1
Active Anomaly Detection in Heterogeneous
Processes
Boshuang Huang, Kobi Cohen, Qing Zhao
Abstract—Anactiveinferenceproblemofdetectinganomalies access [1]), unusual activities in surveillance feedings, frauds
among heterogeneous processes is considered. At each time, a infinancialtransactions,attacksandintrusionsincommunica-
subset of processes can be probed. The objective is to design a
tionandcomputernetworks,anomaliesininfrastructures(such
sequential probing strategy that dynamically determines which
as bridges, buildings, and the power grid) that may indicate
processes to observe at each time and when to terminate the
search so that the expected detection time is minimized under catastrophes. Depending on the application, a cell may refer
a constraint on the probability of misclassifying any process. to an autonomous data stream with a continuous data flow or
This problem falls into the general setting of sequential design a system component that only generates data when probed.
of experiments pioneered by Chernoff in 1959, in which a
randomized strategy, referred to as the Chernoff test, was
proposed and shown to be asymptotically optimal as the error A. Main Results
probability approaches zero. For the problem considered in this
The anomaly detection problem considered in this paper
paper, a low-complexity deterministic test is shown to enjoy the
is a special case of active hypothesis testing originated from
same asymptotic optimality while offering significantly better
performance in the finite regime and faster convergence to the Chernoff’s seminal work on sequential design of experiments
optimal rate function, especially when the number of processes in 1959 [2]. Compared with the classic passive sequential
is large. The computational complexity of the proposed test is hypothesis testing pioneered by Wald [3], where the obser-
also of a significantly lower order.
vation model under each hypothesis is predetermined, active
hypothesis testing has a control aspect that allows the de-
Index Terms—Active hypothesis testing, sequential design
cision maker to choose the experiment to be conducted at
of experiments, anomaly detection, dynamic search, target
each time. Different experiments generate observations from
whereabout.
different distributions under each hypothesis. Intuitively, as
more observations are gathered, the decision maker becomes
I. INTRODUCTION
more certain about the true hypothesis, which in turn leads to
Weconsidertheproblemofdetectingananomalousprocess better choices of experiments.
among M heterogeneous processes. Borrowing terminologies In [2], Chernoff proposed a randomized strategy, referred
fromtargetsearch,werefertotheseprocessesascellsandthe to as the Chernoff test, and established its asymptotic (as the
anomalousprocessasthetargetwhichcanlocateinanyofthe errorprobabilitydiminishes)optimality1.Thisrandomizedtest
M cells. At each time, K (1≤K <M) cells can be probed chooses, at each time, a probability distribution that governs
simultaneously to search for the target. Each search of cell the selection of the experiment to be carried out at this time.
i generates a noisy observation drawn i.i.d. over time from Thisdistributionisobtainedbysolvingaminimaxproblemso
two different distributions f and g , depending on whether that the next observation generated under the random action
i i
the target is absent or present. The objective is to design a canbestdifferentiatethecurrentmaximumlikelihoodestimate
sequential search strategy that dynamically determines which of the true hypothesis (using all past observations) from its
cells to probe at each time and when to terminate the search closest alternative, where the closeness is measured by the
so that the expected detection time is minimized under a Kullback-Liebler (KL) divergence. Due to the complexity in
constraint on the probability of declaring a wrong location solving this minimax problem at each time, the Chernoff test
of the target. can be expensive to compute and cumbersome to implement,
The above problem is prototypical of searching for rare especially when the number of hypotheses or the number of
eventsinalargenumberofdatastreamsoralargesystem.The experiments is large.
rare events could be opportunities (e.g., financial trading op- Itisnotdifficulttoseethattheproblemathandisaspecial
portunities or transmission opportunities in dynamic spectrum case of the general active hypothesis testing problem. Specif-
ically, the available experiments are in the form of different
Boshuang Huang and Qing Zhao are with the School of Electrical and subsets of K cells to probe, and the number of experiments is
Computer Engineering, Cornell University, Ithaca, NY, 14853, USA. Email: (cid:0)M(cid:1)
. Under each hypothesis that cell m (m = 1,...,M) is
{bh467,qz16}@cornell.edu.KobiCoheniswiththeDepartmentofElectrical K
andComputerEngineering,Ben-GurionUniversityoftheNegev,Beer-Sheva the target, the distribution of the next observation (a vector
84105,Israel.Email:yakovsec@bgu.ac.il. of dimension K) depends on which K cells are chosen.
ThisworkwassupportedbytheNationalScienceFoundationunderGrant
The Chernoff test thus directly applies. Unfortunately, with
CCF-1815559andbytheArmyResearchOfficeunderGrantW911NF-17-1-
0464.TheworkofKobiCohenwassupportedbytheCyberSecurityResearch
CenteratBen-GurionUniversityoftheNegev,andtheU.S.-IsraelBinational 1The asymptotic optimality of the Chernoff test was shown under the
ScienceFoundation(BSF)undergrant2017723. assumptionthatthehypothesesaredistinguishableundereveryexperiment.
8102
guA
82
]TI.sc[
3v66700.4071:viXra
2
the large number of hypotheses and the large number of model.TheproblemofsearchingamongGaussiansignalswith
experiments, it can be computationally prohibitive to obtain rare mean and variance values was studied and an adaptive
the Chernoff test. group sampling strategy was developed in [17]. In [18], the
In this paper, we show that the anomaly detection prob- problem of quickly detecting anomalous components under
lem considered here exhibits sufficient structures to admit a the objective of minimizing system-wide cost incurred by all
low-complexity deterministic policy with strong performance. anomalouscomponentswasstudied.In[19],animportantcase
In particular, we develop a deterministic test that explicitly of multichannel sequential change detection is studied and an
specifies which K cells to search at each given time and asymptotic framework in which the number of sensors tends
show that this test enjoys the same asymptotic optimality as to infinity was proposed.
the Chernoff test2. Furthermore, extensive simulation exam- Asymptotically optimal search policies over homogeneous
ples have demonstrated significant performance gain over the processes were established in [21] under a non-parametric
Chernofftestinthefiniteregimeandfasterconvergencetothe setting with finite discrete distributions and in [22] under
optimal rate function, especially when M is large. In contrast a parametric composite hypothesis setting with continuous
to the Chernoff test, the proposed test requires little offline distributions. The objective of minimizing operational cost
or online computation. The test can also be extended to cases as opposed to detection delay led to a different problem
withmultipletargetsasdiscussedinSectionV.Itsasymptotic from the one considered in this paper. Other related work on
optimality is preserved for K =1. quickest search over multiple processes under various models
Often,whenasolutionissimpler,establishingitsoptimality andformulationsincludes[10],[14],[20],[23]andreferences
becomesharder.Thisisindeedthecasehere.InChernofftest, therein. Sequential spectrum sensing within both the passive
since the distribution of the random action depends only on and active hypothesis testing frameworks has also received
the current maximum likelihood estimate of the underlying extensiveattentionintheapplicationdomainofcognitiveradio
hypothesiswhichbecomestime-invariantafteraninitialphase networks (see, for example, [24]–[27] and references therein).
with a bounded duration, the stochastic behaviors of the The readers are also referred to [28] for a comprehensive
test statistics, namely, the log-likelihood ratios (LLRs), are survey on the problem of detecting outlying sequences.
independent over time. In contrast, the deterministic actions A prior study by Cohen and Zhao considered the problem
under the proposed policy result in strong time and spacial forhomogeneousprocesses(i.e.,f ≡f andg ≡g)[15].This
i i
(across processes) dependencies in the dynamic evolutions work builds upon this prior work and addresses the problem
of the LLRs. Establishing the asymptotic optimality becomes in heterogeneous systems where the absence distribution f
i
much more involved. and the presencedistribution g are different acrossprocesses.
i
Allowing heterogeneity significantly complicates the design
of the test and the analysis of asymptotic optimality. Since
B. Related Work
each process has different observation distributions, the rate
Chernoff’s pioneering work on sequential design of exper-
at which the state of a cell can be inferred is different across
iments focuses on binary composite hypothesis testing [2].
processes. To achieve asymptotic optimality, the decision
Variations and extensions have been studied in [4]–[9], where
maker must carefully balance the search time among the
the problem was referred to as controlled sensing for hypoth-
observed processes, which makes both the algorithm design
esis testing in [5]–[7] and active hypothesis testing in [8], [9].
and the performance analysis much more involved under the
AsvariantsoftheChernofftest,thetestsdevelopedin[4]–[9]
heterogeneouscase.Specifically,intermsofalgorithmdesign,
are all randomized tests.
whendealingwithhomogeneousprocesses,thesearchstrategy
Thereisanextensiveliteratureondynamicsearchandtarget
is often static in nature [10], [12], [15], [21]. In contrast,
whereabout problems under various scenarios. We discuss
the asymptotically optimal search strategy developed here for
here existing studies within the sequential inference setting,
heterogeneous processes dynamically changes based on the
which is the most relevant to this work. Two models on prior
current belief about the location of the target. In terms of per-
information about the targets have been considered in the
formanceanalysis,whendealingwithhomogeneousprocesses,
literature: the exclusive model which assumes a fixed number
the resulting rate function (which is inversely proportional to
oftargetsandtheindependentmodelwhichassumeseachcell
the search time) always obeys a certain averaging over the
maycontainatargetwithagivenpriorprobabilityindependent
KLdivergencesbetweennormalandabnormaldistributionsof
of other cells. These two models were juxtaposed in [10],
all processes. This observation follows from the fact that the
[11] under different objective functions. The studies in [12]–
decision maker completes gathering the required information
[16]focusontheexclusivemodel.Inparticular,homogeneous
from all the processes at approximately the same time due
Poisson point processes with unknown rates was investigated
to the homogeneity. In contrast, when searching over hetero-
and an asymptotically optimal randomized test was developed
geneous processes, the overall rate function does not always
in[12].In[13],theproblemoftrackingatargetthatmovesas
obey a simple averaging across the KL divergences of all
aMarkovChaininafinitediscreteenvironmentisstudiedand
processes. In Section IV, we show that the search time can
a search strategy that provides the most confident estimate is
be analyzed by considering two separate scenarios, referred
developed. The studies in [17]–[20] focus on the independent
to as the balanced and the unbalanced cases. The balanced
case holds when a judicious allocation of probing resources
2The asymptotic optimality of the proposed test holds for all but at most
threesingularvaluesofK (seeTheorem3). canensuretheinformationgatheringfromalltheprocessesbe
3
completed at approximately the same time, in which case the which is denoted as
rate function is a weighted average among the heterogeneous
R(Γ∗)∼infR(Γ). (5)
processes.Theunbalancedcaseoccurswhenthereisaprocess
Γ
with a sufficiently small KL divergence that it dominates the
overall rate function of the search. This case is unique to III. THEDETERMINISTICDGFIPOLICY
the heterogeneous processes considered here and needs to be
In this section we propose a deterministic policy, re-
addressed with new analytical techniques.
ferred to as the DGFi policy, indicating the key quantities
Besides the active inference approach to anomaly detection
{D(g ||f ), D(f ||g )}M thatgoverntheselectionruleofthe
considered in this paper, there is a growing body of literature i i i i i=1
proposed policy.
on various approaches to the general problem of anomaly
detection.Wereferthereadersto[29],[30]forcomprehensive
surveys on this topic. A. DGFi under Single-Cell Probing
We first consider the case of K = 1. Let 1 (n) be the
II. PROBLEMFORMULATION m
indicator function, where 1 (n) = 1 if cell m is observed
m
Weconsidertheproblemofdetectingasingletargetlocated
at time n, and 1 (n) = 0 otherwise. This indicator function
m
in one of M cells. If the target is in cell m, we say that
clearly depends on the selection rule, which we omit in the
hypothesis H is true. The a priori probability that H is
m m notation for simplicity. Let
true is denoted by π , where
(cid:80)M
π =1. To avoid trivial
m m=1 m
solutions, it is assumed that 0<π m <1 for all m. (cid:96) (n)(cid:44)log g m (y m (n)) , (6)
When cell m is observed at time n, an observation y m (n) m f m (y m (n))
is drawn, independent of previous observations. If cell m
and
contains a target, y (n) follows distribution g (y). Oth- n
m m (cid:88)
erwise, y m (n) follows distribution f m (y). Let P m be the S m (n)(cid:44) (cid:96) m (t)1 m (t) (7)
probabilitymeasureunderhypothesisH andE theoperator t=1
m m
of expectation with respect to the measure P m . be the LLR and the observed sum LLRs of cell m at time n,
An active search strategy Γ consists of a stopping rule τ respectively. Let D(g||f) denote the KL divergence between
governing when to terminate the search, a decision rule δ for two distributions g and f which is given by3
determining the location of the target at the time of stopping,
(cid:90) ∞ g(x)
andasequenceofselectionrules{φ(n)} n≥1 governingwhich D(g||f)(cid:44) log g(x)dx. (8)
K cells to probed at each time n. Let y(n) be the set of all −∞ f(x)
cell selections and observations up to time n. A deterministic
selection rule φ(n) at time n is a mapping from y(n − 1)
to {1,2,...,M}K. A randomized selection rule φ(n) is a
mapping from y(n − 1) to probability mass functions over
{1,2,...,M}K.
We adopt a Bayesian approach as in Chernoff’s original
study [2] by assigning a cost of c for each observation and
a loss of 1 for a wrong declaration. Note that c represents
the ratio of the sampling cost to the cost of wrong detections.
The Bayes risk under strategy Γ when hypothesis H is true
m
is given by:
R (Γ)(cid:44)α (Γ)+cE (τ|Γ), (1)
m m m
Fig. 1: Typical sample paths of sum LLRs.
where α (Γ)=P (δ (cid:54)=m|Γ) is the probability of declaring
m m
δ (cid:54)= m under H m and E m (τ|Γ) is the detection delay under Illustrated in Fig. 1 are typical sample paths of the sum
H m . The average Bayes risk is given by: LLRsofM =4cells,where,withoutlossofgenerality,weas-
M sumethatcell1isthetarget.NotethatthesumLLRofcell1is
(cid:88)
R(Γ)= π R (Γ)=P (Γ)+cE(τ|Γ), (2) a random walk with a positive expected increment D(g ||f ),
m m e 1 1
m=1 whereas the sum LLR of cell m (m = 2,3,4) is a random
where P e (Γ) and E(τ|Γ) are the error probability and detec- walk with a negative expected increment −D(f m ||g m ). Thus,
tiondelayaveragedunderthegivenprior{π }.Theobjective when the gap between the largest sum LLR and the second
m
is to find a strategy Γ that minimizes the Bayes risk R(Γ): largest sum LLR is sufficiently large, we can declare with a
sufficient accuracy that the cell with the largest sum LLR is
inf R(Γ). (3)
the target. This is the intuition behind the stopping rule and
Γ
the decision rule under DGFi. Specifically, we define m(i)(n)
A strategy Γ∗ is asymptotically optimal if
lim
R(Γ∗)
=1, (4)
3We assume that gi is absolutely continuous with respect to fi (i =
c→0 inf Γ R(Γ) 1,...,M)andviseversa,whichensuresthatallKLdivergencesarefinite.
4
astheindexofthecellwiththeith largestobservedsumLLRs B. DGFi under Multiple Simultaneous Observations
at time n. Let Now we consider the case of K > 1. The stopping rule
and the decision rule remains the same as given in (10), (11),
∆S(n)(cid:44)S (n)−S (n) (9)
m(1)(n) m(2)(n) whereas the selection rule requires a significant modification.
The main reason is that when K cells can be observed
denote the difference between the largest and the second
simultaneously, the asymptotic increasing rate of S (n)
largest observed sum LLRs at time n. The stopping rule and m(1)(n)
and the asymptotic decreasing rate of S (n) are much
the decision rule under the DGFi policy are given by: m(2)(n)
more involved to analyze.
τ =inf{n : ∆S(n)≥−logc} , (10) The selection rule φ(n), at each time n, chooses either the
K cells with the top K largest sum LLRs or those with the
and second to the (K+1)th largest sums LLRs as in (14) where
δ =m(1)(τ). (11)
F m (κ)(cid:44)min{κF¯ m , minD(f j ||g j )}. (15)
We now specify the selection rule of the DGFi policy. The j(cid:54)=m
intuitionbehindtheselectionruleistoselectacellfromwhich Notethat(15)reducesto(13)atK =1(i.e.,F (1)=F¯ ),
m m
the observation can increase ∆S(n) at the fastest rate. The in which case the minimum is always attained at the first
selection rule is thus given by comparing the rate at which term. Similar to the case with K =1, the intuition behind the
S m(1)(n) (n) increases with the rate at which S m(2)(n) (n) selectionruleistoselectK cellsfromwhichtheobservations
decreases. If S m(1)(n) (n) is expected to increase faster than increase ∆S(n) at the fastest rate. Specifically, F m(1)(n) (K)
S (n)decreases,cellm(1)(n)ischosen.Otherwise,cell istheasymptoticdecreasingrateofS (n)whenK cells
m(2)(n) m(2)(n)
m(2)(n) is chosen. This leads to the following selection rule: withthesecondlargesttothe(K+1)thlargestsumLLRsare
probed each time. When the cell with the top K largest sum
(cid:26) m(1)(n), if D(g ||f )≥F¯
φ(n)= m(1)(n) m(1)(n) m(1)(n) , LLRs are probed each time, the asymptotic increasing rate
m(2)(n), otherwise of ∆S(n) is D(g ||f )+F (K−1), where
m(1)(n) m(1)(n) m(1)(n)
(12)
D(g ||f ) is the asymptotic increasing rate of
m(1)(n) m(1)(n)
where
S (n)andF (K−1)istheasymptoticdecreasing
1 m(1)(n) m(1)(n)
F¯ m (cid:44) (cid:80) 1 . (13) rate of S m(2)(n) (n) with K −1 drivers. It is easy to see that
j(cid:54)=m D(fj||gj) when K = 1, the policy reduces to the one described in
section III-A.
The selection rule in (12) can be intuitively understood by
noticing that D(g ||f ) is the asymptotic increas-
m(1)(n) m(1)(n)
ing rate of S (n) when cell m(1) is probed at each time.
m(1)
This is due to the fact that m(1)(n) is the true target after an
initial phase (defined by the last passage time that m(1)(n)
is an empty cell) which can be shown to have a bounded
expectedduration.Similarly,eventhoughmuchmoreinvolved
to prove, F¯ is the asymptotic rate at which S (n)
m(1)(n) m(2)(n)
decreases when cell m(2)(n) is probed at each time. To see
the expression of F¯ for any m as given in (13), consider
m
the following analogy. Consider M − 1 cars being driven
by a single driver from 0 to −∞. Car j (j = 1,...,M,
j (cid:54)= m) has a constant speed of D(f ||g ). At each time,
j j
the car closest to the origin is chosen by the driver and driven
Fig. 2: The piecewise linear property of F (κ).
by one unit of time. We are interested in the average moving m
speed of the position of the closest car to the origin. It is not
difficult to see that it is given by F¯ in (13). This analogy, The behavior of F m (κ) as a function of κ (extending κ
m
to all positive real values) is crucial in understanding and
concerned with deterministic processes, only serves as an
intuitive explanation for the expression of F¯ . As detailed analyzing the asymptotic optimality of DGFi for K > 1. It
m
in Sec. IV, proving F¯ to be the asymptotic decreasing is easy to see that the first term in the right hand of (15) is
m(1)(n)
a linearly increasing function of κ and the second term is a
rate of S (n) requires analyzing the trajectories of the
m(2)(n)
M sum LLRs {S (n)}M , which are stochastic processes constant.Thisreadilyleadstothepiecewiselinearpropertyof
m m=1 F (κ) as illustrated in Fig. 2. Let K˜ denote the switching
withcomplexdependenciesbothintimeandacrossprocesses. m m
point between the increasing and constant regions, we have
(cid:40)(cid:0) m(1)(n),m(2)(n),...,m(K)(n) (cid:1) if D(g ||f )+F (K−1)≥F (K)
φ(n)= m(1)(n) m(1)(n) m(1)(n) m(1)(n) (14)
(cid:0) m(2)(n),m(3)(n),...,m(K+1)(n) (cid:1) otherwise
5
from all the cells at the same time, we carry out the analysis
by treating the balanced and the unbalanced cases separately.
K˜ m = min j(cid:54)=m F¯ D(f j ||g j ) = (cid:88) min j D (cid:54)= ( m f D || ( g f j ) ||g j ) . (16) Next we estabilsh a lower bound on the Bayes risk achiev-
m j j
j(cid:54)=m able by any policy. Define
The constant value of F (κ) for κ ≥ K˜ can be
m m
explained with the same car analogy. This constant value I∗ (cid:44) max uD(g ||f )+F (K−u). (20)
m m m m
min j(cid:54)=m D(f j ||g j ) is the speed of the slowest car among the u∈[0,1]
M −1 cars (excluding the mth car). When the speed of the 1
I∗ (cid:44) . (21)
slowest car is sufficiently small, this car always lags behind (cid:80)M πm
m=1 I∗
even with a dedicated driver. This car becomes the bottleneck m
that caps the value of F (κ) even when the number κ of Using the same car analogy, we can interpret I∗ as the
m m
drivers increases (note that each car can at most have one maximumincreasingrateof∆S(n)underhypothesisH m with
driver assigned). We refer to this case as the unbalanced case, an optimal allocation of u∗ ∈ [0,1] driver to the target car.
which presents the most challenge in proving the asymptotic Comparing with the rate of DGFi under H m in (17), we see
optimalityofDGFi.Thelinearlyincreasingregionofκ<K˜ that the deterministic nature of DGFi forces the allocation of
m
isreferredtoasthebalancedcase,whereF (κ)isaweighted driverstothetargettobeeither0or1.AsshowninTheorem2
m
average among the M −1 cars. below,I∗isanupperboundontheratefunctionforanypolicy.
Theorem 2: Let R(Γ) be the Bayes risk under an arbitrary
policy Γ. We have
IV. PERFORMANCEANALYSIS
Inthissection,weestablishtheasymptoticoptimalityofthe −clogc
inf R(Γ) ∼ (22)
DGFi policy. While the intuitive exposition of DGFi given in Γ I∗
Sec. III may make its asymptotic optimality seem expected,
Proof: The outline of the proof is as follows. We first
constructing a proof is much more involved. In particular,
provethatiftheBayesriskissufficientlysmallunderstrategy
bounding the detection time of DGFi requires analyzing the
trajectoriesoftheM stochasticprocesses{S (n)}M which Γ, i.e., R(Γ) = O(−clogc), the difference between the
m m=1
largest sum LLRs and the second largest sum LLRs must
exhibit complex dependencies both over time and across
be sufficiently large when the test terminates, i.e. ∆S(τ) =
processes as induced by the deterministic selection rule.
Ω(−logc). Otherwise, it is not possible to achieve a risk
The asymptotic optimality of DGFi is established by com-
O(−clogc) due to a large error probability. We then show
paring its Bayes risk (given in Theorem 1) with a lower
that in order to make ∆S(n) sufficiently large, the sample
bound on achievable Bayes risk (given in Theorem 2). We
size must be large enough, i.e., E[τ|Γ] ≥ −logc. Since each
first analyze the rate function of DGFi. Define I∗
samplecostsc,thetotalriskwillbelowerboundedby −clogc
I m (Γ DGFi )(cid:44)max{D(g m ||f m )+F m (K−1),F m (K)}, as desired. The detailed proof can be found in Appendix I B ∗ .
(17)
which is the increasing rate of ∆S(n) under hypothesis H m Establishing the asymptotic optimality of DGFi rests on
when DGFi is employed. For a given a priori distribution comparing its rate function I(Γ ) with the optimal rate
DGFi
{π m }M m=1 , define function I∗. The key thus lies in analyzing the optimizer u∗ m
1 in the right hand of (20) and showing whether and when it
I(Γ )(cid:44) .
DGFi (cid:80)M πm (18) assumes integer values of 0 and 1 as used in DGFi. This is
m=1 Im(ΓDGFi) established in Lemma 1 that leads to the following necessary
As shown in Theorem 1 below, I(Γ ) is the rate function andsufficientconditionfortheasymptoticoptimalityofDGFi.
DGFi
of the Bayes risk of the DGFi policy.
Theorem 3: A necessary and sufficient condition for the
Theorem 1: The Bayes risk R(Γ ) of the DGFi policy asymptotic optimality of the DGFi policy is that, for each
DGFi
is given by m=1,...,M, at least one of the following three statements
−clogc is true
R(Γ ) ∼ . (19)
DGFi I(Γ DGFi ) (a) D(g m ||f m )≥F¯ m .
(b) K ≤K˜ .
m
Proof:Hereweprovideasketchoftheproof.Thedetailed (c) K ≥K˜ +1.
m
proof can be found in Appendix A. First, we show that
when ∆S(τ) is large, the probability of error is small, i.e. Proof: We first establish the following lemma on the
P = O(c). As a result, by the definition of the Bayes risk, maximizer u∗ that attains I∗ given in (20). The proof of
e m m
it suffices to show that the detection time is upper bounded this lemma is in Appendix C.
by −logc/I(Γ ). By the definition of I(Γ ) in (18), it Lemma 1: Define
DGFi DGFi
suffices to show that the detection time is upper bounded by
−logc/I (Γ ) under hypothesis H . Since the decision
m DGFi m u∗ (cid:44)arg max uD(g ||f )+F (K−u). (23)
maker might not complete to gather the required information m m m m
u∈[0,1]
6
Then, For the selection rule, define, for a given set D ⊂
(cid:40) {1,2,...,M} with |D|=L,
1, if D(g ||f )≥F¯
u∗ = m m m . 1
m min{max{K−K˜ m ,0},1}, if D(g m ||f m )<F¯ m F¯ D (cid:44) (cid:80) 1 . (28)
(24) j∈/D D(fj||gj)
From (24) in Lemma 1, u∗ takes the integer value of 0 or Similar to F¯ defined in (13), F can be viewed as the
m m D
1 if and only if at least one of the Statements (a), (b), (c) is asymptotic increasing rate of ∆S (n) when the L targets are
L
true. Theorem 3 thus follows. givenbysetDandweprobethecellwiththe(L+1)th largest
sum LLR. We also define
Corollary 1: The DGFi policy is asymptotically optimal
1
except for at most three values of K ∈ {2,3,...,M} G¯ D (cid:44) (cid:80) 1 , (29)
for every given problem instance specified by j∈D D(gj||fj)
{M,{D(g ||f ),D(f ||g )}M }.
i i i i i=1 which can be viewed as the asymptotic increasing rate for
Proof:FromTheorem3,itiseasytoseethatforeachm, ∆S (n) when we probe the cell with the Lth largest sum
there is only one possible K =(cid:100)K˜ (cid:101), which is the least inte- L
m LLR.
gergreaterthanorequaltoK˜ ,thatmakesI (Γ )<I∗.
m m DGFi m The selection rule follows the same design principle of
Let j(cid:48) =argmin D(f ||g ). Since there is only one possible
j j j maximizing the asymptotic increasing rate of ∆S (n), and
K = (cid:100)K˜ (cid:101) that makes I (Γ ) < I∗, it remains to show L
j(cid:48) j(cid:48) DGFi j(cid:48) is given by
that there are only two possible values of K = (cid:100)K˜ m (cid:101) that (cid:26) m(L)(n), if G¯ ≥F¯
makes I m (Γ DGFi )<I m ∗ when m(cid:54)=j(cid:48). Let φ(n)= m(L+1)(n), other D w (n is ) e D(n) , (30)
M
V (cid:44) (cid:88)D(f j(cid:48) ||g j(cid:48) ) . where
D(f ||g )
j=1 j j D(n)={m(1)(n),m(2)(n),...,m(L)(n)}. (31)
Since 0≤
D(f j(cid:48)||g j(cid:48))
≤1, we have
ItisnotdifficulttoseethatwhenL=1,thepolicyreduces
D(fm||gm) to the one described in Section III.
K˜ = (cid:88) min j(cid:54)=m D(f j ||g j ) =V− D(f j(cid:48) ||g j(cid:48) ) ∈[V−1,V] Next, we establish the asymptotic optimality of the DGFi
m D(f ||g ) D(f ||g ) policyforL>1andK =1.LetD denoteasubsetofLcells
j j m m
j(cid:54)=m and π the prior probability of hypothesis H (i.e, the target
D D
forallm(cid:54)=j(cid:48).Thisimpliesthat(cid:100)K˜ (cid:101)(m(cid:54)=j(cid:48))canonlytake cells are given by D). Define
m
two possible integers as desired. I (cid:44)max{F¯ ,G¯ },
D D D
I∗ (cid:44) 1 , (32)
TheabovecorollaryalsoindicatesthatforK =1,theDGFi L (cid:80) πD
policy is always asymptotically optimal. This can be easily D ID
seensinceStatement(b)alwaysholdsforK =1.Tofindthose where I L ∗ is again the optimal rate function of the Bayes risk
pathologicalvaluesofK forwhichDGFiisnotasymptotically asshowninthetheorembelow,andreducestotheonedefined
optimal, we can compute (cid:100)K˜ (cid:101) defined in (16) for each m= in (20) when L=1.
m
1,2,...,M. Since for each m, (cid:100)K˜ m (cid:101) only requires O(M) Theorem 4: Let R L (Γ DGFi ) and R L (Γ) be the Bayes risks
number of multiplication and summation, the computational under the DGFi policy and an arbitrary policy Γ, respectively.
complexity of finding those pathological values is O(M2). For K =1, we have,
−clogc
R (Γ ) ∼ ∼ inf R(Γ). (33)
V. EXTENSIONTODETECTINGMULTIPLETARGETS L DGFi I∗ Γ
L
In this section we extend the DGFi policy to the case with
Proof: See Appendix D.
L>1 targets. The number of hypotheses in this case is
(cid:0)M(cid:1)
.
L
We consider first K =1. The stopping rule and decision rule For K >1, the stopping rule and the decision rule remain
of DGFi for L > 1 are given below, similar in principle to the same. For the selection rule, define
those for L=1 as described in Section III: F D (κ)(cid:44)min{κF¯ D , minD(f j ||g j )}. (34)
j∈/D
τ =inf{n : ∆S L (n)≥−logc}, (25) Similar to F m (κ) defined in (15), F D (κ) can be viewed as
the asymptotic increasing rate of ∆S (n) when the L targets
L
δ ={m(1)(τ),m(2)(τ),...,m(L)(τ)}, (26) are given by set D and we probe those κ cells with the (L+
1)th to the (L+κ)th largest sum LLR. Similarly,
where
G D (κ)(cid:44)min{κG¯ D , minD(g j ||f j )}, (35)
∆S (n)(cid:44)S (n)−S (n) (27) j∈D
L m(L)(n) m(L+1)(n)
which can be viewed as the asymptotic increasing rate of
denotes the difference between the Lth and the (L + 1)th ∆S (n) when we probe the cells with the (L − κ + 1)th
L
largest observed sum LLRs at time n. to the Lth largest sum LLR.
7
Let which is the increasing rate of ∆S(n) under hypothesis H
m
k D ∗ (cid:44)arg k=0 m ,1 a ,. x ..,K F D (K−k)+G D (k), (36) w Ch h e e r n no th ff e t C es h t er u n n o d f e f r te a st g i i s ve e n m p p r l i o o y r ed { . π m Th } e M m r = a 1 te c f a u n nc b t e io s n im of ila th ry e
obtained as in (18).
which can be interpreted as the optimal number of target
We point out that in [2], while proving I (Γ ) equals
cells that should be probed at each time for maximizing the m C
the optimal rate I∗, Chernoff did not provide an explicit
asymptotic increasing rate of ∆S (n). The selection rule of m
L expression for I∗ or I (Γ ). Both were given, as in (41),
DGFi is thus given by m m C
inexplicitly in terms of the optimizer q∗ of the maximin
m
φ(n)={m(L−k D ∗ (n) +1)(n),...,m(L−k D ∗ (n) +K)(n)}, (37) problem in (40). Even for the problem studied here, a special
case of that considered by Chernoff4, solving for q∗ numeri-
where m
cally is computationally expensive (see a detailed analysis on
D(n)={m(1)(n),m(2)(n),...,m(L)(n)}. (38) computationalcomplexityinthenextsubsection).Theexplicit
characterization of I∗ in (20), which equals to I (Γ )
m m DGFi
The asymptotic optimality of DGFi for L > 1 and K > 1 in (17) under the necessary and sufficient condition given in
remains open. Following the same insight in the single- Theorem 3, is a contribution of this work.
target case, however, we have strong belief of the following
conjecture. B. Comparison in Computational Complexity
While both the Chernoff test and the DGFi policy are
Conjecture 1: The DGFi policy preserves its asymptotic
asymptotically optimal, i.e., I(Γ ) = I(Γ ) = I∗, they
optimality if DGFi C
differ drastically in computational complexity. Specifically,
u∗ (cid:44)arg max F (K−u)+G (u) (39) the Chernoff test can be expensive to compute especially
D D D
u∈[0,K]
whenthenumberofhypothesesorthenumberofexperiments
is an integer for all D, where we allow the domain of F (·) is large. Consider the case of a single target (L = 1).
D
and G (·) to be real numbers. ComputingtheselectionruleoftheChernofftestgivenin(40)
D
requiressolvingM minimaxproblems,eachcorrespondingto
aparticularvalueoftheMLestimateˆi(n)∈{1,...,M}.One
VI. COMPARISONWITHTHECHERNOFFTEST
efficient way of solving minimax problems is through linear
Inthissection,wecomparetheperformanceoftheproposed programming, which takes polynomial time with respect to
DGFi policy and the Chernoff test in terms of both computa- the number of variables and constraints. For this problem,
tional complexity and sample complexity. the number of variables is
(cid:0)M(cid:1)
, which can be exponential
K
in M in the worst case. Calculating the rate function given
in(41)requirestheoptimalselectiondistributionq∗ forallm,
A. The Chernoff Test m
thus bears similar computational complexity. For multi-target
The Chernoff test has a randomized selection rule. Specif-
detection,thenumberofhypothesesis
(cid:0)M(cid:1)
,furtherincreasing
ically, let q be a probability mass function over a set of ω L
the complexity.
available experiments {u }ω that the decision maker can
choose from. Note that
i
in
i=
o
1
ur case, ω =
(cid:0)M
K
(cid:1)
. For each is
T
(1
h
5
e
)
o
,
n
w
ly
h
c
ic
o
h
mp
re
u
q
ta
u
t
i
i
r
o
e
n
s
in
M
vol
s
v
u
e
m
d
m
in
a
t
t
h
io
e
n
s
s
ele
e
c
a
t
c
i
h
on
w
ru
it
l
h
eo
M
fD
−
GF
1
i
hypothesis m = 1,2,...,M, the optimal action distribution elements.Asaresult,thecomputationaltimeisO(M2),which
is given by is independent of K. Similarly, the computational complexity
q∗ =arg max min (cid:88) quiD(pui||pui), (40) for calculating the rate function I(Γ DGFi ) is O(M2) as well.
m m j
q j(cid:54)=m
ui C. Comparison in Sample Complexity
where pu j i is the observation distribution under hypothesis In this subsection, we compare the performance of DGFi
j when action u i is taken, and qui is the ith element of with that of the Chernoff test in the finite regime (i.e., when
q (i.e., the probability of choosing experiment u i under q). the sample cost c is bounded away from 0).
The rationale behind (40) is a zero-sum game formulation of Consider a uniform prior and exponentially distributed ob-
the problem, and the optimal mixed strategy q∗ m leads to a servations: f m ∼ exp(λ( f m)) and g m ∼ exp(λ( g m)). The KL
randomobservationthatbestdifferentiatesH m fromitsclosest divergences can be easily computed as follows.
alternative.
λ(m)
The action at time n under the Chernoff test is drawn from D(g ||f )=log(λ(m))−log(λ(m))+ f −1,
a distribution q∗ , whereˆi(n) is the ML estimate of the true m m g f λ(m)
ˆi(n) g
hypothesis at time n based on past actions and observations. λ(m)
The stopping rule and the decision rule are the same as in D(f m ||g m )=log(λ( f m))−log(λ( g m))+ λ g (m) −1.
(10), (11). f
The rate function of the Chernoff test Γ under hypothesis
C 4Note that the asymptotic optimality of the Chernoff test requires the
H m is given by assumptionofpositiveKLdiverencebetweeneverypairofhypothesesunder
(cid:88) every experiment. This does not hold for the problem at hand. However, it
I m (Γ C )= min q m ∗uiD(pu m i||pu j i), (41) canbeshownthattheChernofftestpreservesitsasymptoticoptimalityinthis
j(cid:54)=m case.
ui
8
20 30 40 50 60 70 80 90 100
M
yaleD
noitceteD
600
DGFi policy
Chernoff test 500
400
300
200
100
0
20 30 40 50 60 70 80 90 100
M
rorrE
fo
ytilibaborP
0.018
DGFi policy
0.016 Chernoff test
0.014
0.012
0.01
0.008
0.006
0.004
0.002
0
20 30 40 50 60 70 80 90 100
M
ksiR
seyaB
20 30 40 50 60 70 80 90 100
M
0.6
DGFi policy
Chernoff test 0.5
0.4
0.3
0.2
0.1
0
Fig. 3: Performance comparison (K = 1,λ(m) = 9 +
g
m,λ(m) =0.0188,c=10−3).
f
Shown in Fig. 3 is the performance comparison between
DGFi policy and Chernoff test for L = 1 and K = 1.
The figure clearly demonstrates the significant reduction in
detection delay and Bayes risk offered by the DGFi policy
as compared with the Chernoff test. The performance gain
increases drastically as M increases. The probability of errors
for Chernoff test and DGFi policy are about the same order
as shown. A similar comparison is observed in Fig. 4 with
L = 1,K = 2. The performance comparison for a case with
multiple targets is shown in Fig. 5 with L=2,K =1.
Next, we provide an intuition argument for the superior
finite-time performance of DGFi. Consider a short horizon
scenario where the sampling cost c is sufficiently high such
that D(f||g)>−logc. This implies that each empty cell can
be distinguished from the target with, on the average, a single
probing to achieve the required accuracy as determined by
c. We can cast this as the coupon collection problem, where
each empty cell is a coupon and the goal is to collect all
M −1 coupons. Consider a special case where K = 1 and
all f and g are identical, i.e., f ≡ f and g ≡ g. Assume i i i i
thatD(f||g)>(M−1)D(g||f).Inthiscase,theDGFipolicy
chooses,ateachtime,thecellwiththesecondlargestsumLLR
whereas the Chernoff test randomly and uniformly chooses a
cell from all but the one with the largest sum LLR at each
time (this can be shown by solving (40)). Since Chernoff test
chooses empty cells with equal probability, based on results
in coupon collectors problem, the expected probing time will
yaleD
noitceteD
350
DGFi policy
300 Chernoff test
250
200
150
100
50
0
20 30 40 50 60 70 80 90 100
M
rorrE
fo
ytilibaborP
0.018
DGFi policy
0.016 Chernoff test
0.014
0.012
0.01
0.008
0.006
0.004
0.002
0
20 30 40 50 60 70 80 90 100
M
ksiR
seyaB
0.35
DGFi policy
0.3 Chernoff test
0.25
0.2
0.15
0.1
0.05
0
Fig. 4: Performance comparison (K = 2,λ(m) = 9 +
g
m,λ(m) =0.0188,c=10−3).
f
be roughly MlogM. The DGFi policy, on the other hand, is
deterministic and guaranteed to collect a new coupon at each
time. The expected probing time is thus M.
VII. CONCLUSION
The problem of detecting anomalies among a large number
ofheterogeneousprocesseswasconsidered.Alow-complexity
deterministic test was developed and shown to be asymptot-
ically optimal. Its finite-time performance and computational
complexity were shown to be superior to the classic Chernoff
testforactivehypothesistesting,especiallywhentheproblem
size is large.
APPENDIXA:PROOFOFTHEOREM1
Throughoutthissection,weusethefollowingnotations.Let
n
(cid:88)
N (n)(cid:44) 1 (t) (42)
j j
t=1
be the number of times that cell j has been observed up to
time n. Let
∆S (n)(cid:44)S (n)−S (n) (43)
m,j m j
be the difference between the observed sum of LLRs of cells
m and j. We also define
∆S (n)(cid:44)min∆S (n). (44) m m,j
j(cid:54)=m
9
20 30 40 50 60 70 80 90 100
M
yaleD
noitceteD
900
DGFi policy
800 Chernoff test
700
600
500
400
300
200
100
0
20 30 40 50 60 70 80 90 100
M
rorrE
fo
ytilibaborP
0.012
DGFi policy
Chernoff test
0.01
0.008
0.006
0.004
0.002
0
20 30 40 50 60 70 80 90 100
M
ksiR
seyaB
−logc which implies ∆S ≥−logc. Hence, for all j (cid:54)=m
j,m
we have:
α =P (δ =j)
m,j m
≤P (∆S (τ)≥−logc) (48) m j,m
≤cP (∆S (τ)≥−logc)≤c,
j j,m
where changing the measure in the second inequality follows
by the fact that ∆S (τ)≥−logc. As a result, j,m
(cid:88)
α = α ≤(M −1)c
m m,j
j(cid:54)=m
and (47) thus follows.
Next we show that the expected detection time of DGFi
is bounded by −logc/I (Γ ) under hypothesis H . To
m DGFi m
show this, we partition the detection process into three stages,
all defined by certain last passage times. The first stage is
defined by the last passage time, denoted by τ , that the
1
0.9 maximum likelihood estimate is not the true hypothesis H .
DGFi policy m
0.8 Chernoff test The second stage defined by a last passage time τ , indicates
0.7 2
thatthetruehypothesisH canbedistinguishedfromatleast
0.6 m
0.5 onefalsehypothesiswithsufficientlyhighaccuracy.Thethird
0.4 stage defined by last passage time τ , indicates that H can
3 m
0.3 be distinguished from all the other M −1 hypotheses with
0.2
sufficientaccuracy.Theformaldefinitionsofτ ,τ ,τ aregive
0.1 1 2 3
0 below:
τ (cid:44)min{t:∀j (cid:54)=m,∀n≥t,S (n)≥S (n)}
Fig. 5: Performance comparison (L = 2,K = 1,λ(m) = 9+ 1 m j
g
m,λ(m) =0.0188,c=10−3). τ
2
(cid:44)min{t:∃j (cid:54)=m,∀n≥t,S
m
(n)−S
j
(n)≥−logc}
f
τ (cid:44)min{t:∀j (cid:54)=m,∀n≥t,S (n)−S (n)≥−logc}.
3 m j
(49)
As a result, we have: Here, we assume that the selection rule of DGFi policy is
implemented indefinitely, which means we probe the cells
∆S(n)=S (n)−S (n)=max∆S (n). (45)
m(1)(n) m(2)(n) m according to the selection rule of DGFi as given in (14)
m
indefinitely, while the stopping rule is disregarded. Note that
Without loss of generality we prove the theorem under τ ,τ ,τ are not stopping times since they depend on the
1 2 3
hypothesis H m . We define future.
Sinceτ ≤τ basedonthestoppingruleofDGFi,itsuffices
 3
(cid:96) (i)−D(g ||f ), if k =m,
 k k k toshowτ isboundedby−logc/I (Γ )underhypothesis
(cid:96)˜ (i)= (46) 3 m DGFi
k H . Let n = τ − τ and n = τ − τ . In Lemma 4
(cid:96) (i)+D(f ||g ), if k (cid:54)=m, m 2 2 1 3 3 2
k k k and Lemma 7, we show that τ and n are sufficiently
1 3
small with high probability. In Lemma 5 we show that the
which is a zero-mean random variable under hypothesis H .
m probability that n is greater than n decays exponentially
For the ease of presentation, we first provide the proof for 2
with n when n is greater than −logc/I (Γ ). Since
the case of K =1. m DGFi
n = τ + n + n , the expected detection time of DGFi
3 1 2 3
is bounded by −logc/I (Γ ) under hypothesis H as
m DGFi m
desired.
A. Proof for K =1
Lemma 3: There exist constants C > 0 and γ > 0 such
WefirstboundtheerrorprobabilityofDGFiasgivenbelow. that for any fixed 0 < q < 1, under any arbitrary policy, the
Lemma2:IfDGFipolicyisused,thentheerrorprobability following statements hold:
is upper bounded by:
P (S (n)≥S (n),N (n)≥qn)≤Ce−γn , (50)
m j m j
P ≤(M −1)c. (47)
e and
Proof: Let α m,j = P m (δ = j) for all j (cid:54)= m. Thus, P (S (n)≥S (n),N (n)≥qn)≤Ce−γn , (51)
(cid:80) m j m m
α = α . By the definition of the stopping rule
m j(cid:54)=m m,j
underDGFi(see(10)),acceptingH isdonewhen∆S (n)≥ for m=1,2,...,M and j (cid:54)=m.
j j
10
Proof: We start with proving (50). Note that Note that 0<ρ ≤1/16. Thus, we can write
m
N (n),N (n) can take integer values N (n) =
j m j
P (S (n)≥S (n))
(cid:100)qn(cid:101),(cid:100)qn(cid:101) + 1,...n, and N (n) = 0,...,n. Using the m j m
m
i.i.d. property of the observations across time yield: ≤P (S (n)≥S (n),N (n)<ρ n,N (n)<ρ n)
m j m j m m m
P m (S j (n)≥S m (n),N j (n)≥qn) +P m (S j (n)≥S m (n),N j (n)≥ρ m n)
(cid:88) n (cid:88) n (cid:32) (cid:88) r (cid:88) k (cid:33) +P m (S j (n)≥S m (n),N m (n)≥ρ m n).
≤ P (cid:96) (i)+ −(cid:96) (i)≥0
m j m (58)
(52)
r=(cid:100)qn(cid:101) k=0 i=1 i=1 The second and the third terms on the RHS of (58) decay
n n exponentially with n by Lemma 3. Thus, it remains to show
(cid:88) (cid:88)(cid:104) (cid:16) (cid:17)(cid:105)r(cid:104) (cid:16) (cid:17)(cid:105)k
≤ E m es(cid:96)j(1) E m es(−(cid:96)m(1)) that the first term decays exponentially with n as well. Note
r=(cid:100)qn(cid:101) k=0 that the event (N j (n)<ρ m n,N m (n)<ρ m n) implies that at
leastn˜ =n−N (n)−N (n)≥n(1−2ρ )timescellsj,m
wherewehaveusedthefollowinggenericChernoffboundfor j m m
a random variable X: are not probed. We define N(cid:101)r (n) as the number of times in
which cell r (cid:54)=j,m has been probed and cells j,m have not
E[eλX]
P(X ≥a)≤ , (53) been probed by time n. There exists a cell r (cid:54)=j,m such that
eλa N(cid:101)r (n)≥
M
n˜
−2
= n(1
M
−
−
2ρ
2
m). Hence, we can upper bound (58)
whereitisassumethatthemomentgeneratingfunctionE[eλX] as follows:
exists locally in an interval around λ=0. Since the moment
P (S (n)≥S (n))
generatingfunctionisequaltooneats=0andE ((cid:96) (1))= m j m
m j
(cid:18)
−D(f j ||g j ) < 0, E m (−(cid:96) m (1)) = −D(g m ||f m ) < 0 are ≤ (cid:88) P N˜ (n)> n(1−2ρ m ) ,
strictlynegative,differentiatingtheMGFsof(cid:96) j (1),(cid:96) m (1)with m r M −2 (59)
respect to s yields strictly negative derivatives at s=0. As a r(cid:54)=j,m
E
re
m
sul
(cid:0)
t
e
,
s
t
(
h
−
e
(cid:96)
r
m
e
(1
e
)
x
)
i
(cid:1)
st
a
s
re
>
str
0
ict
a
l
n
y
d
le
γ
s 1 s
>
tha
0
n
s
e
u
−
c
γ
h
1
t
<
ha
1
t
.
E
H m en
(cid:0)
c
e
e
s
,
(cid:96)j
t
(
h
1
e
)
r
(cid:1)
e
,
N j (n)<ρ m n,N m (n)<ρ m n
(cid:19)
+2De−γ1n,
exist C >0 and γ =γ q >0 such that
1
where the second and third terms on the RHS of (58) are
P m (S j (n)−S m (n)≥0,N j (n)≥qn) upper bounded by De−γ1n (there exist such D > 0,γ 1 > 0
n n by Lemma 3), and the first term on the RHS of (58) is upper
(cid:88) (cid:88) (54)
≤ e−γ1r e−γ1k ≤Ce−γn . bounded by the first term (i.e., the summation term) on the
r=(cid:100)qn(cid:101) k=0 RHS of (59). Next, we show that each term in the summation
decays exponentially with n to get the desired result.
Note that (51) can be proved with minor modifications.
Let t˜r,t˜r,...,t˜r be the indices for the time instants in
1 2 N˜ r(n)
Lemma 4: If the selection rule of DGFi is implemented which cell r (cid:54)=j,m has been probed and cells j,m have not
indefinitely, there exist C >0 and γ >0 such that been probed by time n. Let
P (τ >n)≤Ce−γn , (55) 1−2ρ
m 1 ζ (cid:44) m . (60)
2(M −2)
for m=1,2,...,M.
Proof: We focus on proving for M > 2. Proving for Note that the event S (t˜r )≤S (t˜r ) or S (t˜r )≤S (t˜r )
j ζn r ζn m ζn r ζn
M =2 is straightforward. Note that the event τ 1 >n implies must occur (otherwise, cell j or m will be probed). Hence5,
that there exists a time instant t with t≥n such that S (t)>
j
(cid:16)
S m (t) for some j (cid:54)=m. Hence, P m N˜ r (n)> n(1 M − − 2ρ 2 m),
(cid:18) (cid:19)
P (τ >n)≤P max sup (S (t)−S (t))≥0 N (n)<ρ n,N (n)<ρ n)
m 1 m j m j m m m
j(cid:54)=m t≥n  
∞
n
(cid:88)
−ζn ρ (cid:88)mn
(cid:88)
n(cid:48) ζ
(cid:88)
n+q
≤ (cid:88) (cid:88) P (S (t)≥S (t)) . = P m (cid:96) j (i)≤ (cid:96) r (i) (61)
m j m
q=0 n(cid:48)=0 i=1 i=1
j(cid:54)=m t=n
(56)  
Following (56), it suffices to show that there exist C >0 and
n
(cid:88)
−ζn ρ (cid:88)mn
(cid:88)
n(cid:48) ζ
(cid:88)
n+q
γ >0 such that P (S (n)≥S (n))≤Ce−γn. + P m (cid:96) m (i)≤ (cid:96) r (i).
m j m q=0 n(cid:48)=0 i=1 i=1
We next establish the required exponential decay. Let
ForupperboundingthefirsttermontheRHSof(61)wewrite
k = max j(cid:54)=m D(f j ||g j ) , the sum LLRs as follows:
m min D(f ||g )
j(cid:54)=m j j
j m =argminD(f j ||g j ), (57) 5Fortheeaseofpresentation,throughouttheproofweassumethatζn,ρmn
j(cid:54)=m areintegers.Thisassumptiondoesnotaffecttheexponentialdecaybutonly
1 theexactvalueofC>0in(55)(sinceαn−1≤(cid:98)αn(cid:99)≤(cid:100)αn(cid:101)≤αn+1
ρ = .
m 8(k +1)(M −2) holdsforallα≥0foralln=0,1,...).
m
11
and
ζn+q n(cid:48)
(cid:88) (cid:88)
(cid:96) (i)+ −(cid:96) (i)
r j
 
i=1 i=1 n
(cid:88)
−ζn ρ (cid:88)mn
(cid:88)
n(cid:48) ζ
(cid:88)
n+q
= ζ (cid:88) n+q (cid:96)˜ r (i)+ (cid:88) n(cid:48) (cid:96)˜ j (i) q=0 n(cid:48)=0 P m i=1 (cid:96) j (i)≤ i=1 (cid:96) r (i)
(68)
i=1 i=1
−D(f r ||g r )(ζn+q)+D(f n(cid:48) ||g n(cid:48) )n(cid:48) ≤e−γ2n
n
(cid:88)
−ζn
e−γ2q
ρ (cid:88)mn
e−γ2n(cid:48) ≤C 2 e−γ2n ,
ζn+q n(cid:48) q=0 n(cid:48)=0
≤ (cid:88) (cid:96)˜(i)+ (cid:88) −(cid:96)˜(i)−D(f ||g )(ζn+q−k n(cid:48)),
r j jm jm m
i=1 i=1
(62)
and by the definitions of ζ,k m ,ρ m in (57) and (60), we have where C 2 =(1−e−γ2) −2 .
ζn+q−k n(cid:48) ≥ζn+q−k n(cid:48)−(k +1)(ρ n−n(cid:48))
m m m m
1 A similar technique can be applied to upper bound the
=n(ζ−(k +1)ρ )+q+n(cid:48) ≥ n+q+n(cid:48)
m m 4(M −2) second term on the RHS of (61).
1
≥ (n+q+n(cid:48)), Lemma 5: If the selection rule of DGFi is implemented
4(M −2)
indefinitely, then for every fixed (cid:15)>0 there exist C >0 and
for all n(cid:48) ≤ρ n. Therefore, γ >0 such that
m
ζn+q n(cid:48)
(cid:88) (cid:88)
(cid:96) (i)+ −(cid:96) (i)≥0 (63)
r j
i=1 i=1 P (n >n)≤Ce−γn ∀n>−(1+(cid:15))logc/I (Γ ),
m 2 m DGFi
implies (69)
for all m=1,2,...,M.
ζn+q n(cid:48)
(cid:88) (cid:96)˜(i)+ (cid:88) −(cid:96)˜(i)≥C (n+q+n(cid:48)), (64)
r j 1
Proof: First, we consider the case where I (Γ ) >
i=1 i=1 m DGFi
D(g ||f ).Notethatcellmisnotobservedforalln≥τ in
where
C 1 =
D
4
(
(
f
M
jm
−
||g
2
jm
)
)
>0. (65) a
th
r
i
g
s
m
c m a
a
s
x
e.
j
m D
N
e
j (cid:48)
fi
(τ
n
1
e
+
N j
t
(cid:48)(
)
τ
D
1
(
+
f j
t)
||g
=
j )
(cid:80)
. T
i τ =
h
1+
u
τ1
s
t +
,
1 1 j (i)andj∗(τ 1 +t 1 )=
Then we have
 
n(cid:48) ζn+q
(cid:88) (cid:88) P (n >n)
P m (cid:96) j (i)≤ (cid:96) r (i) m 2
i=1 i=1 (cid:32) τ (cid:88)1+t (cid:33)
≤P m


ζ
(cid:88)
n+q
(cid:96)˜ r (i)+ (cid:88)
n(cid:48)
−(cid:96)˜ j (i)≥C 1 (n+q+n(cid:48))


≤P m s
t≥
up
n i=τ1+1
(cid:96) j∗(τ1+t) (i)1 j∗(τ1+t) (i)≥logc
(
.
70)
i=1 i=1 Since t is the total number of observation from τ to τ +t,
1 1
(cid:104) (cid:16) (cid:17)(cid:105)ζn+q(cid:104) (cid:16) (cid:17)(cid:105)n(cid:48) by the definition of j∗(t) we have
≤ E es(cid:96)˜ r(1) E es(−(cid:96)˜ j(1))
m m
×e−sC1 (n+q+n(cid:48))
=
(cid:104)
E
m
(cid:16)
es((cid:96)˜ r(1)−C1 )
(cid:17)(cid:105)ζn+q(cid:104)
E
m
(cid:16)
es(−(cid:96)˜ j(1)−C1 )
(cid:17)(cid:105)n(cid:48) t= (cid:88) N
j
(cid:48)(τ
1
+t)= (cid:88) N j (cid:48)(τ 1
D
+
(f
t
j
)
|
D
|g
(
j
f
)
j ||g j )
j(cid:54)=m j(cid:54)=m
(71)
×e−sC1(n−ζn) .
(66) ≤
(cid:88) N j (cid:48) ∗(τ1+t) (τ 1 +t)D(f j∗(τ1+t) ||g j∗(τ1+t) )
.
D(f ||g )
j j
for all s>0. j(cid:54)=m
SinceE ((cid:96)˜(1)−C )=−C <0andE (−(cid:96)˜(1)−C )=
m r 1 1 m j 1
−C <0arestrictlynegative,byapplyingasimilarargument
1
as at the end of the proof of Lemma 3, there exist s>0 and Let (cid:15) = I (Γ )(cid:15)/(1 + (cid:15)). Since I (Γ ) =
(cid:16) (cid:17) (cid:16) (cid:17) 1 m DGFi m DGFi
γ 2 >0suchthatE m e(s(cid:96)˜ r(1)−C1) ,E m es(−(cid:96)˜ j(1)−C1) and (cid:80) j(cid:54)=m 1/D(f j ||g j ), we have
e−sC1 are strictly less than e−γ2 <1. Hence,
 
n(cid:48) ζn+q
P m (cid:88) (cid:96) j (i)≤ (cid:88) (cid:96) r (i)≤e−γ2 (n+q+n(cid:48)) , (67) (cid:15) = (cid:15) . (72)
1 (1+(cid:15)) (cid:80) 1/D(f ||g )
i=1 i=1 j(cid:54)=m j j
12
Then, ¯j = arg max S (τ ),j = arg min S (τ ). Let
j(cid:54)=m j 2 j(cid:54)=m j 2
τ (cid:88)1+t
(cid:96) j∗(τ1+t) (i)1 j∗(τ1+t) (i)−logc t
t
0
0
<
be
t
t
≤
he
τ 2
s
.
m
A
a
s
lle
a
st
re
i
s
n
u
t
l
e
t
g
,
e
Ψ
r
(
s
τ
u
2
c
)
h
>
th
(cid:15) 1
a
n
t S
im
j
(
p
t
l
)
ies
≤ S¯j (t) for all
i=τ1+1 (cid:88) τ2 (cid:88) τ2
=
τ (cid:88)1+t
(cid:96)˜ (i)1 (i)
(cid:96)¯j (t)1¯j (t)− (cid:96)
j
(t)1
j
(t)>(cid:15)
1
n.
j∗(τ1+t) j∗(τ1+t) t=t0 t=t0
i=τ1+1 NotethatthesecondtermontheRHSof(76)canberewritten
−N(cid:48) (τ +t)D(f ||g )−logc as:
j∗(τ1+t) 1 j∗(τ1+t) j∗(τ1+t)
P (Ψ(τ )>(cid:15) n,τ ≤n)
≤
τ (cid:88)1+t
(cid:96)˜ (i)1 (i)
m 2 1 2
j∗(τ1+t) j∗(τ1+t) =P
m
(Ψ(τ
2
)>(cid:15)
1
n,τ
2
≤n,t
0
≥τ
1
) (77)
i=τ1+1
(73)
+P (Ψ(τ )>(cid:15) n,τ ≤n,t <τ )
t m 2 1 2 0 1
− −logc
(cid:80) 1/D(f ||g ) First, we upper bound the first term on the RHS of (77).
j(cid:54)=m j j
Notethatforallτ ≤t <t≤τ ,wehave1 (t)=0.Hence,
1 0 2 j
≤
τ (cid:88)1+t
(cid:96)˜ (i)1 (i)−tI (Γ )
j∗(τ1+t) j∗(τ1+t) m DGFi
(cid:88)
τ2
(cid:88)
τ2
(cid:88)
τ2
i=τ1+1 (cid:96)¯j (t)1¯j (t)− (cid:96)
j
(t)1
j
(t)= (cid:96)¯j (t)1¯j (t)
+tI
m
(Γ
DGFi
)/(1+(cid:15)) t=t0 t=t0 t=t0
(78)
≤
τ (cid:88)1+t
(cid:96)˜ j∗(τ1+t) (i)1 j∗(τ1+t) (i)−t(cid:15) 1 = (cid:88)
τ2
(cid:96)˜ ¯j (t)1¯j (t)−D(f¯j ||g¯j )≤ (cid:88)
τ2
(cid:96)˜ ¯j (t)1¯j (t)
i=τ1+1 t=t0 t=t0
for all t ≥ n > −(1+(cid:15))logc/I (Γ ). By applying the Then, applying the generic Chernoff bound given in (53)
m DGFi
completes the proof for this case.
generic Chernoff bound given in (53), it can be shown that
there exists γ > 0 such that P ( (cid:80)τ1+t −(cid:96)˜ (i) ≥ Next, we upper bound the second term on the RHS of (77).
t(cid:15) 1 ) < e−γ1t 1 for all t ≥ n > − m (1 + τ1 (cid:15) + ) 1 logc j / ∗( I τ m 1+ (Γ t) DGFi ). Let (cid:15) 3 (cid:44) 4maxj (cid:15) D 1 (fi||gi) >0. Note that
Hence, there exist C 1 > 0 and γ 1 > 0 such that P m (n 2 > P (Ψ(τ )>(cid:15) n,τ ≤n,t <τ )
m 2 1 2 0 1
n)≤C
1
e−γ1n for all n>−(1+(cid:15))logc/I
m
(Γ
DGFi
). A simi-
larargumentappliesforcasewhereI (Γ )≤D(g ||f ). ≤P m (τ 1 >(cid:15) 3 n) (79)
m DGFi m m
+P (Ψ(τ )>(cid:15) n,τ ≤n,t <τ ,τ ≤(cid:15) n) .
m 2 1 2 0 1 1 3
To show that n is sufficiently small, we define a random The first term on the RHS of (79) decreases exponen-
3
variable Ψ(t) as the dynamic range between sum LLRs of tially with n by Lemma 4. Thus, it remains to show
empty cells: that the second term on the RHS of (79) decreases ex-
ponentially with n. Note that Ψ(τ ) > (cid:15) n implies
Ψ(t)(cid:44)m
j(cid:54)=
a
m
xS j (t)−
j
m
(cid:54)=
i
m
nS j (t). (74) (cid:80)τ
t=
1
t0
(cid:96)¯j 1¯j (t)+ (cid:80)τ
t=
2
τ1+1
(cid:96)¯j 1¯j (t)>(cid:15) 2
1
n. The 1 refore, the
second term on the RHS of (79) can be rewritten as:
Note that the dynamic range at time τ can be viewed as
2
P (Ψ(τ )>(cid:15) n,τ ≤n,t <τ ,τ ≤(cid:15) n)
a measure of the amount of information remains to gather m 2 1 2 0 1 1 3
i L n em or m de a r 6 to be d l i o s w ting sh u o is w h s H th m at f t r h o e m dy an n y am o i t c he r r an f g a e lse at h t y im po e th τ e
2
si i s s . ≤P
m
(cid:32) (cid:88) τ1 (cid:96)¯j (t)1¯j (t)> (cid:15) 1
2
n ,τ
2
≤n,t
0
<τ
1
,τ
1
≤(cid:15)
3
n (cid:33)
sufficiently small. t=t0
ind
L
e
e
fi
m
n
m
ite
a
ly
6
.
:
Th
If
en
t
,
he
fo
s
r
e
e
le
v
c
e
t
r
i
y
on
fix
ru
ed
le
(cid:15)
o
1
f
>
D
0
G
,
F
(cid:15)
i
2
i
>
s i
0
m
t
p
h
l
e
e
r
m
e
e
e
n
x
t
i
e
s
d
t +P
m
(cid:32)
(cid:88)
τ2
(cid:96)¯j (t)1¯j (t)> (cid:15) 1
2
n ,τ
2
≤n,t
0
<τ
1
,τ
1
≤(cid:15)
3
n
(cid:33)
C >0 and γ >0 such that t=τ1+1
(80)
P (Ψ(τ )>(cid:15) n)≤Ce−γn,
m 2 1 The second term on the RHS of (80) decreases exponentially
(75)
∀n>−(1+(cid:15) )logc/I (Γ ) with n using a similar argument as in (78). Next, it remains
2 m DGFi
to show that the first term on the RHS of (80) decreases
for all m=1,2,...,M.
exponentially with n. Note that
Proof: Note that
(cid:88)
τ1
(cid:88)
τ1
(cid:96)¯j (t)1¯j (t)− (cid:96)
j
(t)1
j
(t)
P (Ψ(τ )>(cid:15) n)
m 2 1 t=t0 t=t0
(76)
≤P
m
(τ
2
>n)+P
m
(Ψ(τ
2
)>(cid:15)
1
n,τ
2
≤n)
≤ (cid:88)
τ1
(cid:96)˜ ¯j (t)1¯j (t)− (cid:88)
τ1
(cid:96)˜ j (t)1 j (t)+maxD(f j ||g j )τ 1
j
Since τ
2
= τ
1
+ n
2
, applying Lemmas 4, 5 implies that t=t0 t=t0
t w h i e th fir n st fo te r rm all on n t > he − RH (1 S + of (cid:15) 2 ( ) 7 l 6 o ) gc d / e I c m re ( a Γ se D s GF e i x ) po fo n r en e ti v a e l r ly y ≤ (cid:88) τ1 (cid:104) (cid:96)˜ ¯j (t)1¯j (t)−(cid:96)˜ j (t)1 j (t) (cid:105) + (cid:15) 4 1n
fixed (cid:15)
2
> 0. It remains to show that the second term t=t0
on the RHS of (76) decreases exponentially with n. Let (81)
13
for all τ ≤(cid:15) n. Proof: Since the actual detection time under DGFi is
1 3
As a result, upper bounded by: τ ≤ τ = τ + n + n , combining
3 1 2 3
(cid:88)
τ1
(cid:96)¯j (t)1¯j (t)−(cid:96) j (t)1 j (t)> (cid:15) 2 1n (82)
Le
C
m
o
m
m
a
b
s
in
4
i
,
n
5
g
a
L
n
e
d
m
7
m
p
a
r
2
ov
a
e
n
s
d
th
L
e
em
st
m
at
a
em
8,
en
T
t
h
.
eorem1followsfor
t=t0
the case of K =1.
implies
(cid:88)
τ1
(cid:104) (cid:96)˜ ¯j (t)1¯j (t)−(cid:96)˜ j (t)1 j (t) (cid:105) > (cid:15) 4 1n (83) B. Proof for K >1
t=t0 We focus on the case where F
m
(K) > D(g
m
||f
m
) +
F (K −1). The case where the inequality is reversed can
for all τ ≤(cid:15) n. Applying the generic Chernoff bound given m
1 3
be proven with minor modifications.
in (53), we arrive at the lemma.
We consider the balanced case and the unbalanced case
separately. For the balanced case, the proof in Subsection A
Lemma 7: If the selection rule of DGFi is implemented directly applies. For the unbalanced case, the proof has to
indefinitely, then for every fixed (cid:15)>0 there exist C >0 and be constructed differently. This is because in the unbalanced
γ >0 such that case, there is a process with a sufficiently small information
P (n >n)≤Ce−γn ∀n>−(cid:15)logc/I (Γ ), (84) acquisition rate D(f j ||g j ) such that it becomes the bottleneck
m 3 m DGFi
of the detection process and determines the asymptotic in-
for all m=1,2,...,M. creasing rate of ∆S(n). Directly bounding the dynamic range
Proof:ToprovetheLemma,wefirstdefineτj (cid:44)max{t: of all sum LLR trajectories is no longer tractable. Instead,
∀n≥t,S (n)−S (n)≥−logc}andNj asthe 3 totalnumber the proof is built upon the analysis of the trajectory of the
m j 3 sum LLR with the smallest expected increment. In particular,
of observations that the decision maker collected from cell j
between τ and τj. Since n ≤ (cid:80) Nj and τ = max τj, we recognize that the key in handling the imbalance in the
2 3 3 j 3 3 j 3 information acquisition rates among empty cells is to define
weonlyneedtoshowthatP (Nj >n)decaysexponentially
m 3 a last passage time as the last time at which the empty cell
with n. We can write P (Nj >n) as follows:
m 3 with the smallest D(f ||g ) is not probed and then analyze,
j j
(cid:18) (cid:19)
P (Nj >n)≤P Ψ(τ )>n min j D(f j ||g j ) separately, the detection process before and after this last
m 3 m 2 2 passage time.
(85)
(cid:18) (cid:19) The proof proceeds as follows. First, by directly applying
min D(f ||g )
+P
m
N
3
j >n|Ψ(τ
2
)≤n j
2
j j Lemma2,theerrorprobabilityunderDGFiisO(c).Then,we
show that the expected detection time of DGFi is bounded.
Lemma 6 provides the desired decay for the first term on the Similar to the case of K = 1, we partition the detection
RHS. We next show the desired decay for the second term. process into three stages with minor modifications. The first
Let t 1 ,t 2 ,... denote the time indices when cell j is observed and the third stage are defined by the same last passage times
between τ 2 and τ 3 j. We can write: τ 1 andτ 3 givenin(49).Thesecondstage,however,isdefined
P m (cid:18) N 3 j >n|Ψ(τ 2 )≤n min j D 2 (f j ||g j ) (cid:19) d th if e fe s r m en al t l l e y st b K y L τ˜ 2 d , i i v n e d rg ic e a n t c e e s i t s ha s t m t a h l e le s r u t m han LL − R lo o g f c t . h B e y ce d l i l re w c i t t l h y
≤P (cid:32) inf (cid:88) r −(cid:96) (t )<n min j D(f j ||g j ) (cid:33) a w p i p th ly h in ig g h L p e r m o m ba a b s il 3 ity a . nd4,weshowthatτ 1 issufficientlysmall
m r>n j i 2 (86)
≤P m (cid:32) (cid:88) r (cid:96)˜ i j = ( 1 t i )>r min j D 2 (f j ||g j ) (cid:33) . i o s bs T b e h o r e u v n e n , d de w d s e um b p y ro L I v L m e − R (Γ l t o D h g a e G ( m c F ) f o i o ) n . l g lo L w t e h i m n e g m e a L m e 9 p m ty s m ta a c t s e e l s l t s o th i a s s t ho s t w u h f e fi t c l h a i a e r t g n e t τ l s y 3 t
i=1
large as required with high probability. Lemma 10 states that
Using the i.i.d. property of (cid:96)˜ j (t i ) yields: the smallest observed sum LLR among the empty cells is
(cid:32) n (cid:33) sufficientlysmallasrequiredwithhighprobability.Lemma11
P (cid:88) (cid:96)˜(t )>n min j D(f j ||g j ) <C e−γn (87) shows the difference between the (K+1)th largest sum LLR
m j i 2 3
i=1 andtheMth largestsumLLRissufficientlysmallasrequired
for some C ,γ which completes the proof. with high probability. Lemma 12 states that the sum LLR of
3 3
the cell with the smallest KL divergence is sufficiently small
The following Lemma provides an upper bound on the
(whichwilldeterminetheratefunctionfunctionforthesearch
detection time when DGFi policy is implemented.
in this case) with high probability. Lemma 13 shows that the
Lemma8:IfDGFipolicyisimplemented,thentheexpected
sumLLRofothercellsaresmallerthanthatofthecellwiththe
detection time τ is upper bounded by:
smallest KL divergence at time when t>τ˜ . Finally, Lemma
2
log(c) 14 upper bounds the last passage time τ .
E (τ)≤−(1+o(1)) , (88) 3
m I (Γ ) Define
m DGFi
for m=1,...,M. U(n)(cid:44)minS (n), L(n)(cid:44)maxS (n), (89)
j j
j(cid:54)=m j(cid:54)=m
14
Proof:Weprovebyinductionwithrespecttot.Whent=
Ψj2(n)(cid:44)S (n)−S (n), (90) 1, using the generic Chernoff bound completes the induction
j1 m(j2)(n) m(j1)(n)
base. If the statement is true for t−1, then for t we have
j(t)(cid:44)argminN (t)D(f ||g ), (91) P (ΨK+1(t)>maxD(f ||g )+n(cid:15))
j j j m M j j
j(cid:54)=m j(cid:54)=m
=P (ΨK+1(t)>max D(f ||g )+n(cid:15),
m M j(cid:54)=m j j
¯j(t)(cid:44)argmaxN j (t)D(f j ||g j ), (92) m(M)(t)=m(M)(t−1)) (103)
j(cid:54)=m
+P (ΨK+1(t)>max D(f ||g )+n(cid:15),
m M j(cid:54)=m j j
j(cid:48) =argminD(f j ||g j ). (93) m(M)(t)(cid:54)=m(M)(t−1)).
j(cid:54)=m
Lemma 9: For any selection rule, ∀t,∀(cid:15) > 0, there exist For the first term on the RHS, we have
C,γ >0 such that
P (ΨK+1(t)>maxD(f ||g )+n(cid:15),
P (L(t)<−tKF¯ −n(cid:15))<Ce−γn ∀n>t. (94) m M j(cid:54)=m j j
m m
m(M)(t)=m(M)(t−1))
n(cid:15)
Proof: Note that ≤P (ΨK+1(t−1)>maxD(f ||g )+ ,
m M j(cid:54)=m j j 2
P m (L(t)<−tKF¯ m −n(cid:15))≤P(S j(t) (t)<−tKF¯ m −n(cid:15)), m(M)(t)=m(M)(t−1) or l (t)<− n(cid:15) ,
(95) m(K+1)(t−1) 2
and
m(M)(t)=m(M)(t−1))
t n(cid:15)
S j(t) (t)=−N j(t) (t)D(f j(t) ||g j(t) )+
(cid:88)˜l
j(t) (i)1 j(t) (i).
≤P
m
(ΨK
M
+1(t−1)>m
j(cid:54)=
a
m
xD(f
j
||g
j
)+
2
)
i=1 n(cid:15)
(96) +P (l (t)<− )
m m(K+1)(t−1) 2
SinceKtisthetotalnumberofobservationsbytimet,bythe
definition of j(t) we have ≤C 1 e−γ1n,
(104)
Kt= (cid:88) N (t)= (cid:88)N j (t)D(f j ||g j ) where the first term can be bounded using assumptions on
j D(f j ||g j ) t−1 and the second term can be bounded using the generic
j j
N (t)D(f ||g ) (97) Chernoff bound.
(cid:88) j(t) j(t) j(t)
≥ . For the second term on the RHS of (103), we have
D(f ||g )
j j
j
P (ΨK+1(t)>maxD(f ||g )+n(cid:15),r(M)(t)(cid:54)=r(M)(t−1))
Hence, m M j j
j(cid:54)=m
N
j(t)
(t)D(f
j(t)
||g
j(t)
)≤Kt·
(cid:80) 1/D
1
(f ||g )
=t·KF¯
m
. ≤P m (l m(M)(t) (t)>m
j(cid:54)=
a
m
xD(f j ||g j )+n(cid:15))
j j j
(98) ≤P (˜l (t)>n(cid:15))<C e−γ2n.
m m(M)(t) 2
Therefore, (105)
S (t)<−tKF¯ −n(cid:15) (99) Combining (103), (104), (105) completes the proof.
j(t) m
Lemma 12: If DGFi policy is implemented, ∀t > τ ,∀(cid:15),
1
implies
there exist C,γ >0 such that
t
(cid:88)˜l
(i)1 (i)<−n(cid:15). (100)
j(t) j(t) P (S (t)−S (τ )>−(t−τ )D(f ||g )+n(cid:15))<Ce−γn
m j(cid:48) j(cid:48) 1 1 j(cid:48) j(cid:48)
i=1
∀n>t.
Then, applying the generic Chernoff bound completes the
(106)
proof.
Proof: Define t as the smallest integer such that cell j(cid:48)
0
Lemma10:Foranyselectionrule,∀t,∀(cid:15),thereexistC,γ > isobservedattimeiforallt 0 <i≤t.Then,byourselection
0 such that rule, cell j(cid:48) is the one of the top K sum LLRs at time t 0 .
Then, by applying t=t to Lemma 11 we have
0
P (U(t)>−tKF¯ +n(cid:15))<Ce−γn ∀n>t. (101)
m m
P
m
(U(t
0
)−S
j(cid:48)
(t
0
)<−n(cid:15))<C
1
e−γ1n ∀n>t
0
, (107)
Proof: The proof follows similarly with Lemma 9.
for some C ,γ . Substituting t=t in Lemma 10 we have:
1 1 0
Lemma 11: If DGFi policy is implemented, ∀t,∀(cid:15), there
exist C,γ >0 such that P m (U(t 0 )>−t 0 KF¯ m +n(cid:15))<C 2 e−γ2n ∀n>t 0 , (108)
P m (ΨK M +1(t)>maxD(f j ||g j )+n(cid:15))<Ce−γn ∀n>t. for some C 2 ,γ 2 . Hence,
j(cid:54)=m
(102)
P
m
(S
j(cid:48)
(t
0
)>−t
0
KF¯
m
+n(cid:15))<C
3
e−γ3n ∀n>t
0
, (109)
15
for some C ,γ . Then, by the definition of t and using the Proof: By substituting t=τ˜ in Lemma 12 we have:
3 3 0 2
generic Chernoff bound we have
P
m
(S
j(cid:48)
(τ˜
2
)>logc+n(cid:15))<C
1
e−γ1n ∀n>τ˜
2
(117)
P (S (t)−S (t )>−(t−t )D(f ||g )+n(cid:15))
m j(cid:48) j(cid:48) 0 0 j(cid:48) j(cid:48)
(110)
for some C ,γ . By applying Lemma 13, we have:
<C e−γ4n ∀n>(t−t ). 1 1
4 0
Since KF¯ >D(f ||g ), we have: P m (S j (τ˜ 2 )>logc+n(cid:15))<C 2 e−γ2n
m j(cid:48) j(cid:48) (118)
∀n>τ˜ ,j =1,2,··· ,m
P m (S j(cid:48) (t)−S j(cid:48) (τ 1 )>−(t−τ 1 )D(f j(cid:48) ||g j(cid:48) )+n(cid:15)) 2
<C 5 e−γ5n ∀n>t for some C 2 ,γ 2 >0.
(111)
as desired. LetN˜j denotethattotalnumberofobservations,takenfrom
3
cell j between τ˜ and τj. Since n˜ ≤ (cid:80) N˜j, it suffices to
Define τ˜ 2 = τ 1 + D( − f j l (cid:48) o | g |g c j(cid:48)) . Next we show that the sum show that P(N˜ 3 j 2 >n) de 3 cays expon 3 entially w 3 ith n. Note that
LLRs of other cells are smaller than cell j(cid:48) at time τ˜ .
2
P (N˜j >n)
m 3
Lemma 13: For every fixed (cid:15) > 0, there exists C > 0 and
(cid:18) (cid:19)
D(f ||g )
γ >0, such that for all j we have: ≤P S (τ˜ )>logc+n j(cid:48) j(cid:48)
m j 2 2 (119)
P (S (τ˜ )−S (τ˜ )<−(cid:15)n)≤Ce−γn, ∀n>τ˜ . (112)
m j(cid:48) 2 j 2 2 (cid:18) (cid:19)
D(f ||g )
+P N˜j >n|S (τ˜ )≤logc+n j(cid:48) j(cid:48) .
Proof: For fixed j, define tj as the smallest integer m 3 j 2 2
0
such that S (n) < S (n) for all tj < i ≤ τ˜ . By def-
j(cid:48) j 0 2 By (118) it remains to show that the second term decays
inition, S j(cid:48) (tj 0 ) ≥ S j (tj 0 ). Then, by our selection rule, for exponentially with n. Let t 1 ,t 2 ,··· denote the time indices
all tj 0 < i ≤ τ˜ 2 , whenever cell j(cid:48) is observed, cell j must when cell j is observed between τ˜ 2 and τ 3 j. Then,
be observed based on their ranking of sum LLRs. Note that
D(f j(cid:48) ||g j(cid:48) )≤D(f j ||g j ). Thus, P (N˜j >n|S (τ˜ )≤logc+n D(f j(cid:48) ||g j(cid:48) ) )
m 3 j 2 2
(cid:88) τ˜2 l j (i)1 j (i)− (cid:88) τ˜2 l j(cid:48) (i)1 j(cid:48) (i) ≤P (cid:32) inf (cid:88) r l (t )<n D(f j(cid:48) ||g j(cid:48) ) (cid:33)
i=tj
0
i=tj
0
m r>n
i=1
j i 2
= (cid:88) τ˜2 ˜l j (i)1 j (i)− (cid:88) τ˜2 ˜l j(cid:48) (i)1 j(cid:48) (i) ≤P m (cid:32) (cid:88) r ˜l j (t i )>r D(f j 2 (cid:48) ||g j(cid:48) ) (cid:33) .
i=tj i=tj i=1
0 0 (113)
(cid:88)
τ˜2
(cid:88)
τ˜2
Applying the generic Chernoff bound and using the i.i.d.
+D(f ||g ) 1 (i)−D(f ||g ) 1 (i)
j j j j(cid:48) j(cid:48) j(cid:48) property of ˜l (t ) across time we have
j i
i=tj i=tj
≥ (cid:88)
τ˜2
˜l (i)1 (i
0
)− (cid:88)
τ˜2
˜l (i)1 (i),
0
P
(cid:32)
(cid:88)
r
˜l (t )>r D(f j(cid:48) ||g j(cid:48) )
(cid:33)
<C e−γn (120)
j j j(cid:48) j(cid:48) m j i 2 3
i=tj i=tj i=1
0 0
whichindicatesthattheLHShaspositivemeans.Byapplying for some C 3 ,γ 3 which completes the proof.
the generic Chernoff bound and using the i.i.d. property of The following Lemma provides an upper bound on the
˜l (t ) we have: detection time for the unbalanced case.
j i
Lemma 15: If DGFi policy is implemented, for the unbal-
P (S (τ˜ )−S (tj)−(S (τ˜ )−S (tj))<−(cid:15)n)≤Ce−γn,
m j(cid:48) 2 j(cid:48) 0 j 2 j 0 anced case, the expected detection time τ is upper bounded
(114)
by:
for some C,γ. Since S (tj)≥S (tj), we have:
j(cid:48) 0 j 0 log(c)
E (τ)≤−(1+o(1)) , (121)
P m (S j(cid:48) (τ˜ 2 )−S j (τ˜ 2 )<−(cid:15)n) m I m (Γ DGFi )
≤P (S (τ˜ )−S (tj)−(S (τ˜ )−S (tj))<−(cid:15)n) for m=1,...,M.
m j(cid:48) 2 j(cid:48) 0 j 2 j 0
≤Ce−γn, ∀n>τ˜ Proof: Since the actual detection time under DGFi is
2
(115) upper bounded by: τ ≤τ =τ˜ +n˜ =τ + −log(c) +n˜ ,
as desired.
3 2 3 1 Im(ΓDGFi) 3
combining Lemmas 4 and 14 proves the statement.
Let n˜ 3 (cid:44)τ 3 −τ˜ 2 denotes the total amount of time between Combining Lemma 2 and Lemma 15, Theorem 1 follows
τ˜ 2 and τ 3 . for the case of K >1.
Lemma 14: For every fixed (cid:15) > 0, there exists C > 0 and
γ >0 such that APPENDIXB:PROOFOFTHEOREM2
P (n˜ >n)<Ce−γn, ∀n>−(cid:15)logc/D(f ||g ). FirstweshowthatinordertoachieveasmallorderofBayes
m 3 j(cid:48) j(cid:48)
(116) Risk, ∆S (τ) defined in (44) need to be sufficient large.
m
16
Lemma 16: Assume that α (Γ) = O(−clogc) for all j = which completes the proof.
j
1,...,M. Let 0<(cid:15)<1. Then:
For the next lemma we define
P (∆S (τ)<−(1−(cid:15))logc|Γ)=O(−c(cid:15)logc), (122)
m m
j∗(t)(cid:44)argminN (t)D(f ||g ), (130)
for all m=1,...,M. j j j
j(cid:54)=m
Proof: Note that:
and
P (∆S (τ)<−(1−(cid:15))logc|Γ)
m m t t
=P m (∆S m (τ)<−(1−(cid:15))logc, δ =m|Γ) W m ∗(t)(cid:44) (cid:88) (cid:96)˜ m (i)1 m (i)− (cid:88) (cid:96)˜ j∗(t) (i)1 j∗(t) (i), (131)
i=1 i=1
+P (∆S (τ)<−(1−(cid:15))logc, δ (cid:54)=m|Γ)
m m
which is a sum of zero-mean random variable
≤P (∆S (τ)<−(1−(cid:15))logc, δ =m|Γ)+α (Γ),
m m m
(123)
Lemma 18: For every fixed (cid:15) > 0 there exist C > 0 and
where α (Γ)=O(−clogc) by assumption. In what follows,
m γ >0 such that
we upper bound
(cid:18) (cid:19)
P max W∗(t)≥n(cid:15)|Γ ≤Ce−γn (132)
P (∆S (τ)<−(1−(cid:15))logc, δ =m|Γ). m m
m m 1≤t≤n
Similar to [2, Lemma 4] we can show that for all j (cid:54)= m
for all m=1,...,M and for any policy Γ.
there exists G>0 such that:
Proof: We upper bound (132) by summing over any
−Gclogc≥P (δ (cid:54)=j|Γ)≥P (δ =m|Γ)
j j possible values that N (t),N (t) can take and using the
m j∗(t)
≥P (∆S (τ)≤−(1−(cid:15))logc, δ =m|Γ) generic Chernoff bound given in (53):
j m,j
≥c1−(cid:15)P (∆S (τ)<−(1−(cid:15))logc, δ =m|Γ) , (cid:18) (cid:19)
m m,j (124) P m max W m ∗(t)≥n(cid:15)|Γ
1≤t≤n
where the last inequality holds by changing the measure as in n t t (cid:32) t
[2, Lemma 4]. Thus, = (cid:88) (cid:88) (cid:88) P (cid:88) (cid:96)˜ (r)1 (r)
m m m
P (∆S (τ)<−(1−(cid:15))logc, δ =m|Γ) t=1 i=0 j=0 r=1
m m,j
(125) t (cid:33)
=O(−c(cid:15)logc) ∀j (cid:54)=m. + (cid:88) −(cid:96)˜ (r)1 (r)≥n(cid:15),N (t)=i,N =j|Γ
j∗(t) j∗(t) m j∗(t)
r=1
As a result,
n t t
≤ (cid:88) (cid:88) (cid:88)(cid:104) E (cid:16) es((cid:96)˜ m(1)−(cid:15)/2) (cid:17)(cid:105)i
P (∆S (τ)<−(1−(cid:15))logc, δ =m|Γ) m
m m
t=1 i=0 j=0
(cid:88)
≤
j(cid:54)=m
P m (∆S m,j (τ)<−(1−(cid:15))logc, δ =m|Γ) × (cid:104) E
m
(cid:16) es(−(cid:96)˜ j∗(t)(1)−(cid:15)/2) (cid:17)(cid:105)j ×exp (cid:110) −s
2
(cid:15) (2n−i−j) (cid:111) ,
(133)
=O(−c(cid:15)logc) .
for all s>0.
(126) Since E ((cid:96)˜ (1)−(cid:15)/2)=−(cid:15)/2<0 and E (−(cid:96)˜ (1)−
m m m j∗(t)
Finally, (cid:15)/2) = −(cid:15)/2 < 0 are strictly negative, using a similar
argument as at the end of the proof of Lemma 3, there
(cid:16) (cid:17)
P m (∆S m (τ)<−(1−(cid:15))logc|Γ)=O(−c(cid:15)logc). exist s > 0 and γ(cid:48) > 0 such that E m es((cid:96)˜ m(1)−(cid:15)/2) ,
(127) (cid:16) (cid:17)
E
m
es(−(cid:96)˜ j∗(t)(1)−(cid:15)/2) and e−s(cid:15)/2 are strictly less than
e−γ(cid:48) < 1. Since 2n − i − j ≥ 0, there exist C > 0 and
Lemma 17: Assume that
γ >0, such that summing over t,i,j yields (132).
1
D(g ||f )≥ . (128)
m m (cid:80) 1 Lemma 19: For any fixed (cid:15)>0,
j(cid:54)=m D(fj||gj)
Then, the function: (cid:18) (cid:19)
P max ∆S (t)≥n(I∗ +(cid:15)) |Γ →0 as n→∞,
(cid:34) (cid:35) m m m
n −1 1≤t≤n
d(t)(cid:44)t D(g ||f )+ t (129) (134)
m m (cid:80) 1
j(cid:54)=m D(fj||gj) for all m=1,...,M and for any policy Γ.
is monotonically increasing with t for 0≤t≤n. Proof: We next show exponential decay of (134) (which
Proof: Differentiation d(t) with respect to t yields: is stronger than the polynomial decay shown under the binary
composite hypothesis testing case in [2, Lemma 5]). Let
∂d(t) 1
=D(g ||f )− ≥0,
∂t m m (cid:80) 1 ∆S∗(t)(cid:44)S (t)−S (t).
j(cid:54)=m D(fj||gj) m m j∗(t)
17
logc
Note that ∆S m (t)≤∆S m ∗(t) for all m and t. As a result, Proof:Forany(cid:15)>0letn c =−(1−(cid:15)) I∗ +(cid:15) .Notethat
m
(cid:18) (cid:19)
P m max ∆S m (t)≥n(I m ∗ +(cid:15))|Γ P m (τ ≤n c |Γ)
1≤t≤n
(135) =P (τ ≤n , ∆S (τ)≥−(1−(cid:15))logc|Γ)
(cid:18) (cid:19) m c m
≤P max ∆S∗(t)≥n(I∗ +(cid:15))|Γ .
m m m +P (τ ≤n , ∆S (τ)<−(1−(cid:15))logc|Γ)
1≤t≤n m c m
(141)
(cid:18) (cid:19)
We next prove the lemma for the case where I m ∗ =F m (K) ≤P m max∆S m (t)≥−(1−(cid:15))logc|Γ
and u∗ =0. Proving the lemma for the cases where u∗ >0 t≤nc
m m
applies with minor modifications. +P m (∆S m (τ)<−(1−(cid:15))logc|Γ).
Note that:
Both terms on the RHS approaches zero as c → 0 by
∆S m ∗(t)=W m ∗(t)+N m (t)D(g m ||f m ) Lemmas 16, 19. Hence,
+N (t)D(f ||g ) ∞
j∗(t) j∗(t) j∗(t) (cid:88)
E (τ|Γ)≥ nP (τ =n|Γ)
1 (136) m m
≤W
m
∗(t)+N
m
(t)·
(cid:80) 1/D(f ||g )
n=nc+1 (142)
j(cid:54)=m j j ≥n P (τ ≥n +1|Γ)→n as c→0
c m c c
+N (t)D(f ||g ).
j∗(t) j∗(t) j∗(t)
Since (cid:15) > 0 is arbitrarily small we have E (τ|Γ) ≥
m
Since that j∗(t) = argmin j(cid:54)=m N j (t)D(f j ||g j ) and Kt − −(1+o(1))log(c)/I m ∗. As a result, R m (Γ) ≥ cE m (τ|Γ) ≥
N
m
(t) is the total number of observations taken from M −1 −(1+o(1))clog(c)/I∗.
m
cells j (cid:54)=m, we have:
(cid:88) N j∗(t) D(f j∗(t) ||g j∗(t) )
≤Kt−N (t)≤Kn−N (t).
D(f j ||g j ) m m APPENDIXC:PROOFOFLEMMA1
j(cid:54)=m
(137) Define
Hence,
h (u)=uD(g ||f )+F (K−u). (143)
m m m m
1
∆S m ∗(t)≤W m ∗(t)+Kn (cid:80) 1/D(f ||g ) By taking the derivative of h m (u), we have
j(cid:54)=m j j (138)
h(cid:48) (u)=D(g ||f )−F(cid:48) (K−u), (144)
=W∗(t)+nI∗ . m m m m
m m
where
Therefore,

∆S m ∗(t)≥n(I m ∗ +(cid:15)) F m (cid:48) (v)=   (cid:80) 0, j(cid:54)=m D 1 (fj 1 ||gj) , i i f f v v ≤ > K K ˜ ˜ m . (145)
m
implies
Since F(cid:48) (v) is piecewise constant with a breakpoint K˜ ,
W m ∗(t)≥n(cid:15). h(cid:48) (u) is m piecewise constant with a breakpoint K − K˜ m .
m m
By Lemma 18 we have: Therefore,
1) If D(g ||f )≥F¯ , then h(cid:48) (u)>0 and u∗ =1.
(cid:18) (cid:19) m m m m m
P m 1 m ≤t a ≤ x n ∆S m (t)≥n(I m ∗ +(cid:15)) 2) p If os K itiv > e c K o ˜ n m sta + nt 1 a , nd the u n ∗ h = (cid:48) m 1 (u) = D(f j ||g j ) > 0 is a
(cid:18) (cid:19) m
≤P m 1 m ≤t a ≤ x n W m ∗(t)≥n(cid:15) (139) 3) D If ( D g (g || m f ||f ) m < ) F¯ < < F¯ m 0is an a d ne K gati < ve K c ˜ o m nsta th n e t n an h d (cid:48) m u ( ∗ u) = = 0
≤Ce−γn →0 as n→∞. 4) If no m ne o m f the a m bove is true, then h(cid:48) (u) > 0 fo m r u <
m
K −K˜ and h(cid:48) (u) > 0 for u < K −K˜ . Therefore,
m m m
u∗ =K−K˜
m m
Finally,weshowthattheBayesriskcannotbemadesmaller
than −clog(c): APPENDIXD:PROOFOFTHEOREM4
I∗
m
Lemma20:AnypolicyΓthatsatisfiesR (Γ)=O(−clogc) We now focus on proving asymptotic optimality for L>1,
j
for all j =1,...,M must satisfy: and K = 1. For L > 1, we define τ as the smallest integer
1
such that S (n)>S (n) for all m∈D, j (cid:54)=D and n≥τ .
m j 1
clog(c)
Note that when K =1 and n≥τ the decision maker always
R (Γ)≥−(1+o(1)) . (140) 1
m I m ∗ probe the consistent cell (target or not depending on the order
of G¯ and F¯ ) for making the difference between the Lth
for all m=1,...,M. D D
and (L+1)th largest sum LLRs greater than the threshold
−logc. As a result, the decision maker can always balance
18
the detection time so that the difference between the largest It suffices to show that there exist constants C,γ such that
sum LLR and the sum LLRs of any other cell exceeds the
P (N (t)>qn,∃m∈D :S (t)>S (t))≤Ce−γn (152)
threshold −logc approximately at the same time as c → 0. D j j m
Thus, proving the asymptotic optimality of DGFi for L > 1 for all t≤n.
and K =1 follows similar arguments as in the balanced case First we have
intheproofofTheorem1giveninAppendixB,andwefocus
P (N (t)>qn,∃m∈D :S (t)>S (t))
here only on the key modifications. Let D j j m
(cid:88) (153)
∆S (n)(cid:44) min ∆S (n), (146) ≤ P D (N j (t)>qn,S j (t)>S m (t)).
D m,j
m∈D,j∈/D m∈D
where∆S m,j (n)isdefinedin(43).Withoutlossofgenerality Fix m, then we have
we prove the theorem when set D contains all the targets. We
P (N (t)>qn,S (t)>S (t))
define D j j m
 n n (cid:32) n k (cid:33)
(cid:96) (i)−D(g ||f ), if k ∈D, (cid:88) (cid:88) (cid:88) (cid:88)
(cid:96)˜ (i)=  k k k (147) ≤ P D (cid:96) j (i)+ −(cid:96) m (i)≥0 (154)
k
(cid:96) (i)+D(f ||g ), if k ∈/ D, r=(cid:100)qn(cid:101)k=0 i=1 k=1
k k k
≤C e−γmn.
which is a zero-mean random variable. m
The last inequality can be shown using the generic Chernoff
We start by showing the upper bound on the Bayes risk
bound given in (53).
obtained by DGFi. Similar to Lemma 2, we can show that
(cid:80)
To show (152), we let C = C ,γ =min γ , which
theerror probabilityunderDGFi isO(c).Specifically, wecan m m m m
completes the proof.
show that the error probability is upper bounded by:
Lemma 22: For all m∈D, and (cid:15)>0, there exist C,γ >0
P ≤(M −L)L·c. (148)
e
such that
We can show this by letting α D =P D (δ (cid:54)=D) and α D,j = (cid:18) G¯ (cid:19)
P (j ∈ δ) for all j ∈/ D, where the subscript D denotes P N (n)> D ·n ≤Ce−γn (155)
D D m D(g ||f )−(cid:15)
the measure when set D contains all the targets. Thus, α ≤ m m
D
(cid:80)
α . By the stopping rule, accepting j ∈ δ implies
j∈/D D,j Proof: For each m, define tm(n) as the time when cell
∆S ≥ −logc for some m ∈ D. Hence, for all j ∈/ D we
j,m misobservedforthenth time.ByDGFiselectionrule,ifcell
have:
m is observed at time t, either there exists j ∈/ D such that
α D,j =P D (j ∈D) S j (n)>S m (n) or S m(cid:48) (n)>S m (n) for all m(cid:48) ∈D. Similar
≤ (cid:80) P (∆S (τ)≥−logc) to (151), it suffices to show that
m∈D D j,m (149)
(cid:16) (cid:17)
≤
(cid:88)
cP D∪j\m (∆S j,m (τ)≥−logc)≤L·c, P D N m (t)> D(gm
G¯
|| D fm)−(cid:15) ·n,∃j ∈/ D :S j (t)>S m (t)
m∈D ≤Ce−γn
where we changed the measure in the second inequality. As a
(156)
result,
and
α D ≤
j
(cid:88)
∈/D
α D,j ≤(M −L)L·c, P D (cid:16) N m (t)> D(gm G¯ || D fm)−(cid:15) ·n,∀m(cid:48) ∈D :S m(cid:48) (t)>S m (t) (cid:17)
which yields (148).
≤Ce−γn
(157)
Here we consider the case where I =G¯ , the case I = for all t<n.
D D D
F¯ applies with minor modifications. For showing that τ is Since (156) can be shown similarly as in (152), it remains to
D 1
sufficientlysmallweneedtoshowfirstthefollowingLemmas: show(157).BythedefinitionofG¯ D ,ifN m (t)> D(gm G¯ || D fm)−(cid:15) ·
Lemma21:Forallj ∈/ D,∀0<q <1,thereexistC,γ >0 n, there exists m(cid:48) ∈ D and (cid:15)(cid:48) > 0 such that N (t) <
m(cid:48)
such that G¯ D ·t. Hence,
D(g(cid:48) ||f(cid:48) )+(cid:15)(cid:48)
P (N (n)>qn)<Ce−γn (150) m m
D j (cid:18) G¯
P N (t)> D ·n,∀m(cid:48) ∈D :
Proof: For each j, define tj(n) as the time when cell j D m D(g ||f )−(cid:15)
m m
is observed for the nth time. By DGFi selection rule, if cell (cid:19)
j is observed at time t, then there exists m ∈ D such that S m(cid:48) (t)>S m (t)
S (t)≥S (t). Hence,
j m
P (N (n)>qn) ≤ (cid:88) P
(cid:18)
N (t)>
G¯
D ·n,S (t)>S (t),
D j D m D(g ||f )−(cid:15) m(cid:48) m
m m
n m(cid:48)∈D
(cid:88)
≤ P D (N j (t)>qn,∃m∈D :S j (t)>S m (t)) (151) N(cid:48) (t)< G¯ D ·t (cid:19) .
t=1 m D(g(cid:48) ||f(cid:48) )+(cid:15)(cid:48)
m m
×P (tj((cid:100)qn(cid:101))=t). (158)
D
19
Th
F
en
ix
, w
m
e
(cid:48),
ha
a
v
n
e
d let s 1 = D(gm G¯ || D fm)−(cid:15) , s 2 = D(g m(cid:48) G¯ || D f m(cid:48))+(cid:15) .
d
F
e
o
c
ll
a
o
y
w
s
i
e
n
x
g
p
(
o
1
n
6
e
2
n
)
t
,
ia
it
ll
s
y
uf
w
fi
i
c
t
e
h
s
n
to
.
s
N
h
o
o
t
w
e t
t
h
h
a
a
t
tP D (S j (n)≥S m (n))
P (S (n)≥S (n))
P D (cid:18) N m (t)> D(g m G | ¯ |f D m )−(cid:15) ·n,S m(cid:48) (t)>S m (t) ≤ D P D j (cid:18) S j (n)≥ m S m (n),N m (n)≥( 2D(g G¯ D ||f ) )n (cid:19)
m m
,N m (cid:48) (t)< D(g m(cid:48) | G | ¯ f D m(cid:48) )+(cid:15)(cid:48) ·t (cid:19) +P D (cid:18) N m (n)<( 2D(g G¯ m D ||f m ) )n (cid:19)
(163)
(cid:88) n (cid:98) (cid:88) s2t(cid:99) (cid:32) (cid:88) r (cid:88) k (cid:33) The first term decays exponentially with n by Lemma 3 (with
≤ P −(cid:96) (i)+ (cid:96) (i)≥0
D m m(cid:48) minor modifications). The second term decays exponentially
r=(cid:100)s1n(cid:101) k=0 i=1 i=1 with n by Lemma 23.
(cid:88)
n (cid:98)
(cid:88)
s2t(cid:99) (cid:32)
(cid:88)
r Notethatweobtainedthattheexpectationofτ 1 isbounded,
≤ P D(g ||f )−(cid:15)−(cid:96) (i) and we can use similar arguments as in the balanced case of
D m m m
Theorem 1 in Appendix B to obtain the detection rate I
r=(cid:100)s1n(cid:101) k=0 i=1 D
for n ≥ τ . Combining these results yields that the expected
k (cid:33) 1
+ (cid:88) (cid:96) (i)−D(g ||f )−(cid:15)(cid:48) ≥0 detection time τ under the DGFi policy is upper bounded by:
m(cid:48) m(cid:48) m(cid:48)
log(c)
i=1
E (τ)≤−(1+o(1)) , (164)
≤ (cid:88) n (cid:98) (cid:88) s2t(cid:99) (cid:104) E
D
(cid:16) es(−(cid:96)˜ m(1)−(cid:15)) (cid:17)(cid:105)r(cid:104) E
D
(cid:16) es((cid:96) m(cid:48)(1)−(cid:15)(cid:48)) (cid:17)(cid:105)k for m=1,...,M D . I D
r=(cid:100)s1n(cid:101) k=0 Finally, showing that the asymptotic Bayes risk is lower
≤C
m(cid:48)
e−γ m(cid:48)n bounded by −clogc/I
L
∗ follows a similar outline as in Ap-
(159) pendix B. Specifically, similar to Lemma 16, if α (Γ) =
D
The last inequality can be shown using the generic Cher- O(−clogc) for all D, and we let 0<(cid:15)<1, then:
noff bound given in (53). To show (158), we let C =
(cid:80) C ,γ =min γ , which completes the proof. P D (∆S m (τ)<−(1−(cid:15))logc|Γ)=O(−c(cid:15)logc), (165)
m(cid:48) m(cid:48) m(cid:48) m(cid:48)
for all D and m∈D. Then, we define:
Lemma 23: For all m ∈ D, ∀(cid:15) > 0, there exist C,γ > 0
such that j∗(t)(cid:44)argminN (t)D(f ||g ), (166)
j j j
j∈/D
(cid:18) G¯ (cid:19)
P D N m (n)<( 2D(g m D ||f m ) )n ≤Ce−γn. (160) m∗(t)(cid:44)arg m m ∈ in D N m∗(t) (t)D(g m ||f m ), (167)
and
t t
Lemm Pr a o 2 o 2 f: su B c y h t c h h a o t o (cid:80) sin j g q j + q j (cid:80) a (cid:48) m nd (cid:15)(cid:48) m (cid:15)(cid:48) m = i 2 n D( ¯¯ g L G m e D | m |f m m) a , w 21 e h a a n v d e W D ∗(t)(cid:44) (cid:88) i=1 (cid:96)˜ m∗(t) (i)1 m∗(t) (i)− (cid:88) i=1 (cid:96)˜ j∗(t) (i)1 j∗(t) (i),
(168)
(cid:18) G¯ (cid:19) where W D ∗(t) is a sum of zero-mean random variable. Using
P N (n)<( D )n these definitions, similar to Lemma 18, we can show that for
D m 2D(g ||f )
(cid:88) m m every fixed (cid:15)>0 there exist C >0 and γ >0 such that
≤ P (N (n)>q n)
D j j (cid:18) (cid:19)
j∈/D P max W∗(t)≥n(cid:15)|Γ ≤Ce−γn (169)
+ (cid:88) P
(cid:18)
N(cid:48) (n)>(
G¯
D +(cid:15)(cid:48) )n
(cid:19)
≤C e−γn
D
1≤t≤n
D
D m D(g(cid:48) ||f(cid:48) ) m m(cid:48) for all D and for any policy Γ.
m(cid:48)∈D m m
(161) Next, similar to Lemma 19 we can show that for any fixed
as desired. (cid:15)>0,
(cid:18) (cid:19)
Next,similartoLemma4,wecanshowthattheprobability
P max ∆S (t)≥n(I +(cid:15)) |Γ →0
D D D
that τ is greater than n decreases exponentially with n. This 1≤t≤n
1
result is used when evaluating the asymptotic expected search as n→∞,
time to show that it is not affected by τ 1 . We can show this (170)
by noting that for all D and for any policy Γ.
Finally, similar to Lemma 20, we can show that any policy
(cid:18) (cid:19)
Γ that satisfies R (Γ)=O(−clogc) for all D must satisfy:
P (τ >n)≤P max sup (S (t)−S (t))≥0 D
D 1 D j m
j∈/D,m∈D t≥n clog(c)
R (Γ)≥−(1+o(1)) . (171)
∞ D I
(cid:88) (cid:88) D
≤ P (S (t)≥S (t)) .
D j m for all D.
j∈/D,m∈D t=n
(162)
20
REFERENCES [27] M.Egan,J.-M.Gorce,andL.Cardoso,“Fastinitializationofcognitive
radio systems,” in IEEE International Workshop on Signal Processing
[1] Q. Zhao and B. M. Sadler, “A survey of dynamic spectrum access,” AdvancesinWirelessCommunications,2017.
IEEEsignalprocessingmagazine,vol.24,no.3,pp.79–89,2007. [28] A.Tajer,V.V.Veeravalli,andH.V.Poor,“Outlyingsequencedetection
[2] H.Chernoff,“Sequentialdesignofexperiments,”TheAnnalsofMath- in large data sets: A data-driven approach,” IEEE Signal Processing
ematicalStatistics,vol.30,no.3,pp.755–770,1959. Magazine,vol.31,no.5,pp.44–56,2014.
[3] A.Wald,“Sequentialanalysis.1947,”Zbl0029,vol.15805,1947. [29] V.Chandola,A.Banerjee,andV.Kumar,“Anomalydetection:Asurvey,”
[4] S. A. Bessler, “Theory and applications of the sequential design of ACMcomputingsurveys(CSUR),vol.41,no.3,p.15,2009.
experiments,k-actionsandinfinitelymanyexperiments.parti.theory,” [30] M.H.Bhuyan,D.K.Bhattacharyya,andJ.K.Kalita,“Networkanomaly
tech.rep.,DTICDocument,1960. detection:methods,systemsandtools,”IEEECommunicationsSurveys
[5] S. Nitinawarat, G. K. Atia, and V. V. Veeravalli, “Controlled sensing &Tutorials,vol.16,no.1,pp.303–336,2014.
for hypothesis testing,” in 2012 IEEE International Conference on [31] B. Huang, K. Cohen, and Q. Zhao, “Sequential active detec-
Acoustics, Speech and Signal Processing (ICASSP), pp. 5277–5280, tion of anomalies in heterogeneous processes,” arXiv preprint
IEEE,2012. arXiv:1704.00766,2017.
[6] S. Nitinawarat, G. K. Atia, and V. V. Veeravalli, “Controlled sensing
for multihypothesis testing,” IEEE Transactions on Automatic Control,
vol.58,no.10,pp.2451–2464,2013.
[7] S.NitinawaratandV.V.Veeravalli,“Controlledsensingforsequential
multihypothesistestingwithcontrolledmarkovianobservationsandnon-
uniform control cost,” Sequential Analysis, vol. 34, no. 1, pp. 1–24,
2015.
[8] M.NaghshvarandT.Javidi,“Activesequentialhypothesistesting,”The
AnnalsofStatistics,vol.41,no.6,pp.2703–2738,2013.
[9] M. Naghshvar and T. Javidi, “Sequentiality and adaptivity gains in
active hypothesis testing,” IEEE Journal of Selected Topics in Signal
Processing,vol.7,no.5,pp.768–782,2013.
[10] D. A. Castanon, “Optimal search strategies in dynamic hypothesis
testing,” IEEE transactions on systems, man, and cybernetics, vol. 25,
no.7,pp.1130–1138,1995.
[11] K.Cohen,Q.Zhao,andA.Swami,“Optimalindexpoliciesforanomaly
localizationinresource-constrainedcybersystems,”IEEETransactions
onSignalProcessing,vol.62,no.16,pp.4224–4236,2014.
[12] N. K. Vaidhiyan and R. Sundaresan, “Learning to detect an oddball
target,”arXivpreprintarXiv:1508.05572,2015.
[13] K. Leahy and M. Schwager, “Always choose second best: Tracking
a moving target on a graph with a noisy binary sensor,” in Control
Conference(ECC),2016European,pp.1715–1721,IEEE,2016.
[14] J.Heydari,A.Tajer,andH.V.Poor,“Quickestlinearsearchovercor-
relatedsequences,”IEEETransactionsonInformationTheory,vol.62,
no.10,pp.5786–5808,2016.
[15] K. Cohen and Q. Zhao, “Active hypothesis testing for anomaly de-
tection,” IEEE Transactions on Information Theory, vol. 61, no. 3,
pp.1432–1450,2015.
[16] K. S. Zigangirov, “On a problem in optimal scanning,” Theory of
Probability&ItsApplications,vol.11,no.2,pp.294–298,1966.
[17] A.TajerandH.V.Poor,“Quicksearchforrareevents,”IEEETransac-
tionsonInformationTheory,vol.59,no.7,pp.4462–4481,2013.
[18] K.CohenandQ.Zhao,“Asymptoticallyoptimalanomalydetectionvia
sequential testing,” IEEE Transactions on Signal Processing, vol. 63,
no.11,pp.2929–2941,2015.
[19] G. Fellouris, G. V. Moustakides, and V. V. Veeravalli, “Multistream
quickestchangedetection:Asymptoticoptimalityunderasparsesignal,”
in Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE
InternationalConferenceon,pp.6444–6447,IEEE,2017.
[20] L. Lai, H. V. Poor, Y. Xin, and G. Georgiadis, “Quickest search over
multiplesequences,”IEEETransactionsonInformationTheory,vol.57,
no.8,pp.5375–5386,2011.
[21] S. Nitinawarat and V. V. Veeravalli, “Universal scheme for optimal
search and stop,” in Information Theory and Applications Workshop
(ITA),2015,pp.322–328,IEEE,2015.
[22] B. Hemo, K. Cohen, and Q. Zhao, “Asymptotically optimal search of
unknown anomalies,” in Proc. of the 16th IEEE Symposium on Signal
Processing and Information Technology (ISSPIT), (Limassol, Cyprus),
Dec.2016.
[23] M.L.Malloy,G.Tang,andR.D.Nowak,“Quickestsearchforarare
distribution,” in Information Sciences and Systems (CISS), 2012 46th
AnnualConferenceon,pp.1–6,IEEE,2012.
[24] Y.Pei,Y.-C.Liang,K.C.Teh,andK.H.Li,“Energy-efficientdesignof
sequentialchannelsensingincognitiveradionetworks:optimalsensing
strategy,powerallocation,andsensingorder,”IEEEJournalonSelected
AreasinCommunications,vol.29,no.8,pp.1648–1659,2011.
[25] R. Caromi, Y. Xin, and L. Lai, “Fast multiband spectrum scanning
for cognitive radio systems,” IEEE Transactions on Communications,
vol.61,no.1,pp.63–75,2013.
[26] L. Ferrari, Q. Zhao, and A. Scaglione, “Utility maximizing sequential
sensingoverafinitehorizon,”IEEETransactionsonSignalProcessing,
vol.65,no.13,pp.3430–3445,2017.