Marginal and Joint Cross-Entropies & Predictives for Online
Bayesian Inference, Active Learning, and Active Sampling
Andreas Kirsch andreas.kirsch@cs.ox.ac.uk
Jannik Kossen jannik.kossen@cs.ox.ac.uk
Yarin Gal yarin.gal@cs.ox.ac.uk
OATML, Department of Computer Science
University of Oxford
Abstract
PrincipledBayesiandeeplearning(BDL)doesnotliveuptoitspotentialwhenweonlyfocus
onmarginalpredictivedistributions(marginalpredictives). Recentworkshavehighlightedthe
importance of joint predictives for (Bayesian) sequential decision making from a theoretical
and synthetic perspective. We provide additional practical arguments grounded in real-
world applications for focusing on joint predictives: we discuss online Bayesian inference,
which would allow us to make predictions while taking into account additional data without
retraining, and we propose new challenging evaluation settings using active learning and
active sampling. These settings are motivated by an examination of marginal and joint
predictives, their respective cross-entropies, and their place in offline and online learning.
Theyaremorerealisticthanpreviouslysuggestedones,buildingonworkbyWenetal.(2021)
and Osband et al. (2022), and focus on evaluating the performance of approximate BNNs in
an online supervised setting. Initial experiments, however, raise questions on the feasibility
of these ideas in high-dimensional parameter spaces with current BDL inference techniques,
and we suggest experiments that might help shed further light on the practicality of current
research for these problems. Importantly, our work highlights previously unidentified gaps in
current research and the need for better approximate joint predictives.
1 Introduction
Deep learning has seen tremendous success in recent years. Beyond deep ensembles (Lakshminarayanan et al.,
2016), however, more principled methods of deep learning that attempt to be (approximately) Bayesian,
commonly referred to as (approximate) Bayesian Neural Networks (BNN), have arguably not lived up to
their full potential (Ovadia et al., 2019; Beluch et al., 2018). This might be because the focus in their
evaluation has been on marginal predictions q(y|x), where they can only provide marginal:D improvements
over unprincipled regular NNs. Yet the strength of a Bayesian approach for deep learning might not solely lie
in marginal predictions but in allowing for online learning via online Bayesian inference.
With ‘online Bayesian inference’, we refer to incorporating additional data into the posterior predictive
without retraining in the common sense, i.e. by computing gradients and optimizing the model parameters
further. This could offer important performance benefits for applications that would otherwise require
repeated retraining like active learning and could have important implications for how we could use large
supervised models in production: currently, they are seen as strictly static, however online Bayesian inference
would allow them to dynamically adapt to new data on the fly.
Generally, the difference between an approximate BNN and a regular NN is that the former assumes a
distribution q(ω) over the model parameters ω, where q(ω) approximates the Bayesian posterior p(ω|D ),
train
which is the optimal distribution given prior information p(ω) and training data D : q(ω)≈p(ω|D ).
train train
Online Bayesian Inference. Toincorporatenewdata{y ,x }∼pˆ (y,x)n,viaonlineBayesianinference,
i i data
we simply apply Bayes’ theorem: for a test point x, the predictive q(y|x,y ,x ,...,y ,x ) is proportional
n n 1 1
1
2202
yaM
81
]GL.sc[
1v66780.5022:viXra
to its joint predictive. We obtain:
q(y,y ,...,y |x,x ,...,x )
q(y|x,y ,x ,...,y ,x )= n 1 n 1 (1)
n n 1 1 q(y ...,y |x ,...,x )
n 1 n 1
∝q(y,y ,...,y |x,x ,...,x ). (2)
n 1 n 1
We can thus use a joint predictive q(y,y ,...,y |x,x ,...,x ) to incorporate fixed {y ,x }n and make
1 n 1 n i i
predictions for x without explicit retraining.
Hence, for online Bayesian inference, we require joint predictives, which only Bayesian methods can give
us:1 through BNNs in the parametric case or through Gaussian processes in the non-parameteric case, for
example. This strongly contrasts with marginal predictives which can also be modelled by regular NNs.
Can we perform online Bayesian inference well for high-dimensional inputs and parameters using current
approximate BNNs? The quality of the resulting predictions crucially depends on the joint predictives.
However, computing joint predictives can be challenging. For example, in Bayesian literature, the joint
predictive of all samples in the training set marginalized over the prior distribution is just the well-known
marginal likelihood, which can be used for model selection (MacKay, 2003; Lyle et al., 2020; Llorente et al.,
2020), and is known to be difficult to estimate in high-dimensional parameter spaces (Lotfi et al., 2022).
Similar challenges can be expected for performing online Bayesian inference using approximate posterior
distributions.
Related Work. Osband et al. (2022); Wen et al. (2021); Osband et al. (2021b;a) mention the importance of
joint predictives in the context of combinatorial decision problems, sequential predictions and multi-armed
banditsinlowdimensions. Comparedtothesepreviousworks,weexploreimportantconnectionstosupervised
learning, e.g. in active learning (Atlas et al., 1989; Settles, 2009) and Bayesian optimal experimental design
(Lindley, 1956; Foster, 2022), and focus on online Bayesian inference in high dimensions. In addition, we
clarify that both marginal and joint cross-entropies have their use, and it is not the case that one is always
preferable over the other. Simply put, we will argue that they capture different quantities that are separately
useful in offline and online learning, and we will combine them to evaluate the performance of online Bayesian
inference using approximate BNNs. We provide more details in §5.
Marginal Cross-Entropy. As will become evident, the marginal cross-entropy for a fixed predictive model
captures the expected performance (under log loss) when the model does not adapt to data at test time.
For supervised tasks, the marginal cross-entropy is what is commonly referred to as cross-entropy loss and
represents common practice: we obtain a fixed set of parameters by training the model on a training set, and
we re-use these parameters at test time without any further updates. The performance of the model does
not change as it observes more test data, and there is no feedback loop of any sort. In this offline learning
setting, the marginal cross-entropy is the right choice to estimate performance.
Joint Cross-Entropy. On the other hand, the joint cross-entropy for a predictive model captures the
performance in a sequential learning setting, where sequential model updates take place. Here, the parameter
distribution q(ω) serves as a prior for online Bayesian inference. This fits the context used in Wen et al.
(2021) in which the model makes a prediction for the next step, observes the outcome, and then updates the
model (agent).
Applications & Experiments. We will also see that the joint predictive is important for data selection in
active learning and active sampling as we detail in §2.3. We further connect several recent works (Wen et al.,
2021; Osband et al., 2022) to active learning and active sampling and present new realistic and challenging
experimental settings. Most importantly, we examine online Bayesian inference within these contexts as it
allows us to avoid retraining across acquisitions.
Outline. In §2.1, we introduce the model setting; in §2.2, we investigate marginal and joint cross-entropies;
in §2.3, we show the connections to active learning and active sampling; in §2.4, we look at online Bayesian
inference; in §3, we present evaluation settings and applications, including for active learning and online
1Forconsistentjointpredictives,adheringtothechainruleofprobability,amodelneedstoadheretothe“Bayesianupdate
rule”,i.e.Bayes’theorem,andthusisBayesian. Seealsoeq.(17)later.
2
Bayesian inference; in §4, we suggest experiments and detail initial results on MNIST using MC dropout; and
finally discuss related work in §5.
2 Background
In this section, we introduce our setting, revisit marginal and joint cross-entropies, examine the connection to
active learning and active sampling, and explain online Bayesian inference in detail.
2.1 Setting
In the following, we will assume an underlying parametric predictive model p(y|x,ω) for input samples x
with targets or labels y with a prior parameter distribution p(ω) over ω, i.e. our model is Bayesian. The
marginalpredictivep(y|x)isthenobtainedbymarginalizingoverp(ω): p(y|x)=E [p(y|x,ω)].Similarly,
p(ω)
the joint predictive is obtained by marginalizing the joint p(y ,...,y |x ,...,x ,ω) over p(ω): p(y ,...,y |
1 n 1 n 1 n
x ,...,x )=E [p(y ,...,y |x ,...,x ,ω)]=E [Q p(y |x ,ω)].
1 n p(ω) 1 n 1 n p(ω) i i i
We are interested in using the posterior parameter distribution p(ω |D ) to obtain marginal or joint
train
predictives p(·|D ) given training data D , where we use Bayes’ theorem to obtain the posterior given
train train
a prior distribution p(ω).
In general, capturing the true posterior distribution p(ω|D ) is infeasible, however. We assume we have
train
an approximate distribution q(ω)≈p(ω|D ) that we use instead of the true posterior.
train
For example, q(ω) could be based on a deep ensemble (Lakshminarayanan et al., 2016) as a mixture of Dirac
delta distributions positioned at the parameters of individually trained ensemble members, or it could be an
MC dropout model that is trained using variational inference (Gal and Ghahramani, 2015). As above, we
use q(y|x) to denote the predictions after marginalizing over q(ω): q(y|x)=E [p(y|x,ω)]. Note that
q(ω)
the underlying discriminative model p(y|x,ω) stays the same—we only exchange the distribution over its
parameters ω.
Notation. We use notation from Kirsch and Gal (2021) for information-theoretic quantities. In particular,
this means that we take expectations over random variables (capital letters) and condition on outcomes
(lower-case letters): e.g. for entropies H[A,b|C,d]=E [−logp(a,b|c,d)]. As examples:
p(a,c|b,d)
H[x]:=−logp(x), (3)
H[X]=E [H[x]], (4)
p(x)
H[x|y]=−logp(x|y), (5)
H[x|Y]=E [H[x|y]], (6)
p(y|x)
H[X|Y]=E [H[x|y]], (7)
p(x,y)
H[X,y]=E [H[x,y]], (8)
p(x|y)
and so on. We define the cross-entropy as (only one example shown):
H [Y |x]:=E [−logq(y|x)]. (9)
pkq p(y|x)
Similarly, the mutual information is consistently defined via:
I[x;y]:=H[x]−H[x|y], (10)
I[X;y]=H[X]−H[X|y], (11)
I[x;Y]=H[x]−H[x|Y], (12)
I[X;Y]=H[X]−H[X|Y]. (13)
This matches the well-known standard definitions and extends the notation to mixing random variables and
outcomes. The mixed mutual information terms capture the notion of information gain and information-
theoretic surprise, respectively—see Kirsch and Gal (2021) for more details. We will add the distribution as a
subscript to disambiguate the notation: e.g. H [X].
q
3
2.2 Marginal and Joint Cross-Entropy
We start by comparing marginal and joint predictive cross-entropies and revisiting how they are useful for
offline and online learning separately.
Marginal Cross-Entropy. Given an underlying, possibly empirical, data distribution pˆ (x,y), the
data
marginal cross-entropy is:
H [Y |X]=E [−logE [p(y|x,ω)]]
pˆ kq pˆ (x,y) q(ω)
train data
=E [−logq(y|x)], (14)
pˆ (x,y)
data
where we use H [Y |X] to denote the cross-entropy. This cross-entropy is the population loss when q(ω)
pˆ kq
is not updated atrfatienr seeing new samples. Each sample x,y is treated independently. Hence, the marginal
cross-entropy captures the expected performance in an offline learning setting.
Joint Cross-Entropy. On the other hand, given an initial parameter distribution q(ω) above, the joint
cross-entropy measures how well the parameter distribution can adapt to new data D.
To show how this connects to joint cross-entropies, we can look at the joint cross-entropy of specific samples
D ={y ,x }n (without taking an expectation). The joint cross-entropy for these specific samples is just the
i i i=1
sum of (negative) log marginal likelihoods using the chain rule, where each y is conditioned on all ‘previous’
i
observations D :
<i
H [y ,...,y |x ,...,x ]=−logq(y ,...,y |x ,...,x ) (15)
pˆ kq 1 n 1 n 1 n 1 n
train
Y
=−log q(y |x ,y ,x ,...,y ,x ) (16)
i i i−1 i−1 1 1
i
X
=− logq(y |x ,D ) (17)
i i <i
i
X
= H [y |x ,D ], (18)
pˆ kq i i <i
train
i
where D denotes “y ,x ,...,y ,x ”, and the marginal predictive is: q(y |x ,D )=E [p(y |
<i i−1 i−1 1 1 i i <i q(ω|D<i) i
x ,ω)].Semantically,wecomputethefollowingineachiterationofthesum: weupdate theparameterposterior
i
q(ω|D ),computethelossesforourpredictionsatoutcomesy forx ,andthenincludey ,x inourobserved
<i i i i i
data. We will denote this as the online learning setting.
When we are interested in the expected loss given arbitrary data, we can compute the following joint
cross-entropy:
OLL(n):=E [−logq(y ,...,y |x ,...x )] (19)
(xi,yi)i∼pˆ
data
(xi,yi)n 1 n 1 n
=E [H [y ,...,y |x ,...x ]] (20)
(xi,yi)i∼pˆ
data
(xi,yi)n pˆ
train
kq 1 n 1 n
=H [Y ,...,Y |X ,...X ], (21)
pˆ kq 1 n 1 n
train
where OLL stands for “online learning loss”.
Connection to the Conditional Cross-Entropy Rate. As an aside, if we let n → ∞, we also have
OLL(n) → ∞. This is not helpful, so instead we can look at the average: 1OLL(n). In the limit, this
n
average is just the cross-entropy rate:
1
H [Y |X]:= lim H [Y ,...,Y |X ,...X ], (22)
pˆ train kq n→∞n pˆ train kq 1 n 1 n
which we define analogously to the entropy rate in Cover and Thomas (1991).2
Summary. The marginal cross-entropy is useful for offline learning as it predicts the performance of a
fixed model on the data distribution. The joint cross-entropy is useful for online learning as it predicts the
performance of a model as it adapts to additional data. Hence, it is important for sequential decision making
where we often cannot afford to retrain the model dynamically.
2Cf.theentropyrate,whichis: H[X]=limn→∞
n
1 H[X1,...Xn].
4
2.3 Connection to Active Learning and Active Sampling
Marginal and joint cross-entropies and the related predictives play an important role in active learning and
active sampling as we will examine here. In active learning (Atlas et al., 1989; Settles, 2009), we do not know
the outcome (label) for every sample and have a budget for the additional labels we can acquire for training
from an unlabeled pool set. In active sampling (also data subset selection or coreset selection) (Campbell
and Broderick, 2017; Mirzasoleiman et al., 2019; Borsos et al., 2020), on the other hand, we have access to
the labels, but we assume that we have a budget for the samples we can use to train the model on, and our
goal is to pick the best subset of labeled samples.
In each case, we have a budget of n additional samples we can condition on and we are usually interested
in maximizing the performance of the model as end-goal, which is equivalent to finding x ,...,x which
1 n
minimize the following cross-entropy (assuming the usual cross-entropy loss):
H [Y |X,y ,x ,...,y ,x ] or H [Y |X,Y ,x ,...,Y ,x ], (23)
pˆ kq n n 1 1 pˆ kq n n 1 1
train train
depending on whether we have access to the labels (active sampling) or not (active learning).
These objectives depend on a joint predictive via eq. (1) but in expectation over X and Y sampled from the
data distribution. As such it is a hybrid between the joint and marginal cross-entropy.
Transductive Active Sampling. If we have access to the labels y ,...,y , this leads to active sampling
1 n
approaches, where we want to find the best samples to train on to increase model performance. Using
I [Y;y ,...,y |X,x ,...,x ]=H [Y |X]−H [Y |X,y ,x ,...,y ,x ], (24)
pˆ kq n 1 n 1 pˆ kq pˆ kq n n 1 1
train train train
where I [·] is a “cross mutual information”, minimizing above cross-conditional entropy is equivalent
pˆ kq
to maxim tr i a z in ing the cross-mutual information:
argminH [Y |X,y ,x ,...,y ,x ]=argmaxI [Y;y ,...,x |X,x ,...,x ], (25)
pˆ kq n n 1 1 pˆ kq n 1 n 1
train train
{xi}1..n {xi}1..n
because H [Y |X] is constant (independent of the x ).
pˆ kq i
train
Approximations of this term lead to the reducible hold-out loss in Mindermann et al. (2021). Note that this is
a transductive approach (Yu et al., 2006) because we examine performance in regards to samples x,y from
the data distribution.
Transductive Active Learning. If we do not have access to the labels, the Bayesian-optimal approach
is to take the expectation using labels drawn from the joint predictive of the model (Lindley, 1956). This
leads to minimizing conditional entropies or, equivalently, maximizing mutual information terms (MacKay,
1992; McCallum and Nigam, 1998; Yu et al., 2006; Wang et al., 2020; Kirsch et al., 2021). Assuming we have
access to no labels beyond the current training set at all, we have the following objectives:
argminH [Y |X,Y ,x ,...,Y ,x ]=argmaxI [Y;Y ,...,Y |X,x ,...,x ]. (26)
q n n 1 1 q n 1 n 1
{xi}1..n {xi}1..n
The latter is exactly the Expected Predictive Information Gain (EPIG) acquisition function from Kirsch et al.
(2021).3
Active Learning. Many active learning approaches, especially the ones that are not transductive, maximize
the expected information gain, also called BALD (Houlsby et al., 2011), as an acquisition function—or,
equivalently, minimize the model uncertainty. The corresponding information-theoretic expression is I [Ω;Y |
q
x], and in the batch case, we have:
argmaxI [Ω;Y ,...,Y |x ,...,x ]=argmaxH [Y ,...,Y |x ,...,x ]−H [Y ,...,Y |x ,...,x ,Ω]
q n 1 n 1 q n 1 n 1 q n 1 n 1
{xi}1..n {xi}1..n
X
=argmaxH [Y ,...,Y |x ,...,x ]− H [Y , |x ,Ω], (27)
q n 1 n 1 q i i
{xi}1..n i
3EPIGisformulatedusinganunlabeledevaluationsetthatspecifiesthetargetdomain. Here,weusethepoolsetitselfasan
evaluationsettosimplifytheexposition.
5
where the last step follows from the fact that predictions factorize conditioned on the model parameters
(Kirsch et al., 2019). The corresponding active sampling acquisition function I [Ω;y ,...,y |x ,...,x ],
q n 1 n 1
which quantifies the information gain, is examined further in Kirsch and Gal (2021).
Summary. All of the above objectives depend on the joint predictive in some way to compute acquisition
scores without explicit retraining. However, it is common to update models after acquiring a set of labels to
take into account the newly acquired data.
2.4 Online Bayesian Inference
To see what we mean by incorporating new data, assume we have sampled n additional points x ,y ∼
i i
pˆ (x ,y ). Traditionally, wewouldnowupdatetheposteriorapproximationq(w)totakethisnewdatainto
data i i
account for our predictions at future test points. However, this can be prohibitively expensive—especially in
applications that require frequent retraining. Instead, online Bayesian inference allows Bayesian models to
adapt their predictions without explicitly updating the posterior approximation.
Following eq. (1), for a test point x, the predictive q(y |x,y ,x ,...,y ,x ) is proportional to the joint
n n 1 1
predictive:
q(y,y ,...,y |x,x ,...,x )
q(y|x,y ,x ,...,y ,x )= n 1 n 1 (28)
n n 1 1 q(y ...,y |x ,...,x )
n 1 n 1
∝q(y,y ,...,y |x,x ,...,x ), (29)
n 1 n 1
since the normalization constant q(y ...,y |x ,...,x ) is independent of y and x. Hence, this allows us to
n 1 n 1
make predictions that take into account new data without explicit retraining by simply computing the joint
predictive of the test point and newly observed data.
We refer to this as online Bayesian inference (OBI). While this inference is precisely Bayesian, q(ω) is
commonly only an approximate posterior, and thus the quality of this inference depends on the properties of
the approximation and how we estimate the joint predictive.
The simplest approach to estimate the joint predictive is via sampling, which applies to e.g. Monte-
Carlo dropout, deep ensembles, and deep ensembles with prior functions (Gal and Ghahramani, 2015;
Lakshminarayanan et al., 2016; Osband et al., 2018), by factorizing the joint:
q(y,y ,...,y |x,x ,...,x )=E [p(y,y ,...,y |x,x ,...,x ,ω)] (30)
n 1 n 1 q(ω) n 1 n 1
" n #
Y
=E p(y|x,ω) p(y |x ,ω) . (31)
q(ω) i i
i=1
Thus, if we draw fixed parameter samples ω ∼q(ω), we can pre-compute Qn p(y |x ,ω ) for each j and
j i=1 i i j
estimate the joint predictive.
Finally, we can view Qn p(y |x ,ω) as unnormalised importance weights:
i=1 i i
n
Y
q(ω) p(y |x ,ω)∝q(ω|y ,x ,...,y ,x ), (32)
i i n n 1 1
i=1
and hence, overall:
" n #
Y
E p(y|x,ω) p(y |x ,ω) ∝E [p(y|x,ω)]. (33)
q(ω) i i q(ω|yn,xn,...,y1,x1)
i=1
To evaluate the performance, we can use these predictions to compute the following marginal cross-entropy
which incorporates the additional samples using OBI:
H [Y |X,y ,x ,...,y ,x ], (34)
pˆ kq n n 1 1
train
where X,Y are sampled from the data distribution. Comparing this entropy with the performance of a fully
retrained model allows us to obtain a practical estimate for the quality of the approximate posterior q(ω).
6
3 New Evaluations & Applications
Wesuggestnewexperimentalsettingsthatallowustoevaluatethequalityofthejointpredictiveandcompare
it to ones suggested in prior work.
3.1 Performance in Active Learning and Active Sampling Methods
Aconceptuallysimplesetofdownstreamtasksistoevaluatetheperformanceofthejointpredictivesindifferent
active learning or active sampling settings using different approximate model architectures (e.g. based on
Epistemic Neural Networks Osband et al. (2021a) as abstraction). Wang et al. (2020) show that performance
for transductive active learning is correlated to the quality of the joint predictive. Similarly, Kirsch et al.
(2019)foundthejointpredictivetoperformmuchbetterinbatchactivelearningthanafactorizeddistribution
relying on marginal predictives.
We suggest, similar to Repeated-MNIST (Kirsch et al., 2019), to duplicate the underlying pool sets or
to simply allow the same sample to be selected multiple times. Importantly, approximate BNNs that do
not provide good joint predictives will greedily select the same sample over and over again or degrade to
uninformed data acquisitions. This avoids an issue pointed out by Wang et al. (2020) and Osband et al.
(2022) as we explain next.
Connection to Total Correlation. Wang et al. (2020) argue that the joint cross-entropy is dominated by
the sum of the individual marginal cross-entropy scores. This is equivalent to saying that the total correlation
between samples is negligible since the difference between the joint cross-entropy and its individual marginal
cross-entropies for specific y ,x is just the total correlation:
i i
X
TC [y ,...,y |x ,...,x ]:= H [y |x ]−H [y ,...,y |x ,...,x ]. (35)
q 1 n 1 n q i i q 1 n 1 n
i
The total correlation measures the amount of information shared between the samples.
Forrandombatches,thetotalcorrelationis,indeed,likelygoingtobenegligiblebecausemostrandombatches
are not very informative overall, and importantly, on curated datasets, they are most likely uncorrelated as it
is unlikely that observing y , x in the batch informs prediction for y , x for most j 6=i as curated datasets
i i j j
are usually as diverse as possible.
Only with increasing redundancy in the dataset, e.g. by duplicating samples like in Repeated-MNIST (Kirsch
et al., 2019), random batches will become more correlated on average and the total correlation larger.
This setup is similar to the dyadic sampling proposed by Osband et al. (2022) which repeatedly samples
yj for x , with i∈{1,2} and evaluates the joint predictive. However, this setting in essence only measures
i i
the ability of the approximate model to perform Bayesian updates on two fixed training samples at a time.
Hence, we suggest that a better adaption to evaluate joint predictives using active learning is to duplicate
the dataset or to simply allow the same sample to be selected multiple times.
3.2 Performance of Online Bayesian Inference
As a practical “ground truth”, we can compare the performance of retrained models after acquiring additional
samples with the performance of OBI as explained in section 2.4.
A particular challenging scenario for OBI is to use acquisition sequences x ,y ,...x ,y that were collected
1 1 T T
using active learning or active sampling on the dataset. We can evaluate OBI on models trained at different
D = {y ,x }t and increasing subsets of online data {y ,x }T from these acquisition sequences.
train,t i i i=1 i i i=t+1
This scenario is particularly challenging for OBI because the sequence of acquisition is selected to result in
large changes in the predictives and posterior distributions.
TheaverageperformancedifferencebetweenOBIandfullyretrainedmodelsacrossdifferenttrainingacquisition
sequences will tell us how good a given joint predictive is for “meaningful” online learning. The expectation
is that for most approximate BNNs, OBI will quickly suffer from degraded performance compared to the
retrained models.
7
Table 1: Comparison between online Bayesian inference and retraining for 5 additional samples on MNIST.
OBI performs worse than the baseline (ie. not taking into account new samples at all) when using an active
acquisition sequence. On the random acquisition sequence, it only performs just as well as not updating at
all. (Mean for 5 model trials and 5 OBI trials for each.)
Baseline vs OBI: +5 samples Retraining: +5 samples
Avg ∆ Cross-Entropy (↓) ∆ Accuracy (↑) ∆ Cross-Entropy (↓) ∆ Accuracy (↑)
Active Acquisition Sequence 0.16 -5.7% -0.062 2.0%
Random Acquisition Sequence 0.00 0.0% -0.075 1.8%
That is, we compare the performance of OBI as we acquire new samples x ,y to an approximate BNN
i i
retrained with the same additional data for increasing n:
H [Y |X,y ,x ,...,y ,x ]−H [Y |X], (36)
pˆ kq n n 1 1 pˆ kq0
train train
where q0(ω)≈p(ω|y ,x ,...,y ,x ,D ) is the parameter distribution of an (approximate) BNN after
n n 1 1 train
retraining with the additional y ,x ,...,y ,x .
n n 1 1
Ideally, we would compare to the predictions from the correct updated posterior distribution; however, this is
infeasible in most practical scenarios. Instead, when we use an approximate BNN q0(ω) that is similar to the
one used for q(ω), we can measure the practical degradation between OBI and retraining. Here, the ideal
would be for OBI to behave exactly like a fully retrained model—even if the latter does not match exact
Bayesian inference—as such an approach would be self-consistent. Note that with exact Bayesian inference,
we would have q(ω)=q0(ω) and the above would be zero.
Comparison to Dyadic Sampling. Unlike Osband et al. (2022) which focuses on selecting labels for
dyadic samples repeatedly, this experiment setting is both more practical and more insightful: active learning
picks samples that are the most informative and are supposed to update the posterior the most. This is
because, for informative samples, we would expect the changes in model predictions to be the largest. Hence,
one could expect that these samples pose the most significant challenge to approximate BNNs and their joint
predictives. Ideally, we would hope that OBI would keep up with retrained model, but this might prove to be
challenging in high-dimensional scenarios.
Sample Selection Bias. The suggested evaluation is orthogonal to any sample selection bias that is added
through the data acquisition process itself as we use the same training data at each step for both OBI and
retraining in eq. (36). Specifically, Farquhar et al. (2021) observed that active learning introduces a bias by
sampling from the data distribution using an acquisition function and not uniformly.
3.3 Application: Active Learning with Online Bayesian Inference
In many settings, retraining models for when only few new samples are added is prohibitively expensive. This
motivates batch active learning, where batches are acquired instead of individual samples. Expanding on this,
when equipped with models that perform well under OBI, one could avoid retraining models when acquiring
new data by using OBI. Only when OBI degrades, fully retraining will become necessary.
We can evaluate this both for individual acquisition as well as for batch acquisition.
4 Experiments
Following the newly suggested experimental settings, we want to run comparisons using different kinds
of approximate BNNs in active learning and active sampling settings. Moreover, given fixed acquisition
sequences, determined using active learning and active sampling, we want to evaluate the difference between
OBI and fully retrained models.
8
0.9
0.8
0.7
0.6
20 30 40 50 60 70 80 90
Training Set Size
ycaruccA
Random Acquisition Sequence Active Acquisition Sequence
20 30 40 50 60 70 80 90
Training Set Size
Baseline OBI: +5 samples Retraining: +5 samples
1.5
1.0
0.5
20 30 40 50 60 70 80 90
Training Set Size
yportnE-ssorC
Random Acquisition Sequence Active Acquisition Sequence
20 30 40 50 60 70 80 90
Training Set Size
Baseline OBI: +5 samples Retraining: +5 samples
Figure 1: Comparison between online Bayesian inference and retraining for 5 additional samples on MNIST.
We compare the dynamics between using a random acquisition sequence and using an active acquisition
sequence, drawn using active sampling. While OBI does not appear to be significantly worse or better on
the random acquisition sequence, it markedly deteriorates when using the active acquisition sequence. OBI
struggles with the informative samples that change the posterior a lot: the active acquisition sequence reaches
90% accuracy with just 70 samples, unlike the random one.
An initial experiment shows that OBI in high dimensions might not be feasible using simple Monte Carlo
approximationsoftheexpectationsinparameterspace. Thisislikelybecauseofthemuchhigherdimensionality
of the problems we consider—especially in comparison to Osband et al. (2021b).
Specifically, we use an acquisition sequence created using active sampling which achieves 90% accuracy on
MNIST with just 70 samples and use the same model and training setup as Kirsch and Gal (2021). After
every new data point (starting from 20), we evaluate a retrained model using OBI with 5 additional data
points and compare it to a fully retrained model with the same 5 additional data points as well as to a model
that is not retrained at all. We train the models (5 trials) for each training set size and (bootstrap) sample
10000 MC dropout samples for OBI 5 times out of 20000 MC dropout samples using consistent MC dropout
for each model (Kirsch et al., 2019; Gal and Ghahramani, 2015) (5 sub-trials) to reduce the variance.
Ideally, whenusingOBI,weshouldrecoverthesameperformanceasifwefullyretrainedthemodelsusingthe
additional data. However, as is visible in Figure 1, this is not the case when using the challenging acquisition
sequence from active sampling.
Table1showstheaverageperformancedifferencewhenusingOBIwithadditionalsamplesfromtheacquisition
sequence and when fully retraining. In all cases, OBI performs worse than retraining. On the random
acquisition sequence, it performs only as well as not updating at all, while on the active acquisition sequence,
it always performs worse.
9
5 Related Work
The most relevant works and indeed one of the inspirations for this work are Wen et al. (2021) and Osband
et al. (2022), which are recommended reading. We see our work as a contribution that provides a different
and differentiated position on the benefits of marginal versus joint predictives and respective cross-entropies
as performance metrics and that puts greater focus on OBI.
Moreover, our suggested experimental settings expand on these prior works and draw attention to active
learning and active sampling as more realistic use-cases. Measuring the error between OBI and retrained
models expands on the experiments in Wen et al. (2021) while evaluating performance in active learning and
active sampling on highly redundant datasets (allowing to reselect previously selected points) expands on the
idea of dyadic sampling from Osband et al. (2022).
Lastly, Wenetal.(2021)focusontheKLdivergencebetweentheexactBayesianjointpredictiveandthejoint
predictive of an approximate Bayesian model for different numbers of samples in the joint. Our suggested
experimental settings focus on evaluating downstream tasks.
Wang et al. (2020) also examine the quality of joint predictives on low-dimensional datasets using a more
synthetic evaluation method, the cross-normalized log-likelihood. They also evaluate the quality of joint
predictives in regression settings using active learning experiments. Our evaluation settings from §3 extend
these.
6 Conclusion
We have revisited the difference between marginal and joint cross-entropies and predictives, clarifying in
which contexts either is appropriate: for offline learning, the marginal cross-entropy is the right choice to
evaluate performance while for online learning, it is the joint cross-entropy. We have also shown how the
joint predictive plays an important role in information-theoretic acquisition functions in active learning and
active sampling.
Importantly, we argue that online Bayesian inference could provide many benefits and have proposed new
more practical and challenging experimental settings which expand on prior art by using active learning and
active sampling.
Given the results of the presented experiment, it is an open question how much better other sampling-based
approaches can be when using high-dimensional parameter spaces. Especially deep ensembles which usually
provide a much smaller “sample count” (i.e. number of ensemble members) might not perform well under
online Bayesian inference because the hypothesis space will be exhausted faster—even when the ensemble
members are diverse.
In a future revision, we will offer further experimental evaluation following §3, e.g. improving the quality
of online Bayesian inference by studying higher quality posterior distributions such as those from HMC or
efficient low-dimensional posterior approximations that might make parameter-space integrals tractable, and
we will investigate if prior research into failures of Bayesian model averaging under model misspecification
might provide further insights and paths to improvements (Minka, 2002).
Acknowledgements
The authors would like to thank the members of OATML in general for their feedback at various stages of
the project. AK is supported by the UK EPSRC CDT in Autonomous Intelligent Machines and Systems
(grant reference EP/L015897/1). JK is supported by New College Yeotown Scholarship.
References
Les Atlas, David Cohn, and Richard Ladner. Training connectionist networks with queries and selective
sampling. Advances in neural information processing systems, 2, 1989.
10
William H Beluch, Tim Genewein, Andreas Nürnberger, and Jan M Köhler. The power of ensembles for
active learning in image classification. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 9368–9377, 2018.
Zalán Borsos, Mojmír Mutný, and Andreas Krause. Coresets via bilevel optimization for continual learning
and streaming, 2020.
Trevor Campbell and Tamara Broderick. Automated scalable bayesian inference via hilbert coresets, 2017.
Thomas M Cover and Joy A Thomas. Information theory and statistics. Elements of information theory, 1
(1):279–335, 1991.
Sebastian Farquhar, Yarin Gal, and Tom Rainforth. On statistical bias in active learning: How and when to
fix it. International Conference on Learning Representations, 2021.
AE Foster. Variational, Monte Carlo and policy-based approaches to Bayesian experimental design. PhD
thesis, University of Oxford, 2022.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty
in deep learning, 2015.
NeilHoulsby,FerencHuszár,ZoubinGhahramani,andMátéLengyel. Bayesianactivelearningforclassification
and preference learning. arXiv preprint arXiv:1112.5745, 2011.
Andreas Kirsch and Yarin Gal. A practical & unified notation for information-theoretic quantities in ml.
arXiv preprint arXiv:2106.12062, 2021.
Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch acquisition for
deep bayesian active learning, 2019.
AndreasKirsch,TomRainforth,andYarinGal. Testdistribution-awareactivelearning: Aprincipledapproach
against distribution shift and outliers, 2021.
BalajiLakshminarayanan,AlexanderPritzel,andCharlesBlundell. Simpleandscalablepredictiveuncertainty
estimation using deep ensembles. arXiv preprint arXiv:1612.01474, 2016.
Dennis V Lindley. On a measure of the information provided by an experiment. The Annals of Mathematical
Statistics, pages 986–1005, 1956.
FernandoLlorente,LucaMartino,DavidDelgado,andJavierLopez-Santiago.Marginallikelihoodcomputation
for model selection and hypothesis testing: an extensive review, 2020.
Sanae Lotfi, Pavel Izmailov, Gregory Benton, Micah Goldblum, and Andrew Gordon Wilson. Bayesian model
selection, the marginal likelihood, and generalization, 2022.
Clare Lyle, Lisa Schut, Binxin Ru, Yarin Gal, and Mark van der Wilk. A bayesian perspective on training
speed and model selection, 2020.
David JC MacKay. Information-based objective functions for active data selection. Neural computation, 4(4):
590–604, 1992.
David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003.
Andrew McCallum and Kamal Nigam. Employing em and pool-based active learning for text classification.
In ICML, 1998.
Sören Mindermann, Muhammed Razzak, Winnie Xu, Andreas Kirsch, Mrinank Sharma, Adrien Morisot,
Aidan N. Gomez, Sebastian Farquhar, Jan Brauner, and Yarin Gal. Prioritized training on points that are
learnable, worth learning, and not yet learned, 2021.
Thomas P. Minka. Bayesian model averaging is not model combination. 2002.
11
Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. Coresets for data-efficient training of machine
learning models, 2019.
Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement learning,
2018.
Ian Osband, Zheng Wen, Mohammad Asghari, Morteza Ibrahimi, Xiyuan Lu, and Benjamin Van Roy.
Epistemic neural networks, 2021a.
Ian Osband, Zheng Wen, Seyed Mohammad Asghari, Vikranth Dwaracherla, Botao Hao, Morteza Ibrahimi,
Dieterich Lawson, Xiuyuan Lu, Brendan O’Donoghue, and Benjamin Van Roy. The neural testbed:
Evaluating predictive distributions, 2021b.
Ian Osband, Zheng Wen, Seyed Mohammad Asghari, Vikranth Dwaracherla, Xiuyuan Lu, and Benjamin Van
Roy. Evaluating high-order predictive distributions in deep learning, 2022.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua V. Dillon, Balaji
Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating predictive
uncertainty under dataset shift, 2019.
Burr Settles. Active learning literature survey. 2009.
ChaoqiWang,ShengyangSun,andRogerGrosse. Beyondmarginaluncertainty: Howaccuratelycanbayesian
regression models estimate posterior predictive correlations?, 2020.
Zheng Wen, Ian Osband, Chao Qin, Xiuyuan Lu, Morteza Ibrahimi, Vikranth Dwaracherla, Mohammad
Asghari, and Benjamin Van Roy. From predictions to decisions: The importance of joint predictive
distributions, 2021.
Kai Yu, Jinbo Bi, and Volker Tresp. Active learning via transductive experimental design. In Proceedings of
the 23rd international conference on Machine learning, pages 1081–1088, 2006.
12