Graphical Abstract
Bridging Probabilistic Inference and Behavior Trees: An Interactive Framework for Adap-
tive Multi-Robot Cooperation
Chaoran Wang, Jingyuan Sun, Yanhui Zhang, Changju Wu
Root
TaskCondition
Root
TaskCondition
Minimize Free Energy
Planning
Interactive Inference
Robot1
Root
Condition Task
Continuous-Time
Environment
Observations
communication
communication
communication
arXiv:2512.04404v1  [cs.RO]  4 Dec 2025
Highlights
Bridging Probabilistic Inference and Behavior Trees: An Interactive Framework for Adap-
tive Multi-Robot Cooperation
Chaoran Wang, Jingyuan Sun, Yanhui Zhang, Changju Wu
•A novel Interactive Inference Behavior Tree (IIBT) framework is proposed.
•Integrates Active Inference with Behavior Trees for multi-robot decision-making.
•Enables distributed and adaptive cooperation under uncertainty and partial observability.
•Introduces joint preference matrices for inter-robot reasoning and policy alignment.
•Validated through simulation and real-world multi-robot tasks, showing robustness gains.
Bridging Probabilistic Inference and Behavior Trees: An
Interactive Framework for Adaptive Multi-Robot Cooperation
Chaoran Wanga,1, Jingyuan Sunb, Yanhui Zhanga, Changju Wua
aSchool of Aeronautic and Astronautics, Zhejiang University, No. 866, Yuhangtang Road, Xihu
District, Hangzhou, 310027, Zhejiang, China
bShanghai Huawei Technologies Co., Ltd, Shanghai, 201799, China
Abstract
This paper proposes an Interactive Inference Behavior Tree (IIBT) framework that integrates be-
havior trees (BTs) with active inference under the free energy principle for distributed multi-robot
decision-making. The proposed IIBT node extends conventional BTs with probabilistic reason-
ing, enabling online joint planning and execution across multiple robots. It remains fully com-
patible with standard BT architectures, allowing seamless integration into existing multi-robot
control systems. Within this framework, multi-robot cooperation is formulated as a free-energy
minimization process, where each robot dynamically updates its preference matrix based on per-
ceptual inputs and peer intentions, thereby achieving adaptive coordination in partially observ-
able and dynamic environments. The proposed approach is validated through both simulation
and real-world experiments, including a multi-robot maze navigation and a collaborative ma-
nipulation task, compared against traditional BTs(https://youtu.be/KX_oT3IDTf4). Experimental
results demonstrate that the IIBT framework reduces BT node complexity by over 70%, while
maintaining robust, interpretable, and adaptive cooperative behavior under environmental uncer-
tainty.
Keywords:
Interactive inference, behavior tree, multirobot, joint action.
1. Introduction
Cooperative decision-making among multiple robots operating in dynamic and uncertain en-
vironments remains a fundamental challenge in autonomous systems [1, 2]. As task complexity
grows—ranging from industrial assembly and warehouse logistics to search-and-rescue opera-
tions—robot teams must not only coordinate actions and share information but also adapt their
behaviors to changing environmental conditions in real time [3, 4]. Traditional centralized plan-
ning approaches provide global coordination capabilities but often suffer from high computa-
tional cost and limited scalability [5, 6, 7, 8]. Conversely, fully distributed or reactive control
architectures offer rapid responses but frequently fail to maintain coherent team-level strategies
under uncertainty.
∗Corresponding author
Email addresses:chaoran_w@zju.edu.cn(Chaoran Wang ),sunjingyuan1@huawei.com(Jingyuan Sun)
Root
TaskCondition
Root
TaskCondition
Minimize Free Energy
Planning
Interactive Inference
Robot1
Root
Condition Task
Continuous-Time
Environment
Observations
communication
communication
communication
Figure 1: Overview of the proposed framework. Multiple robots perform interactive inference to jointly minimize free
energy, dynamically update their policies, and coordinate actions in a shared, continuously evolving environment.
To balance structured decision-making with real-time adaptability, BTs have emerged as a
widely adopted control paradigm in both robotics and game AI. BTs offer a modular, hierarchical,
and interpretable framework that enhances code reusability, debugging efficiency, and system
transparency [9, 10]. Their execution semantics—based on ticking nodes that returnSuccess,
Failure, orRunningstatuses—allow robots to reactively adapt to changing conditions without
requiring a complete redesign of the control logic [11, 12]. However, once a BT structure is
constructed, it remains inherently deterministic [13]. This limitation makes it challenging to
apply BTs in scenarios characterized by partial observability, dynamic task dependencies, and
evolving cooperation requirements [14, 15]. As a result, BT-based systems often rely on static
decision paths, which can degrade performance when environmental or task-related conditions
deviate from design-time assumptions.
Meanwhile, interactive inference, grounded in the free energy principle, provides a probabilis-
tic foundation for perception, prediction, and decision-making [16, 17]. By minimizing expected
free energy, agents can iteratively infer hidden states and select action policies that balance ex-
ploratory information gathering with goal-directed behavior [18]. Despite its success in cognitive
modeling and single-robot active perception, the application of interactive inference to multi-
robot systems remains limited. More importantly, existing inference frameworks are typically
monolithic or centralized, making them difficult to embed into modular, node-based decision ar-
chitectures like BTs. Consequently, a fundamental gap persists between the interpretability and
modularity of BTs and the probabilistic adaptability of inference-based methods.
To bridge this gap, this paper proposes anInteractive Inference Behavior Tree(IIBT) frame-
work that embeds free-energy–based probabilistic reasoning directly into BT execution nodes.
The proposed framework preserves the modularity and interpretability of BTs while enabling
each node to perform adaptive inference based on contextual observations. Through this integra-
tion, multiple robots can jointly infer and update their policies online, dynamically adapting to
environmental variations and the actions of other agents during task execution.
The main contributions of this work are summarized as follows:
•Integration of probabilistic inference into BTs:We propose a novel IIBT node that seam-
lessly integrates free-energy–based inference into BT execution semantics, enabling online
adaptation without altering the BT structure.
2
•Distributed cooperative policy selection:Each node performs local inference based on
expected free energy, supporting scalable, decentralized, and coherent decision-making
among multiple robots.
•Comprehensive experimental validation:The proposed approach is evaluated through
both simulation and real-world experiments, demonstrating significant improvements in
task success rate, decision efficiency, and BT complexity compared to conventional ap-
proaches.
The remainder of this paper is organized as follows. Section 2 reviews related work on inter-
active inference and BT-based planning. Section 3 formulates the cooperative decision-making
problem and introduces the theoretical foundation. Section 4 presents the proposed IIBT frame-
work in detail. Implementation and case studies are described in Section 5, while Section 6
reports experimental results. Finally, Section 7 discusses the findings and concludes the paper.
2. Related Work
2.1. Interactive Inference in Robotic Systems
Interactive inference, grounded in the free energy principle, has emerged as a powerful
paradigm for unified perception, prediction, and decision-making under uncertainty [4, 3]. By
formulating control as a process of minimizing expected free energy, agents can iteratively up-
date their beliefs about hidden states and select policies that balance epistemic exploration with
pragmatic goal-directed actions [1, 18].
Initial research primarily focused on cognitive modeling and single-robot active percep-
tion [16, 17]. More recent studies have extended these ideas to multi-robot contexts, including
distributed control [5], federated inference and belief sharing [19], and collective state estima-
tion in partially observable environments [20]. Additionally, recent works have explored implicit
coordination mechanisms where robots coordinate without explicit communication by inferring
the latent intentions of teammates [21].
While these approaches demonstrate the versatility of active inference in robotics, most rely on
centralized generative models or global state synchronization, which limit scalability in realistic
multi-robot deployments. Furthermore, these methods typically lack structured representations
for hierarchical task decomposition, which constrains their integration into modular decision-
making architectures.
2.2. Behavior Trees for Robotic Planning
BTs have become a prominent alternative to classical decision architectures such as finite
state machines, offering a modular, hierarchical, and interpretable control framework [22]. By
decomposing complex behaviors into control nodes (e.g., Sequence, Selector) and execution
nodes (e.g., Condition, Action), BTs allow developers to build scalable decision policies that are
reusable and easily debuggable [14, 11].
BTs have been widely applied in robotic navigation [15], manipulation [13], multi-robot coor-
dination [8], and human-robot collaboration [12]. To enhance adaptability, researchers have in-
tegrated BTs with machine learning [23], symbolic planning [24], and probabilistic models [25],
as well as studied BT performance metrics and design evaluation methodologies [26].
Despite these advances, conventional BTs remain largely deterministic and static once defined.
Most extensions focus on offline learning or external probabilistic reasoning layers, rather than
3
incorporating probabilistic inference directly into BT execution semantics. As a result, current
approaches still struggle to handle dynamic task priorities, partial observability, and emergent
multi-robot interactions in a unified framework.
2.3. Research Gap and Motivation
In summary, two complementary research lines have emerged: inference-based methods offer
probabilistic reasoning and adaptability but lack modular structure, while BT-based approaches
provide interpretability and composability but cannot reason probabilistically or adapt online. A
few attempts have combined active inference and BTs for reactive single-agent control [18], but
these efforts stop short of embedding free-energy–based reasoningwithinBT execution nodes.
To the best of our knowledge, no prior work has integrated interactive inference directly
into the execution semantics of BTs to enable distributed, adaptive, and cooperative multi-robot
decision-making. This gap motivates the present work, which aims to unify these complementary
strengths through the proposedInteractive Inference Behavior Tree (IIBT)framework.
3. Preliminary
Notation
To facilitate the following derivations, we summarize the main symbols and their definitions
in Table 1.
Table 1: Notation summary used in the preliminary section.
Symbol Description
Ri Thei-th robot in a team ofNrobots
Oi
τ & Observation received by robotiat timeτ
si Hidden state of robotRi
si
τ State of robotiat timeτ
Πi ={πi
1,...,π i
K }Policy set available to roboti
πi
k Thek-th policy for roboti
πi
∗ The optimal policy for roboti
πi
τ The policy for robotiat timeτ
PGenerative distribution
QVariational posterior distribution
Ai Observation likelihood matrixP(Oi
τ|si
τ)
Bi
π State transition matrixP(s i
τ|si
τ−1,πi
k)
Ci Outcome preference prior
Di Initial state priorP(s i
1)
EPrior preference over policies
F(πi
k) Variational free energy under policyπ i
k
G(πi
k) Expected free energy under policyπ i
k
γPrecision parameter balancing planning and inference
σ(·) Softmax function
4
π
β
γ
-th Robot
-th Robot
(π)
π
Figure 2: The figure illustrates the interactive inference process between robotsRi andRj using a generative model.
3.1. Generative Model for Multi-Robot Inference
We consider a team ofNcooperative robotsR={R 1,...,R N }operating in a dynamic environ-
ment. Each robotRi maintains an internal model of the environment through a generative process
that relates hidden states, actions (policies), and observations over time. This model serves as
the foundation for inference and planning.
The joint probability of the observation sequenceO i
1:T , latent statess i
1:T , and policyπ i
k for
robotican be expressed as:
P

Oi
1:T ,¯si
1:T |πi
=P

si
1
 TY
τ=1
P

Oi
τ|si
τ
 TY
τ=2
P

si
τ|si
τ−1,πi
=s i
1 ·Di
TY
τ=1
Oi
τ ·Ai si
τ
TY
τ=1
si
τ ·Bi si
τ−1,
(1)
Where the terms are interpreted as follows:
•P(s i
1): Prior over the initial hidden state, encoded byDi.
•P(π i
k): Prior over policies, influenced byEandC i.
•P(O i
τ|si
τ): Likelihood of observation given the state, parameterized byAi.
•P(s i
τ|si
τ−1,πi
k): Transition model under policyπi
k, parameterized byBi
π.
This generative structure forms the basis for both state estimation and action planning, cap-
turing the causal relationships between actions, latent states, and observations in multi-robot
collaboration.
5
3.2. Variational Inference and Evidence Lower Bound
In probabilistic robotics and decision-making, computing the exact posterior distribution over
hidden states and strategies,P(s i
1:T ,πi
k|Oi
1:T ), is intractable due to the exponential growth of the
state space and the nonlinear observation models involved [27, 28]. Variational inference pro-
vides a tractable approximation by introducing a surrogate distributionQ(s i
1:T ,πi
k) and minimiz-
ing its Kullback-Leibler (KL) divergence from the true posterior:
KL
h
Q(si
1:T ,πi
k)∥P(s i
1:T ,πi
k|Oi
1:T )
i
.(2)
This objective is equivalent to maximizing theEvidence Lower Bound(ELBO), a standard
formulation in Bayesian inference [29, 27]:
L=E Q
h
lnP(Oi
1:T ,s i
1:T ,πi
k)
i
−E Q
h
lnQ(s i
1:T ,πi
k)
i
.(3)
The negative ELBO is referred to as thevariational free energy[30, 31], which can be ex-
pressed as:
F(πi
k)=KL
h
Q(si
1:T ,πi
k)∥P(s i
1:T ,πi
k|Oi
1:T )
i
−lnP(Oi
1:T ).(4)
Since the marginal likelihood lnP(Oi
1:T ) is constant with respect to the optimization objective,
minimizingFis equivalent to minimizing the KL divergence. Expanding the terms yields:
F(πi
k)=KL
h
Q(si
τ|πi
k)∥P(s i
τ|si
τ−1,πi
k)
i
|                                {z                                }
Complexity
−EQ
h
lnP(Oi
τ|si
τ)
i
|              {z              }
Accuracy
.(5)
Here, thecomplexity termpenalizes divergence between the posterior and the prior transition
model, constraining the internal model’s deviation from known dynamics. Theaccuracy term,
in contrast, rewards beliefs that better explain the observed sensory data.
By minimizing the variational free energy, the robot continuously aligns its internal generative
model with external observations, enabling robust state estimation and situational awareness [32,
33]. This mechanism provides the foundation for interactive decision-making and collaborative
policy selection in multi-agent systems.
3.3. Expected Free Energy and Policy Selection
While the variational free energyFgovernsperceptionby inferring latent states from current
observations, decision-making in uncertain environments requires reasoning aboutfuture out-
comes. This is achieved by minimizing theexpected free energy(EFE)Gfor each candidate
policyπi
k [32]:
G(πi
k)=KL
h
Q(Oi
τ|πi
k)∥P(O i
τ)
i
|                       {z                       }
Extrinsic value (goal alignment)
−E Q
h
H(Oi
τ|si
τ)
i
|           {z           }
Intrinsic value (information gain)
(6)
The expected free energy can be interpreted as the sum of two complementary terms:
-Extrinsic value:The first term is the Kullback-Leibler divergence between predicted out-
comes and prior preferencesCi, which encourages policies that lead to outcomes consistent with
task goals and desired states. -Intrinsic value:The second term represents expected informa-
tion gain about hidden states. Maximizing this term promotes epistemic exploration by selecting
policies that reduce uncertainty, improving future state estimation.
6
Together, these two components balancegoal-directed exploitationanduncertainty-reducing
exploration[34, 25], a property particularly important for multi-robot coordination where future
contingencies cannot be exhaustively enumerated.
The posterior probability of executing a specific policy is then defined by a softmax distribu-
tion that integrates both variational and expected free energy terms:
p(πi
k)=σ

lnEk −F(π i
k)−γG(π i
k)

(7)
WhereEk denotes a prior over policies (often uniform),Fencodes the current-state evidence,
andGcaptures the expected future utility of a policy. The hyperparameterγregulates the relative
precision of decision-making: a largerγemphasizes epistemic actions (information-seeking),
while a smallerγbiases decisions toward exploiting known rewards.
This formulation ensures that each robot selects strategies that jointly minimize epistemic
uncertainty and maximize task-relevant outcomes, enabling coherent and adaptive multi-robot
coordination even under partial observability and environmental uncertainty.
4. Methodology
4.1. Approach Overview and System Architecture
To enable distributed, adaptive, and cooperative decision-making in multi-robot systems, we
propose anInteractive Inference Behavior Tree(IIBT) framework that tightly integrates proba-
bilistic inference with the modular decision-making structure of BTs. Conventional BT-based
methods typically rely on pre-defined control logic, which limits their adaptability and robust-
ness under uncertainty. In contrast, the proposed IIBT architecture embeds inference capabilities
directly within BT execution nodes, allowing robots to update their beliefs online, dynamically
adjust decision priorities, and coordinate with teammates in partially observable environments.
         Policy Choose
（Intreactive Inference）
status
Collaborative Task Model
?
         Policy Choose
（Intreactive Inference）
status
Behavior Tree
Update belief
Transfer Logic
Collaborative Task Model
Robot n
?
         Policy Choose
（Intreactive Inference）
status
Update belief
Collaborative Task Model
?
         Policy Choose
（Intreactive Inference）
status
Behavior Tree
Update belief
Transfer Logic
Collaborative Task Model
Robot 2
?
2
1
n
Planning
         Policy Choose
（Intreactive Inference）
status
Update belief
Collaborative Task Model
?
         Policy Choose
（Intreactive Inference）
BT status
Behavior Tree
Share Intention
Intention
Update belief
Environment
Transfer Logic
Collaborative Task Model
Robot 1
?
1
n
2
Figure 3: Workflow of interactive nodes in the BT. Each robot collects its local observationsO i
τ, abstracts them into
logical variablesLi, and updates its beliefs i
τ. The BT emits a preference matrixC i to the inference module, which
queries task models forAi,Bi
π,Di, incorporates other robots’ intentions, and returns state/policy information back to the
BT for execution.
Each robotRi maintains an internal generative model
Mi ={Ai,Bi
π,Ci,Di},
7
as defined in Table 1, which captures the probabilistic relationships between sensory observa-
tions, latent states, action dynamics, and task preferences. Based on this model, the inference
layer estimates latent state beliefss i
τ, predicts possible outcomes under candidate policiesπ i
k,
and evaluates their expected free energy. The resulting posterior beliefs are then fed into the BT
layer, which orchestrates task execution through a structured hierarchy of condition and action
nodes.
This integration transforms the BT from a static execution tree into a dynamic, belief-aware
control architecture. It inherits the interpretability, modularity, and maintainability of BTs while
gaining the adaptivity, robustness, and coordination capabilities associated with probabilistic in-
ference. This hybrid design is particularly advantageous in multi-robot scenarios where agents
must make decisions based on partial observations, dynamically changing objectives, and uncer-
tain intentions communicated by peers.
4.2. Interactive Inference BT Node (IIBT) Design
The IIBT node serves as the core interface that bridges the reactive execution flow of a behavior
tree with the probabilistic reasoning of the inference module. At each tick, the node not only
decides which action policy to execute but also updates its belief and task preference in light
of new observations and teammates’ inferred intentions. Algorithm 1 outlines the complete
reasoning and execution cycle of the node for robotRi.
8
Algorithm 1:Strategy Selection Process within an IIBT Node for RobotRi
1Tick(ℑ task )
2ℑ task .r←running
3RetrieveM i ={Ai,Bi
π,Ci,Di}from task model
4Retrieve preference matrixC i fromℑtask
5Acquire current observationO i
τ from environment
6Update belief states i
τ and logic variablesLi
7Construct candidate policy setΠ i
8fork∈{1,...,N}\ido
9ReceiveR k policy intentionπk
10Π i ←Π i ∪{πk}
11end
12π i
τ ←InteractiveIn f er(s i
τ,Mi,Π i)
13ifπ i
τ ≡πi
stop then
14ℑ task .r←success
15returnℑ task .r
16else
17whileL i <πi.precdo
18L i =f L(πi.prec)
19C i ←Ci +Li
20ReconstructΠ i
task
21fork∈{1,...,N}\ido
22ReceiveR k policy intentionπk
23Π i ←Π i ∪{πk}
24end
25π i
τ ←InteractiveIn f er(s i
τ,Mi,Π i,Oi
τ)
26ifTimeoutthen
27ℑ task .r←f ailure
28returnℑ task .r
29end
30end
31ifcountConnectedRobot=N−1then
32Execute(π i
τ)
33C i ←Ci −Li
34else
35Execute(π i
wait)
36end
37ifπ i
τ ∈πi
stop .precthen
38π i
τ ←πi
stop
39end
40ℑ task .r←running
41returnℑ task .r
42end
Role in the architecture..As shown in Fig. 3, the IIBT node is the execution-time interface
that closes the loop between probabilistic inference and the BT tick-cycle. At every tick, it (i)
reads the current belief and observation, (ii) updates the preference matrix using symbolic logical
evidence, (iii) incorporates peer intentions, (iv) runs interactive inference over the candidate
policies, and (v) executes or defers actions depending on team connectivity and preconditions.
The full tick routine is given in Alg. 1.
As shown in Fig. 3, the IIBT node closes the perception–inference–action loop during each
BT tick. At runtime, it (i) collects local observations and current beliefs, (ii) transforms symbolic
9
logic into quantitative preference updates, (iii) exchanges policy intentions with peers, and (iv)
performs interactive inference to select the most plausible cooperative policy (Alg. 1). This
makes each node an autonomous reasoning unit that aligns its local execution policy with both
environmental feedback and team-level belief consistency.
Inputs and maintained state..For robotR i, the node retrieves its generative modelM i =
{Ai,Bi
π,Ci,Di}(line 3), obtains sensory observationO i
τ (line 5), and updates both the filtered
beliefs i
τ and its symbolic abstractionLi (line 6). The candidate policy poolΠ i (line 7) is contin-
uously expanded with peer intentionsπ k from other robots (lines 8–11), creating a belief-aware
and context-sensitive action space.
Model Semantics..Each robot maintains a latent state vector
si
τ =[s i
loc,s i
hold,s i
place,s i
free]⊤,
representing the robot’s belief in reaching a location, grasping, placing, or being idle. The infer-
ence process operates on the joint state
sjoint
τ =[s 1
τ,s 2
τ,...,s N
τ ,s result
τ ]⊤,
corresponding respectively to the robots’ beliefs and the global task outcome. Each state di-
mension is normalized as a probability distribution representing belief strength (e.g.,s i
hold =
[0.9,0.1] ⊤indicates 90% confidence of holding the object).
Here,s result
τ is a global latent variable summarizing the cooperative task outcome (e.g., overall
success, failure, or pending status). This concatenated representation allows the inference pro-
cess to capture cross-robot dependencies such as temporal ordering, spatial coupling, and shared
resource constraints. In practice, each IIBT node maintains and updates its own marginal belief
si
τ at timeτ, but exchanges summarized information about other agents’ inferred states during
coordination, thereby realizing a distributed yet coherent joint inference process across the team.
This formulation captures inter-robot dependencies within a unified generative process Eq.(1).
Observation representation and joint observation matrix..For a system consisting ofNcooper-
ative robots, the overall observation at discrete time stepτis represented as
Ojoint
τ =[O1
τ,O2
τ,...,O N
τ ,Oresult
τ ]⊤,
whereOjoint
τ denotes thejoint observation vectorof the multi-robot system. It concatenates all
individual robots’ local observations and a task-level outcome vector, forming the sensory inter-
face between the physical environment and the inference process. Each element ofOi
τ is a binary
or probabilistic indicator that reflects whether a certain physical or symbolic event is currently
observed.
For each robotRi (i=1,...,N), the local observation vector
Oi
τ =[o i
1,o i
2,...,o i
mi ]⊤
encodes the status of its task-relevant latent variables. Each componento i
j (j=1,...,m i) is a
binary observation associated with thej-th latent states i
j:
oi
j =

1,if the states i
j is achieved at timeτ,
0,otherwise.
10
Hence,Oi
τ provides a direct logical mapping from the robot’s perception space to its latent belief
states i
τ. In practice, these elements can be computed from sensor feedback, symbolic condition
checks, or communication messages. For instance, if robotR1 successfully grasps an object, then
o1
hold =1 while other entries remain zero.
In addition to local observations, the system maintains a task-level observation vectorO result
τ
that summarizes the global cooperative outcome at the current time step:
Oresult
τ =[o result
success,o result
failure,o result
null ]⊤.
Each entry inOresult
τ is a binary indicator specifying whether the global task has reached a corre-
sponding terminal or intermediate state:
oresult
k =

1,if the overall task is in statuskat timeτ,
0,otherwise.
This vector serves as a shared global signal that allows all robots to condition their inference
on collective task progress, enabling synchronization and cooperative adaptation among team
members.
By vertically stacking all local and global observations, the joint observation matrix can be
expressed as
Ojoint
τ =

o1
loc o1
hold o1
place o1
free
o2
loc o2
hold o2
place o2
free
... ... ... ...
oresult
success oresult
failure oresult
null –

⊤
,
where “–” indicates a null or non-applicable entry. Each row corresponds to one agent (including
the global result layer), while each column denotes a semantic dimension of the task space, such
as localization, grasping, placement, or idle status. This joint structure allows the inference
module to integrate heterogeneous sensory and symbolic information across agents, providing a
unified observation basis for distributed belief updating within the IIBT framework.
Observation and transition model matrices..Consequently, the likelihood matrixA i and transi-
tion matrixBi
π are constructed in block-diagonal form to represent both individual robot dynam-
ics and inter-robot dependencies:
Ai =

Ai
r1 0 0···0
0A i
r2 0···0
... ... ... ... ...
0 0 0A i
rN 0
0 0 0··· A i
result

,
Bi
π =

B11 B12 ··· B 1N
B21 B22 ··· B 2N
... ... ... ...
BN1 BN2 ··· B NN

.
11
Here, each blockAi
r j andBi j corresponds to robotRj’s local observation and transition model,
respectively. The diagonal termsB ii encode the self-dynamics of each robot, while the off-
diagonal termsBi j (i,j) represent the coupling effects between robots, such as physical inter-
ference, task dependencies, or coordination constraints. The bottom-right termA i
result describes
the observation likelihood of the global task outcome, associated with the global observation
vectorOresult
τ defined earlier.
This block-structured formulation enables the inference module to reason about multi-agent
dependencies probabilistically, while each IIBT node still performs local updates through decen-
tralized message passing.
Local likelihood model..For each robotR i, the observation (likelihood) matrixA i defines the
conditional probability of receiving a particular observation given the current latent state:
P(Oi
τ |s i
τ)=A i si
τ,
whereAi ∈R mi×ni maps then i-dimensional latent state space to them i-dimensional observation
space. Each column ofA i specifies the likelihood distribution over observable outcomes when
the system is in a particular hidden state.
Physically,Ai captures the reliability of the perception or sensing process. For instance, a
diagonal entry of 0.9 indicates that the vision or gripper sensor correctly reflects the true state
90% of the time, while the remaining 0.1 models observation noise caused by occlusion, light-
ing change, or sensor failure. When the task involves symbolic communication (e.g., “object
placed”), the same formulation applies, treating message acknowledgment as an observation
channel.
Local transition model..The state-transition matrixB i
π encodes how each robot’s internal belief
evolves over time given its executed policyπi. Formally,
P(si
τ+1 |s i
τ,πi)=B i
πi si
τ,
whereBi
πi ∈R ni×ni is the action-specific transition matrix associated with policyπi. Each column
ofBi
πi represents the probability distribution of the next state given the current state and action.
Different actions correspond to different transition matrices:
Bmove,Bpick,Bplace,Bidle,
where, for example,B move increases the probability ofs i
loc =1 as navigation proceeds,B pick
increases the probability ofs i
hold =1 after a successful grasp,B place increases the probability of
si
place =1 once placement is achieved, andB idle ≈I 4 maintains the current belief state when no
action is executed.
These transition probabilities are empirically estimated from execution logs, using suc-
cess/failure ratios or temporal statistics, and are normalized column-wise to ensure valid prob-
ability distributions. By combiningA i andBi
πi , each robot maintains a physically grounded
generative model that connects sensory uncertainty, action dynamics, and latent belief updates
within the Interactive Inference Behavior Tree framework.
12
Prior state distribution..The prior matrixD i defines the initial belief over the latent task states
of robotRi before task execution begins. Formally,Di ∈R ni is a column vector representing the
initial probability distribution of the latent states i
τ=0:
P(si
τ=0)=D i,
wheren i denotes the number of task-relevant hidden states maintained by robotRi. Each element
di
j ofDi =[d i
1,d i
2,...,d i
ni ]⊤specifies the prior probability of thej-th latent state being true at the
initial time step. This initialization encodes the assumption that the robot has not yet reached a
target location, grasped an object, or completed a placement at timeτ=0.
From a probabilistic perspective,D i serves as the starting point of the generative process,
defining the prior beliefP(s i
0) in the joint distribution Eq.(1), whereP(si
0) corresponds exactly to
Di. This ensures that all subsequent inferences and belief updates within the IIBT framework re-
main grounded in a consistent probabilistic prior, promoting stable initialization and reproducible
behavior across different robot agents.
The preference matrixCi defines the extrinsic desirability of potential observation outcomes,
guiding each robot’s policy selection toward goal-consistent behaviors. In the multi-robot setup,
Ci is modeled jointly rather than independently per agent to ensure that all preferences are aligned
with both individual objectives and collective task constraints. Formally, the joint preference
matrix at time stepτis expressed as
Ci =[Ci
r1,Ci
r2, ...,C i
rN ,Ci
result ]⊤,
where each blockCi
r j ∈R mj encodes the preference distribution of robotRj over its local obser-
vation spaceOj
τ, andCi
result represents the team-level preference over global outcomes (e.g., task
success or failure).
Each preference vector is defined as
Ci
r j =[c i
1,c i
2, ...,c i
mj ]⊤,
wherec i
k denotes the desirability of observing outcomeo j
k under the current task objective. High
preference values bias the inference process toward policies expected to produce those outcomes,
as reflected in the expected free energy computation of the Interactive Inference step.
The preference matrixC i specifies desirable observation outcomes derived from task goals,
e.g., high weight onl hold during grasping phases and onlplace during assembly. During execution,
Ci is adaptively updated through the logic-to-preference mappingf L, which injects symbolic
conditions (e.g., preconditions or unmet goals) into the preference vector to bias the expected
free energy.
Overall, the combination{Ai,Bi
π,Ci,Di}defines a compact yet physically grounded genera-
tive model that links sensory uncertainty, action dynamics, and goal preference. These matrices
can be tuned directly from empirical robot data or analytically set based on system reliability,
providing a clear interface between probabilistic inference and the robot’s physical control do-
main.
Tick-cycle flow..At the beginning of each tick the node sets its return status torunning(line 2)
and constructs a local candidate policy poolΠ i (line 7). It then collects the most recent policy
intentions broadcast by other robots (lines 8–11) and augmentsΠ i accordingly, yielding a peer-
aware candidate set. Given (s i
τ,Mi,Π i), the node calls InteractiveInfer(line 12) to obtain the
13
current policyπ i
τ via the free-energy based posterior (cf. Sec. 3.3). If the selected policy is the
terminal policyπi
stop (line 13), the node returnssuccess(lines 14–15).
Algorithm 2:InteractiveInfer(lite): One-step Active-Inference Scoring forRi
Input:Beliefs i
τ, modelMi ={Ai,Bi
π,Ci,Di}, candidatesΠ i, current observationOi
τ, policy priorE, precision
γ
Output:Selected policyπ i
τ and posteriorQ(πi)
1P(O i
τ)←Softmax(C i)// preference-induced outcome prior
2foreachπ∈Π i do
3b ′←Bi(·|π)s i
τ // next-belief (one-step rollout)
4q(O i
τ)←A i b′ // predicted outcome marginal
5E ext ←D KL
 q(Oi
τ)∥P(Oi
τ)
6E int ←P
s b′(s)H Ai(:,s) 
7G(π)←E ext −E int
8F(π)←− P
s si
τ(s) logAi Oi
τ,s 
9Score[π]←lnE(π)−F(π)−γG(π)
10end
11Q(π i)←Softmax  Score[·] // log-sum-exp stabilized
12π i
τ ←arg max π∈Πi Q(πi =π)
13returnπ i
τ,Q(πi)
Logical-to-preference shaping..In our multi-robot setup, the preference matrixC i is also mod-
eled jointly rather than independently per agent. Specifically,C i is structured as a column-wise
concatenationCi =[Ci
r1,Ci
r2,...,C i
result]⊤, where each block encodes the extrinsic preferences of
one robot.C 1 andC2 represent, respectively, the desired observation likelihoods for robotsR 1
andR2. Each elementc i
k inCi
r j =[c i
0,c i
1,...,c i
M]⊤ corresponds to the desirability of observing
outcomeounder robotR i’s current task objective.
If preconditions forπ i
τ are not satisfied (Li <π i
τ.prec, line 17), the node computes the mini-
mal logical evidence required to satisfy the precondition using the mappingf L(·) (line 18), and
applies it as an additive update onCi (line 19):
Li =f L(πi
τ.prec),C i ←Ci +Li.
Intuitively, this raises the extrinsic preference for outcomes that make the precondition true,
thereby biasing the EFE toward prerequisite-achieving actions. The node then reconstructs the
task-specific policy poolΠ i
task (line 20), refreshes peer intentions (lines 21–24), and re-runs In-
teractiveInfer(line 25). If a timeout occurs (lines 26–29), the node fails fast, returningfailure.
Execution and synchronization..When the communication layer reports that all teammates are
connected (countConnectedRobot=N−1, line 31), the node executes the selected policyπ i
τ
(line 32). Immediately after dispatch, itrolls backthe temporary preference boost associated
with the just-satisfied logical increment (line 33), i.e.,
Ci ←Ci −Li,
restoring the baseline preferences to avoid long-term drift. If full connectivity is not met, the
node executes a wait policyπ i
wait (line 35), preserving safety and coordination while messages
converge.
Termination guard..If the currently selected policy becomes a member of the stop precondition
set (line 37), the node promotes it toπ i
stop (line 38). Otherwise, it continues inrunningstate
(line 40) and returns control to the parent BT composite (line 41).
14
Discussion and interface.The IIBT node exposes two light-weight interfaces to the rest of the
BT: (i) alogic-to-preferenceadapterf L that transforms symbolic BT conditions into additive
updates onCi, and (ii) the InteractiveInfercall that converts (si
τ,Mi,Π i) into a softmax posterior
over policies, using the variational free energyF(perception term) and expected free energyG
(prospection term) defined in Secs. 3.3. This design preserves BT interpretability and reactivity
while endowing each node with uncertainty-aware, preference-driven adaptation.
5. Implementation of Interactive Inference Nodes
5.1. Robots Interactive Inference: A Simple Example
This section presents a case study on interactive inference in a multirobot system, showing
how robots plan behaviors under unknown objectives while minimizing free energy.
p1p0 p3 p4 p5 p6
p8p7 p9 p10 p11 p12 p13
p15p14 p16 p17 p18 p19 p20
p22p21 p23 p25 p27
p29
p35
p30 p31 p32 p33 p34
p36 p37 p38 p39 p40 p41
p43p42 p44 p45 p46 p47 p48
goal1 goal2
Figure 4: Two robots,R1 andR2, operate in a 7x7 grid, with cell positions labeled as{p 0,p 1,...,p 48}. The environment
features two goals,goal 1 andgoal 2, and each robot creates paths to both goals.
We examine two robots,R1 andR2, in an environment shown in Fig. 4 with two goals (goal1
andgoal 2). The robots cannot identify their goals but can see each other’s positions on a grid
map with locations{p 0,p 1,...,p 48}.
The strategy setΠ 1 includes the strategies for robotR 1 to achieve two goals. The paths
togoal 1 are{p 28,p 29,p 30,p 31,p 24}and{p 28,p 35,p 36,p 37,p 38,p 31,p 24}. The paths togoal 2
are{p 28,p 29,p 30,p 31,p 32,p 33,p 26}and{p 28,p 35,p 36,p 37,p 38,p 39,p 40,p 41,p 34,p 27,p 26}.
The strategy set for robotR 2 (Π2) includes paths togoal 1:{p 2,p 1,p 8,p 15,p 22,p 23,p 24}
and{p 2,p 3,p 10,p 17,p 24}; and paths togoal 2:{p 2,p 3,p 4,p 11,p 18,p 25,p 26}and
{p2,p 3,p 4,p 5,p 12,p 19,p 26}.
The task requires robotsR1 andR2 to reach different goals simultaneously. The combination
of strategy selections is defined asΠ 1 ×Π 2 ={(π1,π2)|π 1 ∈Π 1,π2 ∈Π 2}.
For robotR 1, the hidden states ares 1 ={s 1
τ,s 2
τ}, wheres 1
τ =[p 0,p 1,...,p 48]⊤ ands 2
τ =
[p0,p 1,...,p 48]⊤.
We define the observation sets as
O1
τ ={Or1
τ ,Or2
τ ,Oresult
τ },
15
（b
Exist(obstacle)
）（a）
IsArrived
(goal1,  goal2)
RootRoot
<
MoveTo(goal2)
1,goal2d1,goal1d MoveTo(goal1)
?
IsArrived
(goal1 or goal2)
Figure 5: Fig.(a) shows the traditional method for robotR 1 to select the nearest goal while considering other robots’
states. Fig.(b) illustrates the interactive inference node, which selects a strategy to minimize free energy. Both figures
demonstrate the same functionality.
whereOr1
τ is robotR1’s position observation,Or2
τ contains observations from robotR2, andOresult
τ
indicates whether both robots reached their goals simultaneously. The likelihood matrix is de-
fined as
A1 ={A1
r1,A1
r2,A1
result}.
The matrixA 1
r1 illustrates the relationship between robotR 1’s observable position and its
hidden state, with a probability of accurately determining its position at 0.9952. Formally,
A1
r1{pi,p i,:}=0.9952 fori∈[0,48], indicating that robotR 1 correctly determines its own
position with probability 0.9952. The matrixA 1
r2{pi,p i,:}=0.904 fori∈[0,48] reflects the
accuracy of estimating the other robot’s position. LetA 1
result denote the joint inference result.
Success occurs when robotsR 1 andR2 achieve different goals simultaneously, represented by
A1
result(1,p 24,p 26)=1, or vice versa. If the robots fail to achieve their goals simultaneously or
select the same goal, it results in a task failure, indicated byA1
result(2,p 24,p 26)=1, or vice versa.
If neither robot reaches a goal, the task result isnull, denoted asA 1
result(0,:,:).
The transition matrixB 1 describes how hidden states evolve over timeτbased on control
actionsa τ ∈U. A sequence of control actions is represented asπ i ={a τ=1,a τ=2,...,a τ=n}, with
πi ∈Π i, whereΠ i includes all strategies for robotRi. The matrixB1 ={B1
r1,B1
r2}consists of the
state transition matrices for robotR1 and robotR2. SetBi{pnext,p cur,a τ}=1.0, wherea τ is the
robot’s action at timeτ,p cur is its current position, andp next is the position after actiona τ.
Table 2: Setting Strategy Priorities in Experiments
Planning Precondition Postcondition
πi
obs Exist(obstacle) l i
obs =−(max(Ci
ri)+1)
πi
points
!Exist(obstacle)
And!IsArrived(points) li
add =max(Ci
ri)+1
πi
goal
!Exist(obstacle)
And!IsArrived(goal)
AndIsArrived(points)
li
goal =max(Ci
ri)+1
πi
stop IsArrived(goal)-
The preference matrixC i, aligned with the observation matrixO i
τ, indicates preferences for
goal locations regarding task outcomes, as defined in
C1 ={C1
r1,C1
r2,C1
result},
whereC1
{r1,r2}{p24,p 26}=1, andC 1
result{success}=1. These are set based on the Pref-Weights in
Table 5.1.
16
We derive the probability distributions of strategies inΠ1 ×Π 2 based on robot configurations.
The robots select and execute the strategies with the highest probabilities, resulting in the fol-
lowing distributions:
Pg1−g1 =[0.151×10 −7,0.566×10 −6,0.995×10 −7,0.210×10 −6],
Pg1−g2 =[0.405×10 −7,0.357×10 −7,0.081,0.072],
Pg2−g1 =[0.746×10 −7,0.847,0.682×10 −7,0.573×10 −6],
Pg2−g2 =[0.133×10 −7,0.117×10 −7,0.408×10 −7,0.360×10 −7].
Here,Pgi−gj represents the probability distribution of robotR1 moving towardgoal i while robot
R2 moves towardgoal j. RobotR 1 will follow the path{p 28,p 29,p 30,p 31,p 32,p 33,p 26}, while
robotR2 will follow the path{p 2,p 1,p 8,p 15,p 22,p 23,p 24}.
5.2. Interactive Inference Nodes for Conflict Handling
We demonstrate how robots resolve task conflicts in a dynamic environment, aiming to meet
the expected postcondition matrixπ1
goal.postc for achieving the strategyπ1
stop (see Table 5.1).
a1
a'3
a'4
Robot1 Policy Robot2 Policy
Interactive Inferencea2
a3
a4
a'6
a5
b1
b2
b'3
b'5
b'4
b5
b3
b4
a'5 b'6
Obstacle
Figure 6: RobotR 1, located at positiona 2, detects an obstacle at positiona 3. Consequently, robotR 1 abandons its
original path{a 1,a 2,a 3,a 4,a 5}and modifies it to
n
a2,a ′
3,a ′
4,a ′
5,a ′
6,a 5
o
. To ensure that it reaches the destinationb 5
simultaneously with robotR1, robotR2, positioned atb 2, engages in interactive inference with robotR1 and acquires a
new path
n
b2,b ′
3,b ′
4,b ′
5,b ′
6,b 5
o
.
While pursuing their goals, the robots evaluate execution conditions in real time at the inter-
active inference nodes (Algorithm 1, Line 17). When executingπ 1
goal, robots must ensure no
obstacles block the current path, a preconditionπ 1
goal.prec={E 1
r1,E1
r2,E1
result}. In the current
strategy,E1
r1{p29,p 30,p 31,p 32,p 33,p 26}=0 andE 1
r2{p1,p 8,p 15,p 22,p 23,p 24}=0. If a tempo-
rary obstacle is added atp 30 on robotR1’s path, the observation logical quantity for robotR1 is
L1 ={l 1
r1,l 1
r2,l 1
result}, wherel 1
r1{p30}=l 1
obs =−1. Thus,L 1 <π 1
goal.prec. The environmental logi-
cal variable is added to the preference matrixC 1, resulting inC1
r1{p30}=−1,C 1
r1{p24,p 26}=1,
andC1
r2{p24,p 26}=1.
At this stage, robotR 1 generates several obstacle avoidance strategies, denoted as
Π1
obs. These strategies are transformed into the path sets{p 29,p 22,p 15,p 16,p 17,p 24}and
{p29,p 36,p 37,p 38,p 31,p 32,p 33,p 26}.
Consequently, the strategy set for robotR 1 becomesΠ 1 = Π 1
obs ∪π1
goal. Meanwhile, robot
R2, located atp 8, generates a new strategy setΠ 2
goal, which includes{p 1,p 8,p 9,p 10,p 17,p 24}
17
and{p 1,p 8,p 9,p 10,p 11,p 18,p 19,p 26}. RobotR 2 incorporates its current strategy intoΠ 2 =
Π2
goal ∪π2
goal. After combining the two strategy sets and executing interactive inference, the result-
ing strategy distribution isP(Π1 ×Π2)=[0.784×10 −6,0.166×10 −6,0.521×10 −7,0.999,0.156×
10−6,0.134×10 −7,0.898×10 −13, 0.256×10 −7,0.554×10 −14].Based on the principle of min-
imizing free energy, robotR 1 will move along the path{p 29,p 36,p 37,p 38,p 31,p 32,p 33}, while
robotR2 will choose the path{p 1,p 8,p 9,p 10,p 17,p 24}. Due to the introduction of obstacles, both
robots abandon their original strategies in favor of new ones that minimize free energy.
6. Experiments
6.1. Cooperative Navigation without Predefined Goals
Goal Obstacle
Charger 
Station
Robot
Figure 7: The factory scene features three robots, movable obstacles, and charging stations, with the robots collaborating
to reach designated goals.
We previously demonstrated how interactive inference nodes manage tasks and conflicts. In
this subsection, we use these nodes to create a complex BT for mobile robots performing col-
laborative tasks in dynamic environments. In our simulation 1, three robots autonomously select
goals and adapt to changes, coordinating their movements through a BT controlled by interactive
nodes. The task settings for the robot group are shown in Table I. Fig. 8 shows the independent
BT control strategies for each robot. Traditional BTs require 21 nodes, while our approach uses
only 5, achieving a 76.2% reduction in design complexity.
RobotsR1,R2, andR 3 start at positionsp 2,p 28, andp 45, respectively. Thegoal 1,goal 2,
andgoal 3 are located atp 24,p 26, andp 32, while their charging stations are atp 13,p 27, andp 41.
In subsequent experiments, strategy setsΠ 1,Π 2, andΠ 3 employ the A* algorithm to design
paths for each goal, incorporatinga τ =waitinto the action strategies for each robot. To reduce
computational load, we utilize pairwise interactions—such as betweenR 1 andR2, and between
R1 andR3—as illustrated in Fig. 9.
For instance,R1 interacts with bothR2 andR3. RobotR1 generates strategies forgoal 1,goal 2,
orgoal 3 (Algorithm 1, Line 7). RobotsR1 andR2 then apply the interactive inference procedure
(Algorithm 1, Line 10) using the defined likelihood matrixA1 and transition matrixB1. RobotR1
selects the strategy with the highest posterior probability. The same inference process is repeated
between robotsR1 andR3. In the preference matrixC 1, we setC1
{r1,r2,r3}{p24,p 26,p 32}=1 and
C1
result{success}=1.
1https://youtu.be/KX_oT3IDTf4
18
Joint Arrival? Min(Sum(Policy1,Policy2))
?
IsArrived
(goal1 or goal2)
Exist Obstacle?
Construct 
Policy(Obstacle) Exist WayPoints?
Construct 
Policy(WayPoints)
?
Policy1:R1 →G1 MoveTo(G1)
MoveTo(G2)
?
Battery Enough?
MoveTo(Charger)
?
IsArrived
(goal1,  goal2)
Figure 8: The blue area shows the replacement of traditional control frameworks with interactive inference nodes, em-
phasizing their compression effects relative to traditional BT nodes.
0.0
 0.2
 0.4
 0.6
 0.8
 1.0
Probability
R3-R1
R3-R2Groups
0.162×10 20
0.920×10 20
0.100×10 19
0.101×10 26
0.110×10 26
0.327×10 6
0.467
0.441×10 6
0.472×10 6
0.533
0.119×10 19
0.148×10 19
0.161×10 19
0.184×10 26
0.201×10 26
0.568×10 6
0.723×10 6
0.802×10 6
0.857×10 6
1.000
3 Strategies Distribution( 3)
0.0
 0.2
 0.4
 0.6
 0.8
 1.0
Probability
R3-R1 R3-R2
0.162×10 20
0.920×10 20
0.100×10 19
0.101×10 26
0.110×10 26
0.327×10 6
0.467
0.441×10 6
0.472×10 6
0.533
0.119×10 19
0.148×10 19
0.161×10 19
0.184×10 26
0.201×10 26
0.568×10 6
0.723×10 6
0.802×10 6
0.857×10 6
0.999
3 Strategies Distribution( 3)
0.0 0.2 0.4 0.6 0.8 1.0
Probability
R2-R1 R2-R3
2 Strategies Distribution( 2)
0.0 0.2 0.4 0.6 0.8 1.0
Probability
R1-R2 R1-R3
1 Strategies Distribution( 1)
0.124×10 6
0.565×10 6
0.567×10 6
0.554
0.153×10 13
0.432×10 14
0.406×10 6
0.446
0.576×10 6
0.586×10 6
0.125×10 1
0.595×10 1
0.177
0.316
0.385×10 7
0.770×10 7
0.924×10 1
0.220
0.606×10 1
0.617×10 1
0.303×10 8
0.156×10 7
0.775×10 2
0.117×10 1
0.427
0.553
0.204×10 8
0.919×10 14
0.102×10 8
0.155×10 8
0.998×10 8
0.884×10 3
0.395×10 2
0.104×10 1
0.306
0.679
0.169×10 14
0.117×10 9
0.523×10 9
0.139×10 8
Figure 9: The probability distributions for robotsR1,R2, andR3 in the left, middle, and right figures, respectively.
The sixth strategy has the highest posterior probability:P(π 1
6)=0.553,P(π 2
6)=0.679. Robot
R1 follows the path{p 28,p 21,p 22,p 23,p 24,p 25,p 26}and communicates this policy intention to
robotR2. RobotR 2 then infers its strategy with probabilities of 0.554 and 0.316, selecting
{p2,p 9,p 16,p 16,p 23,p 23,p 24}to relay to robotR 3. RobotR 3 performs inference based on the
intentions of robotsR 1 andR2, selecting{p 45,p 38,p 31,p 31,p 31,p 31,p 32}. This demonstrates
distributed interactive inference among the robots.
This section details the task execution process for robotR1. While moving to positionp 21, it
encounters an obstacle atp 23. The robot updates its environmental observationOr1
τ at each cycle,
generating logical variables
L1 ={l 1
r1,l 1
r2,l 1
r3,l 1
result}.
RobotR1 assigns a preference value of−1 to the obstacle atp 23, resulting in a conflict be-
tween the obstacle-avoidance and current strategies, asL1{l1
r1}<!Exist(obstacle)(Algorithm 1,
Line 17). RobotR 1 addsL1{l1
r1}to matrixC 1 (Algorithm 1, Line 18) and guides robotsR 1,
R2, andR3 to develop new strategies (Algorithm 1, Line 19). The preference matrix updates
toC1
r1 =C1
r1 +l 1
r1. RobotR 1 then performs inference (Algorithm 1, Line 21), yielding maxi-
mum strategy probabilities of 0.166 and 0.252, leading to the path{p 21,p 14,p 15,p 16,p 17,p 24}.
RobotR2 selects{p 9,p 10,p 17,p 24,p 31,p 32}, and robotR 3 chooses{p 38,p 38,p 31,p 24,p 25,p 26}.
19
An obstacle atp 23 causes robotR1 to switch fromgoal 2 togoal 1, prompting robotsR2 andR3 to
modify their goals and strategies.
0.0
 0.2
 0.4
 0.6
 0.8
 1.0
Probability
0.162×10 20
0.920×10 20
0.100×10 19
0.101×10 26
0.110×10 26
0.327×10 6
0.467
0.441×10 6
0.472×10 6
0.533
0.119×10 19
0.148×10 19
0.161×10 19
0.184×10 26
0.201×10 26
0.568×10 6
0.723×10 6
0.802×10 6
0.857×10 6
0.999
3 Strategies Distribution( 3)
0.0 0.2 0.4 0.6 0.8 1.0
Probability
2 Strategies Distribution( 2)
0.0 0.2 0.4 0.6 0.8 1.0
Probability
1 Strategies Distribution( 1)
0.124×10 6
0.565×10 6
0.567×10 6
0.554
0.153×10 13
0.432×10 14
0.406×10 6
0.446
0.576×10 6
0.586×10 6
0.125×10 1
0.595×10 1
0.177
0.316
0.385×10 7
0.770×10 7
0.924×10 1
0.220
0.606×10 1
0.617×10 1
0.303×10 8
0.156×10 7
0.775×10 2
0.117×10 1
0.427
0.553
0.204×10 8
0.919×10 14
0.102×10 8
0.155×10 8
0.998×10 8
0.884×10 3
0.395×10 2
0.104×10 1
0.306
0.679
0.169×10 14
0.117×10 9
0.523×10 9
0.139×10 8
0.0 0.2 0.4 0.6 0.8 1.0
Probability
)1 Strategies Distribution( 1
0.0
 0.2
 0.4
 0.6
 0.8
 1.0
Probability
0.105×10 12
0.128×10 12
0.138×10 12
0.342×10 13
0.850×10 6
0.999
0.121×10 12
0.133×10 12
0.142×10 12
0.164×10 6
0.105×10 12
0.128×10 12
0.139×10 12
0.162×10 6
0.853×10 6
 0.999
0.121×10 12
0.134×10 12
0.142×10 12
0.388×10 13
3 Strategies Distribution( 3)
R3-R1 R3-R2
R2-R1 R2-R3
R1-R2 R1-R3
0.107
0.123
0.166
0.689×10 1
0.447×10 1
0.548×10 1
0.768×10 1
0.107
0.125
0.127
0.119×10 5
0.128
0.252
0.504×10 5
0.705×10 6
0.117
0.240
0.119×10 5
0.130
0.132
0.0 0.2 0.4 0.6 0.8 1.0
Probability
2 Strategies Distribution( 2)
0.237×10 7
0.249×10 7
0.251×10 7
0.256×10 7
0.163
0.166
0.170
0.165
0.167
0.170
0.206×10 12
0.473×10 12
0.498×10 7
0.100×10 6
0.431×10 5
0.983×10 5
0.165×10 4
0.207×10 5
0.331
0.669
Figure 10: After introducing obstacles into the planned path of robotR1, the distribution of planning strategies for robots
R1,R2, andR3.
When robotR1 moves top 14, a new obstacle atp31 blocks robotR3, while the original obstacle
remains. The preference matrix for robotR 3 updates toC1
r3 =C1
r3 +l 1
r3. The updated strategies
are{p 14,p 15,p 16,p 17,p 24,p 25,p 26}for robotR 1,{p 16,p 17,p 18,p 18,p 25,p 25,p 32}for robotR 2,
and{p 38,p 39,p 32,p 25,p 18,p 17,p 24}for robotR 3.
0.0
 0.2
 0.4
 0.6
 0.8
 1.0
Probability
0.162×10 20
0.920×10 20
0.100×10 19
0.101×10 26
0.110×10 26
0.327×10 6
0.467
0.441×10 6
0.472×10 6
0.533
0.119×10 19
0.148×10 19
0.161×10 19
0.184×10 26
0.201×10 26
0.568×10 6
0.723×10 6
0.802×10 6
0.857×10 6
0.999
3 Strategies Distribution( 3)
0.0 0.2 0.4 0.6 0.8 1.0
Probability
2 Strategies Distribution( 2)
0.0 0.2 0.4 0.6 0.8 1.0
Probability
1 Strategies Distribution( 1)
0.124×10 6
0.565×10 6
0.567×10 6
0.554
0.153×10 13
0.432×10 14
0.406×10 6
0.446
0.576×10 6
0.586×10 6
0.125×10 1
0.595×10 1
0.177
0.316
0.385×10 7
0.770×10 7
0.924×10 1
0.220
0.606×10 1
0.617×10 1
0.303×10 8
0.156×10 7
0.775×10 2
0.117×10 1
0.427
0.553
0.204×10 8
0.919×10 14
0.102×10 8
0.155×10 8
0.998×10 8
0.884×10 3
0.395×10 2
0.104×10 1
0.306
0.679
0.169×10 14
0.117×10 9
0.523×10 9
0.139×10 8
0.0
 0.2
 0.4
 0.6
 0.8
 1.0
Probability
R3-R1 R3-R2
0.102×10 5
 0.999
0.154×10 19
0.165×10 19
0.465×10 20
0.105×10 12
0.144×10 6
0.134×10 12
0.142×10 12
0.159×10 6
0.879×10 6
0.999
0.148×10 19
0.160×10 19
0.186×10 13
0.972×10 13
0.119×10 12
0.130×10 12
0.137×10 12
0.329×10 13
3 Strategies Distribution( 3)
0.0 0.2 0.4 0.6 0.8 1.0
Probability
R1-R2 R1-R3
1 Strategies Distribution( 1)
0.128×10 7
0.338×10 2
0.557×10 2
0.745×10 2
0.435
0.527
0.447×10 2
0.602×10 2
0.661×10 7
0.103×10 1
0.240×10 12
0.865×10 8
0.305×10 7
0.588×10 7
0.425
0.575
0.812×10 7
0.123×10 6
0.720×10 7
0.153×10 6
0.0 0.2 0.4 0.6 0.8 1.0
Probability
R2-R1 R2-R3
2 Strategies Distribution( 2)
0.404×10 6
0.108×10 6
0.555×10 6
0.568×10 6
0.552
0.577×10 13
0.597×10 13
0.165×10 13
0.405×10 6
0.448
0.718×10 6
0.113×10 5
0.223×10 5
0.377×10 5
0.568×10 5
0.116×10 6
0.565×10 1
0.813×10 1
0.354
0.509
Figure 11: After introducing obstacles into the planned path of robotR3, the distribution of planning strategies for robots
R1,R2, andR3.
During task execution, we introduce temporary waypoints for robotsR1 andR3 (see Table I).
These waypoints take precedence overπ i
goal. When robotR 1 moves top 17, robotsR2 andR3
will move top 25 andp 39, respectively. RobotsR 1 andR2 add waypointsp 4 andp 35, with
l1
r1(p4)=2 andl 1
r3(p35)=2. The preference matrices are updated asC 1
r1 =C1
r1 +l 1
r1 andC1
r3 =
C1
r3 +l 1
r3. To minimize free energy and reach their goals simultaneously, robotR1 selects the path
{p17,p 10,p 3,p 4,p 4,p 3,p 10,p 17,p 24}, robotR2 chooses{p 18,p 25,p 25,p 25,p 25,p 25,p 25,p 25,p 32},
and robotR3 adopts{p 25,p 32,p 39,p 39,p 32,p 25,p 25,p 25,p 26}.
Distributed BTs allow interactive BT nodes to cooperate with traditional BT nodes, ensuring
robot coordination and autonomous decision-making. Each robot has aBattery Enough?con-
dition node and aMoveToChargeraction node, enabling them to autonomously detach from the
swarm and reach a charging station when battery levels are low. All strategies must meet the
20
0.0 0.2 0.4 0.6 0.8 1.0
Probability
R1-R2 R1-R3
1 Strategies Distribution( 1)
0.0 0.2 0.4 0.6 0.8 1.0
Probability
R2-R1 R2-R3
2 Strategies Distribution( 2)
0.0 0.2 0.4 0.6 0.8 1.0
Probability
R3-R1 R3-R2
3 Strategies Distribution( 3)
0.394×10 8
0.332
0.115×10 42
0.131×10 55
0.313
0.715×10 36
0.838×10 49
0.355
0.101×10 42
0.119×10 55
0.922×10 63
0.355
0.889×10 63
0.139×10 68
0.323
0.635×10 50
0.101×10 55
0.322
0.133×10 62
0.141×10 68
0.637×10 20
0.690×10 20
0.263×10 20
0.299×10 6
0.311×10 6
0.461
0.345×10 6
0.351×10 6
0.539
0.368×10 6
0.258
0.444×10 6
0.468×10 6
0.245
0.438×10 6
0.459×10 6
0.497
0.548×10 6
0.454×10 6
0.462×10 6
0.110×10 82
0.521
0.479
0.579×10 7
0.260×10 82
0.268×10 82
0.239×10 82
0.235×10 82
0.267×10 82
0.267×10 82
0.111×10 82
0.499
0.501
0.582×10 7
0.249×10 82
0.261×10 82
0.250×10 82
0.261×10 82
0.255×10 82
0.261×10 82
Figure 12: After adding waypoints for robotsR 1 andR3, the distribution of planning strategies for robotsR 1,R2, and
R3.
Battery Enoughprecondition. When a robot runs the interactive nodeMoveToGoal, it returns
arunningstatus each cycle. If the battery is low, theMoveToGoalnode returns afailuresta-
tus, causing disconnection. When theBattery Enough?node shows afailurestatus, the robot
activates theMoveToChargernode. For instance, robotR 1 moves autonomously from position
p4 to Charging Station 1 (p 13), disconnecting from robotsR 2 andR3. RobotsR 2 andR3 wait
forR1 to reconnect after recharging. Once charged,R 1 returns to its position and resumes its
strategy. If robotR 3 detects a low battery atp 25, it goes to Charging Station 3 (p 41) whileR1
andR2 wait. Upon reunion, all three reach their destination together, adjusting their strategies
toπ{1,2,3} =π stop. This experiment shows that robots can effectively use multi-robot interactive
nodes with traditional BT nodes, maintaining coordinated movement and autonomous disen-
gagement during group tasks.
Table 3: QUANTITATIVE COMPARISON OF BEHA VIOR TREE COMPLEXITY
Traditional IIBT-Node Absolute Relative
Component BT Approach Reduction Reduction
Sequence Nodes 5 1 4 80%
Fallback Nodes 4 1 3 75%
Condition Nodes 7 1 6 85%
Action Nodes 5 2 3 60%
Total Nodes 21 5 16 76.2%
Tree Depth 8 2 6 75%
Design Complexity High Low - -
As illustrated in Fig. 8, the proposed IIBT architecture replaces large portions of traditional BT
control logic (shown in blue) with compact interactive inference nodes. These nodes integrate
action selection, precondition checking, and inter-robot coordination within a single probabilis-
tic framework, significantly simplifying the tree topology. While a conventional BT relies on
multiple layers of Sequence, Fallback, and Condition nodes to handle task transitions and re-
covery behaviors, the IIBT node internally performs these functions through belief updates and
free-energy minimization.
To quantitatively evaluate this structural compression, Table 3 compares the node composition
and tree depth of the traditional BT against the proposed IIBT approach. The results confirm that
embedding inference capabilities into BT nodes yields a 76.2% reduction in node count and a
75% decrease in tree depth, effectively transforming a deep, rule-based hierarchy into a compact,
21
adaptive decision structure.
6.2. Multirobot Cooperative Object Placement Real-World Experiment
Building upon the navigation experiments presented in the previous subsection, this study
extends the proposed Interactive Inference Behavior Tree (IIBT) framework from multi-robot
motion coordination to a real-world collaborative object placement task. The goal of this exper-
iment is to quantitatively evaluate the generality and robustness of the IIBT-Node architecture
under physical conditions with sensory noise and actuation uncertainty. Specifically, the exper-
iment aims to verify (1) the practical deployability of the BT-based control framework on real
robot platforms, and (2) the robustness of the interactive inference mechanism when faced with
perceptual and control disturbances.
Within this setup, the robot agents are capable of sharing real-time task states and executing
coordinated movements. The action space for each robot is defined as
U={moveTo(goal),pick(obj),place(obj),idle}.
To encompass the full range of potential cooperative behaviors, a joint strategy setΠ {1,2}is con-
structed as
Π{1,2}={π0,π1,...,π 15},
where each joint policyπi is a tupleπi =(a R1 ,a R2 ), witha Rj ∈U. Accordingly, the complete set
of joint policy combinations is enumerated as:
π0 =(moveTo(goal),moveTo(goal)), π 1 =(moveTo(goal),pick(obj)),
π2 =(moveTo(goal),place(obj)), π 3 =(moveTo(goal),idle),
π4 =(pick(obj),moveTo(goal)), π 5 =(pick(obj),pick(obj)),
π6 =(pick(obj),place(obj)), π 7 =(pick(obj),idle),
π8 =(place(obj),moveTo(goal)), π 9 =(place(obj),pick(obj)),
π10 =(place(obj),place(obj)), π 11 =(place(obj),idle),
π12 =(idle,moveTo(goal)), π 13 =(idle,pick(obj)),
π14 =(idle,place(obj)), π 15 =(idle,idle).
This joint strategy space represents all possible combinations of cooperative actions between
robotsR1 andR2, forming the foundation for subsequent inference-based policy selection.
Within the proposed framework, the update of each robot’s task preference matrixC i is mod-
ulated by its corresponding logical variable set
Li ={l i
loc,l i
hold,l i
place,l i
free}.
The logical variablesLi serve as symbolic representations of discrete task states and act as logical
priors that shape the preference update inCi according to the robot’s perceived progress and task
requirements. The definitions of the hidden state vectorsi and the corresponding logical variables
Li used in this experiment are listed in Table 4.
Table 4: Definitions of Robot Hidden States and Logical Variables (li
· ∈Li)
Hidden State Logical Variable Semantic Description
si
loc li
loc Posterior belief of reaching goal location
si
hold li
hold Posterior belief of grasping the object
si
place li
place Posterior belief of object placement
si
free li
free Posterior belief of being idle or task-free
22
The preconditions and postconditions governing the execution of each robotic action are de-
fined in Table 5. Each action corresponds to a logical transition that determines whether the
associated condition can be executed within the IIBT-Node. Upon action completion, the post-
conditions update the logical variable setLi and indirectly modify the task preference matrixCi
through additive adjustments to the relevant entries.
Table 5: Action specifications with preconditions and postconditions
Action Preconditions Postconditions
moveTo(loc) !IsReached(loc)l i
loc =max(Ci)+1
pick(obj) IsReached(loc)
!IsHolding li
hold =max(Ci)+1
place(obj)
IsReached(loc)
IsHolding(obj)
!IsPlaced(obj,loc)
li
place =max(Ci)+1
As shown, each postcondition reflects the logical progression of the task: for instance, exe-
cutingmoveTo(loc)increases the preference for theIsReached(loc)state (l i
loc), whilepick(obj)
andplace(obj)increment the correspondings i
hold ands i
place beliefs. Theidleaction, in contrast,
maintains thes i
free belief, indicating no active task engagement.
To enable probabilistic reasoning within the IIBT-Node, this section formalizes the probabilis-
tic matrices that constitute the core of the inference process. A likelihood matrixA i is defined
for each robotRi, modeling the conditional relationship between the hidden states i and the ob-
servation matrixOi
τ asP(Oi
τ |s i). This matrix captures how sensory evidence updates the robot’s
belief state under perceptual uncertainty.
Ai =

0.9 0.025 0.025 0.025
0.025 0.9 0.025 0.025
0.025 0.025 0.9 0.025
0.025 0.025 0.025 0.9

.(8)
Each row ofAi corresponds to a specific hidden states i
k and represents the conditional prob-
ability distributionP(Oi
τ |s i
k) over possible observations. For example, the diagonal entry of
0.9 in the first row indicates a 90% probability of correctly perceiving the intended feature (e.g.,
target location) when the system is in that state. The low off-diagonal probabilities (0.025) rep-
resent potential sensor noise or ambiguous observations. The observationO i
τ is thus generated
according to this mapping, providing the basis for Bayesian belief updates during inference.
Subsequently, a set of state transition matricesB i
π is defined to characterize the probabilistic
dynamics ofs i under the execution of each action policyπ. Each elementB i
π(s′|s) denotes the
probability of transitioning from statestos ′givenπ. Together withA i, these transition models
complete the probabilistic generative structure of the IIBT-Node.
Having defined the probabilistic model and action semantics of the IIBT-Node, this section
introduces the specific joint task designed for real-world multi-robot evaluation. As illustrated
in Fig. 13, the experimental environment comprises three key locations: Goal A, Goal B, and
the Rendezvous Point C. This setup extends the previous navigation experiment into a coopera-
tive manipulation domain, enabling evaluation of the proposed framework under more complex
physical interactions and goal dependencies.
23
Two quadruped robots,R1 andR2, are deployed to execute a collaborative object placement
mission. Each robot is equipped with a front-mounted manipulator and onboard cameras for
local perception, allowing them to share task-relevant state information in real time. RobotR 1
is assigned to navigate toward Goal A, grasp a bottle, and transport it to Rendezvous Point C.
RobotR2 is tasked with navigating to Goal B, picking up a plate, and likewise delivering it to
the same rendezvous point. The shared objective is forR 1 to precisely place the bottle onto
the plate held byR2 at Location C, thereby completing the cooperative manipulation task. This
configuration allows both agents to infer and update their action policies from the joint policy set
Π{1,2}={π0,π1,...,π 15}based on their joint observationsO{1,2}
τ and preference matricesC{1,2}.
A
B
C
Robot 2
Robot 1
Bottle
Plate
Figure 13: Experimental scenario illustrating collaborative object grasping and placement by two quadruped robots.
In this cooperative manipulation task, temporal dependencies between actions emerge natu-
rally. A critical inter-robot dependency exists: the successful execution of theplaceaction byR 1
is contingent uponR2 having already positioned the plate at Location C. IfR1 reaches Location
C beforeR2 completes its placement action, it must enter a waiting state (s1
free) until it detects that
the plate is in place. This dependency requires each robot to reason not only about its own latent
states i
τ and selected actiona i
τ, but also to infer and adapt to the evolving strategy of its partner
in real time, thereby encapsulating the core challenge of multi-agent interactive inference.
To evaluate this mechanism, a baseline was implemented using a traditional BT design ap-
proach. Fig. 14 presents the executable BT structures forR 1 andR2, both incorporating the
proposed interactive inference nodes (IIBT-nodes). For robotR 1, the IIBT-nodes includeIsH-
olding(Bottle)andIsPlaced(Bottle,Plate), complemented by conventional BT nodes such as the
conditionIsReached(Goal A)and the actionmoveTo(Goal C). For robotR 2, the IIBT-nodes in-
cludeIsHolding(Plate)andIsMovingWith(Bottle,Plate), together with standard nodes such as the
conditionIsReached(Goal C)and the actionmoveTo(Goal C).
Fig. 15 further illustrates the BTs that are functionally equivalent to the aforementioned IIBT-
nodes, demonstrating the structural expansion required to achieve the same logical expressive-
24
IsReached(Goal C) Construct 
Policy(Obstacle)
IsHolding(Bottle) IsPlacedAt(Bottle,plate)?
Goal CMoveTo( )
IsHolding(Plate) Goal C)MoveTo(
Figure 14: Interactive inference behavior trees for (left) robotR1 and (right) robotR2.
ness in a conventional BT framework. Specifically, the equivalent BT for theIsHolding(Bottle)
node ofR 1 is shown in Fig. 15(a), while the one corresponding toIsPlaced(Bottle,Plate)is
depicted in Fig. 15(b). Similarly, forR 2, the equivalent BTs for theIsHolding(Plate)andIs-
MovingWith(Bottle,Plate)nodes are presented in Fig. 16(a) and Fig. 16(b), respectively. Due to
distinct task objectives, the architectures of the two robots’ BTs differ, highlighting the modular
adaptability of the proposed IIBT framework.
To quantify the efficacy of our approach, Table 6 provides a node count comparison between
the interactive inference BT (Fig. 14(a)) and its functionally equivalent traditional counterpart
(Fig. 15(a)). The conventional implementation necessitates 4 Sequence nodes, 9 Fallback nodes,
9 Condition nodes, and 11 Action nodes, totaling 33 nodes. In contrast, our approach reduces
these requirements to 1 Sequence node, 1 Fallback node, 1 Condition node, and 3 Action nodes,
totaling merely 6 nodes—achieving a compression rate of 81.8%. Furthermore, the structural
depth is compressed from 7 layers to 3 layers, substantially alleviating design complexity.
Similarly, Table 7 presents comparisons for robotR 1’s complete decision-making control
model. The traditional BT requires 10 total nodes, compressed to just 6 nodes using our
method—a 70% reduction—with depth reduced from 5 layers to 2 layers.
（a）
IsReached(Bottle)
IsHolding(Bottle)
MoveTo(Goal C)
IsHolding(Bottle)?
?IsReached(R2,Goal B) Pick(Bottle)
（b）
IsPlacedAt(Bottle,plate)
IsHolding(Bottle)
IsPlacedAt(Bottle,plate)
?
?  IsReached
(R2,Goal C) Place(Bottle,Plate)IsHolding 
(R2,Plate)
Pick(Bottle)
Figure 15: Structural compression comparison of BT nodes for RobotR1.
IsReached(Goal B)
IsHolding(Plate)
MoveTo(Goal B)
IsHolding(Plate)?
? Pick(Plate)
 
IsReached(R1,Goal A)
Figure 16: Structural compression comparison of BT nodes for RobotR2.
At the initial time step (τ=0), both robotsR 1 andR2 are in theidlestate. Their joint
observation matrixO{1,2}
τ=0 , composed of individual observation vectorsO1
τ=0 andO2
τ=0 and a task
result observationOresult
τ=0 , is defined as:
25
Table 6: QUANTITATIVE COMPARISON OFR1 BEHA VIOR TREE COMPLEXITY
Traditional Ours Absolute Relative
Component BT BT Reduction Reduction
Sequence Nodes 4 1 3 75%
Fallback Nodes 9 1 8 88.9%
Condition Nodes 9 1 8 88.9%
Action Nodes 11 3 8 73%
Total Nodes 33 6 27 81.8%
Tree Depth 7 3 4 57.1%
Design Complexity High Low - -
Table 7: QUANTITATIVE COMPARISON OFR2 BEHA VIOR TREE COMPLEXITY
Traditional Ours Absolute Relative
Component BT BT Reduction Reduction
Sequence Nodes 2 1 1 50%
Fallback Nodes 2 0 2 100%
Condition Nodes 3 0 3 100%
Action Nodes 3 2 1 33.3%
Total Nodes 10 3 7 70%
Tree Depth 5 2 3 60%
Design Complexity High Low - -
O{1,2}
τ=0 =

0 0 0 1
0 0 0 1
0 0 1−
,(9)
where each row corresponds to one robot’s observation vectorOi
τ=0 =[o i
loc,o i
hold,o i
place,o i
free].
A value of “1” in the last column indicates that both robots are currently in thes i
free state. This
observation serves as the initial condition for belief inference within the IIBT-node, from which
each agent begins reasoning about its subsequent actions based onC{1,2}andΠ {1,2}.
At this initial stage, the value “1” inO {1,2}
τ=0 indicates that both robots are in thes i
free state,
signifying that the task execution has not yet begun. Under this condition, the IIBT-Node of
R1 directs its reasoning focus toward the interactive nodeIsHolding(Bottle), whileR 2 remains
inactive to prevent potential conflicts arising from parallel policy execution. To formalize this
coordination logic, a joint extrinsic preference matrixC{1,2}is defined as follows:
C1 =

0 1 0 0
0 0 0 1
1 0 0−
.(10)
Each row inC 1 corresponds to one robot’s preference distribution over the logical state set
{lloc,l hold,l place,l free}. The first row representsR 1’s extrinsic preference, encouraging transition
towards 1
hold, while the second row expressesR2’s preference to remain “free.” During inference,
R1 will thus select thepick(obj)action only wheno 2
free =1, thereby establishing a logically
consistent division of labor between the two robots.
26
However, the current observational stateO{1,2}
τ ,{l i
loc,l i
free}(Algorithm 1, line 17), indicating
that the preconditionl i
loc for thepick(obj)action is not yet satisfied. Consequently,C i must be
adaptively adjusted through logical evidenceLi to align the desired goal state with environmental
constraints. This adaptive adjustment mechanism forms the foundation of interactive inference
within the IIBT framework.
To satisfy the unsatisfied preconditionli
loc forpick(obj), the originalCi is augmented by a sup-
plementary logical constraint matrixLi, introducing the necessary condition for spatial reacha-
bility:
Li =

2 0 0 0
0 0 0 0
0 0 0−
.(11)
The entry “2” in the first row explicitly encodes a higher-priority constraint on the
moveTo(goal)action. This adjustment ensures that, given the current observation, the agent
prioritizes navigating to the goal location before attempting to grasp the object. Consequently,
the updated comprehensive preference matrix becomes:
Ci =Ci +Li =

2 1 0 0
0 0 0 1
1 0 0−
.
This update demonstrates thatC i evolves dynamically during task execution, incorporating
contextual information and logical constraints derived fromO i
τ andLi. Following Expected
Free Energy (EFE) inference overΠ {1,2} ={π 0,π1,...,π 15}, the IIBT-node determines the MAP
joint action asπ 3 =(moveTo(goal),idle), withP(π 3)=0.255 (Fig. 17). This indicates thatR 1
navigates whileR2 remains idle, achieving conflict-free coordination.
Figure 17: Probability distribution over joint strategies after EFE inference.
Upon completion ofmoveTo(goal), the temporary logical constraintLi is removed:
Ci =Ci −Li =

0 1 0 0
0 0 0 1
1 0 0−
.
27
EFE inference is then repeated, yieldingπ 7 =(pick(obj),idle) withP(π 7)=0.205 (Fig. 18).
This drivesR1 to executepick(obj), demonstrating dynamic re-evaluation and adaptive coopera-
tion.
Figure 18: Updated probability distribution over joint strategies after completion of the moveTo(goal) action.
Next, robotR2 enters the interactive nodeIsHolding(Plate). It constructs its local constraint
matrixL2, encoding both intrinsic preconditions and inter-agent requirements:
L2 =

0 0 0 0
0 2 0 0
0 0 0−
.(12)
The first row imposes the dependency thatR 1 must be free beforeR 2 executespick(plate),
while the second encodesR 2’s intrinsic preference for holding. EFE inference overΠ {1,2} pro-
duces distributions shown in Fig. 19 and Fig. 20.
Figure 19: Probability distribution over joint strategies forR2 after completingmoveTo(goal).
The selected joint strategies areπ 12 =(idle,moveTo(Goal B)) andπ 13 =(idle,pick(plate)),
withP(π 12)=0.255 andP(π 13)=0.205. Afterward, both robots executemoveTo(Goal C).
28
Figure 20: Updated probability distribution over joint strategies forR2 afterpick(plate).
Upon arrival,R1 activatesIsPlaced(Bottle,Plate), triggering another inference iteration to evalu-
ate placement readiness (Fig. 21).
Figure 21: Final probability distribution over joint strategies during the collaborative object placement phase.
The final preference matrixC2 is updated as:
C2 =

0 1 0 0
0 1 0 0
1 0 0−
.
Here, the first row denotesR 1’s elevated preference forplace(obj), while the second main-
tainsR2’s preference fors 2
free, ensuring stability during placement. Through iterative inference
and preference adjustment, the IIBT-Node effectively coordinates inter-robot dependencies and
converges toward successful task completion.
29
7. Conclusion
This work presented the Interactive Inference Behavior Tree (IIBT-Node), a unified control
node that integrates active inference with the modular architecture of Behavior Trees (BTs) for
decentralized multi-robot cooperation. By embedding a dynamic preference matrix within each
node, the proposed framework enables robots to infer, adapt, and coordinate their actions under
uncertainty while preserving the interpretability and modularity of BTs. Extensive validation
was conducted through both simulation and physical experiments using quadruped robots. In
the simulated multi-robot navigation tasks, the IIBT-Node reduced the BT structural complexity
by 76.2%, while in real-world collaborative manipulation experiments, an equivalent reduction
ranging from 70% to 81.8% was achieved. These results confirm that the proposed approach
generalizes effectively across different action spaces, maintaining consistent reasoning perfor-
mance and robust coordination under partial observability. Overall, the IIBT-Node provides a
scalable and interpretable mechanism for multi-robot systems to achieve autonomous coopera-
tion and conflict-free decision-making. Future work will focus on extending the framework to
heterogeneous robot teams and exploring real-time learning of preference matrices in large-scale
environments.
References
[1] P. Lanillos, C. Meo, C. Pezzato, A. A. Meera, M. Baioumy, W. Ohata, A. Tschantz, B. Mil-
lidge, M. Wisse, C. L. Buckleyet al., “Active inference in robotics and artificial agents:
Survey and challenges,”arXiv preprint arXiv:2112.01871, 2021.
[2] M. Priorelli, I. P. Stoianov, and G. Pezzulo, “Embodied decisions as active inference,”
bioRxiv, pp. 2024–05, 2024.
[3] G. Pezzulo, T. Parr, and K. Friston, “Active inference as a theory of sentient behavior,”
Biological Psychology, p. 108741, 2024.
[4] K. J. Friston, T. Parr, C. Heins, A. Constant, D. Friedman, T. Isomura, C. Fields, T. Ver-
belen, M. Ramstead, J. Clippingeret al., “Federated inference and belief sharing,”Neuro-
science&Biobehavioral Reviews, p. 105500, 2023.
[5] D. Maisto, F. Donnarumma, and G. Pezzulo, “Interactive inference: a multi-agent model of
cooperative joint actions,”IEEE Transactions on Systems, Man, and Cybernetics: Systems,
2023.
[6] N. Wirkuttis and J. Tani, “Leading or following? dyadic robot imitative interaction using
the active inference framework,”IEEE Robotics and Automation Letters, vol. 6, no. 3, pp.
6024–6031, 2021.
[7] D. A. Friedman, A. Tschantz, M. J. Ramstead, K. Friston, and A. Constant, “Active in-
ferants: An active inference framework for ant colony behavior,”Frontiers in behavioral
neuroscience, vol. 15, p. 647732, 2021.
[8] A. Clodic and R. Alami, “What is it to implement a human-robot joint action?”Robotics,
AI, and humanity: Science, ethics, and policy, pp. 229–238, 2021.
30
[9] M. Ghallab, D. Nau, and P. Traverso, “The actor’s view of automated planning and acting:
A position paper,”Artificial Intelligence, vol. 208, pp. 1–17, 2014.
[10] M. Nixon, R. B. Havekost, L. O. Jundt, M. G. Ott, A. Webb, D. Stevenson, M. Lucas, and
K. J. Beoughter, “Process control system using a control strategy implemented in a layered
hierarchy of control modules,” Jan. 19 1999, uS Patent 5,862,052.
[11] S. S. O. Venkata, R. Parasuraman, and R. Pidaparti, “Kt-bt: A framework for knowledge
transfer through behavior trees in multirobot systems,”IEEE Transactions on Robotics,
2023.
[12] R. Hull, D. Moratuwage, E. Scheide, R. Fitch, and G. Best, “Communicating intent as
behaviour trees for decentralised multi-robot coordination,” in2024 IEEE International
Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 7215–7221.
[13] S. Gugliermo, E. Schaffernicht, C. Koniaris, and F. Pecora, “Learning behavior trees from
planning experts using decision tree and logic factorization,”IEEE Robotics and Automa-
tion Letters, vol. 8, no. 6, pp. 3534–3541, 2023.
[14] M. Colledanchise, D. Almeida, and P. Ögren, “Towards blended reactive planning and act-
ing using behavior trees,” in2019 international conference on robotics and automation
(ICRA). IEEE, 2019, pp. 8839–8845.
[15] Y . Luo, H. Bai, D. Hsu, and W. S. Lee, “Importance sampling for online planning under
uncertainty,”The International Journal of Robotics Research, vol. 38, no. 2-3, pp. 162–181,
2019.
[16] J.-H. Kim and P. Vadakkepat, “Multi-agent systems: a survey from the robot-soccer per-
spective,”Intelligent Automation&Soft Computing, vol. 6, no. 1, pp. 3–17, 2000.
[17] J. Li, C. Hua, H. Ma, J. Park, V . Dax, and M. J. Kochenderfer, “Multi-agent dynamic
relational reasoning for social robot navigation,”arXiv preprint arXiv:2401.12275, 2024.
[18] C. Pezzato, C. H. Corbato, S. Bonhof, and M. Wisse, “Active inference and behavior trees
for reactive action planning and execution in robotics,”IEEE Transactions on Robotics,
vol. 39, no. 2, pp. 1050–1069, 2023.
[19] K. J. Friston, T. Parr, C. Heins, A. Constant, D. Friedman, T. Isomura, C. Fields, T. Ver-
belen, M. Ramstead, J. Clippingeret al., “Federated inference and belief sharing,”Neuro-
science&Biobehavioral Reviews, vol. 156, p. 105500, 2024.
[20] S. Wakayama, A. Candela, P. Hayne, and N. Ahmed, “Active inference in contextual multi-
armed bandits for autonomous robotic exploration,”arXiv preprint arXiv:2408.04119,
2024.
[21] L. Bramblett, J. Reasoner, and N. Bezzo, “Implicit coordination using active epistemic
inference,”arXiv e-prints, pp. arXiv–2501, 2025.
[22] M. Colledanchise and L. Natale, “On the implementation of behavior trees in robotics,”
IEEE Robotics and Automation Letters, vol. 6, no. 3, pp. 5929–5936, 2021.
31
[23] X. Li, Y . Li, J. Zhang, X. Xu, and D. Liu, “Embedding multi-agent reinforcement learning
into behavior trees with unexpected interruptions,”Complex&Intelligent Systems, vol. 10,
no. 3, pp. 3273–3282, 2024.
[24] R. Liu, G. Wan, M. Jiang, H. Chen, and P. Zeng, “Autonomous robot task execution in
flexible manufacturing: Integrating pddl and behavior trees in ariac 2023,”Biomimetics,
vol. 9, no. 10, p. 612, 2024.
[25] E. Scheide, G. Best, and G. A. Hollinger, “Synthesizing compact behavior trees for proba-
bilistic robotics domains,”Autonomous Robots, vol. 49, no. 1, p. 3, 2025.
[26] S. Gugliermo, D. C. Dominguez, M. Iannotta, T. Stoyanov, and E. Schaffernicht, “Evaluat-
ing behavior trees,”Robotics and Autonomous Systems, vol. 178, p. 104714, 2024.
[27] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, “Variational inference: A review for statis-
ticians,”Journal of the American statistical Association, vol. 112, no. 518, pp. 859–877,
2017.
[28] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul, “An introduction to variational
methods for graphical models,”Machine learning, vol. 37, no. 2, pp. 183–233, 1999.
[29] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” inInternational Confer-
ence on Learning Representations (ICLR), 2014.
[30] K. Friston, “The free-energy principle: a unified brain theory?”Nature reviews neuro-
science, vol. 11, no. 2, pp. 127–138, 2010.
[31] K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, G. Pezzuloet al., “Active inference
and learning,”Neuroscience&Biobehavioral Reviews, vol. 68, pp. 862–879, 2016.
[32] T. Parr and K. J. Friston, “Generalised free energy and active inference,”Biological cyber-
netics, vol. 113, no. 5, pp. 495–513, 2019.
[33] G. Pezzulo, F. Rigoli, and K. Friston, “Active inference, homeostatic regulation and adap-
tive behavioural control,”Progress in neurobiology, vol. 134, pp. 17–35, 2015.
[34] P. Schwartenbeck, J. Passecker, T. U. Hauser, T. H. FitzGerald, M. Kronbichler, and K. J.
Friston, “Computational mechanisms of curiosity and goal-directed exploration,”elife,
vol. 8, p. e41703, 2019.
32