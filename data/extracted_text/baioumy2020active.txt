Active Inference for Integrated State-Estimation, Control, and Learning.
Mohamed Baioumy Paul Duckworth Bruno Lacerda Nick Hawes
Abstract—Thisworkpresentsanapproachforcontrol,state-
estimation and learning model (hyper)parameters for robotic
manipulators. It is based on the active inference framework,
prominent in computational neuroscience as a theory of the
brain,wherebehaviourarisesfromminimizingvariationalfree-
energy. First, we show there is a direct relationship between
active inference controllers, and classic methods such as PID
control.Wedemonstrateitsapplicationforadaptiveandrobust
behaviour of a robotic manipulator that rivals state-of-the-art.
Additionally, we show that by learning specific hyperparame-
ters, our approach can deal with unmodeled dynamics, damps
oscillations, and is robust against poor initial parameters. The
approach is validated on the ‘Franka Emika Panda’ 7 DoF
manipulator.Finally,wehighlightlimitationsofactiveinference
controllers for robotic systems.
Supplementary Material
Fig. 1: The Franka Emika Panda 7 DoF manipulator real
https://github.com/MoBaioumy/active_
robot (left) and simulated robot in Gazebo (right).
inference_panda_paper.
I. INTRODUCTION
controllers (such as a PID [14]) require a target state a
It is crucial for intelligent robots to be able to adapt to withlong the current state. A PID controller then provides
the presence of unmodeled dynamics. This is necessary for a control law to reach the target from the current state. In
applicationssuchasaerialvehiclesencounteringunpredicted theAIChowever,state-estimationrequiresthetargetandthe
wind dynamics [1], manipulators handling objects of un- observationswhichisusualfromacontrolperspective;those
knownweight[2],andautonomousvehiclesonslipperyroad areusedtoproducesa‘biased’belief.Thecontrolpartofthe
surfaces [3]. Humans are skilled in this regard and recent AICrequirestheobservationandthe‘biased’belief.Itisthus
work in robotics has taken inspiration from active inference unclear how to systematically tune such a controller (since
[4], a neuroscientific theory of the brain and behaviour. tuning the controller affects state-estimation). Furthermore,
Active inference provides a framework for understanding even slight changes to the parameters of the state-estimation
decision-makingofbiologicalagentswhereoptimalbehavior component could cause the controller to be unstable.
arises from minimising variational free-energy: a measure Thispaperhasthreecontributions.First,wepresentafor-
of the fit between an internal model and (past) sensory mulationwhichincludesatemporalparameterτ.Second,we
observations.Additionally,agentsactinafashionthatfulfills highlight theoretical results showing the exact relationship
prior beliefs about preferred future observations [5]. This betweenourformulationandexistingmethods.Forexample,
frameworkhasbeenemployedtoexplainandsimulateawide if our introduced parameter τ approaches zero, the approach
range of complex behaviors including abstract rule learning, converts to a classic PID controller. When τ → ∞, the
speech generation, planning and navigation [6], [7], [4]. approach converts to a filter. If τ is set to the unity, the for-
The active inference controller (AIC) [8], allows an agent mulationpresentedin[8]isrecovered.Thethirdcontribution
to jointly perform state-estimation and control by minimiz- is presenting a method to automatically tune the controller.
ing a single quantity: variational free-energy. Subsequent We show that the gains of the controller correspond to the
work has utilized the AIC in robotics [9], [10], [11]. The covariances of the observation model and that learning the
control and state-estimation parts are coupled and the AIC optimal covarinaces results in learning the optimal gains for
architecture is unusual compared to methods commonly the controller. However, the covariances do not converge
used in robotics. Usually, when performing state-estimation when the systems reaches the target. We thus present an
(for instance using a Kalman or particle filter [12], [13]) alternative by learning the introduced temporal parameter τ.
observations are required. The aim is then to find a belief This reduces oscillations and improves robustness.
state that is close to the true (hidden) state. Additionally, As a running example throughout the paper, the mass-
spring-damper [15] system is used. However, we validate
Authors are with the Oxford Robotics Institute, University of Oxford,
theapproachona‘FrankaEmikaPanda’7DoFmanipulator
United Kingdom. For correspondence: {mohamed, pduckworth, bruno,
nickh}@robots.ox.ac.uk in the results section.
1202
raM
03
]OR.sc[
2v49850.5002:viXra
II. RELATEDWORK isalsooftenreferredtoastheevidencelowerbound(ELBO)
in the Machine Learning community2. If we choose q(s)
There are only a few attempts to use active inference in
to be a Gaussian distribution with mean µ, and utilize the
robotics and control. In [16] a simulated PR2 robot is con-
Laplaceapproximation[30],thefree-energyexpressionfrom
trolled by open-loop active inference but the computational
Equation 1 simplifies to:
complexity made an online implementation unfeasible. In
[17], the authors use free-energy minimization for adaptive
F ≈−lnp(µ,o). (2)
body perception. The same authors extended their work to
include control in [9]. In [10] an implementation of active The expression for F is solely dependent on one param-
inference is presented with real hardware on a humanoid eter, µ, which is referred to as the ‘belief state’ or simply
robot capable of performing reaching behaviors with both ‘belief’. The objective is to find the µ which minimizes F
arms along with active head object tracking in the presence andthusminimizingthedivergencebetweenq(s)andp(s|o).
noisyobservations.Thisworkisextendedin[18]usingdeep For a robotic manipulator the set of observations (o) and
neural networks as function approximators. beliefs (µ) are vectors with length depending on the number
In [11] an approach for control of robotic manipulators of degrees of freedom.
is presented. Adaptive behaviouris demonstrated against the Generalised motions (GM) [31] are used to represent the
state-of-the-art model-reference adaptive control (MRAC) 1. (belief) states of a dynamical system, using increasingly
Additionally, in [21], the authors use active inference to higher order derivatives of the system state. This means
achieve fault-tolerant control for sensory faults [22], [23]. that the n-dimensional state µ and its higher order time
The robot can detect whether its sensors are faulty and derivativesarecombinedinµ˜ (i.e.µ˜ =[µ,µ(cid:48)...]).Similarly,
the control law is automatically adjusted to account for the observations are combined as ˜o = [o,o(cid:48) ...]. In the context
detected faults. In [24], free-energy minimization is used to ofaroboticmanipulator,omayrepresentthesensoryobser-
improve state-estimation under coloured noise. In [25], [26], vation of joint positions, while o(cid:48) represents the observation
[27] active inference with factor graphs is proposed as an of joint velocities. Similarly, µ represents the belief about
effective control method; however, results are provided only all joint positions and µ(cid:48) the joint velocities.
for a simulated toy example.
B. Observation model and state transition model
III. ACTIVEINFERENCEFRAMEWORK Taking generalized motions into account, the joint proba-
bility from Equation (2) can be written as:
Thissectionintroducesactiveinferenceasageneralframe-
workandderivesthekeyequationsforfree-energy(F).Free-
energy is used in later sections to achieve state-estimation,
p(o˜,µ˜)=p(o˜|µ˜)p(µ˜)=p(o|µ)p(o(cid:48)|µ(cid:48))p(µ(cid:48)|µ)p(µ(cid:48)(cid:48)|µ(cid:48)),
control and hyperparameter learning. (3)
where p(o|µ) is the probability of receiving an observation
A. Variational free energy
o while in (belief) state µ, and p(µ(cid:48)|µ) is the state tran-
We consider an agent in a dynamic environment that re-
sition model (also referred to as the dynamic model or the
ceivesobservationoaboutthehiddenstates.Givenamodel
generative model). The state transition model predicts the
of the agent’s world, Bayes’ rule can be used to find p(s|o).
state evolution given the current state. These distributions
However,thenormalizationtermp(o)inBayes’ruleinvolves
are assumed Gaussian according to:
calculating an integral making calculations of all but trivial
p(o|µ)=N(g(µ),Σ ), p(o(cid:48)|µ(cid:48))=N(g(cid:48)(µ(cid:48)),Σ ),
examples infeasible. Instead, the agent can approximate the o o(cid:48)
posterior distribution p(s|o) with a variational distribution q p(µ(cid:48)|µ)=N(f(µ),Σ ), p(µ(cid:48)(cid:48)|µ(cid:48))=N(f(cid:48)(µ(cid:48)),Σ ),
µ µ(cid:48)
overstates,whichwecandefinetohaveasimplerform(such (4)
as a Gaussian). The goal is then to minimize the difference
where the functions g(µ) and g(cid:48)(µ(cid:48)) represent a mapping
between the two distributions. The mismatch between the
between observations and states. For many applications in
twodistributioncanbecomputedusingtheKullbackLeibler
robotics the state is directly observable. For instance, in the
(KL) divergence [28], [29]:
context of a robotic manipulator the state consists of the
positions and velocities of all joints and the manipulator
(cid:90) q(s) is provided with position and velocity encoders. Thus we
KL(q(s)||p(s|o))= q(s)ln ds+lnp(o)
p(s,o) (1) assume: g(µ) = µ and g(cid:48)(µ(cid:48)) = µ(cid:48). The functions f(µ)
=F +lnp(o). and f(µ(cid:48)) represent the evolution of the belief state over
time. We encode the agent’s target state µ in f(µ). Our
d
The quantity F is referred to as the (variational) free-
contribution is to introduce τ as part of these funcitons:
energy and minimizing F minimizes the KL-divergence. F
f(µ) = (µ −µ)τ−1 and f(cid:48)(µ(cid:48)) = τ−1µ(cid:48), where µ is
d d
the desired state and τ a time scale (explained in Section
1Model-referenceadaptivecontrol[19],findsacontrollawthatwillguide
the system to behave as specified by a chosen reference model. Another IV-D).
commonmethodusedforadaptivecontrolis‘self-tuningadaptivecontrol’,
whichrepresenttherobotasalineardiscrete-timemodelandestimatesthe 2Minimizing the ELBO and thus the KL divergence is common in
unknownparametersonline,substitutingtheminthecontrollaw[20]. variationalinference,amethodforapproximatingprobabilitydensities[28].
To simplify notion, we define the following error terms:
ε =µ(cid:48)−(µ −µ)τ−1, ε =µ(cid:48)(cid:48)+τ−1µ(cid:48), ε =o−µ,
µ d µ(cid:48) o 1.0
ε =o(cid:48)−µ(cid:48). Now that all the terms have been defined, F
o(cid:48)
can be expanded to: 0.5
F = 1 2 (cid:88)(cid:0) ε(cid:62) i Σ− i 1ε i +ln|Σ i | (cid:1) +C, (5) 0.0
i
wherei∈ {o, o(cid:48), µ, µ(cid:48)},C isaconstant,andthecovariance −0.5
matricesaredefinedinEquation4.Equation(5)differsfrom
−1.0
the work presented in [11], [10] in the terms with ln|Σ|,
0 2 4 6 8 10 12 14
which are not explicitly included but rather added to the Time (s)
constant.Thesignificanceofthisdifferenceishighlightedin
Section V-A.
IV. STATE-ESTIMATIONANDCONTROL
We now introduce how to perform state-estimation and
control by minimizing F. We show how the estimation step
biases the belief towards the goal state. The control step
then steers the system from its observation o to its (biased)
belief µ. In Section IV-E we show that if τ−1 → ∞, the
approach converts to a classic PID controller. Additionally,
if τ−1 →0, the approach reduces to a filter.
A. State estimation by minimizing free-energy
Estimating the state of our system is achieved by finding
a value µ that minimizes F. Gradient descent is a simple
way to accomplish that:
∂F
µ˜˙ =Dµ˜−κ , (6)
µ∂µ˜
whereκ isatuningparameterandD istemporalderivative
µ
operator(Dµ=µ(cid:48)).Thedotreferstothedifferencebetween
twotime-steps(µ˙ =µ[t+1]−µ[t]).UsingEquation(6)the
agent takes one-step in the gradient descent at every time-
step. In this case the equation expands to:
µ˙ =µ(cid:48)+κ Σ−1ε −τ−1κ Σ−1ε
µ o o µ µ µ
µ˙(cid:48) =µ(cid:48)(cid:48)+κ µ Σ− o(cid:48) 1ε o(cid:48) −κ µ Σ− µ 1ε µ −τ−1κ µ(cid:48) Σ− µ(cid:48) 1ε µ(cid:48) (7)
µ˙(cid:48)(cid:48) =−κ Σ−1ε
µ(cid:48) µ o(cid:48)
Thefirstequationstatesthatbeliefisrefinedusingtheterm
κ Σ−1ε which moves our new belief towards the value
µ o o
justobserved.Additionally,thetermτ−1κ Σ−1ε ,shiftsthe
µ µ µ
belief towards the target µ since ε =µ(cid:48)−(µ −µ)τ−1.
d µ d
Essentially, this ‘biases’ the belief towards preferred future
states (target state µ ). The degree to which the system is d
biased depends on the the values τ−1 and Σ−1.
µ
B. Control by minimizing free-energy
Next to state-estimation, the agent can apply an action (cid:97)
to the environment. In the context of a robotics manipulator
this is a vector containing torques of all joints. To find the
control action which minimizes F, gradient descent is used:
∂F ∂F ∂o˜
a˙ =−κ =−κ , (8) a∂a a∂o˜ ∂a
)m(
noitisoP
Belief (μ(t)) about position (x) for different values of τ−1
Reference μ(τ−1=2)
x (True position) μ(τ−1=8)
μ(τ−1=0.1)
Fig. 2: Separately performing state-estimation for different
values of τ−1. Higher values of τ−1 result in more bias.
where κ is a tuning parameter. The term ∂o˜ is assumed
a ∂a
linear, and equal to the identity matrix (multiplied by a
constant) similar to existing work [11], [10]. Actions are
then computed as:
a˙ =−κ (Σ−1(o−µ)+Σ−1(o(cid:48)−µ(cid:48))). (9) a o o(cid:48)
This controller essentially steers the system from its ob-
served state o to its (biased) belief µ.
Note how the current control law does not contain any
information about the dynamical system, it is thus a reactive
controller. The control law only requires o and µ. This con-
troller thus operates in the presence of unmodeled dynamics
similar to a PID controller.
C. Simultaneous state-estimation and control
Ourapproachperformsstateestimationandcontrolsimul-
taneously. The estimation and control step are dependent.
This is because the estimation step biases the belief µ
towards the target µ . The controller then steers the system
d
from the observation o to the biased estimated state µ. If
τ−1 and Σ−1 are large, the belief µ is biased more towards
µ
the target µ . d
Toillustratethis,considerthemass-spring-dampersystem
given by the equation: x¨=a(t)−k x−k x˙, where s is the 1 2
position of the mass, a(t) the control action, k the spring
1
constant (set to 1N/m), k the damping coefficient (set to
2
0.1Ns/m) and the system has unit mass. It’s simulated
with initial conditions x(0) = −0.5m, s˙(0) = −1m/s
and a(t) = 0N. Equations (7) are used to perform state
estimation. To challenge the system, the initial beliefs are
inaccurate (µ(0)=0m and µ(cid:48)(0)=−1.5m/s). The system
is simulated for different values of τ−1 and presented in
Figure 2.
It is clear that higher values of τ−1 provide more bias
towards the target. For τ−1 = 8 (green line in Figure 2),
the estimate is close to the target (black dashed line) and far
away form the actual position (blue dashed line) as opposed
to setting τ−1 = 0.1 (red line), the belief is closer to the
real trajectory (the latent state). If τ−1 → 0, the estimation
step reduces to a pure estimator, which would follow the
trajectory without any bias towards the target.
1.25
1.00
0.75
0.50
0.25
0.00
−0.25
−0.50
−0.75
0 2 4 6 8 10 12 14
Time (s)
)m(
noitisoP
Figure 2 shows that for very small values of τ−1, the belief
Control behaviour for different values of τ−1
is close to the latent state (true position) without bias.
V. LEARNINGTHEHYPERPARAMETERS
We have shown that state estimation and control can be
performed using gradient decent on the free-energy F. In
additiontousinggradientdecentofthefreeenergyforstate-
Reference estimation and control, we apply it to learn the hyperparam-
x(τ−1=0.1)
eters online. Previous work such as [11] does not update
x(τ−1=1.2)
x(τ−1=2.4) the hyperprameters online which make the performance
extremelysensitivetoinitialization,pronetoinstabilitiesand
reduces overall performance.
Fig. 3: True position (x) for different values of τ−1. Higher
A. Learning model variances
values of τ−1 provide more bias towards the target and thus
As illustrated in Section IV-E, the model variances Σ−1
more aggressive control causing overshoot oscillations. o
andΣ−1canbeconsideredasgainsforthecontroller,similar
o(cid:48)
to the ‘P’ and ‘I’ gains in a PID controller. Additionally, the
Enabling control steers the system to its target. The τ−1 values Σ− µ 1 and Σ− µ(cid:48) 1 affect how much the estimation step
biases the controller towards the desired position.
in this case affects how aggressive the controller is. Larger
WecanupdateΣ andΣ usinggradientdecentonF as:
values give more bias towards the target, the term (o−µ) o o(cid:48)
is larger and thus the controller is more aggressive. Figure 3 ∂F ∂F
shows an illustration for different values of τ−1. Σ˙ o =−κ σ∂Σ , Σ˙ o(cid:48) =−κ σ∂Σ . (10)
o o(cid:48)
D. Understanding the temporal parameter τ The presented update rules have several practical issues.
First, in any high dimensional case, Σ would be a matrix.
The generative model specified by Equations (3) and (4) o
Since in most equations presented so far, the inverse Σ−1
include the function f(µ) which determines how the belief o
state evolves over time i.e. f(µ)=(µ −µ)τ−1. is used, updating the covariance matrix using Equations 10
d
then inverting it would be computationally expensive. A
The belief state is specified to evolve over time as the
workaroundistosimplyupdatetheinversecovariancematrix
derivative between the current belief µ and target µ . This
d
(the precision matrix), as done in [32]:
can be evaluated as the (µ - µ) divided by a time scale τ.
d
Thesmallerτ,thelargerthederivative.Ifτ approacheszero ∂F ∂F
(τ−1 →∞),thevaluef(µ)approaches∞.Asaresults,the Σ˙− o 1 =−κ σ ∂Σ−1 , Σ˙− o(cid:48) 1 =−κ σ ∂Σ−1 . (11)
belief is infinitely biased towards the target and µ≈µ . o o(cid:48)
d
Additionally, a lower-bound on the diagonal elements is
E. Relationship to a classic PID Controller and filters set to keep the matrix positive semi-definite as suggested in
A classic PID controller defines an error term e=(µ − [33].
d
o). The control action is then chosen as: We demonstrate the effect of updating the covarince by
keeping Σ−1 fixed at 0.5 and Σ−1 will be varied. If Σ−1 is
(cid:90) de µ(cid:48) µ µ
a=P ·e+I edt+D , toohigh,thesystemssuffersfromoscillationsandovershoot.
dt However, if Σ−1 and Σ−1 are updated during run-time, the
o o(cid:48)
where P, I and D are tuning parameters. controller shows improved behaviour. Results are shown in
Forthecontrollawdefinedbyactiveinference,our(o−µ) Figure 4. The convergence of Σ−1 occurs when ∂F = 0.
is similar to the error term. Additionally, as explained in the Since the observations change a o nd have a certai ∂ n Σ− ole 1 vel of
previous section, when τ−1 → ∞ then µ ≈ µ . Now the noise, Σ converges to the expected value of ε ε(cid:62). This
d o o o
control law of active inference can be rewritten in terms of does not necessarily happen upon reaching the target state.
the error term as:
B. Learning the temporal parameter
de
a˙ =κ Σ−1e+κ Σ−1 . We previously showed the importance importance of
a o a o(cid:48) dt
choosingappropriatevaluesforτ−1:Ifthevalueistoohigh,
This means than if τ−1 → ∞, the active inference the controller suffers from overshoot and oscillations, see
controller is equivalent to a PI Controller i.e. PID with Figure 3. On the other hand, a low value results in a slow
D = 0, a P gain of κ a Σ− o(cid:48) 1 and an I gain of κ a Σ− o 1. If response. Ideally, the value for τ would be high towards the
one considers the generalized motions (section III) up to a start but decrease as the system reaches the target. This is
thirdorder,thecontrollawwouldincludeanon-zeroDterm. the value that minimizes F and can be found using gradient
The relationship to a pure estimator (plain filter) is descent on F as:
straightforward. As previously mentioned, if τ−1 → 0, the
estimation step reduces to a filter. Essentially, this indicates ∂F
=−2Σ ε (µ −µ)+2Σ ε µ(cid:48). (12)
that the estimation step has zero bias towards the target. As ∂τ−1 µ(cid:48) µ d µ(cid:48) µ(cid:48)
1.00
0.75
0.50
0.25
0.00
−0.25
−0.50
0 2 4 6 8 10 12 14
Time (s)
)m(
noitisoP
Control behaviour when updating control variances
Reference
Σμ=0.1 no update
Σμ=0.6 no update
Σμ=0.6 with update
Fig.4:Effectofupdatingthethecontrolvariances(Σ−1 and
o
Σ−1).WhenthevalueofΣ−1isinitializedathighvalues,the
o(cid:48) µ
systemoscillates.UpdatingΣ−1 andΣ−1 essentially‘tunes’
o o(cid:48)
the controller and ensures robust performance.
1.25
1.00
0.75
0.50
0.25
0.00
−0.25
−0.50
0 2 4 6 8 10 12 14
Time (s)
)m(
noitisoP
parts of the robotic system, this could be problematic.
Additionally, the terms Σ−1 and Σ−1 are defined as part
o o(cid:48)
of the sensory model. In the context of a filter, this would
indicatethelevelofGaussiannoiseaffectingthepositionand
velocityencoders.However,sincethebeliefsarebiased,Σ−1
o
and Σ−1 converge to a value that is much higher than the
o(cid:48)
actual Gaussian noise affecting the system. The values for
these variables therefore lose their physical meaning. This
bias can also cause false-positives when reasoning about the
accuracy of the sensor. Work in [21] explores this problem.
Finally,sinceestimationandcontrolaredependentoneach
other, it is not straight-forward to tune all the parameters
separately.LearningtheparametersΣ−1 andΣ−1 booststhe
o o(cid:48)
performance as shown in the previous section. However, if
Σ−1 and Σ−1 are also updated, the performance decreases µ µ(cid:48)
significantly.
Control behaviour when updating τ−1. VI. RESULTSONAROBOTICMANIPULATOR
This section evaluates the presented approach against the
active inference controller (AIC) from [11]. We show that
our approach outperforms the AIC from [11] at the task of
manipulating different payloads, and setting different initial
parameters for the variances and different values of τ. More
Reference Σμ=0.6, τ−1=1 results (including time-varying target states) can be found in
Σμ=0.6, τ−1=2.5 our other work [32], [34].
Σμ=0.6, τ−1=5.5 with updating τ only
A. Robustness of intial settings
The AIC from [11] achieves adaptive control without an
Fig.5:Effectofupdatingthevalueofτ−1 duringoperation.
explicit dynamics model. However, it is sensitive to the
This figure shows that increasing both Σ or τ−1 results in
µ initialization of its parameters. By slightly changing Σ−1
overshoots and severe oscillations. Rather than tuning Σ−1 µ
o for instance, the system suffers from severe oscillations and
and Σ−1, updating τ−1 can be sufficient.
o(cid:48) never converges to the target state.
If the AIC is tuned optimally (Σ−1 =1.5I, Σ−1 =0.5I,
o o(cid:48)
Σ−1 = 0.1I and Σ−1 = 0.5I), this results in satisfactory
Note how the inverse τ−1 is updated rather than τ directly. µ µ(cid:48)
behaviour. In this case, I refers to the 7x7 identity matrix.
Similar to the variances, all previous equations contain τ−1
However, if we vary Σ−1 = 0.1I or to other values (0.3I
and since inverting a matrix is computationally expensive, µ
and 0.5I), the performance gets considerably worse. In our
theinverseisdirectlyupdated.Additionally,τ−1 requiresus
approach, we update τ, Σ and Σ online to retune the
to define a lowerbound. The optimization can results in τ−1 o o(cid:48)
controller. We ran the experiment for a pick-and-place task
approaching zero which means the controller converts to a
(as in [11]) for several values of Σ−1 and recorded in Table
pure estimator. In this work, τ−1 is set to have a minimum µ
I the Mean Absolute Error (MAE) defined as:
value of 0.5 on the diagonal elements and zero elsewhere.
UsingEquation12,theoscillationscanbedampedaswell 1 (cid:88) nt
MAE = |µ −µ|.
as improving settling time as shown in Figure 5. Note how n d
t
updatingτ−1 onlyissatisfactorytoeliminatetheoscillations j=1
(no update of Σ− o 1 or Σ− o(cid:48) 1 is necessary). The convergence When the AIC is properly tuned Σ− µ 1 = 0.1I, the two
of τ−1 occurs at µ d =µ and µ(cid:48) =0 which corresponds to cases have the same MAE (tuning does not matter since
the controller settling at its target position. The updates for the initialization was optimal). However, when Σ−1 =0.3I,
µ
τ will thus retune the controller appropriately until the the the controller becomes does not converge to the target state
target is reached. This provides a preference for updating τ (visualized in Figure 6) . The MAE increases to more than
rather than updating Σ− o 1 or Σ− o(cid:48) 1 in most cases. triple its value while in the case of tuning the hyperparam-
eters, the MAE actually decreases. This is due to the fact
C. Limitations
that increasing Σ−1 makes the controller more aggressive
µ
In the active inference setting, the belief is intentionally and when tuned, it does not oscillate and also has a slightly
biased towards the target to achieve control. However, the fasterresponse.Inasimilarfashion,resultsforchangingthe
belief is biased and thus not accurate unless the system value of τ−1 are recorded in Table II. Again, the MAE is
reaches the target. If an accurate belief is required for other much lower when using our method.
0.00
-0.20
-0.40
-0.60
-0.80
-1.00
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
Time [s]
]dar[
noitisop
tnioJ
Joint 1
0.60
0.50
0.40
0.30
0.20
0.10
0.00
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
Time [s]
]dar[
noitisop
tnioJ
Joint 2
0.04
0.02
0.00
-0.02
-0.04
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
Time [s]
]dar[
noitisop
tnioJ
Joint 3
-0.20
-0.40
-0.60
-0.80
-1.00
-1.20
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
Time [s]
]dar[
noitisop
tnioJ
Joint 4
0.03
0.02
0.01
0.00
-0.01
-0.02
-0.03
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
Time [s]
]dar[
noitisop
tnioJ
Joint 5
1.50
1.25
1.00
0.75
0.50
0.25
0.00
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
Time [s]
]dar[
noitisop
tnioJ
Joint 6
AIC - No updates AIC - Updating τ, Σo and Σo0 Reference
Fig. 6: Step response for the 6 joints of the robot arm with and without updating hyperparameters in the AIC. This graphs
correspondstothesecondcolumnofTableI(Σ−1 =0.3I).Itiscleartheupdatingthehyperparametersreducesoscillations.
µ
B. Results on adaptive behaviour AkeypropertyoftheAICisthecouplingbetweencontrol
Forthelastexperiment,therobotcarriesvaryingpayloads. and state-estimation. As shown, to perform any meaningful
All controllers are tuned to have identical performance in control,thestatemustbebiasedtowardsthetarget.Thusour
the no payload case. Subsequently, we test three different belief about the true state is only accurate when the systems
payloads: m = 1kg, m = 2kg and m = 3kg (max payload reaches the target. The coupling between estimation and
for the Panda arm). The MAE for these cases is recorded in control makes some quantities lose their physical meaning.
the Table III. For instance Σ o does not represent the uncertainty of the
joint position encoders anymore. It is rather a combination
Σ− µ 1 =0.1I 0.3I 0.5I of the encoder’s uncertainty and how far the target is form
Noupdates 0.028 0.088 0.118
thecurrentposition.Futureworkwillinvestigatethisfurther.
Updatingτ−1,Σo −1 andΣ−
o(cid:48)
1 0.028 0.025 0.032
VIII. CONCLUSIONS
TABLE I: MAE for different values of Σ−1.
µ
In this paper we demonstrate how a minimizing a single
quantity: variational free-energy, effective state-estimation,
control and learning can be performed for robotic manipula-
τ−1 =2I τ−1 =3I
tor. Online estimation of relevant quantities can be achieved
Noupdates 0.091 0.123
Updatingτ−1,Σ− o 1 andΣ− o(cid:48) 1 0.025 0.032 using gradient descent on the free-energy for each iteration
of the controller. We introduce a temporal parameter and
TABLE II: MAE for different values of τ−1.
show that when τ approaches zero, the approach converts
to a PID controller and if τ approaches ∞, it converts
to a filter. We then demonstrated the effectiveness of the
m=1kg m=2kg m=3kg frameworkfora7DOFroboticarmandshowedadaptability
AICnoupdates 0.024 0.029 0.027
and robustness ourperforming previous work. We showed
AICwithupdates(ours) 0.020 0.020 0.021
that out approach improves robustness, damps oscillations
TABLE III: Mean Absolute Error (MAE) for different pay-
and adapts to different payloads.
loads in case of updating hyperparameters and no updates.
ACKNOWLEDGMENT
The authors thank Mees Vanderbroeck, Matias Mattamala
VII. DISCUSSIONANDFUTUREWORK and Charlie Street for helpful comments and feedback. This
In the AIC, actions are not explicitly modelled. In Sec- work was supported by UK Research and Innovation and
tion III, the generative model was selected to have the form EPSRC through the Robotics and Artificial Intelligence for
p(o,s)whichdoesnotexplicitlyincludeanynotionofanac- Nuclear (RAIN), and Offshore Robotics for Certification of
tiona.Thustochoosetheactionthatminimizesfree-energy, Assets (ORCA) hubs [EP/R026084/1, EP/R026173/1].
the chain rule was utilized (Equation 8). Alternatively, the
REFERENCES
actions could be explicitly added in the generative model
[1] Jesu´s Enrique Sierra and Matilde Santos. Wind and payload distur-
p(o,s,a). When doing so, this problem can be efficiently
bance rejection control based on adaptive neural estimators: applica-
solved using factor graphs [35], [26], [27]. tiononquadrotors. Complexity,2019.
[2] Bin Wei. Adaptive control design and stability analysis of robotic processes by message passing. Frontiers in Robotics and AI, 6(20),
manipulators. In Actuators, volume 7, page 89. Multidisciplinary 2019.
DigitalPublishingInstitute,2018. [26] Thijs van de Laar, Ayc¸a O¨zc¸elikkale, and Henk Wymeersch. Appli-
[3] BustanulArifin,BhaktiYudoSuprapto,SriArttiniDwiPrasetyowati, cation of the free energy principle to estimation and control. arXiv
andZainuddin Nawawi. Thelateralcontrolof autonomousvehicles: preprintarXiv:1910.09823,2019.
Areview.In2019InternationalConferenceonElectricalEngineering [27] Mees Vanderbroeck, Mohamed Baioumy, Daan van der Lans, Rens
andComputerScience(ICECOS),pages277–282.IEEE,2019. deRooij,andTiisvanderWerf. Activeinferenceforrobotcontrol:A
[4] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp factor graph approach. Student Undergraduate Research E-journal!,
Schwartenbeck, and Giovanni Pezzulo. Active inference: a process 5:1–5,2019.
theory. Neuralcomputation,29(1):1–49,2017. [28] Charles W Fox and Stephen J Roberts. A tutorial on variational
[5] Karl Friston. Friston k. the free-energy principle: a rough guide to bayesianinference. Artificialintelligencereview,38(2):85–95,2012.
thebrain?trendscognsci13:293-301. Trendsincognitivesciences, [29] Martin J Wainwright, Michael I Jordan, et al. Graphical models,
13:293–301,072009. exponential families, and variational inference. Foundations and
[6] GiovanniPezzulo,FrancescoRigoli,andKarlJFriston. Hierarchical Trends®inMachineLearning,1(1–2):1–305,2008.
active inference: A theory of motivated control. Trends in cognitive [30] Christopher M Bishop. Pattern recognition and machine learning.
sciences,22(4):294–306,2018. springer,2006.
[31] Karl Friston. Hierarchical models in the brain. PLoS computational
[7] RaphaelKaplanandKarlJFriston. Planningandnavigationasactive
biology,4(11):e1000211,2008.
inference. Biologicalcybernetics,112(4):323–343,2018.
[32] MohamedBaioumy,MatiasMattamala,PaulDuckworth,BrunoLac-
[8] Christopher L Buckley, Chang Sub Kim, Simon McGregor, and
erda, and Nick Hawes. Adaptive manipulator control using active
Anil K Seth. The free energy principle for action and perception:
inferencewithprecisionlearning. InUKRAS20Conference:”Robots
Amathematicalreview. JournalofMathematicalPsychology,81:55–
into the real world” Proceedings, Lincoln, United Kingdom, May
79,2017.
2020.
[9] Pablo Lanillos and Gordon Cheng. Active inference with function
[33] RafalBogacz. Atutorialonthefree-energyframeworkformodelling
learning for robot body perception. In International Workshop on
perceptionandlearning.Journalofmathematicalpsychology,76:198–
ContinualUnsupervisedSensorimotorLearning,IEEEDevelopmental
211,2017.
LearningandEpigeneticRobotics(ICDL-Epirob),2018.
[34] MohamedBaioumy,MatiasMattamala,andNickHawes. Variational
[10] GuillermoOliver,PabloLanillos,andGordonCheng.Activeinference
inferenceforpredictiveandreactivecontrollers,2020.
body perception and action for humanoid robots. arXiv preprint
[35] Hans-AndreaLoeliger,JustinDauwels,JunliHu,SaschaKorl,LiPing,
arXiv:1906.03022,2019.
andFrankRKschischang. Thefactorgraphapproachtomodel-based
[11] Corrado Pezzato, Riccardo MG Ferrari, and Carlos Hernandez. A signalprocessing. ProceedingsoftheIEEE,95(6):1295–1322,2007.
novel adaptive controller for robot manipulators based on active
inference. IEEERoboticsandAutomationLetters,2020.
[12] Rudolph Emil Kalman. A new approach to linear filtering and
predictionproblems. 1960.
[13] Neil J Gordon, David J Salmond, and Adrian FM Smith. Novel
approachtononlinear/non-gaussianbayesianstateestimation. InIEE
proceedingsF(radarandsignalprocessing),volume140,pages107–
113.IET,1993.
[14] Karl Johan A˚stro¨m and Tore Ha¨gglund. PID controllers: theory,
design,andtuning,volume2.InstrumentsocietyofAmericaResearch
TrianglePark,NC,1995.
[15] Marie Dillon Dahleh and William T Thomson. Theory of vibration
withapplications. Prentice-HallInc,1998.
[16] Le´o Pio-Lopez, Ange Nizard, Karl Friston, and Giovanni Pezzulo.
Activeinferenceandrobotcontrol:acasestudy.JournalofTheRoyal
SocietyInterface,13(122):20160616,2016.
[17] PabloLanillosandGordonCheng. Adaptiverobotbodylearningand
estimationthroughpredictivecoding.In2018IEEE/RSJInternational
Conference on Intelligent Robots and Systems (IROS), pages 4083–
4090.IEEE,2018.
[18] Cansu Sancaktar and Pablo Lanillos. End-to-end pixel-based deep
active inference for body perception and action. arXiv preprint
arXiv:2001.05847,2019.
[19] Dan Zhang and Bin Wei. A review on model reference adaptive
controlofroboticmanipulators. AnnualReviewsinControl,43:188–
198,2017.
[20] RG Walters and MM Bayoumi. Application of a self-tuning pole-
placementregulatortoanindustrialmanipulator. In198221stIEEE
ConferenceonDecisionandControl,pages323–329.IEEE,1982.
[21] Corrado Pezzato, Mohamed Baioumy, Carlos Hernandez Corbato,
Nick Hawes, Martijn Wisse, and Riccardo Ferrari. Active inference
forfaulttolerantcontrolofrobotmanipulatorswithsensoryfaults. In
1stInternationalWorkshoponActiveInference,ECML/PKDD2020,
Ghent,Belgium,2020.
[22] YouminZhangandJinJiang.Bibliographicalreviewonreconfigurable
fault-tolerantcontrolsystems. AnnualReviewsinControl,32(2):229
–252,2008.
[23] M.L.Visinsky,J.R.Cavallaro,andI.D.Walker.Roboticfaultdetection
and fault tolerance: A survey. Reliability Engineering and System
Safety,46:139–158,1994.
[24] A. Meera and M Wisse. Free energy principle based state and
inputobserverdesignforlinearsystemswithcolorednoise. In2020
AmericanControlConference(ACC),pages5052–5058,2020.
[25] ThijsWvandeLaarandBertdeVries. Simulatingactiveinference