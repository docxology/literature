1 
 
INTERACTIVE INFERENCE: A MUL TI-AGENT MODEL OF 
COOPERATIVE JOINT ACTIONS 
 
Domenico Maisto, Francesco Donnarumma, and Giovanni Pezzulo 
Institute for Cognitive Sciences and Technologies, National Research Council, Rome, Italy  
 
 
Contact: Giovanni Pezzulo (giovanni.pezzulo@istc.cnr.it) 
 
Abstractâ€”We advance a novel computational model of multi -agent, cooperative joint 
actions that is grounded in the cognitive framework of active inference. The model  assumes 
that to solve a joint task, such as pressing together a red or blue button, two (or more) agents 
engage in a process of interactive inference. Each agent maintains probabilistic beliefs about 
the joint goal (e.g., should we press the red or blue b utton?) and updates them by observing 
the other agentâ€™s movements, while in turn selecting movements that make his own 
intentions legible and easy to infer by the other agent (i.e., sensorimotor communication). 
Over time, the interactive inference aligns b oth the beliefs and the behavioral strategies of the 
agents, hence ensuring the success of the joint action. We exemplify the functioning of the 
model in two simulations. The first simulation illustrates a â€œleaderlessâ€ joint action. It shows 
that when two agents lack a strong preference about their joint task goal, they jointly infer it 
by observing each otherâ€™s movements. In turn, this helps the interactive alignment of their 
beliefs and behavioral strategies. The second simulation illustrates a "leader -follower" joint 
action. It shows that when one agent (â€œleaderâ€) knows the true joint goal, it uses sensorimotor 
communication to help the other agent (â€œfollowerâ€) infer it, even if doing this requires 
selecting a more costly individual plan. These simulation s illustrate that interactive inference 
supports successful multi -agent joint actions and reproduces key cognitive and behavioral 
dynamics of â€œleaderlessâ€ and "leader -follower" joint actions observed in human -human 
experiments. In sum, interactive inferenc e provides a cognitively inspired, formal framework 
to realize cooperative joint actions and consensus in multi-agent systems. 
 
Keywords: active inference, consensus, joint action, multi -agent systems, sensorimotor 
communication, shared knowledge, social interaction.  
 
 
  
2 
 
I. INTRODUCTION 
A central challenge of multi -agent systems (MAS) is coordinating the actions of multiple 
autonomous agents in time and space, to accomplish cooperative tasks and achieve joint goals 
[1], [2] . Developing successful multi -agent systems requires addressing controllability 
challenges [3], [4]  and dealing with synchronization control [5], formation control [6], task 
allocation [7] and consensus formation [8]â€“[10]. 
Research in cognitive science may provide guiding principles to address the above 
challenges, by identifying the cognitive strategies that groups of individuals use to 
successfully interact with each other and to make collective decisions [11]â€“[14]. An extensive 
body of research studied how t wo or more people coordinate their actions in time and space 
during cooperative (human -human) joint actions, such as when performing team sports, 
dancing or lifting something together [15], [16] . These studies have shown that successful 
joint actions engage various cognitive mechanisms, whose level of sophistication plausibly 
depends on task complexity. The simplest form s of coordination and imitation in pairs or 
groups of individuals, such as the joint execution of rhythmic patterns, might not require 
sophisticated cognitive processing, but could use simple mechanisms of behavioral 
synchronization â€“ perhaps based on coup led dynamical systems, analogous to the 
synchronization of coupled pendulums [17]. However, m ore sophisticated types of joint 
actions go beyond the mere alignment of behavior. For example, some joint actions require 
making decisions together, e.g., the decision about where to place a table that we are lifting 
together. These sophisticated forms of joint actions and joint decisions might benefit from 
cognitive mechanisms for mutual prediction, mental state inference, sensorimoto r 
communication and shared task representations [16], [18] . The cognitive mechanisms 
supporting joint action have been probed by numerous experiments [19]â€“[29], sometimes 
with the aid of conceptual [30], computational [31]â€“[39], and robotic [40]â€“[43] models. 
However, there is still a paucity of models that implement  advanced cognitive abilities, such 
as the inference of others' plans  and the alignment of task knowledge  across group members, 
which have been identified in empirical studies of joint action. Furthermore, it is unclear 
whether and how it is possible to de velop joint action models from first principles; for 
example, from the perspective of a generic inference or optimization scheme that unifies 
multiple cognitive mechanisms required for joint action. 
We advance an innovative framework for cooperative joint action and consensus in multi -
agent systems, inspired by the cognitive framework of active inference. Active inference is a 
normative theory that describes the brain as a prediction machine, which learns an internal 
(generative) model of the statistical re gularities of the environment â€“ including the statistics 
of social interactions â€“ and uses it to generate predictions that guide perceptual processing 
and action planning [44]. Here, we use the predictive and inferential mechanisms of active 
inference to implement sophisticated forms of joint action in dyads of interacting agents.  The 
model presented here formalizes joint action as a process of interactive inference based on 
shared task knowledge between the agents [2], [45] . We exemplify the functioning of the 
model in a â€œjoint mazeâ€ task. In th e task, two agents have to navigate in a maze, to reach and 
press together either a red or a blue button. Each agent has probabilistic beliefs about the 
joint task that the dyad is performing, which covers his own and the other agent's 
contributions (e.g.,  should we both press a red or a blue button?). Each agent continuously 
infers what the joint task is, based on his (stronger or weaker) prior belief and the 
observation of the other agent's movements towards one of the two buttons. Then, he selects 
an action (red or blue button press), in a way that simultaneously fulfills two objectives. The 
3 
 
former, pragmatic objective consists in achieving the joint task efficiently (e.g., by following 
the shortest route to reach the to -be-pressed button). The latter, epistemic objective consists 
in shaping one's movements to help the other agent inferring what the joint goal is (e.g., by 
selecting a longer route easily associated to the goal of pressing the red button).  
The next sections are organized as follows. First,  we introduce the consensus problem 
(called a â€œjoint mazeâ€) we will use throughout the paper to explain and validate our approach. 
Next, we illustrate the main tenets of the interactive inference model of joint action. Then, we 
present two simulations that  illustrate the functioning of the interactive inference model. The 
first simulation shows that over time, the interactive inference aligns the joint task 
representations of the two agents and their behavior, as observed empirically in several joint 
action studies [18], [23], [46] â€“[49]. In turn, this form of â€œinteractive alignmentâ€ (or 
â€œgeneralized synchronyâ€) optimizes the performance of the dyad. The second simulation 
shows that when agents have asymmetric information about the joint task, the more 
knowledgeable agent (or "leader") systematically modifies his behavior, to reduce the 
uncertainty of the less knowledgeable agent (or "foll ower"), as observed empirically in 
studies of sensorimotor communication [16], [18] . This form of â€œsocial epistemic actionâ€ 
ensures the success of joint action s despite incomplete information. Finally, we disc uss how 
our model of interactive inference could help better understand various facets of ("leaderless" 
and "leader -follower") human joint actions, by providing a coherent formal explanation of 
their dynamics at both brain and behavioral levels. 
II. PROBLEM FORMULATION AND SCENARIO 
To illustrate the mechanisms of the interactive inference model, we focus on the consensus 
problem called â€œjoint mazeâ€ task, which closely mimics the setting used in a previous human 
joint action study [50], see Fig . 1. In this task, two agents (represented as a grey hand and a 
white hand) have to â€œnavigateâ€ in a grid-like maze, reach the location in which the red or blue 
button is located, and then press it together. The task is completed successfully when both 
agents â€œpressâ€ the same button, whatever its color (unless stated otherwise).  
 
 
Fig. 1.  Schematic illustration of the â€œjoint mazeâ€ task. The two (grey and white) agents are 
represented as two hands. Their initial positions are L3 (grey) and L19 (white). Their possible 
goal locations are in blue (L12) and red (L10). The agents can navigate in the maze, by 
following the open paths, but cannot go through walls). 
 

4 
 
At the beginning of each simulation, each agent is equipped with some prior knowledge (or 
preference) about the goal of the task. This prior knowledge is represented as a probabilistic 
belief, i.e., a probability distribution over four possible task states; these are â€œboth agents will 
press redâ€, â€œboth agents will press blueâ€, â€œthe white agent will press red and the grey agent 
will press blueâ€ and â€œthe white agent will press blue and the grey agent will press redâ€. 
Importantly, in different simulations, the prior knowledge of the two agents can be congruent 
(if both assign the highest probability to the same state) or incongruent (if they assign the 
highest probability to different states) ; certain (if the probability mass is peaked in one state) 
or uncertain (if the probability mass is spread across all the states). This creates a variety of 
coordination problems, which span from easy (e.g., if the beliefs of the two agents are 
congruent and certain) to difficult problems (e.g., if the beliefs are incongruent or uncertain).  
Each simulation includes several trials, during which each agent follows a perception -action 
cycle. First, the agent receives an observation about his own position and t he position of the 
other agent. Then, he updates his knowledge about the goal of the task (i.e., joint task 
inference) and forms a plan about how to reach it (i.e., joint plan inference). Finally, he makes 
one movement in the maze (by sampling it probabili stically from the plan that he formed). 
Then, a new perception-action cycle starts.  
III. METHODS 
Here, we provide a brief introduction to the active inference framework for single agents (see 
[44] for details) and then we illustrate the novel , interactive inference model developed here  
to address multi-agent, cooperative joint actions.  
 
A. Active Inference 
Active Inference agents implement perception and action planning through the 
minimization of variational free energy [44]. To minimize free energy, the agents use a 
generative model of the causes of their perceptions, which encodes the joint probability of the 
stochastic variables illustrated in Fig. 2, using the formalism of probabilistic graphical models 
[51].  
 
Fig. 2.  Generative model of an active inference agent, unrolled in time. The circles denote 
stochastic variables; filled circles denote observed variables, whereas unfilled circle s denote 

5 
 
variables that are not observed and need to be inferred. The squares indicate the categorical 
probability distributions that parameterize the generative model. Please see the main text for 
a detailed explanation of the variables. 
 
The agent's generative model is defined as follows: 
ğ‘ƒ(ğ‘œ0:ğ‘‡, ğ‘ 0:ğ‘‡, ğ‘¢1:ğ‘‡, ğ›¾|ğš¯) = ğ‘ƒ(ğ›¾|ğš¯)ğ‘ƒ(ğœ‹|ğ›¾, ğš¯)ğ‘ƒ(ğ‘ 0|ğš¯) âˆ ğ‘ƒ(ğ‘œğ‘¡|ğ‘ ğ‘¡, ğš¯)ğ‘ƒ(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡, ğœ‹ğ‘¡, ğš¯)
ğ‘‡
ğ‘¡=0
 
            (1) 
 
where ğ‘ƒ(ğ‘œğ‘¡ |ğ‘ ğ‘¡, ğš¯) = ğ€, ğ‘ƒ(ğœ‹ğ‘¡|ğ›¾, ğš¯) = ğœ(ln ğ„ âˆ’ ğ›¾ âˆ™ ğ†(ğœ‹ğ‘¡)|ğš¯), ğ‘ƒ(ğ‘ ğ‘¡+1|ğ‘ ğ‘¡, ğœ‹ğ‘¡, ğš¯) =  ğ(ğ‘¢ğ‘¡ = ğœ‹ğ‘¡ ), 
ğ‘ƒ(ğ‘ 0|ğš¯) = ğƒ, ğ‘ƒ(ğ›¾|ğš¯) ~ Î“(ğ›¼, ğ›½). 
The set ğš¯ = {ğ€, ğ, ğ‚, ğƒ, ğ›¼, ğ›½} parameterizes the generative model. The (likelihood) matrix A 
encodes the relations  between the observations O and the hidden causes of observations 
S. The (transition) matrix B defines how hidden states evolve ove r time ğ‘¡, as a function of a 
control state (action) ğ‘¢ğ‘¡; note that a sequence of control states ğ‘¢1, ğ‘¢2, â€¦ , ğ‘¢ğ‘¡, â€¦ defines a policy 
ğœ‹ğ‘¡ (see below for a definition). The matrix ğ‚ is an a -priori probability distribution over 
observations and encodes the a gent's preferences or goals. The matrix ğƒ is the prior belief 
about the initial hidden state, before the agent receives any observation. Finally, Î³ âˆˆ â„ is a 
precision that regulates action selection and is sampled from a Î“ distribution with parameters 
ğ›¼ and ğ›½.  
An active inference agent implements the percepti on-action loop by applying the above  
matrices to hidden states and observations. In this perspective, perception corresponds to 
estimating hidden states on the basis  of observations and of previous hidden states. At the 
beginning of the simulation, the agent has access through D to an initial state estimate S0 and 
receives an observation O0 that permits refining the estimate by using the likelihood matrix A. 
Then, for  ğ‘¡ = 1, â€¦ , ğ‘‡ , the agent infers its current hidden state  Sğ‘¡  based on the 
observations previously collected and by considering the transitions determined by the 
control state ğ‘¢t, as specified in B. Specifically, active inference uses an approximate posterior  
over (past, present and future) hidden states and parameters (ğ‘ 0:ğ‘‡, ğ‘¢1:ğ‘‡, Î³). Assuming a mean 
field approximation, it can be described as: 
ğ‘„(ğ‘ 0:ğ‘‡, ğ‘¢1:ğ‘‡, Î³) =  ğ‘„(ğœ‹)ğ‘„(ğ›¾) âˆ ğ‘„(ğ‘ ğ‘¡|ğœ‹ğ‘¡)ğ‘‡
ğ‘¡=0   (2) 
 
where the sufficient statistics are encoded by the expectations ğ = (ğ’”Ìƒğœ‹, ğ…, ğœ¸), with  ğ’”Ìƒğœ‹ =
ğ’”0
ğœ‹, â€¦ , ğ’”ğ‘»
ğ…. Following a variational approach, the distribution in Eq. (2) best approximates the 
posterior when its sufficient statistics ğ minimise the free energy of the generative model, see 
[44]. This condition holds when the sufficient statistics are: 
ğ’”ğ‘¡
ğœ‹ â‰ˆ ğœ(ln ğ€ âˆ™ oğ‘¡ + ln(ğ(ğœ‹ğ‘¡âˆ’1) âˆ™ ğ’”ğ‘¡âˆ’1
ğœ‹ ))    (3.a) 
 
ğ›‘ = ğœ(lnğ„ âˆ’  ğœ¸ âˆ™ ğ†(ğœ‹ğ‘¡))          (3.b) 
 
ğœ¸ =
ğ›¼
ğ›½âˆ’ğ†(ğ›‘)          (3.c) 
 
where the symbol â€œ â‹…â€ denotes the inner product, defined as ğ€ â‹… ğ = ğ€ğ‘‡ğ, with the two 
arbitrary matrices ğ€ and ğ. 
Action selection is operated by selecting the policy (i.e., sequence of control states 
u1, u2, â€¦ , ut) that is expected to minimize free energy the most in the future. The policy 
distribution ğ›‘ is expressed in (3.b); the term ğœ(âˆ™) is a softmax function, ğ„ encodes a prior over 
6 
 
the policies (reflecting habitual components of action selection), ğ† is the expected free energy 
of the policies (reflecting goal -directed components of action selection) and Î³ is a precision 
term that encodes the confidence of beliefs about ğ†.  
The expected free energy (EFE) ğ†(Ï€t) of each policy Ï€t is defined as: 
ğ†(ğœ‹ğ‘¡) â‰ˆ âˆ‘ âˆ’ğ·KL[ğ‘„(ğ‘œğœ|ğœ‹)||ğ‘ƒ(ğ‘œğœ)] âˆ’ ğ”¼ğ‘„Ìƒ [ğ»[ğ‘ƒ(ğ‘œğœ|ğ‘ ğœ)]]
ğ‘‡
ğœ=ğ‘¡+1
 
                 (4) 
where ğ·KL[âˆ™ || âˆ™] and ğ»[âˆ™] are, respectively, the Kullback-Leibler divergence and the Shannon 
entropy, ğ‘„Ìƒ = ğ‘„(ğ‘œğœ, ğ‘ ğœ|ğœ‹) â‰œ ğ‘ƒ(ğ‘œğœ|ğ‘ ğœ)ğ‘„(ğ‘ ğœ|ğœ‹) is the predicted posterior distribution, ğ‘„(ğ‘œğœ|ğœ‹) =
 âˆ‘ ğ‘„(ğ‘œğœ, ğ‘ ğœ|ğœ‹)ğ‘ ğœ  is the predicted outcome, ğ‘ƒ(ğ‘œğœ) is a categorical distribution representing the 
preferred outcome and encoded by ğ‚, and ğ‘ƒ(ğ‘œğœ|ğ‘ ğœ) is the likelihood of the generative model 
encoded by the matrix ğ€.  
The expected free energy (EFE) can be used as a quality score for the policies  and has two 
terms. The first term of (4) is the Kullback -Leibler divergence between the (approximate) 
posterior and prior over the outcomes and it constitutes the pragmatic (or utility-maximizing) 
component of the quality score. This term favours the policies that entail low risk and 
minimise the difference between predicted ( Q(oÏ„|Ï€)) and preferred ( P(oÏ„) â‰¡  ğ‚) future 
outcomes. The second term of (4) is the expected entr opy under the posterior over hidden 
states and it represents the epistemic component of the quality score.  This term favours 
policies that lead to states that diminish the uncertainty future outcomes H[P(ğ‘œğœ |sÏ„)].  
After scoring all the policies using EFE, ac tion selection is performed by drawing over the 
action posterior expectations derived from the sufficient statistic ğ… computed via (3). Then, 
the selected action is executed, the agent receives a novel observation and the perception -
action cycle starts again. See [44] for more details. 
 
B. Multi-Agent Active Inference 
Here, we extend the active inference framework outlined above to a multi -agent setting [1], 
in which multiple agents (here, two) perform a joint ta sk consisting in navigating in the â€œjoint 
mazeâ€ of Fig 1 to reach simultaneously either the red or th e blue location. The â€œjoint mazeâ€ of 
Fig 1 includes 21 locations {L1, L2, â€¦ , L21}. Two agents, â€œgreyâ€ (â€œ iâ€) and â€œwhiteâ€ (â€œ jâ€), start 
from the locations L3 and L19 and their goal is to reach either the red (L10) or the blue (L12) 
goal locations. To reach the goal, the agents can choose between 24 action sequences or 
policies Ï€ (see Fig. S1 for their full list), which can be divided into two main types: those that 
follow the shorter paths that pass through the central cor ridor or longer paths that go 
through the maze perimeter. The shorter paths of the grey agent to reach the red and blue 
goal locations are (L3, L7, L11, L10) and (L3, L7, L11, L12), respectively. The longer paths of 
the grey agent to reach the red and blue locations  are (L3, L2, L1, L6, L9, L10)  and 
(L3, L4, L5, L8, L13, L12), respectively. The shorter paths of the white agent to reach the red 
and blue goal locations are (L19, L15, L11, L10) and (L19, L15, L11, L12), respectively. The 
longer paths of the grey agent to reach the red and blue locations are 
(L19, L18, L17, L14, L9, L10) and (L19, L20, L21, L16, L13, L12), respectively. Below, w e will 
call the first type of policies that go through the  shorter paths â€œpragmatic policiesâ€ and the 
second type of policies that go through the longer paths â€œsocial epistemic policiesâ€. 
Each agent has a separate generative model, whose structure was shown in Fig 2. In 
simulation 1, the two agents are equipped with identical generative models, except for a 
different estimate of their starting positions, L3 or L19. In simulation 2, there are some 
differences in the A and D tensors of the two agents (see below), reflecting the fact that the 
7 
 
white agent (the â€œleaderâ€) knows the joint task to be performed, where as the grey agent (the 
â€œfollowerâ€) does not. 
When the two generative models are considered together, they can be defined  as 
âŒ©Si, Oi, Ui, Î˜iâŒª, with i = 1, â€¦ , N, where N is the number of agents (see Supplementary Fig. 
S8C). Here, we assume that N = 2, but it is possible to generalise the model to a larger number 
of agents. The hidden states  Si = S1
i â¨‚S2
i â¨‚S3
i  are obtained as a tensorial product of three 
vectors (note that unlike the usual algebraic notation for tensors, the subscripts and 
superscripts do not indicate covariance or contravariance). The three vectors are: the position 
of the agent S1
i , the position of the other agent S2
i , and the joint goal context S3
i , that is the 
agent's belief about which joint goal the agents could achieve (see Fig. 3B). In this study, 
where the potential goals are two, there are four possible combinations: blue -blue, blue-red, 
red-blue, and red-red; hence, the size of the hidden state is 21 Ã— 21 Ã— 4 = 1764.  
The observations  Oi = O1
i â¨‚O2
i â¨‚O3
i â¨‚O4
i  consist of the tensorial product between the 
observed agents' positions O1
i , O2
i , the observed joint goal O3
i , and the associated utilities O4
i  
(see Supplementary Fig. S8B). The  first three vectors encode the observations that 
correspond to the three sets of hidden states described above. The final vector,  the outcome 
utility, includes three observations that correspond to the possible task outcomes: negative, 
neutral or positive / rewarding. The control states Ui = U1
i â¨‚U2
i  denote the joint actions 
available to the agents. Note that in this simulation, each a gent has beliefs about his own and 
the other agentâ€™s control states, even if of course he can only execute his own actions 
The set of tensors Î˜i = {Ai, Bi, Ci, Di} define the structure of the generative model. The 
tensors Di = D1
i â¨‚D2
i â¨‚D3
i  and Ci = C1
i â¨‚C2
i â¨‚C3
i â¨‚C4
i  encode the priors of the hidden states and 
the observations, respectively. The former factor reflects the agentâ€™s prior knowledge about 
its initial state. We a ssume that each agent knows his own and the other agentâ€™s initial 
positions (D1
i  and D2
i  are deterministic), but the belief D3
i  about which goal the other expects is 
uncertain and adjustable as a simulation parameter.  Besides, D3
i  depends on the agentâ€™s r ole, 
â€œleaderâ€ or â€œfollowerâ€. The leader knows the joint task goal and hence it splits the probability 
mass of D3
i  equally between blue -blue and blue -red (or between red -red and red -blue). 
Conversely, the follower only knows that, in order to succeed, both agents have to achieve the 
same goal. Hence, he splits the probability mass of D3
i  equally between blue-blue and red-red. 
The factor C4
i  (i.e., a prior over observations that incentivizes preferred outcomes) depends on 
the agentâ€™s role, in the same way as D3
i . The (likelihood) mapping between  Si and Oi is 
specified through the tensor Ai, defined as the tensorial product Ai =âŠ—k Ak
i , k = 1, â€¦ ,4, where 
each Ak
i , one for each different factor of Oi, is a 4 -order tensor defined on the hidden states. 
The first factor A1
i  is an identity tensor that maps the hidden states that represent the agentâ€™s 
positions in the maze into their corresponding observations.  
The tensor A2
i  maps the hidden states of the agent into the observations O2 that are relative 
to the other agent's location in the maze. This (likelihood) mapping is regulated by a 
modulation factor Î´ s1s2
D
 , which depends on the salience of the positions of the two agen ts (s1 
and s2) and the beliefs about the goals D3
i , i.e., p(o2|s1, s2) âˆ Î´ s1s2
D
 . Intuitively, when the 
agents' positions provide diagnostic information about their joint task goal (e.g., the agents 
are close to one of the goal locations and far from the oth er), they are salient and their 
corresponding observations are not attenuated. Rather, when the agentsâ€™ positions are 
uninformative (e.g., the agents are equidistant from the two goal locations), they are not 
salient and their corresponding observations are attenuated. 
Note that the tensor A2
i  can be intuitively considered as a kind of â€œsalience mapâ€, which 
unlike previous active inference studies [44], is not fixed but varies as a function of the agentâ€™s 
beliefs about the joint task goal, hence prioritizing information processing in task -dependent 
8 
 
ways [52]. In this setting, the salience can be interpreted as a form of attention modulation or 
gain-control (or even situation awareness [53]), in the sense that an agent receives noisy 
observations about the location of the other agent (specified in the  state vector s2) and the 
salience of the location determines the amount of endogenous (i.e., agent -dependent) noise in 
the observation process.  
Mechanistically, we calculate the tensor A2
i  in two steps. The first step consists in calculating 
the salience  of the position si of each agent i as âˆ†vi = |vb
D(si) âˆ’ vr
D(si)|, or the (absolute) 
difference between the salience of the position with respect to the blue goal vb
D(si) and the red 
goal vr
D(si). In turn, the salience with respect to one of the goals (here, the blue goa l) is 
calculated as follows: 
 
Î½b
D(s1) = (1 âˆ’ (max
 
(D3
i ) âˆ’ 0.5)) â‹… (
d(s1,L10)
d(s1,L10)+d(s1,L12))   (5) 
 
According to Equation (5), the salience of the location s1with respect to the blue goal is the 
probability that an agent in that state is pursuing that goal. For simplicity, Equation (5) 
assumes that the closer an agent to the blue goal location (L12 in Fig. 1), the higher the 
probability that the agent is pursui ng it. More specifically, the salience mass function Î½b
D 
relative to the position s1 is inversely proportional to the Euclidean distance between s1 and 
the goal (the second factor of Equation (5)). Furthermore, the salience depends on D3
i  (the 
first factor  of Equation (5)): the higher the D3
i â€™s mode â€“ i.e., the less entropic D3
i  is â€“ the 
smaller the amplitude of the salience. Note that we could have designed a more sophisticated 
salience model that considers the agentâ€™s direction of movement rather than, or  in addition to, 
agent-goal distance; but in this simple setup, agent -goal distance is sufficient. Note also that 
we can define the salience Î½r
D(s1) of the state s1 with respect to the red goal by swapping L10 
and L12 in Equation (5). 
The second step to calc ulate A2
i  consists in using the saliences calculated in the first step to 
calculate the modulation factor Î´ s1s2
D
 = sig(âˆ†v1 âˆ™ âˆ†v2), where sig(x) = 1 (1 + h âˆ™ eâˆ’kx)â„  is the 
parametric logistic function. For the purpose of our simulations, we assume that the 
modulation ranges in the interval (0.75, 1), which we obtain by fixing the parameters h = 10, 
k = 4. Therefore, when the agentsâ€™ positions unambiguously reveal the joint task goal they are 
pursuing, there is no modulation (Î´s1s2 = 1). Rather, the amount of modulation increa ses by a 
maximum of 25% (Î´s1s2 = 0.75) when the agentsâ€™ positions provide poor information about 
their joint task goal. 
The tensor A3
i  encodes the joint position (and salience) of the two agents. Given the hidden 
state s = s1 âŠ— s2 âŠ— s3, by assuming the saliences Î½b
D(s1) and Î½b
D(s2) as independent, it is 
possible to assume that A3 1,2,3
i â‰¡ Î½s3
D (s1, s2) = Î½s3
D (s1) â‹… Î½s3
D (s2). Hence, we can associate with 
the A3
i  components the corresponding joint saliences computed as the hidden state s = s1 âŠ—
s2 âŠ— s3 changes. Supplementary Fig. S9 shows  various salience matrices of A3
i  organized in a 
table, in which where the rows indicate joint goal context s3 and the columns correspond to 
different values of the mode of the initial belief about the task goal i.e., max(D3
i )). Each matrix 
is a grid of si ze 21 Ã— 21, where the rows and the columns represent the positions s1and s2, 
respectively, the colours of the cells correspond to the values of the joint salience Î½s3
D (s1, s2). 
Supplementary Fig. S9 shows that the matrices in the first column ( max(D3
i ) = 0.5) e ncode 
saliencies that are more peaked around the joint task goals. The matrices shown in the next 
two columns encode gradually more uniform and lower -valued saliencies â€“ up until only the 
9 
 
goal locations are salient. Note that in the control simulation (des cribed in the section on 
Simulation 1) in which we prevent interactive inference to take place, we replace the 
A3
i  tensor shown in Supplementary Fig. S9 with a uniform tensor that does not allow inferring 
the goal of the other agent from its position.    
The tensor A4
i  is responsible for modelling the relationship between hidden states and 
outcomes, which can be positive, neutral or negative.  A4
i  is a deterministic sparse tensor. For 
any hidden state s corresponding to a joint position s1 âŠ— s2 that does not i nclude any goal 
location, A4
i  gives a "neutral" outcome. The definition of â€œpositiveâ€ and â€œnegativeâ€ outcomes 
varies depending on the joint task goal and the (leader or follower) roles of the agents. In 
Simulation 1, where both agents are treated as "follo wers", the outcome is positive if the both 
agents are in the same goal location (e.g., both are in L10 or both are in L12). Rather, the 
outcome is negative if only one agent is on a goal location or if the two agents are in two 
distinct goal locations. In Simulation 2, one of the two agents (the "follower") has the same 
tensor A4
i  as in Simulation 1. Rather, the other agent (the "leader") receives a positive 
outcome if both agents are in the correct goal location (e.g., L10 if the joint task goal is "red, 
red") but a negative outcome if at least one agent is in the incorrect goal location (e.g., L12 if 
the joint task goal is "red, red"). 
The tensor  B encodes a deterministic mapping between hidden states, given  the control 
state u. Note that here the control state u corresponds to a joint action, not to the action of a 
single agent; hence it is specified as the tensorial product between the vector of the five 
possible movements of one agent ('up', 'down', 'left', 'right', and 'wait ') and the vector of the 
same five movements of the other agent. The tensor B describes how the spatial locations s1 
and s2 of the agents change as a function of the joint actions ui = u1
i â¨‚u2
i , such as 'up -up', 'up-
down', 'up -left', etc. Note that the transi tions regard exclusively the spatial locations. The 
transitions between joint task goals are not modelled explicitly, but the agents can infer that 
the joint goal changed on the basis of their observations.  
The action -perception cycle of the multi -agent a ctive inference model is the same as the 
single-agent active inference (see Fig. 2), except that the two agents exchange observations 
between them. Specifically, agent â€œ iâ€ receives from the agent â€œ jâ€ the outcome vectors O1
j  and 
the action u1
j  and vice versa (for simplicity, we allow the agents to send actions to each other; 
a slightly more complex generative model could have included as inputs observations about 
others' actions rather than directly their actions). Each agent then uses this information 
(along with the observations O2
i  and the actions u2
i  that they have computed) to update his 
beliefs about the hidden states and control states and then to select a course of actions or 
policy Ï€.  
The two key mechanisms of the model are Task goal inference and Plan inference. Task goal 
inference corresponds to inferring what the goal of the joint task is, i.e., updating the belief 
about the four possible task goals (â€œblue, blueâ€, â€œblue, redâ€, â€œred, blueâ€, â€œred, redâ€). As the task 
goal is specified at the level of  the dyad, in order to infer it, each agent needs to consider both 
his prior knowledge about the task goal and the movements of the other agent, which are 
informative about the other agentâ€™s task knowledge. Specifically, the joint task goal inference 
follows a principle of rational action; namely, the expectation that the other agent will act 
efficiently to achieve his goals [54]. Put simply, if an agent observes the other agent moving 
towards the red (or blue) goal, he updates his belief about the joint task goal, by increasing 
the probability that the goal is red (or blue). Furthermore, at the e nd of each trial, both agents 
receive feedback about success (â€œwinâ€ observation) or failure (â€œloseâ€ observation) and update 
their beliefs about the task goal. Please see the Methods section for more details. 
10 
 
Plan inference corresponds to inferring the cour se of action (or plan) that maximizes task 
success, on the basis of the inferred joint task. In this model, each agent infers both his own 
and the other agentâ€™s plan â€“ although, of course, he can only execute his own plan. The 
inference about oneâ€™s own and  the other agentâ€™s plans needs to consider the utility of 
following different routes (which privileges the shortest route) and the uncertainty about the 
goal (which prompts â€œepistemicâ€ behavior and the selection of informative routes). The 
balance between utilitarian and epistemic components of planning will become important in 
Simulation 2, see later. 
A key thing to notice is that the perception -action cycles of the two agents â€“ and their 
inferential processes â€“ are mutually interdependent, as the movement s of one agent 
determine the observations of the other agent at the next time step. Our simulations will show 
that this interactive inference naturally leads to the alignment of beliefs states and behavioral 
patterns of the two agents, analogous to the syn chronization of neuronal activity and 
kinematics in socially interacting dyads [15], [29], [48] . Furthermore, the simulations will 
show that  â€œsocial epistemic actionsâ€ that aim at reducing the uncertainty of the other agent 
increase the alignment and task success, especially in tasks with asymmetric knowledge.  
IV . SIMULATIONS OF INTERACTIVE INFERENCE 
We present two simulations of interactive inference. The first simulation illustrates the case 
of two agents that start from various (uncertain) prior beliefs about the task goal. T his 
simulation illustrates that over time, interactive inference produces a gradual alignment of 
both belief states and behavior, which permits the agents to successfully complete the task 
(most of the times). The second simulation illustrates the case of two agents that initially have 
asymmetric task knowledge: one of them (the leader) has a certain prior belief about the task 
goal, whereas the other (follower) has an uncertain belief. This simulation illustrates 
sensorimotor communication â€“ and the importance for the leader to select (epistemic) actions 
that reduce the followerâ€™s uncertainty, in order to complete the task successfully. 
 
A. Simulation 1: leaderless interaction 
The goal of Simulation 1 is testing whether and how interactive alignment favors the 
alignment of behavior and belief states of two agents engaged in the â€œjoint mazeâ€ task. This 
simulation comprises 100 trials. For each trial, two identical agents (apart for their prior 
beliefs about the task goal, see later) start from two opposite lo cations of the â€œjoint mazeâ€: the 
grey agent starts in location L3 and the white agent starts in location L19. They can move one 
step at a time, or wait (in which case, they remain the same place), until they reach one of the 
locations that include colored goals (red in L10, blue in L12). There are multiple sequences of 
actions (aka â€œpoliciesâ€) that each agent can take to reach the goal locations, which correspond 
to shorter or longer paths, with or without â€œwaitâ€ actions, etc. The 25 policies used in the 
simulation are specified in the Methods section. What is most important here is that 
irrespective of the selected policy, a trial is only successful if both agents reach the same goal 
/ button location, red (L19) or blue (L12). Specifically, if at the end of  the trial both agents are 
in the red (L10) or the blue (L12) button location, then the trial is successful and the agents 
receive the preferred observation (â€œwinâ€). Otherwise, if the two agents fail to reach the same 
button location (e.g., one is in L10 a nd the other is in L12), the trial is unsuccessful and the 
agents receive an undesirable observation (â€œloseâ€).  
Fig. 3 shows the results of one example simulation, in which the agents start with the same 
prior belief about the task goal. This uncertain belief assigns 0.5 to â€œboth agents will press redâ€ 
11 
 
(in short, â€œred, redâ€), 0.5 to â€œboth agents will press blueâ€ (in short, â€œblue, blueâ€) and zero to the 
two other possible states (â€œred, blue: the white agent will press red and the grey agent will 
press blueâ€ and â€œblue, red: the white agent will press blue and the grey agent will press redâ€). 
 
 
Fig. 3. Results of Simulation 1. The first two panels show the prior beliefs of the white (first 
panel) and grey (second panel) at the beginning of each trial. The vertical bars indicate 
moments in which we manually change the mind of the  â€œwhite agentâ€ and set his belief about 
the joint task goal  is set to 1 for â€œblue, blueâ€ (if its prior belief assigned higher probability to 
â€œred, redâ€) or â€œred, redâ€ (if its prior belief assigned higher probability to â€œblue, blueâ€). The 
third panel shows the outcome of the trials. These include successful trials in which the agents 
press the blue button (blue bars) or the red button (red bars) and failures (black bars).  
 
 
The first two panels  of Fig. 3  show the prior beliefs of the white and grey agents, 
respectively, at the beginning of each trial, from 1 to 100. The agentsâ€™ prior beliefs for the first 
trial are set manually, as discussed above. The agentsâ€™ prior beliefs for the subsequent trials 
are simply the posterior beliefs at the end of the previous trials, as usual in Bayesian inference, 
but multiplied by a fixed (volatility) factor. This ensures that the prior probability of â€œred, redâ€ 
or â€œblue, blueâ€ cannot be higher than 0.7.  The reason for introducing the fixed factor is that in 
many trials, the posterior beliefs reach the value of 1 for â€œred, redâ€ or for â€œblue, blueâ€, 
indicating that the agent is fully sure about the shared task goal. If this posterior value were 
used as the prior value for subsequent trials, there would be little place for variability in 
behavior and c hanges of mind . Introducing the fixed factor amounts to assuming that the 
agents are not fully sure that that joint task goal would remain the same across trials â€“ or in 
other words, believe that the environment has some volatility.  Note that from time to time 
(vertical bars) we manually â€œchange the mindâ€ of agent 1 from â€œred, redâ€ to â€œblue, blueâ€ or vice 
versa â€“ to introduce some variability in the simulation.  
The third panel  of Fig. 3  shows whether the agents completed successfully the trial by 
pressing the same button (the blue bars indicate that both pressed the blue button, whereas 
the red bars indicate that both pressed the red button) or unsuccessfully (black bars). Finally, 
the green vertical bars show trials in which the white agent â€œchanges mindâ€ about the goal 

12 
 
(e.g., from â€œblue, blueâ€ to â€œred, redâ€ or vice -versa). Following a â€œchange of mindâ€, the dyad 
usually requires one or a few trials before re-aligning on the new joint task goal.  
Fig. 3 shows that the two agents end up the trials with aligned belief states most of the times, 
except in the first trials (in which they started with uncertain beliefs) and immediately after 
the changes of mind (vertical bars). Furthermore, the two agents are successfully during most 
of the trials in which their be liefs are aligned and unsuccessful when their beliefs are not 
aligned. As shown in Fig. 3, the errors occur in the very first trials, immediately after the grey 
agent changes mind and in one trial afterwards. The errors on the first trials may occur 
because the agents are uncertain about what to do and they assign the same (expected free 
energy) â€œscoreâ€ to the two policies that go straight to the red button and the blue button; see 
the Methods section for an explanation of (expected free energy) and Fig. S1 for an illustration 
of the expected free energy of the policies of the white agent at the beginning of the first trials. 
When the two agents are very uncertain, there are two possible behaviors. First, both agents 
may select their task goals randomly, whi ch might or might not result in an error (see Fig . S2 
for an illustration of the results of 100 replications of the same experiment, without changes 
of mind). Second, one agent might simply follow the other and be succ essful. This â€œfollower 
effectâ€ is particularly apparent when the agentsâ€™ prior beliefs are weak, as in the first trials. 
Rather, in trials in which the agentsâ€™ prior beliefs are strong, such as after a change of mind, 
they do not simply follow one another, but try to fulfill their prior belief  â€“ and this explains 
why we observe several errors after only one of the agents changes mind. These examples 
illustrate that it is the strength (or the precision) of the beliefs about the joint task goal that 
determines whether or not an imitative response  takes place; see also [40] for a robotic 
demonstration of the importance of prior beliefs in enabling imitative responses. Finally, note 
that some errors can occur randomly, with low probability, since action selection is stochastic. 
To better quantify the interactive alignment of belief states between th e agents across trials 
and its effects on performance, we executed 100 runs of Simulation 1 and plotted a measure 
of the belief alignment of the dyad â€“ the KL divergence between the beliefs  about task goals â€“ 
and their performance, during the first 15 tria ls; see Fig. 4. Fig. 4A shows that the KL distance 
between the prior beliefs of the two agents is initially low , because they both start the same 
by design. While the beliefs are apparently aligned, the alignment regards an uncertain state â€“ 
and this is why the performance is initially low ( Fig. 4 B). During the interaction, t he KL 
divergence initially increases, as the agents consider different hypotheses about the joint task 
goal, but then it rapidly decreases when the agents settle on the same joint task goal (â€œred, redâ€ 
or â€œblue, blueâ€) â€“ and at that point, the performance of the dyad (Fig. 4B) is nearly perfect. 
Note that in this simulation  the initial choice of a particular joint task goal (â€œred, redâ€ or 
â€œblue, blueâ€) is random,  but its persistence ac ross trials depends on a process of interactive 
belief alignment between the agents. The alignment of behavior and of beliefs about task goals 
might occur in two ways . First, it might occur thanks to interactive inference  within trials: 
namely, because eac h agent monitors the movements of the other agent and uses this 
information to update his estimate about the joint task goal and the other agentâ€™s plan, 
following a principle of rational action (i.e., the expectation that the other agent will act 
efficiently to achieve his goals [54]). Second, the alignment might occur because at the end of 
each trial, the agents receive a feedback about their success (â€œwinâ€ observation) or failure 
(â€œloseâ€ observation) and use this feedback to update their beliefs. Second, alignment might be 
the byproduct of a standard reinforcement learning approach to learn beha vior by trial and 
error, without interactive inference within trials.  
 
13 
 
 
Fig. 4. Average results of 100 runs, with the same parameters as Simulation 1, for 15 trials. 
The top panel shows a measure of belief (dis)alignment of the agents: the KL divergence 
between their beliefs about task goals. The mean value is in black and the standard deviation 
of the mean is in grey. The bottom panel shows a histogram of mean success rate. See the 
main text for explanation 
 
To understand whether the first mechanism based on interactive inference is actually useful 
for alignment and task success, we replicated the same experiment, but by preventing 
interactive inference to take place. We did this by removing any useful information from the 
likelihood matrix that maps the othersâ€™ positions into task goals (i.e., by making the A3
ğ‘–  tensor 
uniform; see the Methods section for details). This control simulation shows that without 
interactive inference, the performance decreases drastically and there is little alignment: the 
agents keep switching between red and blue goals and their beliefs do not become 
increasingly aligned over time; see Supplementary Fig. S3 and S4. This control simulation 
shows that despite the task could be addressed using (reinforcement -based) feedback from  
successes and failures, the interactive inference is key to achieve alignment. Increasing the 
weight assigned to feedback information could potentially increase success rate and 
alignment, but this does not seem necessary when interactive inference is in place. 
In sum, Simulation 1 shows that two agents that engage in interactive inference can align 
both their beliefs about the joint task goal and their plans to achieve the joint task goal, 
forming shared task knowledge [55]â€“[57]. The al ignment at both the belief and behavioral 
levels is made possible by a process of interactive, reciprocal inference of goals and plans. The 
two agents initially have weak beliefs about the goal identity and therefore they can â€œfollow 
each otherâ€ until they settle on some joint goal â€“ and successively stick to it.  
 
B. Simulation 2: asymmetric leader-follower interaction 
The goal of Simulation 2 is testing the emergence of â€œleader-followerâ€ dynamics observed in 
human studies using the "joint maze" setup [50] and other related studies in which the agents 
have asymmetric preferences (or information) about the joint task goal [24], [58]â€“[60]. This 

14 
 
simulation is similar to Simulation, 1, but the two agents differ in their p rior beliefs about the 
task goal [24], [50], [58]â€“[60]. Specifically, the white agent (the â€œleaderâ€) knows the task to be 
accomplished â€“ for example, â€œred, redâ€ â€“ whereas the grey a gent (the â€œfollowerâ€) does not. In 
other words, while in Simulation 1 both agents had initially weak beliefs (or preferences) 
about the joint goal and can be therefore considered two â€œfollowersâ€, in Simulation 2 one of 
the two agents is a â€œleaderâ€ and has a strong initial preference about the joint task goal. 
The generative model of the follower is identical to the one used in Simulation 1, whereas 
the generative model of the leader differs from it in two ways. First, the (likelihood) tensor A4
ğ‘–  
of the white agent reflects his knowledge of the true task contin gencies; namely, that the 
preferred â€œwinâ€ observation can only be obtained by achieving the joint task â€œred, redâ€, but 
not â€œblue, blueâ€ (or the opposite when the true task goal is "blue, blue"). Furthermore, the 
prior belief of the white agent is 0.5 for â€œ red, redâ€, 0.5 for â€œred, blueâ€ and 0 for the two other 
joint goals (or the opposite when the true task goal is "blue, blue"). Note this prior belief 
makes the white agent resistant to changes of mind about the task goal.  
Several studies [24], [50], [58] â€“[60] showed that when leaders and followers have 
asymmetric information, the leaders modify their movement kinematics to â€œsignalâ€ their 
intentions and reduce the uncertainty of the followers [61], [62]. For example, consider that in 
the scenario of Fig. 1 the leader (white agent) has a choice between two kinds of action 
sequences or policies to reach the red goal location. The first, â€œpragmatic policiesâ€ follow the 
shortest and hence most e fficient path to the goal: L15, L11 and L10. However, if the leader 
selects the pragmatic policy, he does not offer any cue to the follower about the joint task goal, 
until the last action (to L10). This is because passing through L15 and L11 is equally li kely if 
the intended goals are red or blue and hence does not provide diagnostic information about 
the goal location. The second, â€œsocial epistemic policiesâ€ follow the route through L18, L17, 
L14, L9 and L10, which despite being longer, provides to the fo llower early information to the 
intended goal location. This is because passing through L18, L17, L14, L9 is rational only if the 
goal is the red button â€“ and hence it provides diagnostic evidence that the to -be-pressed 
button is red. The above studies [24], [50], [58] â€“ [60] show that leaders often select â€œsocial 
epistemic policiesâ€: they sacrifice efficiency to reduce the followerâ€™s uncertainty.  
The trade-off between pragmatic and epistemic components of policy selection is automatic 
in active inference, because the expected free energy functional used in active inference to 
score policies includes two components: a â€œpragmatic componentâ€ that maximizes utility and 
prioritizes the sh ortest paths to the goal and an â€œepistemic componentâ€ that minimizes 
uncertainty (see the Methods). We therefore expected the leaders to select â€œsocial epistemic 
policiesâ€ most often when the followers were uncertain â€“ and select â€œpragmatic policiesâ€ 
when uncertainty was reduced. 
The results of an example leader -follower simulation lasting 30 trials are shown in Fig. 5. 
The first and third panels of Fig. 5 show the prior beliefs of the leader (white agent) and the 
follower (grey agent), respectively, at the beginning of each trial. These are largely aligned, 
except in the very first trials. The second and fourth panels show the policies selected by the 
leader and the follower, respectively. As discussed above, we divided policies into two 
categories: â€œpragmatic policiesâ€ (S: shorter red bars) that follow the shortest path to the goal 
and â€œsocial epistemic policiesâ€ (L: longer blue bars) that follow longer bu t more informative 
paths. The second panel of Fig. 5  shows that the leader tends to select â€œsocial epistemic 
policiesâ€ in the first trials, to reduce the followerâ€™s uncertainty (see also Supplementary Fig. S5 
for a visualization of the expected free energy  of the leaderâ€™s policies). Rather, the follower 
has no benefit from selecting epistemic policies and indeed selects pragmatic policies across 
15 
 
almost all the trials. Finally, the bottom panel of Fig. 5 shows that in all but the first two trials 
(short black bars), the agents successfully achieve the â€œred, redâ€ joint goal (long red bars). 
 
 
Fig. 5. Results of Simulation 2: example of lea der-follower dyadic interaction , for 30 trials. 
The first two panels show the prior beliefs of the leader at the end of ea ch trial and the policy 
he selects (shorter red bar: pragmatic policy that follows the shortest path to the goal; longer 
blue bar: social epistemic policy that follows the longer but more informative path to the goal). 
The third and fourth panels show the prior beliefs of the follower at the end of each trial and 
the policy he selects. The fifth panel shows the outcome of the trials. These include successful 
trials in which the agents press the red button (red bars) and failures (black bars). 
 
 
Fig. 6 shows  the results of 100 repetitions of the same simulation  (see also Supplementary 
Fig. S6). The first panel shows that the beliefs of the agents, measured as the KL divergence 
between their prior beliefs about the task goal, aligns over time. The second panel  shows that 
the average performance of the dyads, measured as the number of times they select the 
correct â€œred, redâ€ goal, increases over time. The third panel shows the percentage of â€œsocial 
epistemic policiesâ€ selected by the leaders. Initially, the lead ers have a strong tendency to 
select  â€œsocial epistemic policiesâ€, but this tendency decreases significantly across trials, as the 
followers become increasingly certain about the joint task goal â€“ which is in agreement with 
empirical evidence [24], [50], [58] â€“[60]. This result emerges because in the EFE that active 
inference uses to score policies, the decrease of uncertainty lowers the epistemic value of 
policies, hence lowering the probability that they will be selected [63]. 
 

16 
 
 
Fig. 6. Average results of 100 runs, with the same parameters as Simulation 2 shown in Fig. 7. 
The format of the first two plots is the same as Fig. 6. The third plot shows the percentage of 
policies selected by the leader that we label as â€œsocial epistemic actio nsâ€ and prescribe 
signaling behavior. See the main text for explanation. For example, if the grey agent is the 
leader, he can select an epistemic policy that passes through L3, L2, L1, L6, L9 and L10 (to 
reach the red button) or through L3, L4, L5, L8, L13, L12 (to reach the blue button). 
 
Fig. 7 permits appreciating how the leader balances epistemic and pragmatic policie s over 
time. The first panel shows the expected free energy (EFE) (averaged across 100 repetitions) 
that the leader selects the most usefu l social epistemic policy (red line) and the most useful 
pragmatic policy (green line). It shows that the social epistemic policy has a very high 
probability during the first 5 trials, then its probability decreases until the pragmatic policy 
becomes the m ost likely, starting from trial 10 . The second panel  shows the probability 
(averaged across 100 repetitions) that the leader selects a social epistemic policy as a 
function of his uncertainty about the task goal, quantified as the entropy of his belief about the 
joint task goal.  Note that the task goal is a shared representation that encodes both the 
leaderâ€™s and the followerâ€™s contributions . The entropy over this variable  reflects an estimate 
of the followerâ€™s uncertainty, not of the leaderâ€™s uncertainty ( as the leader  knows the goal) 
and  decreases over time, as the follower becomes less uncertain.  
Notably, the patterns of results shown in Fig. 7 B closely corresponds to the findings of a 
study that uses the â€œjoint mazeâ€ setting [50]. Specifically, the study reports that the 
probability that a (human) participant selects a pragma tic policy is high only when (his or her 
estimate of) the followerâ€™s uncertainty is very low (see Fig. 8A of [50]), which is in good 
agreement with the pattern shown in our Fig. 7B.  

17 
 
 
Fig. 7. How the leader balances epistemic and pragmatic policies in Simulation 2. Top panel: 
expected free energy (EFE) of the most useful epistemic (red) and pragmatic policy (green), in 
the first 10 trials. Bottom panel: frequency of the most useful epistemic policy as a function of 
the entropy of the leaderâ€™s belief about the joint task goal. This entropy provides a measure of 
the (leaderâ€™s estimate of the) followerâ€™s uncertainty.  
 
In sum, Simulation 2 shows that in leader-follower interactions with asymmetric knowledge, 
leaders select â€œsocial epistemic policiesâ€ â€“ and therefore sacrifice some efficiency in their 
choice of movements â€“ to signal their intended goals to the followers and reduce their 
uncertainty. This signaling behavior is progressively reduced,  when the followers become 
more certain about the joint action goal. This form of signaling was shown in previous 
computational models that used goal and plan inference, but the models used ad -hoc 
formulations to promote social epistemic actions [61], [62].  
Rather, social epistemic actions emerge naturally from two aspects of our model. The first 
key aspect of the model that promotes social epistemic behavior is that the expected free 
energy functional that active inference uses to score policies balances automa tically 
pragmatic and epistemic components of action and policy selection. This means that when 
uncertainty resolution is necessary, the expected free energy functional automatically 
promotes epistemic behavior [64]. To illustrate this point, we performed a control simulation  
(Supplementary Fig. S7) that is the same as Simulation 2, except that we removed the 
â€œepistemic componen tâ€ from the expected free energy that is used to score policies (see the 
Methods). The results of this control simulation show that the leader selects significantly less 
social epistemic policies, the behavioral alignment process is slower and the success rate 
grows more slowly compared to the case in which the full expected free energy is used. This 
control simulation shows that social epistemic actions are afforded by  the expected free 
energy minimization and that they enhance  leader-follower interactions  with asymmetric 
information. 

18 
 
Yet, it is important to remark that in most studies of active inference, epistemic behavior 
means lowering oneâ€™s uncertainty, not anotherâ€™s uncertainty, as in our study. The second key 
aspect of our model that promotes social epistemic behavior â€“ and permits planning how to 
lower anotherâ€™s uncertainty as opposed to just oneâ€™s own â€“ is the fact that the leaderâ€™s 
generative model includes beliefs about the shared task goal. When scoring his policies (via 
the expected free energy functional), the leader considers the uncertainty (or the entropy) of 
the shared task goal; therefore, it assigns high probability to standard â€œepistemic policiesâ€ that 
lower his own uncertainty (as shown in previous studies) and to â€œsocial epistemic polic iesâ€ 
that lower the followerâ€™s uncertainty [50]. This is important because it shows that active 
inference agents endowed with shared representations would behave natively in socially -
oriented ways, without the need of ad-hoc incentives, beyond joint action optimization. 
V. CONCLUSION 
Studies of human -human joint actions have shown some robust trends at both behavioral 
and neuronal levels, but we still lack a complete formal theory that explains these findings 
from first principles. Here, we proposed a computational model of interactive inference, in 
which two agents (implemented as active inference agents) coordinate around a joint goal â€“ 
pressing together either a red or a blue button â€“ that they do not know in advance (Simulation 
1) or that only one of them, the leader, knows in advance (Simulation 2).  
Our results show that the interactive inference model can successfully reproduce key 
behavioral and neural signatures of dyadic interactions. Simulation 1 shows that when two 
agents have the same (uncertain) knowledge about the joint ta sk to be performed, they 
spontaneously coordinate around a joint goal, align their behavior and task knowledge (here, 
their beliefs about the joint goal) over time â€“ in keeping with evidence of synchronization of 
both neuronal activity and kinematics durin g joint actions [15], [29], [48]. Furthermore, the 
interactive inference is robust to sudden changes of mind of one of the agents, as indexed by 
the fact that the alignment of behavior and task knowledge is recovered fast. While simple 
joint tasks such as the â€œjoint mazeâ€ that we  adopted could be in principle learned by trial and 
error and without inference, our control simulation (illustrated in Fig. S3 and S4) show s that 
interactive inference within trials promotes better performance and alignment of behavior 
and of belief states. 
Simulation 2 shows that during dyadic interactions in which knowledge about the joint task 
goal is asymmetric â€“ specifically, one agent (the "leader") knows the task to be performed but 
the other agent (the "follower") does not â€“ leaders systematically select â€œsocial epistemic 
policiesâ€ in early trials. The social epistemic policies sacrifice some path efficiency to give the 
follower early cues about the task goal, hence reducing her uncertainty and contributing to 
optimize the joint action. The results of this simulation are in keeping with a large number of 
studies of â€œsensorimotor communicationâ€ during dyadic interactions with asymmetric 
information [24], [50], [58] â€“[60]. Specifically, our model reproduces two key phenomena of 
leader-follower interactions. First, in all these tasks, leaders select an apparently less efficient 
path, which however provides early information about the intended task goal. Second, the 
selection of these more informative (or social epistemic) policies is dependent on the 
follower's uncertainty and it is abolished when the follower is (estimated to be) no longer 
uncertain, as reported in a study that uses our "joint maze" setup [50] and other studies in 
which the uncertainty of the follower varies across trials [24], [60].  
Notably, what renders our model different from previous formalizations is that the leader's 
social epistemic behavior does not require any ad -hoc mechanism [61], [62] . Rather, social 
19 
 
epistemic behavior stems directly from the fact that active inf erence uses an expected free 
energy functional that considers epistemic actions on equal ground with pragmatic actions 
and that the generative model used in our simulation includes shared task knowledge. In 
other words, active inference agents who cooperat e in uncertain conditions and have beliefs 
about their shared goal can natively select â€œepistemicâ€ policies that reduce their own 
uncertainty (as shown in previous simulations [44]) as well as the uncertainty of the other 
agents (as shown in this simulation). Given that policies of the l atter kind â€“ for example, the 
policies selected by a leader to reduce the followerâ€™s uncertainty â€“ are socially oriented, here 
we call them â€œsocial epistemic policiesâ€. 
Another important feature of our model is its flexibility. Simulations 1 and 2 use exac tly the 
same computational model, except for the fact that in Simulation 2 the â€œleaderâ€ knows the 
goal, but the follower does not. This implies that active inference is flexible enough to 
reproduce various aspects of joint action dynamics, without ad -hoc changes of the model. In 
our simulations, the differences between standard, â€œleaderlessâ€ (Simulation 1) and â€œleader -
followerâ€ (Simulation 2) dynamics emerge as an effect of the strength (and the precision) of 
the agentsâ€™ beliefs about the joint goal to be performed. When the agentsâ€™ beliefs are uncertain, 
as in Simulation 1, they tend to follow each other to optimize the joint goal â€“ and update (and 
align) their beliefs afterwards. In this case, the joint outcome (e.g., â€œred, redâ€ or â€œblue, blueâ€) 
can be ini tially stochastic, but is successively stabilized thanks to the interactive inference. 
This setting therefore exemplifies a â€œpeer -to-peerâ€ or a â€œfollower -followerâ€ interaction. Yet, it 
is possible to observe some â€œleader -followerâ€ dynamics, in the sense th at one of the two 
agents drives the choice of one particular joint task goal. However, in Simulation 1, the role of 
the leader is not predefined, but rather emerges during the task, as one of the joint goals is 
stochastically selected during the interactio n â€“ and then the two agents stick to it (note 
however that the situation is different during changes of mind, because the goal is predefined 
by us rather than being stochastically selected during the interaction). 
Rather, Simulation 2 exemplifies the case of a â€œleader -followerâ€ setup in which the role of 
the leader is predefined â€“ because the leader has a strong preference for one of the goals. The 
comparison of Simulations 1 and 2 shows that what defines leaders and followers is simply 
the strength of the prior belief about the joint task goal (and of its associated outcomes). Our 
results in Simulation 2 are in keeping with previous active inference models that showed the 
emergence of behavior synchronization and leader -follower dynamics in joint singing [65] 
and robotic dyadic interactions [40]. These studies nicely illustrate that several facets of joint 
actions emerge when two agents infer each other's goals and plans. However, the results 
reported here go beyond the above studies, by demonstrating the emergence of sensorimotor 
communication and social epistemic actions when the agents have asymmetric information. 
In sum, the simulations illustrated here provide a proof -of-concept that interactive inference 
can reproduce key empirical results of joint action studies. These include, for example, the 
interactive alignment and synchronization of behavior and neuronal activity (which, in our 
model, correspond to the belief dynamics) d uring standard joint actions [15], [29], [48]  and 
the â€œsensorimotor communicationâ€ during dyadic leader -follower joint actions with 
asymmetric information [24], [50], [58] â€“[60]. An open objective for futur e research is 
extending the empirical validation of this framework by adopting it to model more cases of 
joint action, beyond the "joint maze" scenario of [50]. Another objective for future research is 
exploiting this framework to design more effective agents that exploit sensorimotor 
communication to enhance human -robot joint actions. It has been argued by many 
researchers that the ease of human -human collaboration rests on our advanced abilities to 
infer intentions and plans, align representations and select movements that are easily legible 
20 
 
and interpretable by o ur agents. Endowing robots with similar advanced cognitive abilities 
would permit them to achieve unprecedented levels of success in human -robot collaboration 
and plausibly increase the trust in robotic agents [43], [66]â€“[68]. Finally, a key challenge for 
future research is extending the framework developed here beyond the case of cooperative 
joint actions, to also cover competitive and mixed cooperative-competitive interactions, which 
are frequent in multi-agent settings [69].  
 
Acknowledgement 
This research received funding from the EU Horizon 2020 Framework Programme for 
Research and Innovation under the Specific Grant Agreement No. 945539 (Human Brain 
Project SGA3) and the European Research Council under the Grant Agreement No. 820213 
(ThinkAhead) to GP and the MUR - PRIN2020 - Grant No. 2020529PCP to FD. The GEFORCE 
Quadro RTX6000 and Titan GPU cards used for this research were donated by the NVIDIA 
Corporation. The funders had no role in study design, data collection and analysis, decision to 
publish, or preparation of the manuscript.  
 
  
21 
 
REFERENCES 
[1] A. Dorri, S. S. Kanhere, and R. Jurdak, â€œMulti -Agent Systems: A Survey,â€ IEEE Access, vol. 6, pp. 28573 â€“28593, 
2018,  
[2] J. Ferber and G. Weiss, Multi-agent systems: an introduction to distributed artificial intelligence , vol. 1. 
Addison-wesley Reading, 1999. 
[3] B. Liu, H. Su, R. Li, D. Sun, and W. Hu, â€œSwitching controllability of discrete -time multi -agent systems with 
multiple leaders and time-delays,â€ Applied Mathematics and Computation, vol. 228, pp. 571â€“588, 2014. 
[4] B. Liu, T. Chu, L. Wang, Z. Zuo, G. Chen , and H. Su, â€œControllability of switching networks of multi -agent 
systems,â€ International Journal of Robust and Nonlinear Control, vol. 22, no. 6, pp. 630â€“644, 2012. 
[5] S. Su, Z. Lin, and A. Garcia, â€œDistributed synchronization control of multiagent syst ems with unknown 
nonlinearities,â€ IEEE Transactions on Cybernetics, vol. 46, no. 1, pp. 325â€“338, 2015. 
[6] B. Anderson, B. Fidan, C. Yu, and D. Walle , â€œUAV formation control: Theory and application,â€ in Recent 
advances in learning and control, Springer, 2008, pp. 15â€“33. 
[7] N. Krothapalli and A. V. Deshmukh, â€œDistributed task allocation in multi -agent systems,â€ in Proceedings of the 
Institute of Industrial Engineers Annual Conference, 2002. 
[8] W. Ni and D. Cheng, â€œLeader -following consensus of multi -agent systems under fixed and switching 
topologies,â€ Systems & control letters, vol. 59, no. 3â€“4, pp. 209â€“217, 2010. 
[9] V. Trianni, D. De Simone, A. Reina , and A. Baronchelli, â€œEmergence of consensus in a multi -robot network: 
from abstract models to empirical validation,â€ IEEE Robotics and Automation Letters, vol. 1, no. 1, pp. 348 â€“353, 
2016. 
[10] W. He, B. Xu, Q. -L. Han, and F. Qian, â€œAdaptive Consensus Co ntrol of Linear Multiagent Systems With 
Dynamic Event-Triggered Strategies,â€ IEEE Transactions on Cybernetics, vol. 50, no. 7, pp. 2996â€“3008, 2020  
[11] C. Castelfranchi, â€œModeling social interaction for AI agents,â€ in IJCAI-97. Proceedings of the Fifteent h 
International Joint Conference on Artificial Intelligence, 1997, pp. 1567â€“1576. 
[12] R. Conte and C. Castelfranchi, Cognitive and Social Action. London, UK: University College London, 1995. 
[13] C. Castelfranchi and R. Falcone, Trust Theory: A socio-cognitive and computational model. Wiley, 2010. 
[14] R. Sun, â€œCognitive science meets multi-agent systems: A prolegomenon,â€ Philosophical psychology, vol. 14, no. 
1, pp. 5â€“28, 2001. 
[15] N. Sebanz, H. Bekkering, and G. Knoblich, â€œJoint action: bodies and minds  moving together.,â€ Trends Cogn Sci, 
vol. 10, no. 2, pp. 70â€“76, 2006 
[16] G. Pezzulo, F. Donnarumma, H. Dindo, A. Dâ€™Ausilio, I. Konvalinka, and C. Castelfranchi, â€œThe body talks: 
Sensorimotor communication and its brain and kinematic signatures,â€ Physics of Life Reviews , Jun. 2018, doi: 
10.1016/j.plrev.2018.06.014. 
[17] O. Oullier and J. A. Kelso, â€œSocial coordination, from the perspective of coordination dynamics,â€ in 
Encyclopedia of complexity and systems science, Springer, 2009, pp. 8198â€“8213. Accessed: Feb. 23, 2016 
[18] G. Knoblich and N. Sebanz, â€œEvolving intentions for social interaction: from entrainment to joint action.,â€ 
Philos Trans R Soc Lond B Biol Sci, vol. 363, no. 1499, pp. 2021â€“2031, Jun. 2008,  
[19] G. Pezzulo, F. Donnarumma, S. Ferrari -Toniolo, P. Cisek, and A. Battaglia -Mayer, â€œShared population -level 
dynamics in monkey premotor cortex during solo action, joint action and action observation,â€ Progress in 
Neurobiology, vol. 210, p. 102214, Mar. 2022,  
[20] G. Pezzulo, P. Iodice, F. Donnarum ma, H. Dindo, and G. Knoblich, â€œAvoiding accidents at the champagne 
reception: A study of joint lifting and balancing,â€ Psychological Science, 2017. 
[21] G. Pezzulo, L. Roche, and L. Saint -Bauzel, â€œHaptic communication optimises joint decisions and affords  
implicit confidence sharing,â€ Sci Rep, vol. 11, no. 1, p. 1051, Jan. 2021 
[22] G. Knoblich and J. S. Jordan, â€œAction coordination in groups and individuals: learning anticipatory control.,â€ J 
Exp Psychol Learn Mem Cogn, vol. 29, no. 5, pp. 1006â€“1016, Sep. 2003, doi: 10.1037/0278-7393.29.5.1006. 
[23] R. P. R. D. van der Wel, C. Becchio, A. Curioni, and T. Wolf, â€œUnderstanding joint action: Current theoretical 
and empirical approaches,â€ Acta Psychologica, vol. 215, p. 103285, Apr. 2021 
[24] M. Candidi, A. Cu rioni, F. Donnarumma, L. M. Sacheli, and G. Pezzulo, â€œInteractional leader â€“follower 
sensorimotor communication strategies during repetitive joint actions,â€ Journal of The Royal Society Interface, vol. 
12, no. 110, p. 20150644, Sep. 2015 
[25] F. Visco -Comandini et al., â€œDo non -human primates cooperate? Evidences of motor coordination during a 
joint action task in macaque monkeys,â€ Cortex, vol. 70, pp. 115â€“127, 2015. 
[26] C. Becchio, A. Koul, C. Ansuini, C. Bertone, and A. Cavallo, â€œSeeing mental states: An e xperimental strategy for 
measuring the observability of other minds,â€ Phys Life Rev, vol. 24, pp. 67â€“80, Mar. 2018 
[27] M. Coco et al., â€œMultilevel behavioral synchronisation in a joint tower -building task,â€ IEEE Transactions on 
Cognitive and Developmental Systems, vol. PP, no. 99, pp. 1â€“1, 2016 
[28] A. Dâ€™Ausilio, G. Novembre, L. Fadiga, and P. E. Keller, â€œWhat can music tell us about social interaction?,â€ 
Trends Cogn. Sci. (Regul. Ed.), vol. 19, no. 3, pp. 111â€“114, Mar. 2015 
22 
 
[29] P. E. Keller, G. Novembre,  and M. J. Hove, â€œRhythm in joint action: psychological and neurophysiological 
mechanisms for real -time interpersonal coordination,â€ Phil. Trans. R. Soc. B , vol. 369, no. 1658, p. 20130394, 
2014 
[30] D. M. Wolpert, K. Doya, and M. Kawato, â€œA unifying compu tational framework for motor control and social 
interaction.,â€ Philos Trans R Soc Lond B Biol Sci , vol. 358, no. 1431, pp. 593 â€“602, Mar. 2003, doi: 
10.1098/rstb.2002.1238. 
[31] H. Dindo, D. Zambuto, and G. Pezzulo, â€œMotor simulation via coupled internal mo dels using sequential Monte 
Carlo,â€ in Proceedings of IJCAI 2011, 2011, pp. 2113â€“2119. 
[32] H. Dindo, F. Donnarumma, F. Chersi, and G. Pezzulo, â€œThe intentional stance as structure learning: a 
computational perspective on mindreading,â€ Biological cybernetics, vol. 109, no. 4â€“5, pp. 453â€“467, 2015. 
[33] G. Pezzulo and H. Dindo, â€œWhat should I do next? Using shared representations to solve interaction 
problems,â€ Experimental Brain Research, vol. 211, no. 3, pp. 613â€“630, 2011. 
[34] G. Pezzulo, F. Donnarumma, and H. Dindo, â€œHuman Sensorimotor Communication: A Theory of Signaling in 
Online Social Interactions,â€ PLoS ONE, vol. 8, no. 11, p. e79876, Nov. 2013 
[35] C. L. Baker, R. Saxe, and J. B. Tenenbaum, â€œAction understanding as inverse planning,â€ Cognition, vol. 113, no. 
3, pp. 329â€“349, Sep. 2009 
[36] T. D. Ullman et al. , â€œHelp or hinder: Bayesian models of social goal inference,â€ in Proceedings of Neural 
Information Processing Systems, 2009. 
[37] N. Tang, S. Stacy, M. Zhao, G. Marquez, and T. Gao, â€œBootstrapping an Imagined We for Cooperation.,â€ in 
CogSci, 2020. 
[38] J. Hwang, J. Kim, A. Ahmadi, M. Choi, and J. Tani, â€œDealing with large -scale spatio -temporal patterns in 
imitative interaction between a robot and a human by using the predictive coding framework,â€ IEEE 
Transactions on Systems, Man, and Cybernetics: Systems, vol. 50, no. 5, pp. 1918â€“1931, 2018. 
[39] J. Tani, R. Nishimoto, J. Namikawa, and M. Ito,  â€œCodevelopmental learning between human and humanoid 
robot using a dynamic neural-network model,â€ IEEE Trans Syst Man Cybern, vol. 38, no. 1, pp. 43â€“59, 2008 
[40] N. Wirkuttis and J. Tani, â€œLeading or Following? Dyadic Robot Imitative Interaction Using th e Active 
Inference Framework,â€ IEEE Robotics and Automation Letters, vol. 6, no. 3, pp. 6024â€“6031, Jul. 2021 
[41] K. Dautenhahn et al., â€œHow may I serve you? A robot companion approaching a seated person in a helping 
context,â€ in Proc of the 1st ACM SIGCHI/SIGART conference on Human-robot interaction, 2006, pp. 172â€“179. 
[42] A. Clodic and R. Alami, â€œWhat Is It to Implement a Human -Robot Joint Action?,â€ in Robotics, AI, and Humanity, 
Springer, Cham, 2021, pp. 229â€“238. 
[43] A. Clodic, E. Pacherie, R. Alami, and R. Chatila, â€œKey Elements for Human Robot Joint Action,â€ p. 159, 2017. 
[44] T. Parr, G. Pezzulo, and K. J. Friston, Active Inference: The Free Energy Principle in Mind, Brain, and Behavior.  
MIT Press, 2022. 
[45] Q. Song, F. Liu, H. Su, and A. V. Vasila kos, â€œSemi -global and global containment control of multi -agent 
systems with second-order dynamics and input saturation,â€ International Journal of Robust and Nonlinear Control, 
vol. 26, no. 16, pp. 3460â€“3480, 2016. 
[46] I. Konvalinka, P. Vuust, A. Roepstor ff, and C. D. Frith, â€œFollow you, follow me: continuous mutual prediction 
and adaptation in joint tapping.,â€ Q J Exp Psychol (Colchester), vol. 63, no. 11, pp. 2220â€“2230, Nov. 2010 
[47] J. C. Skewes, L. Skewes, J. Michael, and I. Konvalinka, â€œSynchronised and complementary coordination 
mechanisms in an asymmetric joint aiming task,â€ Experimental brain research, vol. 233, no. 2, pp. 551â€“565, 2015. 
[48] G. Novembre, G. Knoblich, L. Dunne, and P. E. Keller, â€œInterpersonal synchrony enhanced through 20 Hz 
phase-coupled dual brain stimulation,â€ Soc Cogn Affect Neurosci, Jan. 2017 
[49] S. Garrod and M. J. Pickering, â€œJoint Action, Interactive Alignment, and Dialog,â€ Topics in Cognitive Science, 
vol. 1, no. 2, pp. 292â€“304, 2009 
[50] G. Pezzulo and H. Dindo, â€œWhat s hould I do next? Using shared representations to solve interaction 
problems,â€ Experimental Brain Research, vol. 211, no. 3, pp. 613â€“630, 2011. 
[51] C. Bishop, Pattern Recognition and Machine Learning. Springer, 2006. 
[52] D. Maisto, F. Donnarumma, and G. P ezzulo, â€œNonparametric Problem -Space Clustering: Learning Efficient 
Codes for Cognitive Control Tasks,â€ Entropy, vol. 18, no. 2, p. 61, Feb. 2016, 
[53] K. Von Fritz, â€œNOOÎ£ and Noein in the Homeric Poems,â€ Classical Philology, vol. 38, no. 2, pp. 79â€“93, 1943. 
[54] G. Gergely and G. Csibra, â€œTeleological reasoning in infancy: the naive theory of rational action,â€ Trends in 
Cognitive Sciences, vol. 7, pp. 287â€“292, 2003. 
[55] N. Sebanz, G. Knoblich, W. Prinz, and E. Wascher, â€œTwin peaks: an ERP study of action planning and control 
in co-acting individuals.,â€ J Cogn Neurosci, vol. 18, no. 5, pp. 859â€“870, May 2006 
[56] N. Sebanz, G. Knoblich, and W. Prinz, â€œHow two share a task: corepresenting stimulus -response mappings.,â€ J 
Exp Psychol Hum Percept Perform, vol. 31, no. 6, pp. 1234â€“1246, Dec. 2005 
[57] G. Knoblich and N. Sebanz, â€œEvolving intentions for social interaction: from entrainment to joint action.,â€ 
Philos Trans R Soc Lond B Biol Sci, vol. 363, no. 1499, pp. 2021â€“2031, Jun. 2008 
23 
 
[58] L. M. Sacheli, E. Tido ni, E. F. Pavone, S. M. Aglioti, and M. Candidi, â€œKinematics fingerprints of leader and 
follower role-taking during cooperative joint actions,â€ Exp Brain Res, vol. 226, no. 4, pp. 473â€“486, May 2013 
[59] C. Vesper and M. J. Richardson, â€œStrategic communicat ion and behavioral coupling in asymmetric joint 
action,â€ Exp Brain Res, May 2014, doi: 10.1007/s00221-014-3982-1. 
[60] F. Leibfried, J. Grau-Moya, and D. A. Braun, â€œSignaling equilibria in sensorimotor interactions,â€ Cognition, vol. 
141, pp. 73â€“86, Aug. 2015,  
[61] G. Pezzulo, F. Donnarumma, and H. Dindo, â€œHuman Sensorimotor Communication: A Theory of Signaling in 
Online Social Interactions,â€ PLoS ONE, vol. 8, no. 11, p. e79876, Nov. 2013 
[62] G. Pezzulo, F. Donnarumma, H. Dindo, A. Dâ€™Ausilio, I. Konvalinka, and C. Castelfranchi, â€œThe body talks: 
Sensorimotor communication and its brain and kinematic signatures,â€ Physics of life reviews, 2018. 
[63] K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, and G. Pezzulo, â€œActive Inference: A Process Theory,â€ 
Neural Comput, vol. 29, no. 1, pp. 1â€“49, Jan. 2017, doi: 10.1162/NECO_a_00912. 
[64] T. Parr, G. Pezzulo, and K. J. Friston, Active Inference: The Free Energy Principle in Mind, Brain, and Behavior. 
MIT Press, 2022. 
[65] K. Friston and C. Frith, â€œA Duet for one,â€ Consciousness and cognition, vol. 36, pp. 390â€“405, 2015. 
[66] K. Belhassein et al., â€œAddressing joint action challenges in HRI: Insights from psychology and philosophy,â€ 
Acta Psych, vol. 222, p. 103476, 2022  
[67] F. Donnarumma, H. Dindo, and G. Pezzulo, â€œSensorimotor communication for humans and robots: improving 
interactive skills by sending coordination signals,â€ IEEE Transactions on Cognitive and Developmental Systems , 
vol. PP, no. 99, pp. 1â€“1, 2017 
[68] A. D. Dragan, K. C. Lee, and S. S . Srinivasa, â€œLegibility and predictability of robot motion,â€ in 2013 8th 
ACM/IEEE International Conference on Human-Robot Interaction (HRI), 2013, pp. 301â€“308. 
[69] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, â€œDeep Reinforcement Learning for Multiagent Systems: A 
Review of Challenges, Solutions, and Applications,â€ IEEE Transactions 
 
 
  
24 
 
Supplementary materials 
 
 
 
Fig. S1. Score (expected free energy) that the grey agent assigns to its 25 policies at the 
beginning of the  first trial of Simulation 1 . The policies are ordered according to their 
expected free energy; please note that the policies having the highest expected free energy (to 
the left) are the least preferred, whereas those having the lowest expected free energ y (to the 
right) are the most preferred. The two equally preferred policies (those having the lowest 
expected free energy) are the 24th and the 25th, which infers that both the grey agent (T1) and 
the white agent (T2) will go straight to the same button, r ed or blue. Here, â€œgoing straightâ€ 
means passing through L7 and L11 (for the grey agent) passing through L15 and L11 (for the 
white agent). Rather, â€œlong policyâ€ means (for example, for the grey agent) going through L4, 
L5, L8 and L13 to reach the blue but ton. Please note also that the two most dis -preferred 
policies (those having the highest expected free energy) are the 1 st and 2nd, which infer that 
the two agents will go directly to the two opposite goals. 
 

25 
 
 
Fig. S2. Results of 100 repetitions of Simulation 1, for 30 trials and without changes of mind. 
The blue and red squares indicate that the agents successfully solved the task by pressing the 
blue and red buttons, respectively. The black squares indicate failures. 
 

26 
 
 
Fig. S3. Results of 100 repetitions of Simulation 1, for 30 trials and without changes of mind, 
when interactive inference is prevented. The blue and red squares indicate that the agents 
successfully solved the task by pressing the blue and red buttons, respective ly. The black 
squares indicate failures. See the main text for explanation. 
 
 
 

27 
 
 
Fig. S4. Average results of 100 runs, with the same parameters as Simulation 1, for 15 trials, 
when interactive inference is prevented. The top panel shows a measure of belief alignment of 
the agents: the KL divergence between their beliefs about task goals. The mean value is in 
black and the standard deviation of the mean is in grey. The bottom panel shows a histogram 
of mean success rate.  
 
 
 
 
 
 
 

28 
 
 
 
Fig. S5. Score (expected free energy) that the grey agent â€“ acting as the leader â€“ assigns to its 
25 policies at the beginning of the first trial of Simulation 2. The policies are ordered 
according to their expected free energy; please note that the policies having the highest 
expected free energy (to the left) are the least preferred, whereas those having the lowest 
expected free energy (to the right) are the most preferred. The most preferred policy, the 25 th, 
infers that the leader will follow the long path to the red goal and t he follower will follow the 
short path to the red goal. The second most preferred policy, the 25 th, infers that both agents 
will follow the long path to the red goal. These (and most of the other preferred) policies are 
â€œsocial epistemic policiesâ€ for the leader. 
 
 

29 
 
 
Fig. S6. Results of 100 repetitions of Simulation 2, for 10 trials. The red squares indicate that 
the agents successfully solved the task by pressing the red buttons. The black squares indicate 
failures. 
 
 

30 
 
 
Fig. S7. Average results of 100 runs, with the same parameters as Simulation 2, but after 
removing the â€œepistemic componentâ€ from the expected free energy Fig.. See the main text for 
explanation. 
 
 
  

31 
 
 
 
 
Fig. S8. Schematic illustration of the multi -agent active inference model used in the current 
article. (A)  Sketch of the â€œjoint mazeâ€ task illustrated in Fig. 1. Two agents, â€œgreyâ€ (â€œ iâ€) and 
â€œwhiteâ€ (â€œjâ€), start from the locations L3 and L19 and their goal is to reach either the red (L10) 
or the blue (L12) goal locations. (B) The stochastic variables of the generative model adopted 
for the agents. Si, the hidden states, are the tensorial product between the position of the ith 
agent, the position of the jth agent  and the joint goal context belief. Oi is the observation 
tensor defined as the product between the observed position of the ith agent, the observed 
position of the jth agent, the observed joint context, and the outcome utility. The symbol ut 
denotes the j oint action at the time t defined as the tensorial product between the action of 
the ith agent and the action of the jth agent. A sequence of joint actions (u1, u2, â€¦ , ut) forms a 
policy, indicated by the Greek letter Ï€t. (C) A probabilistic graph of the gene rative model for 
multi-agent active inference. The plate notation indicates that the structures held in a box 
vary as functions of the specified indexes. Therefore, each agent denoted with an index (from 
1 to N) of the outer box executes the process in the  inner box, whose time horizon is t =
0, â€¦ , T âˆ’ 1. During the execution of that process, each agent sends his position and last action 
and receives the positions and the actions of the other agents. Each agent uses the information 
from the other agents to infer the evolution of the entire scenario and to update his own 
model. Note that this scheme could be extended to multiple agents. Please see the main text 
for details. 
 
 
 

32 
 
 
 
Fig. S 9.  Schematic illustration of the tensor A3
i , playing the role of a â€œsalience mapâ€, which 
varies as a function of the agentâ€™s beliefs about the joint task goal. The figure is organized as a 
table, in which the columns correspond to different values of the mode of the initial belief 
about the task go al i.e., max(D3
i )) and the rows correspond to the four possible joint goals: 
â€œblue, blueâ€, â€œblue, redâ€, â€œred, blueâ€, â€œred, redâ€. Each of the 12 matrices shows a specific tensor 
A3
i , which encodes the (color-coded) salience of the positions of the two agents in the maze. As 
illustrated in the figure, the higher the initial belief, the less informative the â€œsalience mapâ€. 
See the main text for explanation. 
 
