Online system identiﬁcation in a Duﬃng
oscillator by free energy minimisation
Wouter M. Kouw
Bayesian Intelligent Autonomous Systems lab
TU Eindhoven, Eindhoven, 5612AP Netherlands
w.m.kouw@tue.nl
Abstract. Online system identiﬁcation is the estimation of parameters
of a dynamical system, such as mass or friction coeﬃcients, for each mea-
surement of the input and output signals. Here, the nonlinear stochastic
diﬀerential equation of a Duﬃng oscillator is cast to a generative model
and dynamical parameters are inferred using variational message passing
on a factor graph of the model. The approach is validated with an exper-
iment on data from an electronic implementation of a Duﬃng oscillator.
The proposed inference procedure performs as well as oﬄine prediction
error minimisation in a state-of-the-art nonlinear model.
Keywords: Online system identiﬁcation ·Duﬃng oscillator ·Free en-
ergy minimisation ·Variational message passing ·Forney factor graphs
1 Introduction
Natural agents are believed to develop an internal model of their motor system
by generating actions in muscles and observing limb movements [11]. It has been
suggested that forming this internal model is analogous to a form of online system
identiﬁcation [24]. System identiﬁcation, i.e. estimating dynamical parameters
from observed input and output signals, has a rich history in engineering. But
there might still be much to gain from considering biologically-plausible proce-
dures. Here, I explore online system identiﬁcation using a leading theory of how
brains process information: free energy minimisation [8,3].
To test free energy minimisation for use in engineering applications, I consider
a speciﬁc benchmark1 problem called a Duﬃng oscillator. Duﬃng oscillators are
relatively well-behaved nonlinear diﬀerential equations, making them excellent
toy problems for methodological research. Its diﬀerential equation is cast to a
generative model, with a corresponding factor graph. The factor graph admits
a recursive parameter estimation procedure through message passing [14,12].
Speciﬁcally, variational message passing minimises free energy [5,13,18]. Here,
I infer the parameters of a Duﬃng oscillator using online variational message
passing. Experiments show that it performs as well as a nonlinear ARX model
with parameters trained oﬄine using prediction error minimisation [2].
1 http://nonlinearbenchmark.org/
arXiv:2009.00845v1  [cs.LG]  2 Sep 2020
2 W.M. Kouw
2 System
Consider a rigid frame with two prongs facing rightwards (see Figure 1 left). A
steel beam is attached to the top prong. If the frame is driven by a periodic
forcing term, the beam will displace horizontally as a driven damped harmonic
oscillator. Two magnets are attached to the bottom prong, with the steel beam
suspended in between. These act as a nonlinear feedback term on the beam’s
position, attracting or repelling it as it gets closer [15].
Fig. 1: (Left) Example of a physical implementation of a Duﬃng oscillator.
(Right) Example of input and output signals.
Let y(t) be the observed displacement, x(t) the true displacement, and u(t)
the observed driving force. The position of the beam is described as follows [25]:
md2x(t)
dt2 + cdx(t)
dt + ax(t) + bx3(t) = u(t) + w(t) (1a)
y(t) = x(t) + v(t) , (1b)
where m is mass, c is damping, a the linear and b the nonlinear spring stiﬀness
coeﬃcient. Both the state transition as well as the observation likelihood contain
noise terms, which are assumed to be Gaussian distributed: w(t) ∼N (0,τ−1)
(process noise) and v(t) ∼N(0,ξ−1) (measurement noise). The challenge is to
estimate m, c, a, b, τ and ξ such that the output of the system can be predicted
as accurately as possible.
3 Identiﬁcation
First, I discretise the state transition of Equation 1 using a central diﬀerence
for the second derivative and a forward diﬀerence for the ﬁrst derivative. Re-
arranging to form an expression in terms of xt+1 yields:
xt+1 = 2m + cδ−aδ2
m + cδ xt + −bδ2
m + cδx3
t + −m
m + cδxt−1 + δ2
m + cδ(ut + wt) , (2)
Online system identiﬁcation by free energy minimisation 3
where δ is the sample time step. Secondly, to ease inference at a later stage, I
perform the following variable substitutions:
θ1 = 2m+cδ−aδ2
m+cδ , θ2 = −bδ2
m+cδ, θ3 = −m
m+cδ, η= δ2
m+cδ, γ= τ(m+cδ)2
δ4 , (3)
where the square in the numerator for γ stems from absorbing the coeﬃcient
into the noise term ( V[ηwt] = η2V[wt]). Note that the mapping between φ =
(m,c,a,b,τ) and ψ= (θ1,θ2,θ3,η,γ ) can be inverted to recover point estimates:
m = −θ3δ2
η , c = (1 + θ3)δ
η , a = 1 −θ1 −θ3
η , b = −θ2
η , τ = γη2 . (4)
Thirdly, the state transition can be cast to a multivariate ﬁrst-order form:
[
xt+1
xt
]
  
zt
=
[
0 0
1 0
]
  
S
[
xt
xt−1
]
  
zt−1
+
[
1
0
]

s
g(θ,zt−1) +
[
1
0
]
ηut +
[
1
0
]
˜wt, (5)
where g(θ,zt−1) = θ1xt+ θ2x3
t + θ3xt−1 and ˜wt ∼N(0,γ−1). The system is now
a nonlinear autoregressive process. Lastly, integrating out ˜wt and vt produces a
Gaussian state transition and a Gaussian likelihood, respectively:
zt ∼N(f(θ,zt−1,η,u t),V ) (6a)
yt ∼N(s⊤zt,ξ−1) , (6b)
where f(θ,zt−1,η,u t) = Szt−1 + sg(θ,zt−1) + sηut and V =
[γ−1 0 ; 0 ϵ]
. The
number ϵ represents a small noise injection to stabilise inference [6].
To complete the generative model description, priors must be deﬁned. Mass
m and process precision τ are known to be strictly positive parameters, while
the damping and stiﬀness coeﬃcients can be both positive and negative. By
examining the variable substitutions, it can be seen that θ1, θ2, θ3 and η can be
both positive and negative, but γ can only be positive. As such, the following
parametric forms can be chosen for the priors:
θ∼N(m0
θ,V 0
θ ) , η ∼N(m0
η,v0
η) , γ ∼Γ(a0
γ,b0
γ) , ξ ∼Γ(a0
ξ,b0
ξ) . (7)
3.1 Free energy minimisation
Given the generative model, a free energy functional with a recognition model q
can be formed as follows:
−log p(y,u) ≤
∫∫
q(ψ,z) q(ψ,z)
p(y,u,z,ψ) dzdψ = F[q] (8)
where z = ( z1,...,z T), y = ( y1,...,y T) and u = ( u1,...,u T). I assume the
states factor over time and that the parameters are largely independent:
q(ψ,z) = q(θ)q(η)q(γ)q(ξ)
T∏
t=1
q(zt) . (9)
4 W.M. Kouw
All recognition densities are Gaussian distributed, except forq(γ) and q(ξ), which
are Gamma distributed. In free energy minimisation, the parameters of the recog-
nition distributions depend on each other and are iteratively updated.
3.2 Factor graphs and message passing
In online system identiﬁcation, parameter estimates should be updated at each
time-step. That puts time constraints on the inference procedure. Message pass-
ing is an ideal inference procedure due to its eﬃciency in factorised generative
models [12]. Figure 2 is a graphical representation of the generative model, with
nodes for factors and edges for variables. Square nodes with Greek letters rep-
resent stochastic operations while · and = represent deterministic operations.
The node marked ”NLARX” represents the state transition described in Equa-
tion 6a.
NLARX
=
=
=
...
...
...
...
...
...
...
= ...
4 ↓ 8↑
3 ↓ 7↑
2 ↓ 6↑
zt − 1
1
→
9
→
5
← zt
12
→
γ
η
θ
· s
N
yt
=... ...
ξ
10
→
11
←
ut
N
N
N
Γ
Γ
Fig. 2: Forney-style factor graph of the generative model of a Duﬃng oscillator.
Nodes represent conditional distributions and edges represent variables. Nodes
send messages to connected edges. When two messages on an edge collide, the
marginal belief q for the corresponding variable is updated. Each belief update
reduces free energy. By iterating message passing, free energy is minimised.
The terminal nodes on the left represent the initial priors for the states
and dynamical parameters. Inference starts when these nodes pass messages.
The subgraph - separated by columns of dots - represents the structure of a
single time step, recursively applied. Messages 1 , 2 , 3 , 4 and 10 represent
beliefs q from previous time-steps. Message 5 , arriving at the state transition
node, originates from the likelihood node attached to observation yt. Messages
Online system identiﬁcation by free energy minimisation 5
6 , 7 , 8 , 9 and 11 combine priors from previous time steps and likelihoods
of observations, and are used to update beliefs q. Message 12 is the current state
belief and becomes message 1 in the next time step.
The graph actually contains more messages, such as those sent by equal-
ity nodes. I have hidden them to avoid complicating the ﬁgure. Their form has
been extensively described in the literature and can be looked up easily [12,14].
Modern message passing toolboxes, such as Infer.NET and ForneyLab.jl, auto-
matically incorporate them. However, the NLARX node is new. Its messages can
be computed with2:
6 − →ν(θ) = exp
(
Eq(zt)q(zt−1)q(η)q(γ)
[
log N(f(θ,zt−1,η,u t),V )
])
(10a)
7 − →ν(η) = exp
(
Eq(zt)q(zt−1)q(θ)q(γ)
[
log N(f(θ,zt−1,η,u t),V )
])
(10b)
8 − →ν(γ) = exp
(
Eq(zt)q(zt−1)q(θ)q(η)
[
log N(f(θ,zt−1,η,u t),V )
])
(10c)
9 − →ν(zt) = exp
(
Eq(zt−1)q(θ)q(η)q(γ)
[
log N(f(θ,zt−1,η,u t),V )
])
, (10d)
where I use a ﬁrst-order Taylor expansion to approximate the expected value of
the nonlinear autoregressive function g(θ,zt−1).
Loeliger et al. (2007) have written an accessible introduction on message pass-
ing in factor graphs [14]. Variational message passing in autoregressive processes
has been described in detail as well [5,19].
4 Experiment
The Duﬃng oscillator has been implemented in an electronic system called Sil-
verbox [25]. It consists of T = 131702 samples, gathered with a sampling fre-
quency of 610.35 Hz. Figure 3 shows the time-series, plotted at every 80 time
steps. There are two regimes: the ﬁrst 40000 samples are subject to a linearly
increasing amplitude in the input (left of the black line in Figure 3) and the
remaining samples are subject to a constant amplitude but contain only odd
harmonics (right of the black line). The second regime is used as a training data
set, where both input and output data were given and parameters needed to
be inferred. The ﬁrst regime is used as a validation data set, where the inferred
parameters are ﬁxed and the model needs to make predictions for the output
signal.
I performed two experiments3: a 1-step ahead prediction error and a simula-
tion error setting. I used ForneyLab.jl, with NLARX as a custom node, to run the
message passing inference procedure [4]. I call the model above FEM-NLARX,
for Nonlinear Latent Autoregressive model with eXogenous input using Free En-
ergy Minimisation. I implemented two baselines: the ﬁrst is NLARX without the
nonlinearity (i.e. the nonlinear spring coeﬃcient b = 0), dubbed FEM-LARX.
2 Derivations at https://github.com/biaslab/IWAI2020-onlinesysid
3 Experiment notebooks at https://github.com/biaslab/IWAI2020-onlinesysid
6 W.M. Kouw
Fig. 3: Silverbox data set, sampled at every 80 time steps for visualisation. The
black line splits it into validation data (left) and training data (right).
The second is a standard NARX model, implemented using MATLAB’s System
Identiﬁcation Toolbox. I modelled the static nonlinearity with a sigmoid net-
work of 4 units (in line with the 4 coeﬃcients used by NLARX and LARX).
Parameters were inferred oﬄine using Prediction Error Minimisation. Hence,
this baseline is called PEM-NARX.
I chose uninformative priors for the coeﬃcients θ and η: Gaussians centred
at 1 with precisions of 0 .1. The authors of Silverbox indicate that the signal-
to-noise ratio at measurement time was high [25]. I therefore chose informative
priors for the noise parameters: a0
ξ = 1e8 and a0
γ = 1e3 (shape parameters) and
b0
ξ = 1e3 and b0
γ = 1e1 (scale parameters).
4.1 1-step ahead prediction error
At each time-step in the validation data, the models were given the previous
output signal yt−1,yt−2 and the current input signal ut and had to infer the
current output yt. It is a relatively easy task, which is reﬂected in all three
models’ performance. The top row in Figure 4 shows the predictions of all three
models in purple and their squared error with respect to the true output signal
in black. The left column shows the oﬄine NARX baseline (PEM-NARX), the
middle column the linear online latent autoregressive baseline (FEM-LARX)
and the right column the nonlinear online latent autoregressive model (FEM-
NLARX). Note that the errors in the top row seem completely ﬂat. The bottom
row in the ﬁgure plots the errors on a log-scale. PEM-NARX has a mean squared
error of 5.831e-5, FEM-LARX one of 5.945e-5 and FEM-NLARX one of 5.830e-5.
4.2 Simulation error
In this experiment, the models were not given the previous output signal, but
had to use their predictions from the previous time-step. This is a much harder
task, because errors will accumulate. The top row in Figure 5 again shows the
predictions of all three models (purple) and their squared error (black). It can
already be seen that the errors increase as the input signal’s amplitude rises. The
bottom row plots the errors on a log-scale. PEM-NARX has a mean squared error
of 1.000e-3, FEM-LARX one of 1 .002e-3 and FEM-NLARX one of 0 .926e-3.
Online system identiﬁcation by free energy minimisation 7
Fig. 4: 1-step ahead prediction errors. (Left) Oﬄine NARX model with sigmoid
net (PEM-NARX), (middle) online linear model (FEM-LARX) and (right) on-
line nonlinear model (FEM-NLARX). (Top) Predictions (purple) and squared
error (black). (Bottom) Squared prediction errors in log-scale.
5 Discussion
The experimental results seem to justify looking to nature for inspiration. Free
energy minimisation, in the form of variational message passing, seems a gener-
ally applicable and well-performing inference technique. The diﬃculties mostly
lie in deriving variational messages (i.e. Equations 10).
Improvements in the proposed procedure could be made with a richer approx-
imation of the nonlinear autoregressive function (e.g. unscented transform) [20].
Alternatively, a hierarchy of latent Gaussian ﬁlters or autoregressive processes
could be used to obtain time-varying noise parameters or time-varying coeﬃ-
cients [22,19]. Furthermore, instead of discretising such that an auto-regressive
model is obtained, one could express the evolution of the states in generalised
coordinates. Lastly, black-box models could be explored for further performance
improvements.
A natural next step is for an active inference agent to determine the control
signal regime (i.e. optimal design). Unfortunately, this is not straightforward:
the current formulation relies on variational free energy which does not produce
an epistemic term in the objective. The epistemic term is needed to encourage
exploration; i.e. try sub-optimal inputs to reduce uncertainty. To arrive at an
epistemic term, one would need to work with expected free energy [17]. But it
is unclear how expected free energy could be incorporated into factor graphs.
5.1 Related work
Online system identiﬁcation procedures typically employ recursive least-squares
or maximum likelihood inference, with nonlinearities modelled by basis expan-
8 W.M. Kouw
Fig. 5: Simulation errors. (Left) Oﬄine NARX model with sigmoid net (PEM-
NARX), (middle) online linear model (FEM-LARX) and (right) online nonlinear
model (FEM-NLARX). (Top) Predictions (purple) and squared error (black).
(Bottom) Squared prediction errors in log-scale.
sions or neural networks [16,23,7]. Online Bayesian identiﬁcation procedures
come in two ﬂavours: sequential Monte Carlo samplers [10,1] and online vari-
ational Bayes [26,9]. This work is novel in the use of variational message passing
as an eﬃcient implementation of online variational Bayes and its application to
a nonlinear autoregressive model.
6 Conclusion
I have presented a free energy minimisation procedure for online system identiﬁ-
cation. Experimental results showed comparable performance to a state-of-the-
art nonlinear model with parameters estimated oﬄine. This indicates that the
procedure performs well enough to be deployed in engineering applications.
Future work should test variational message passing in more challenging non-
linear identiﬁcation settings, such as a Wiener-Hammerstein benchmark [21].
Furthermore, problems with time-varying dynamical parameters, such as a robotic
arm picking up objects with mass, would be interesting for their connection to
natural agents.
7 Acknowledgements
The author thanks Magnus Koudahl, Albert Podusenko and Thijs van de Laar
for insightful discussions and the reviewers for their constructive feedback.
Online system identiﬁcation by free energy minimisation 9
References
1. Abdessalem, A.B., Dervilis, N., Wagg, D., Worden, K.: Identiﬁcation of nonlinear
dynamical systems using approximate Bayesian computation based on a sequen-
tial Monte Carlo sampler. In: International Conference on Noise and Vibration
Engineering (2016)
2. Aguirre, L.A., Letellier, C.: Modeling nonlinear dynamics and chaos: a review.
Mathematical Problems in Engineering 2009 (2009)
3. Buckley, C.L., Kim, C.S., McGregor, S., Seth, A.K.: The free energy principle for
action and perception: A mathematical review. Journal of Mathematical Psychol-
ogy 81, 55–79 (2017)
4. Cox, M., van de Laar, T., de Vries, B.: Forneylab.jl: Fast and ﬂexible automated
inference through message passing in Julia. In: International Conference on Prob-
abilistic Programming (2018)
5. Dauwels, J.: On variational message passing on factor graphs. In: IEEE Interna-
tional Symposium on Information Theory. pp. 2546–2550 (2007)
6. Dauwels, J., Eckford, A., Korl, S., Loeliger, H.A.: Expectation maximization as
message passing - Part I: Principles and Gaussian messages. arXiv:0910.2832 (2009)
7. Engel, Y., Mannor, S., Meir, R.: The kernel recursive least-squares algorithm. IEEE
Transactions on Signal Processing 52(8), 2275–2285 (2004)
8. Friston, K., Kilner, J., Harrison, L.: A free energy principle for the brain. Journal
of Physiology 100(1-3), 70–87 (2006)
9. Fujimoto, K., Satoh, A., Fukunaga, S.: System identiﬁcation based on variational
Bayes method and the invariance under coordinate transformations. In: IEEE Con-
ference on Decision and Control and European Control Conference. pp. 3882–3888
(2011)
10. Green, P.L.: Bayesian system identiﬁcation of a nonlinear dynamical system using
a novel variant of simulated annealing. Mechanical Systems and Signal Processing
52, 133–146 (2015)
11. de Klerk, C.C., Johnson, M.H., Heyes, C.M., Southgate, V.: Baby steps: Investi-
gating the development of perceptual–motor couplings in infancy. Developmental
Science 18(2), 270–280 (2015)
12. Korl, S.: A factor graph approach to signal modelling, system identiﬁcation and
ﬁltering. Ph.D. thesis, ETH Zurich (2005)
13. van de Laar, T., Cox, M., Senoz, I., Bocharov, I., de Vries, B.: Forneylab: a toolbox
for biologically plausible free energy minimization in dynamic neural models. In:
Conference on Complex Systems (2018)
14. Loeliger, H.A., Dauwels, J., Hu, J., Korl, S., Ping, L., Kschischang, F.R.: The
factor graph approach to model-based signal processing. Proceedings of the IEEE
95(6), 1295–1322 (2007)
15. Moon, F., Holmes, P.J.: A magnetoelastic strange attractor. Journal of Sound and
Vibration 65(2), 275–296 (1979)
16. Paleologu, C., Benesty, J., Ciochina, S.: A robust variable forgetting factor re-
cursive least-squares algorithm for system identiﬁcation. IEEE Signal Processing
Letters 15, 597–600 (2008)
17. Parr, T., Friston, K.J.: Generalised free energy and active inference. Biological
Cybernetics 113(5-6), 495–513 (2019)
18. Parr, T., Markovic, D., Kiebel, S.J., Friston, K.J.: Neuronal message passing us-
ing mean-ﬁeld, Bethe, and marginal approximations. Scientiﬁc reports 9(1), 1–18
(2019)
10 W.M. Kouw
19. Podusenko, A., Kouw, W.M., de Vries, B.: Online variational message passing in
hierarchical autoregressive models. In: IEEE International Symposium on Infor-
mation Theory. pp. 1343–1348 (2020)
20. S¨ arkk¨ a, S.: Bayesian ﬁltering and smoothing, vol. 3. Cambridge University Press
(2013)
21. Schoukens, M., No¨ el, J.P.: Three benchmarks addressing open challenges in non-
linear system identiﬁcation. IFAC-PapersOnLine 50(1), 446–451 (2017)
22. Senoz, I., Podusenko, A., Kouw, W.M., de Vries, B.: Bayesian joint state and
parameter tracking in autoregressive models. In: Conference on Learning for Dy-
namics and Control. pp. 1–10 (2020)
23. Tangirala, A.K.: Principles of system identiﬁcation: theory and practice. CRC Press
(2018)
24. Tin, C., Poon, C.S.: Internal models in sensorimotor integration: perspectives from
adaptive control theory. Journal of Neural Engineering 2(3), S147 (2005)
25. Wigren, T., Schoukens, J.: Three free data sets for development and benchmarking
in nonlinear system identiﬁcation. In: European Control Conference (ECC). pp.
2933–2938 (2013)
26. Yoshimoto, J., Ishii, S., Sato, M.a.: System identiﬁcation based on online varia-
tional Bayes method and its application to reinforcement learning. In: Artiﬁcial
Neural Networks and Neural Information Processing, pp. 123–131. Springer (2003)