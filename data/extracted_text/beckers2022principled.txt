3202
voN
9
]GL.sc[
3v43190.0122:viXra
PRINCIPLED PRUNING OF BAYESIAN NEURAL NETWORKS
THROUGH VARIATIONAL FREE ENERGY MINIMIZATION
A PREPRINT
JimBeckers1 BartvanErp∗,1 ZiyueZhao2 KirillKondrashov2 BertdeVries1,2
1DepartmentofElectricalEngineering,EindhovenUniversityofTechnology,Eindhoven,TheNetherlands
2GNHearing,Eindhoven,TheNetherlands∗
November10,2023
ABSTRACT
Bayesian model reduction provides an efficient approach for comparing the performance of all
nestedsub-modelsofamodel,withoutre-evaluatinganyofthesesub-models. Untilnow,Bayesian
modelreductionhasbeenappliedmainlyinthecomputationalneurosciencecommunityonsimple
models.Inthispaper,weformulateandapplyBayesianmodelreductiontoperformprincipledprun-
ingofBayesianneuralnetworks,basedonvariationalfreeenergyminimization. Directapplication
ofBayesian modelreduction,however,givesrise to approximationerrors. Therefore,a noveliter-
ative pruning algorithm is presented to alleviate the problems arising with naive Bayesian model
reduction,assupportedexperimentallyonthepubliclyavailableUCIdatasetsfordifferentinference
algorithms. This novelparameterpruningscheme solves the shortcomingsof currentstate-of-the-
artpruningmethodsthatareusedbythesignalprocessingcommunity. Theproposedapproachhas
a clear stopping criterion and minimizes the same objective that is used during training. Next to
thesebenefits,ourexperimentsindicatebettermodelperformanceincomparisontostate-of-the-art
pruningschemes.
Keywords BayesianModelReduction·BayesianNeuralNetworks·ParameterPruning·VariationalFreeEnergy
1 Introduction
Current state-of-the-art neural networks excel in a variety of signal processing tasks, such as audio processing [1].
Yet, the largesize of these state-of-the-artmodelslimits their directapplicability in storage- and power-constrained
devices,suchashearingaids. Forexample,neuralnetworksforhearingaidsareoftenrelativelysmall[2–4], where
performance is sacrificed for real-time and efficient operations. These smaller architectures limit expressivity and
assumethatallparametersareusedefficiently. Anotherstrategyistostartwithalargenetworkandtocompressthe
networkinordertoutilizelessstorageandpowerresources[5–7]. Oneapproachtonetworkcompression,parameter
pruning,removesparametersfromthenetworkinordertolowercomputationalandstoragedemands,whilstaiming
to retain good-enoughperformance. Unfortunately, these pruning methods for neural networks are often based on
heuristic criteria with tunable thresholds, thus turning parameter pruning into a lengthy (and costly) trial-and-error
procedure.
Bayesian neural networks (BNNs) are the Bayesian equivalent of neural networks, in which the beliefs about the
parametervaluesofthenetworkarerepresentedbyprobabilitydistributions.BNNsenjoyapropermodelperformance
criterion: the variational free energy (VFE). VFE provides a yardstick to compare different models regardingtheir
performanceon“accuracyminuscomplexity". Inotherwords, optimizingVFE leadstomodelsthatmaximizedata
fit (accuracy) and simultaneously minimize model complexity (Occam’s razor is built-in). These objectives align
withgoodmodelingpractices. Consequently,BNNsprovideseveraladvantagesoverregularneuralnetworkssuchas
allowingforincorporatingpriorknowledge,robustnesstooverfittingandstraightforwardonlineorcontinuallearning
[8].Mostimportantly,theyallowformoreprincipledcompressionmethodsastheyretainbeliefsabouttheirparameter
∗Correspondence: b.v.erp@tue.nl
PrincipledPruningofBayesianNeuralNetworks APREPRINT
values. Asoftoday,pruninginBNNshasseenlittleattentionwhencomparedtopruninginregularneuralnetworks
[9,10], asitis still an activeareaof research[11,12]. Currentstate-of-the-artparameterpruningmethodsinBNNs
lacksomevitalcharacteristics.Firstly,theproposedpruningobjectivesdonotcoincidewiththeobjectivesusedduring
thetrainingoftheBNN[13–15]. Additionally,currentpruningstrategiesdonotprovideaclearoptimalpruningrate
astheyarebasedonheuristiccriteriawithtunablethresholds.
Originating from the computationalneuroscience literature, Bayesian model reduction (BMR) [16,17] provides an
approximatemethodfor comparingVFE amongall nestedsub-modelsof a model, withoutre-evaluatingany of the
sub-models. Inshort,weonlytrainamodelandBMRprovidesacheapandprincipledmethodtoselectprunedsub-
modelswithbetterVFEthanthelargemodel.BMRenjoysbiologicalplausibilityasdescribedinvariouspapersinthe
neuroscienceliterature[16,17]. Sofar,BMRhasreceivedlittleattentionoutsideoftheneuroscienceliteratureandhas
thereforenotbeenappliedtolargecomplexmodels. IfproperlydevelopedforBNNs, itwouldalsoprovideastrong
methodforfastandprincipledpruningoftrainedBNNstosmallerandbetterperformingBNNs,toenableadoptionof
neuralnetworksinresource-constraineddevices.
This paper extends Bayesian model reduction to parameter pruning in BNNs that tackles the above problems with
currentstate-of-the-artpruningapproaches. Specifically,weuse BMRtoevaluatetheperformanceofprunedBNNs
byshrinkingthepriordistributionsovertheirparameters.DirectapplicationofBMRexposesfundamentallimitations
whendealingwithfactorizedposteriordistributions,aswewillshowinSection4. Thispaperalleviatesthisproblem
byproposinganovelpruningprocedureforBNNs,wheretrainingandpruningsharethesameobjective. Weexperi-
mentallyshowthatourprincipledpruningprocedureforBNNsachievesthelowestVFEincomparisontoconventional
pruningheuristicsonthepubliclyavailableUCIdatasets[18]. Furthermorewe showthatourprocedureisinvariant
todifferentinferencealgorithms. BeforeintroducingourprincipledpruningschemeinSection4,wefirstspecifythe
modelof the BNN in Section 2 and touch uponthe inferenceprocedurein Section 3. We postponean overviewof
the related work to Section 6 since we believe that the reader will appreciate the comparison to existing literature
moreaftertheintroductionofourproposedpruningalgorithm. WeverifyandvalidateourcontributionsinSection5.
Section7discussesourapproachandprovidesopportunitiesforfutureresearch.Section8concludesthepaper.
2 Model specification
Beforethepresentationofournovelparameterpruningschemein Section4, wewillfirstformallyspecifytheprob-
abilisticmodelofourBNNinthissection. TheninSection3, theprobabilisticinferenceprocedureusedtoperform
computationsinthismodeliselaboratedupon.
ConsiderthelikelihoodfunctionoftheBNN
N
p(Y |θ,γ)= N y |f (x ),Λ −1 , (1)
n θ n
n=1
Y (cid:0) (cid:1)
where N(·|µ,Σ) depicts a normal distribution with mean µ and covariance Σ. X = {x ,x ,...,x } and Y =
1 2 N
{y ,y ,...,y } denote the sets of possibly multivariate inputs x ∈ RD and outputs y ∈ RM of the BNN,
1 2 N n n
respectively.Here,DandM refertothedimensionalityoftheinputandoutputsamples,andN representsthenumber
ofdatasamples.TheprecisionmatrixΛrepresentsadiagonalmatrixwithpositiveelementsγ =[γ ,γ ,...,γ ]⊤ ∈
1 2 M
RM . The most important term f (·) represents the underlying neural network parameterized by the set of model
>0 θ
parametersθ. ThepruningalgorithmaspresentedinSection4ismodelagnosticandthereforetheexactspecification
of f (·) will only be presented in Section 5.1 as the architecture differs between experiments. Although (1) solely
θ
referstoregressionmodels,extensionstomodelssuitableforclassificationarestraightforwardanddonotchangethe
specificationofthepruningalgorithmofSection4.
In order to complete the specification of the BNN, we will define prior distributionsover θ and γ. We assume the
factorizedprioroverthemodelparameters
L
p(θ)= N θ|µ ,σ2 , (2)
θ θ
Y
l=1θ∈ Yθ(l)
(cid:0) (cid:1)
whichfactorizesovertheLlayersinthemodelandovertheindividualparameterswithineachlayerθ ∈ θ(l). Addi-
tionally,wesetthepriordistribution
M
p(γ)= Γ(γ |α ,β ), (3)
m m m
m=1
Y
whereΓ(·|α,β)representsagammadistributionwithshapeandrateparametersαandβ,respectively.
2
PrincipledPruningofBayesianNeuralNetworks APREPRINT
3 Probabilisticinference
Probabilistic inference concerns the computation of posterior and predictive distributions in the BNN specified by
(1)-(3). Theexactposteriordistributionp(θ,γ|Y)canbecomputedusingBayes’ruleas
p(Y |θ,γ)p(θ)p(γ)
p(θ,γ|Y)= . (4)
p(Y)
However,thecomputationofthisposteriordistributionisintractableformostchoicesoff (·). Asolutiontothisprob-
θ
lemisobtainedbyapproximatingtheexactposteriordistributionp(θ,γ|Y)withavariationalposteriorq(θ,γ)∈Q,
whichisconstrainedtoasimplifiedfamilyofdistributionsQ. Wefindtheoptimalsolutionforq(θ,γ)byminimizing
thevariationalfreeenergy(VFE),givenby
q(θ,γ)
F[p,q]= q(θ,γ)ln dθdγ. (5)
p(Y |θ,γ)p(θ)p(γ)
Z
NotethattheVFEcanbedecomposedinto
F[p,q]=KL[q(θ,γ)kp(θ,γ|Y)]− lnp(Y), (6)
posteriordivergence log-evidence
which, since the Kullback-Leibler(KL) dive|rgenceis a{lwzays non-ne}gati|ve,{izllu}strates that the VFE forms an upper
bound on the negative log-evidence. Therefore the VFE is equivalent to the well-known negative evidence lower
bound(ELBO)criterioninthemachinelearningcommunity.MinimizationoftheVFEminimizestheKL-divergence
betweenthevariationalposteriorq(θ,γ)andtheexactposteriordistributionp(θ,γ|Y),andreducesthegapbetween
theVFEandthenegativelog-evidence,whichisdeterminedbytheobservationsandthemodelspecification.
Optimization of (6) is not possibe as the exact posterior p(θ,γ|Y) is unknown. Thereforewe proceed to derive a
minimizationprocedurethroughthedecomposition
F[p,q]=KL[q(θ,γ)kp(θ)p(γ)]−E [lnp(Y |θ,γ)]. (7)
q(θ)q(γ)
complexity accuracy
Furthermorewe assume thatthe vari|ationalpos{tezriorhasa}mea|n-fieldfact{ozrizationq(θ},γ) = q(θ)q(γ), where the
individualfactorsarefurtherfactorizedas
L
q(θ)= N(θ|µˆ ,σˆ2), (8a)
θ θ
Y
l=1θ∈ Yθ(l)
M
q(γ)= Γ(γ |αˆ ,βˆ ). (8b)
m m m
m=1
Y
Heretheˆ·accentdistinguishestheparametersofthevariationalposteriordistributionsfromtheparametersoftheprior
distributions. Under these simplifying assumptions the KL-divergence between the variational posterior and prior
distributionhasaclosed-formsolutionforthechosenfamilyofdistributions.Theaccuracytermcanbeapproximated
indifferentways, forexampleusingvariancebackpropagation[12], wherethefirstandsecondcentralmomentsare
propagatedthroughtheneuralnetwork,orBayes-by-backprop[14],whichemployssamplingforcomputingtheoutput.
Usingtheseapproximations,theVFEcanbeoptimizedovertheparametersofthevariationalposteriordistributions
using(stochastic)gradientdescent,alsoknownasstochasticvariationalinference[19,20].
4 Parameter pruning
Now thatwe have specified the probabilisticmodelof the BNN in Section 2 and have elaborateduponthe training
procedureinSection3,wecanintroduceournovelparameterpruningscheme.Thisparameterpruningschemeshares
its objectivewith the training stage and has a clear stoppingcriterion, solving the issues with conventionalpruning
approachesashighlightedinSection1.
Our pruning scheme is based on Bayesian model reduction [16,17]. After training the full model, BMR supports
efficientrecomputationofposteriordistributionsandmodelevidenceuponchangestothemodelprior. Considerthe
likelihoodfunctionp(Y |θ)in (1), wherethe γ argumentis omitted to simplify notation, withoutremovingit from
the model. We omit γ to focus on θ, which encompasses the vast majority of model parameters that we wish to
3
PrincipledPruningofBayesianNeuralNetworks APREPRINT
1.5
0
5.
1
1
1 0. 5
0.5 1.5
0.5
1
1.5
2 2 1 0.5 3 5 0 35
−1 0 1
µˆθ
2σˆ θ
signal-to-noiseratio signal-plus-robustness Bayesianmodelreduction
1.5 2.5 1.5
1.5 1.5
.0
5 0
5.
1 2.5 1 0 1
2
2 1
1
0.5 1.5 1 1.5 0.5 0.5 0
−0.5
0.5
1
2 0 2 3 1 0 0.5 0 31 5 5
−1 0 1 −1 0 1
µˆθ µˆθ
Figure 1: An overview of the pruning objectives signal-to-noise ratio, signal-plus-robustness and Bayesian model
reduction.Thepriordistributionwassettop(θ)=N(θ|0,1)andthenewpriortop˜(θ)=N(θ|0,ε),withε=10−16.
Thepruningobjectiveshavebeencomputedwithrespecttothevariationalposteriordistributionq(θ)=N(θ|µˆ ,σˆ2).
θ θ
All three objectives prune from low to high values. Only Bayesian model reduction has a clear stopping criterion,
locatedatzero,asindicatedbytheredcontourintherightplot. Theinteriorofthiscontourissubjecttopruning.
Algorithm1IterativetrainingandpruningofaBayesianneuralnetworkbyVFEminimization.
1: inputs
Bayesianneuralnetworkmodelwithparameterpriorp(θ)
2: whileParameterswereprunedinpreviousiterationdo
3: Train(pruned)Bayesianneuralnetwork
4: forθ ∈θdo ⊲Loopoveractiveparameters
5: p˜ θ (θ)←p(θ\θ)p˜(θ) ⊲Specifynewprior
6: Compute∆F θ forp˜ θ (θ)using(10b)
7: if∆F θ ≤0then
8: θ ←θ\θ ⊲Pruneparameter
9: endif
10: endfor
11: endwhile
prune. Based onthe priordistributionp(θ), we can computetheposteriorp(θ|Y)andmodelevidencep(Y)using
Bayes’ rule. Letusnowintroducea neworreducedpriorp˜(θ), wherethe˜· accentreferstothe newsituation. This
priorsharesthesamelikelihoodfunctionasbefore,meaningthatp(Y |θ)=p˜(Y |θ)holds.Insteadofcomputingthe
newposteriorp˜(θ|Y)andnewevidencep˜(Y)fromscratchagain,whichisoftena costlyoperation,BMR usesthe
originalposteriorandevidenceinordertocomputethenewposteriorandevidenceinasimplifiedmanner.Theshared
likelihoodfunctionallowsustocomputethenewposteriorandlog-evidenceas[16,17]
p˜(θ)p(Y)
p˜(θ|Y)=p(θ|Y) , (9a)
p(θ)p˜(Y)
p˜(θ)
lnp˜(Y)=lnp(Y)+ln p(θ|Y) dθ. (9b)
p(θ)
Z
DetailedderivationsarepresentedinAppendixA.
InBNNs wegenerallydonothaveaccesstotheintractablep(θ|Y)andp(Y)asexplainedinSection3. Therefore
wewillneedtoapproximatetheBMRprocedurebysubstitutingthevariationalposteriorq(θ)fortheexactposterior
p(θ|Y)andtheVFEF[p,q]forthenegativelog-evidence−lnp(Y),basedontheboundin(6). Resultingfromthese
substitutions,weobtainexpressionsforthenewvariationalposteriorq˜(θ)andforthechangeinVFE∆Fas
p˜(θ)
q˜(θ)≈q(θ) exp{∆F}, (10a)
p(θ)
p˜(θ)
∆F=F[p,q]−F[p˜,q˜]≈−ln q(θ) dθ. (10b)
p(θ)
Z
4
PrincipledPruningofBayesianNeuralNetworks APREPRINT
Intuitivelyexp{∆F}canbeobtainedfrom(10a)usingthenormalizationpropertyofq˜(θ). Asaresultofthefactor-
izationassumptionsin(2)and(8a),thechangeinVFEdecomposesin∆F ≈ ∆F ,where∆F isthechange
θ∈θ θ θ
inVFEobtainedbypruningasingleparameterθ.
P
Parameterpruningisperformedbyevaluating∆F foreachparameterindividually,whereallparameterswith∆F ≤
θ θ
0arepruned. Despitetheseemingcomplexityof(10), theexpressionssubmittosimpleclosed-formsolutionswhen
the prior and variational posterior distributions are assumed to be in the same family of distributions as presented
in [17, Table 1]. AppendixB presentsthe closed-formsolutionsfornormaldistributions. For ourchoiceof normal
priorandvariationalposteriordistributions,weperformpruningbysettingtheindividualpriordistributionstop˜(θ)=
N(θ|0,ε),whereθ ∈ θ isasingleparameterinthemodelandwhereεrepresentsasufficientlysmallnumberclose
tozero. Basically,thenewpriorissettocoverasmallregionaroundzero,preventingthecorrespondingvariational
posterior to differ significantly upon new observations. For each of the parameters θ ∈ θ the difference in VFE
∆F is evaluated. Pruning starts from the lowest values of ∆F and stops when ∆F surpasses zero because then
θ θ θ
an improvementin VFE is no longer possible. Figure 1 shows an intuitive comparison between BMR in (10) and
currentstate-of-the-artpruningmethodsforcommonchoicesofpriordistributions. Specifically,wecompareagainst
thesignal-to-noiseratio(SNR)[13,14]andsignal-plus-robustness(SPR)metric[15,Ch. 4]. Moreinformationabout
theserankingfunctionsispresentedinSection6.
The approximation in (10), resulting from the unavailability of the exact posterior distribution, leads to estimation
discrepanciesfortheVFEdifference. ForBNNs,theaccumulatingeffectsofthisapproximationleadtoadivergence
betweentheapproximatedandactualdifferenceinVFE.Simultaneously,becausetheprobabilisticmodelischanged
duringpruning,theobtainedvariationalposteriorq(θ)nolongerisguaranteedto belocatedina stationarypointof
theVFE,whichwasobtainedaftertraining.
Inordertocopewith these problems,we proposean updateto BMR byusingan iterativeprocesswherewe donot
useBMRsolelyasapost-hocmethodbutincorporateitintothetrainingstageofBNNs,similarlyasin[13,21]. The
processstartsbytrainingtheBNNuntilconvergenceoftheVFE.ItthencomputestheVFEchanges∆F forallθ ∈θ
θ
individuallyandgreedilyprunestheparameterswhere∆F ≤0.Theprocessoftrainingandpruningisrepeateduntil
θ
nomoreparameterscanbepruned.ThisiterativeprocedureisformalizedinAlgorithm1.
5 Experiments
Thissectionwillpresentexperimentalresultsfortheverificationofournewpruningalgorithm(Subsection5.2), for
the robustness and performance of our approach using different inference algorithms (Subsection 5.3) and for the
comparisontostate-of-the-artmethods,suchasSNRandSPR(Subsection5.4).
5.1 Experimentalsetup
AllexperimentswereperformedinPython(v.3.9)usingTensorFlow(v.2.9)1.Thenetworkf (·)in(1)isrepresented
θ
eitherbyfullyconnectednetworks(FCNs) or byrecurrentneuralnetworks(RNNs). We followedthe setupof[22],
which uses a subset of the publicly available UCI datasets [18] and an FCN with a single hidden layer of 50 units.
FortheRNNexperiments,weusedasyntheticdatasetconsistingoftime-periodicsignalsandaneuralnetworkwitha
singleGatedRecurrentUnit(GRU)[23]layerof16units. Weusedasequencelengthof8topredictthenextsingle
sample.Bothmodelsweretrainedbyminimizing(7)withAdam[24]usingvariancebackpropagation[12]andBayes-
by-backprop[14]withbothglobalandlocalparameterizations[25,26]. Forcomputingtheaccuracytermin(7)with
Bayes-by-backprop,weuseasinglesampleduringtrainingand10samplesduringtheperformanceevaluation.
5.2 Divergenceassessment
The approximationin and recursive usage of (10) leads to a divergencebetween the approximatedand actual VFE
for differentpruningrates. To illustrate this divergence,we performparameter pruningwith incrementsof 1% and
computethecorrespondingestimatedandexactVFE.Figure2showsthediscrepancybetweentheestimatedandactual
VFEfordifferentpruningratesonthebostondatasetforthedifferentinferencealgorithms. Thisdiscrepancycanbe
observedforalldatasets. Asaresultofthisdiscrepancy,asingleroundofpruninguntiltheoptimalstoppingcriterion
hasbeenreached,mightnotyieldtheminimumoftheVFE.ThisestablishestheneedoftheiterativeAlgorithm1.
1Allexperimentsarepubliclyavailableathttps://github.com/biaslab/PrincipledPruningBNN.
5
PrincipledPruningofBayesianNeuralNetworks APREPRINT
1600
1550
1500
1450
87.9%
1400
0 20 40 60 80 100
pruningrate[%]
]stan[ygreneeerflanoitairav
variancebackpropagation Bayes-by-backprop(global) Bayes-by-backprop(local)
1600 1600
estimated estimated estimated
actual actual actual
1550 1550
1500 1500
1450 1450
77.9% 88.9%
1400 1400
0 20 40 60 80 100 0 20 40 60 80 100
pruningrate[%] pruningrate[%]
Figure2:Estimatedandactualvariationalfreeenergyfordifferentpruningratesforthebostondataset. Thedifferent
plots illustrate the different inference algorithms: variance backpropagation[12] and Bayes-by-backprop[14] with
global [25] and local [26] reparameterization. The dashed line denotes the stopping criterion for a single pruning
iteration.Inallplots,theestimatedandactualoptimalpruningratesdiffer,establishingtheneedofAlgorithm1.
Table1:PerformanceoverviewofasingleiterationoftheBayesianmodelreductionalgorithmandoftheiterativeAl-
gorithm1acrossdifferentdatasets[18]andinferencealgorithms.Performanceisindicatedbytheobtainedvariational
freeenergyandpruningrate(withinbrackets)afterpruning. Variationalfreeenergyvaluesafterregulartrainingare
listedasreference.
variancebackpropagation Bayes-by-backprop(global) Bayes-by-backprop(local)
dataset start 1iteration iterative start 1iteration iterative start 1iteration iterative
boston 1.597 1.471(88%) 1.454(94%) 1.624 1.475(78%) 1.456(93%) 1.601 1.464(89%) 1.451(94%)
concrete 3.707 3.626(92%) 3.558(93%) 3.716 3.623(79%) 3.540(92%) 3.716 3.673(84%) 3.555(92%)
energy 4.581 4.217(75%) 4.026(83%) 4.208 3.827(75%) 3.392(85%) 4.255 3.870(75%) 3.414(84%)
kin8nm -5.906 -6.303(71%) -6.377(80%) -5.286 -5.681(75%) -5.894(85%) -5.419 -5.822(74%) -5.988(84%)
naval -141.498 -142.082(47%) -142.870(98%) -139.557 -141.783(81%) -142.902(99%) -140.424 -142.042(60%) -142.875(99%)
powerplant 29.267 29.164(17%) 28.517(53%) 28.895 28.771(28%) 28.190(68%) 29.251 29.186(10%) 28.262(74%)
wine 1.902 1.679(90%) 1.633(98%) 2.171 1.695(88%) 1.578(98%) 1.995 1.648(89%) 1.581(98%)
yacht 882 656(78%) 572(89%) 829 632(80%) 625(93%) 832 632(77%) 547(89%)
sine -1.081 39.579(89%) -1.221(93%) -943 -1.125(65%) -1.194(89%) -1.053 -1.191(76%) -1.225(89%)
sawtooth -1.042 -241(73%) -1.169(79%) -928 -1.135(74%) -1.201(82%) -1.024 -1.144(84%) -1.218(88%)
square -1.109 -1.211(89%) -1.225(97%) -947 -1.090(68%) -1.185(91%) -1.019 -1.188(82%) -1.216(88%)
5.3 Robustnessandperformance
Table 1 shows the performance of our proposed pruning algorithm for different inference algorithms and datasets.
We perform the pruning for a single iteration of the BMR algorithm according to Algorithm 1, until the stopping
criterion has been reached. For all datasets, we achieve a lower VFE with respect to the full model already for a
singleiterationoftheBMRalgorithm. Ourproposediterativeprocedureperformsbetterinallcases,achievingboth
lowerVFEvaluesaswellashigherpruningrates. Theseresultsareconsistentoverthedifferentdatasetsanddifferent
inferencealgorithms,underliningtherobustnessoftheproposedapproach.Oftentimesthefirstiterationhasthebiggest
contribution,however,thisdependssignificantlyonthedatasetandmodel. Dependingonthe applicationonecould
choosetostopthepruningalgorithmearlier.
5.4 Comparisontostate-of-the-art
Comparisonofourproposedalgorithmagainststate-of-the-artmethods,suchasSNRandSPR,isnotstraightforward,
becauseoftheirtunablepruningthresholds. Thesemethodsrequireevaluatingtheperformanceofthemodelfordif-
ferentpruningratesinordertofindtheoptimalprunedmodel.Ourmethodhasaclearstoppingcriterionandtherefore
doesnotneedtoevaluatethemodelperformancefordifferentpruningrates, whereasSNR- andSPR-based pruning
reliesonatrial-and-errorprocedureforfindingtheoptimalpruningrate.Furthermore,extendingSNR-andSPR-based
pruningwithretrainingisill-foundedastheirpruningandtrainingobjectivesdifferfromeachother. Nonetheless,we
perform a comparisonof the differentpruningmethods (SNR, SPR and BMR) by evaluating the VFE for different
pruningrates,withoutretraining. Figure3showsthecapabilityofreducingtheVFEforallpruningmethodsonthe
boston dataset. Furthermore, the underlying negative accuracy as defined in (7) is plotted to give insights in the
6
PrincipledPruningofBayesianNeuralNetworks APREPRINT
1800
1600
1400
0 20 40 60 80 100
pruningrate[%]
]stan[ygreneeerflanoitairav
]stan[ycaruccaevitagen
variancebackpropagation Bayes-by-backprop(global) Bayes-by-backprop(local)
1800 1800
variationalfreeenergy SNR
negativeaccuracy SPR
BMR
1600 1600
1400 1400
0 20 40 60 80 100 0 20 40 60 80 100
pruningrate[%] pruningrate[%]
Figure 3: Comparison of the signal-to-noise ratio (SNR), signal-plus-robustness(SPR) and Bayesian model reduc-
tion (BMR) pruningmetrics on the bostondataset. The differentplots illustrate the differentinferencealgorithms:
variancebackpropagation[12]andBayes-by-backprop[14]withglobal[25]andlocal[26]reparameterization. Solid
linescorrespondsto thevariationalfreeenergyanddashedlinestothenegativeaccuracyfrom(7). Theirdifference
correspondstothecomplexitytermin(7). Thedifferentcolorsdenotethedifferentpruningmethods.
individualVFEcontributions. Acrossallofourexperiments,theminimumVFEvalueforSNRisobservedtoalways
begreaterorequaltotheminimumvalueforBMR.TheflatregionintheBMRpruningmetriccorrespondstomodel
parameterswhosefullposteriorsareclosetotheirfullpriors. Aftertrainingweobserveanoftensignificantgroupof
parameters, whose sufficientstatistics of the full posteriorare close to those of the full prior. These parametersare
foundnottoaffecttheaccuracyofthemodelsignificantlyduringtraining,andarethereforeoptimizedtobecloseto
theoriginalpriortominimizemodelcomplexity. Asaresult, the clusterofparameterslocatedaroundthesufficient
statisticsofthefullpriorisprunedconsecutivelywithcorresponding∆F ≈0,leadingtotheplateausinFigure3.
θ
From Figure 3 it can be observed that both SNR and BMR reach almost the same minimum at high pruning rates.
ThiscanbeattributedtothesimilarityinpruningcurvesforlargerpruningthresholdsasillustratedinFigure1. While
SNRandBMRarebothabletoreachacertainminimuminasinglepruningstage,wearguethatBMRisthesuperior
method in pruning BNNs. A big advantage of BMR over SNR is the predefined pruning threshold. SNR would
requireasearchfortheminimumVFEoverdifferentpruningthresholds,leadingtoaveryexpensivepruningmethod
formodelswithalargenumberofmodelparameters. BMRisabletoreachtheminimuminasinglestepasweonly
needtoremovethoseparameterswhichdecreasetheVFE.Inaddition,theobjectiveofSNRhasnorelationtoVFE
andoftenin theliteraturedifferentmetricsareused to measurethe effectivenessof SNR-basedpruning[14]. Since
thetrainingandpruningofaBNNhaveadifferentobjective,thereisnoguaranteethattheprunedmodelwillhavea
reachedanoptimumintermsofVFE.WithBMRtheobjectivesareequal,indicatingthebetterperformanceofBMR
inminimizingtheVFEthroughpruning.
6 Related Work
Inthissection,wediscussrelatedworkcorrespondingtopruninginBNNsandcomparethesemethodstoourapproach.
However,beforedoingso,weprovidemotivationforourcommitmenttovariationalinferenceinSection3.
6.1 Probabilisticinference
The intractability of probabilistic inference in BNNs has produced two main groups of approximationmethods: 1)
Markovchain Monte Carlo (MCMC) sampling [27–29], which uses randomizedsamplingto approximateposterior
distributions,and2)variationalinference,whichminimizesaboundonBayesianevidencetoapproximatetheposterior
distribution. Incomparisontovariationalinference,MCMCmethodsscalepoorlytolargemodelswiththousandsor
millionsofparameters.Therefore,inthispaper,wefocusedonvariationalinferencemethodsforBNNtraining.
VariationalinferenceforBNNsiswell-representedintheliterature[11,12,14,22,30,31]withextensionstoclassifica-
tiontasks[32]andnon-normalpriororvariationalposteriordistributions[9,14,33]. Throughouttheexperimentsof
Section5wevalidateourpruningapproachfortwoofthemostcommonvariationalinferencealgorithms: Bayes-by-
backprop[14]andvariancebackpropagation[12]. Bayes-by-backprop[14]approximatestheaccuracytermin(7)by
samplingtheoutputwithdifferentpossibleparameterizations[25,26]. Thevariancebackpropagationalgorithm[12]
propagatesmomentsthroughthenetworkallowingforaclosed-formapproximationoftheaccuracytermintheVFE.
7
PrincipledPruningofBayesianNeuralNetworks APREPRINT
6.2 Modelcompression
Recently, multiple surveypapershave been publishedon the state-of-the-artcompressionmethodsfor regular(non-
Bayesian)neuralnetworks[5–7],whichcanbegroupedintothreemaincategories:networkreduction,networkmodi-
ficationandknowledgedistillation. Thispaperfocusesspecificallyonnetworkreduction.
The general aim of network reduction is to reduce the storage size or computational load of the network without
changing its structure. Within network reduction we can again distinguish between three subcategories: network
pruning,quantizationandlow-rankdecomposition. Thekeyideaofnetworkpruningistoremoveunimportantparts
ofthenetworksuchasparameters,nodes,layersandchannels. Componentsareremovedbasedoncertainheuristics
andafterwardthemodelisoftenretrainedtoregainaccuracy.ThefirstandmostcommonheuristicforpruningBNNs
istheSNR[13,14],definedas
E [θ]
q(θ)
SNR(θ)= , (11)
Var [θ]
(cid:12) q(θ)(cid:12)
(cid:12) (cid:12)
withθ beingaparameterofthemodel. Thecentralmomepntsarecomputedwithrespecttothecorresponding(often
approximate)posteriordistributionq(θ). SNRisthenusedasarankingfunctionwhereparameterswithlowvaluesof
SNRareprunedfirst. ThesecondcommonlyusedheuristicisSPRfrom[15,Ch. 4],definedas
SPR(θ)= E [θ] + Var [θ], (12)
q(θ) q(θ)
(cid:12) (cid:12) q
which is used similarly as a ranking function, whe(cid:12)re param(cid:12)eters with low SPR values are pruned first. [15, Ch. 4]
argues that model parameters with large means and variances should not be pruned, instead of parameters with a
largeSNR. A downsideof bothapproachesis that theylack a clear stoppingcriterion: the user hasto specify what
valuesof SNR or SPR still are acceptable. Choosing these thresholdsis a time-consumingprocedurethat limits its
automatability.
Nexttotheseparameterpruningtechniques,thereexistalternativeBayesiancompressionmethods.In[34],variational
dropout was adopted to sparsify BNNs during training. Earlier work of [26] was extended to allow for learning
dropout rates for individual weights. [9] uses hierarchical priors to prune nodes instead of weights in the network,
which encouragethe posterior to be sparse. Additionally, the posterior uncertainty of the weights has been used to
determinetheoptimalfixed-pointprecisiontoquantizethem.Similarly,thenovelmethodofBayesianautomaticmodel
compression(BAMC)[35]usesDirichletprocessmixturestoinfertheoptimallayer-wisequantizationbit-width.
7 Discussionandfuture work
TheproposedpruningalgorithmforBNNshasbeenshowntobeeffectiveinpruningasignificantnumberofparame-
ters. Althoughthishasledtothecompressionofthemodel,thecurrentlyunstructuredpruningofmodelparameters
mightresultinsparsestructuresthatlimitthegainedcomputationalefficiency.Forexample,sparsematricesareoften
savedinaspecialdataformatthatcouldlimittheimprovedefficiencyafterpruning.Instead,itwouldbemoreefficient
topruneentirecolumnsinthesematricesatonceortopruneentirekernelsinconvolutionallayers. Followingupon
thiscomment,investigationofstructuredpriorandvariationalposteriordistributionscouldleadtosignificantperfor-
manceimprovements.Insteadofassumingafullfactorizationbetweenparameters,thecovariancebetweenparameters
canberetainedusingmultivariatedistributions. Thesestructuredpruningapproachesarestraightforwardextensions
ofourapproach,wheretheVFEdifferenceisevaluatedforthepruningoflargerstructuredsetsofparameters.Yetwe
deemthisextensionanimportantdirectionforfutureresearch.
In the current experiments, we chose normal prior and variational posterior distributions, similarly to most of the
works in the literature. However, simultaneously there is significant effort being invested in selecting performance-
improvingprior distributions [15,36,37]. It would be interesting to investigate our pruning algorithm for different
priorandvariationalposteriordistributions,suchasLaplacedistributions,whichenjoyasparsifyingbehavioralready.
Inourwork,weaimedatprovidingaproper,grounded,BayesianframeworkfortrainingandpruningBNNs. There-
fore, we focused on the definition of our model and the appropriate inference methods. In line with this Bayesian
view,wedidnotincludeanycomparisonstothemethodofVariationalDropout[26,34]. Aspresentedin[38],there
aretwomainissueswithVariationalDropout;Being1)theuseofimproperpriorsand2)thesingularapproximating
distributionsoftheposteriors.[38]statesthatVariationalDropoutisnotanapproximationofvariationalinferenceand
shouldthereforebetreatedasanon-Bayesianmethod. Asaresult,wedonotprovideacomparisontothisapproach
andonlycomparetopost-hocpruningschemes. Thesecannamelybeappliedonalreadytrainednetworks,whereas
variationaldropoutrequiresadaptationofexistingmodels.
8
PrincipledPruningofBayesianNeuralNetworks APREPRINT
Finally, the currentpaper focuseson pruningthe BNN modelin a greedyand iterative fashion. This process could
yieldsuboptimalperformanceasparametersmightbeprunedprematurelyduetotheapproximationsin(10). Itwould
thereforebeinterestingtoinvestigatetheeffectsofrevertingthepruningofqualifyingvariablesinAlgorithm1. This
effectiveexpansionofmodelswouldleadtoabreakthroughinBNNdesign. Modelscanthenpruneandgrowthem-
selves,dependingonthecomplexityofthetask. Astrategycouldbetostartoutwithpriordistributionsrepresenting
pruned parameters and then use BMR to evaluate the VFE improvementcompared to the case where they had not
beenpruned. ThetruepowerofthisapproachisobtainedbyadheringtotheBayesianapproachandfocusingoncon-
tinualin-the-fieldlearning. Intertwiningourpruning(andpossibleextension)procedurewiththiscontinuallearning
approachcouldleadtomodelsthatbuildthemselves,automatingcostlytrial-and-errormodelrefinementtasks.
8 Conclusion
This paper has extended the use of Bayesian model reduction to Bayesian neural networks. We have introduced a
principledparameterpruningapproachforBayesianneuralnetworks,basedonaniterativepruning-trainingscheme.
Importantly,thisapproachminimizesthesameobjectiveasduringthetrainingprocedureandhasanintuitivethreshold-
freestoppingcriterion.Becauseitcarriesthesameoptimizationobjective,retrainingthenetworkafterpruningiswell-
groundedandcaneffectivelyalleviatediscrepanciesoriginatingfromthevariationalapproximations.Ourresultsshow
thatthisalgorithmachievesalowervariationalfreeenergyduringpruningincomparisontostate-of-the-artsignal-to-
noiseratioandsignal-plus-robustnesspruning. Ourpruningapproacheasesthe developmentof neuralnetworksfor
signalprocessingtasksinstorage-andpower-constraineddevices.
Acknowledgments
ThisworkwaspartlyfinancedbyGNHearingA/S.TheauthorswouldliketothanktheBIASlabteammembersand
colleaguesatGNHearingforvariousinsightfuldiscussionsrelatedtothiswork.
References
[1] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. Mcleavey, and I. Sutskever, “Robust Speech
Recognition via Large-Scale Weak Supervision,” in Proceedings of the 40th International Conference
on Machine Learning. PMLR, Jul. 2023, pp. 28492–28518, iSSN: 2640-3498. [Online]. Available:
https://proceedings.mlr.press/v202/radford23a.html
[2] I. Fedorov, M. Stamenovic, C. Jensen, L.-C. Yang, A. Mandell, Y. Gan, M. Mattina, and P. N. Whatmough,
“TinyLSTMs: Efficient Neural Speech Enhancement for Hearing Aids,” in Interspeech 2020, Oct. 2020, pp.
4054–4058,arXiv:2005.11138[cs,eess,stat].[Online].Available:http://arxiv.org/abs/2005.11138
[3] P. U. Diehl, Y. Singer, H. Zilly, U. Schönfeld, P. Meyer-Rachner, M. Berry, H. Sprekeler, E. Sprengel,
A. Pudszuhn, and V. M. Hofmann, “Restoring speech intelligibility for hearing aid users with deep learning,”
ScientificReports, vol.13,no.1,p.2719,Feb.2023,number: 1Publisher: NaturePublishingGroup.[Online].
Available:https://www.nature.com/articles/s41598-023-29871-8
[4] N. L. Westhausenand B. T. Meyer,“Lowbitrate binaurallink forimprovedultra low-latencylow-complexity
multichannel speech enhancement in Hearing Aids,” Jul. 2023, arXiv:2307.08858[eess]. [Online]. Available:
http://arxiv.org/abs/2307.08858
[5] Y.Cheng,D.Wang,P.Zhou,andT.Zhang,“ASurveyofModelCompressionandAccelerationforDeepNeural
Networks,”arXiv:1710.09282[cs],Jun.2020.[Online].Available:http://arxiv.org/abs/1710.09282
[6] R.Mishra,H.P.Gupta,andT.Dutta,“ASurveyonDeepNeuralNetworkCompression: Challenges,Overview,
andSolutions,”arXiv:2010.03954[cs,eess],Oct.2020.[Online].Available:http://arxiv.org/abs/2010.03954
[7] K. Zhang, H. Ying, H.-N. Dai, L. Li, Y. Peng, K. Guo, and H. Yu, “Compacting Deep Neural Networks for
Internet of Things: Methods and Applications,” IEEE Internet of Things Journal, vol. 8, no. 15, pp. 11935–
11959,Aug.2021.
[8] L. V. Jospin, W. Buntine, F. Boussaid, H. Laga, and M. Bennamoun, “Hands-on Bayesian Neural Networks
– a Tutorial for Deep Learning Users,” arXiv:2007.06823 [cs, stat], Sep. 2021. [Online]. Available:
http://arxiv.org/abs/2007.06823
[9] C. Louizos, K. Ullrich, and M. Welling, “Bayesian Compression for Deep Learning,” arXiv:1705.08665 [cs,
stat],Nov.2017.[Online].Available:http://arxiv.org/abs/1705.08665
9
PrincipledPruningofBayesianNeuralNetworks APREPRINT
[10] N. Chirkova, E. Lobacheva, and D. Vetrov, “Bayesian Compression for Natural Language Processing,”
arXiv:1810.10927[cs,stat],Dec.2018.[Online].Available:http://arxiv.org/abs/1810.10927
[11] A. Wu, S. Nowozin, E. Meeds, R. E. Turner, J. M. Hernández-Lobato, and A. L. Gaunt, “Deterministic
VariationalInferenceforRobustBayesianNeuralNetworks,”arXiv:1810.03958[cs,stat],Mar.2019.[Online].
Available:http://arxiv.org/abs/1810.03958
[12] M. Haussmann, F. A. Hamprecht, and M. Kandemir, “Sampling-Free Variational Inference of Bayesian
Neural Networks by Variance Backpropagation,” arXiv:1805.07654[cs, stat], Jun. 2019. [Online]. Available:
http://arxiv.org/abs/1805.07654
[13] A. Graves, “Practical Variational Inference for Neural Networks,” in Advances in Neural In-
formation Processing Systems, vol. 24. Curran Associates, Inc., 2011. [Online]. Available:
https://papers.nips.cc/paper/2011/hash/7eb3c8be3d411e8ebfab08eba5f49632-Abstract.html
[14] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, “Weight Uncertainty in Neural Networks,”
arXiv:1505.05424[cs,stat],May2015.[Online].Available:http://arxiv.org/abs/1505.05424
[15] E. T. Nalisnick, “On Priors for Bayesian Neural Networks,” Ph.D. dissertation, UC Irvine, 2018. [Online].
Available:https://escholarship.org/uc/item/1jq6z904
[16] K. Friston and W. Penny, “Post hoc Bayesian modelselection,” Neuroimage, vol. 56, no. 4-2, pp. 2089–2099,
Jun.2011.[Online].Available:http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3112494/
[17] K.Friston,T.Parr,andP.Zeidman,“Bayesianmodelreduction,”arXiv:1805.07092[stat],May2018.[Online].
Available:http://arxiv.org/abs/1805.07092
[18] D.DuaandC.Graff,“UCImachinelearningrepository,”2017.[Online].Available:http://archive.ics.uci.edu/ml
[19] M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley, “Stochastic Variational Inference,” Jour-
nal of Machine Learning Research, vol. 14, no. 4, pp. 1303–1347, 2013. [Online]. Available:
http://jmlr.org/papers/v14/hoffman13a.html
[20] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, “Variational Inference: A Review for Statisticians,” Journal
of the American Statistical Association, vol. 112, no. 518, pp. 859–877, Apr. 2017. [Online]. Available:
https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773
[21] M. Riera, J. M. Arnau, and A. González, “DNN pruning with principal component analysis and connection
importance estimation,” Journal of Systems Architecture, vol. 122, p. 102336, Jan. 2022. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S1383762121002307
[22] J. M. Hernández-Lobatoand R. P. Adams, “Probabilistic Backpropagationfor Scalable Learning of Bayesian
NeuralNetworks,”arXiv:1502.05336[stat],Jul.2015.[Online].Available:http://arxiv.org/abs/1502.05336
[23] K. Cho, B. van Merrienboer,C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, “Learning
PhraseRepresentationsusingRNNEncoder-DecoderforStatisticalMachineTranslation,”arXiv:1406.1078[cs,
stat],Sep.2014.[Online].Available:http://arxiv.org/abs/1406.1078
[24] D.P.KingmaandJ.Ba,“Adam:Amethodforstochasticoptimization,”CoRR,vol.abs/1412.6980,2015.
[25] D. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” arXiv:1312.6114 [cs, stat], May 2014.
[Online].Available:http://arxiv.org/abs/1312.6114
[26] D. P. Kingma, T. Salimans, and M. Welling, “Variational Dropout and the Local Reparameterization Trick,”
in Advances in Neural Information Processing Systems, vol. 28. Curran Associates, Inc., 2015. [Online].
Available:https://proceedings.neurips.cc/paper/2015/hash/bc7316929fe1545bf0b98d114ee3ecb8-Abstract.html
[27] S. Chib and E. Greenberg, “Understanding the metropolis-hastings algorithm,” The American Statistician,
vol.49,no.4,pp.327–335,1995.[Online].Available:http://www.jstor.org/stable/2684568
[28] R. M. Neal, Handbook of Markov Chain Monte Carlo. Chapman and Hall/CRC, May 2011. [Online].
Available:http://dx.doi.org/10.1201/b10905
[29] M.D.HoffmanandA.Gelman,“Theno-u-turnsampler: Adaptivelysettingpathlengthsinhamiltonianmonte
carlo,”2011.
[30] D. J. C. MacKay, “A practical Bayesian framework for backpropagation networks,” Neural Computation,
vol. 4, no. 3, pp. 448–472, May 1992, number: 3 Publisher: MIT Press. [Online]. Available:
https://resolver.caltech.edu/CaltechAUTHORS:MACnc92b
[31] D. Soudry, I. Hubara, and R. Meir, “Expectation Backpropagation: Parameter-Free Training
of Multilayer Neural Networks with Continuous or Discrete Weights,” in Advances in Neu-
ral Information Processing Systems, vol. 27. Curran Associates, Inc., 2014. [Online]. Available:
https://proceedings.neurips.cc/paper/2014/hash/076a0c97d09cf1a0ec3e19c7f2529f2b-Abstract.html
10
PrincipledPruningofBayesianNeuralNetworks APREPRINT
[32] S. Ghosh, F. M. Delle Fave, and J. Yedidia, “Assumed density filtering methods for learning bayesian neural
networks,”inThirtiethAAAIConferenceonArtificialIntelligence,2016.
[33] S. Sun, C. Chen, and L. Carin, “Learning Structured Weight Uncertainty in Bayesian Neural Networks,” in
Proceedingsof the 20thInternationalConference on ArtificialIntelligenceandStatistics. PMLR, Apr. 2017,
pp.1283–1292,iSSN:2640-3498.[Online].Available:https://proceedings.mlr.press/v54/sun17b.html
[34] D. Molchanov, A. Ashukha, and D. Vetrov, “Variational Dropout Sparsifies Deep Neural Networks,”
arXiv:1701.05369[cs,stat],Jun.2017.[Online].Available:http://arxiv.org/abs/1701.05369
[35] J. Wang, H. Bai, J. Wu, and J. Cheng, “Bayesian Automatic Model Compression,” IEEE Journal of Selected
TopicsinSignalProcessing,vol.14,no.4,pp.727–736,May2020.
[36] V. Fortuin, “Priors in Bayesian Deep Learning: A Review,” arXiv:2105.06868[cs, stat], May 2021.[Online].
Available:http://arxiv.org/abs/2105.06868
[37] V. Fortuin, A. Garriga-Alonso, F. Wenzel, G. Rätsch, R. Turner, M. van der Wilk, and L. Aitchison,
“Bayesian Neural Network Priors Revisited,” arXiv:2102.06571 [cs, stat], Oct. 2021. [Online]. Available:
http://arxiv.org/abs/2102.06571
[38] J.Hron,A.G.d.G.Matthews,andZ.Ghahramani,“VariationalBayesiandropout:pitfallsandfixes,”Jul.2018.
[Online].Available:http://arxiv.org/abs/1807.01969
11
PrincipledPruningofBayesianNeuralNetworks APREPRINT
A Bayesianmodel reduction derivations
Bayesianmodelreduction[16,17]allowsfortheefficientrecomputationoftheposteriordistributionandmodelevi-
denceuponchangesintheoriginalpriordistribution.BMRassumesthefollowingtwoprobabilisticmodels
p(Y,θ)=p(Y |θ)p(θ), (13a)
p˜(Y,θ)=p(Y |θ)p˜(θ). (13b)
Bothmodelssharethesamelikelihoodmodelp(Y |θ)anddifferthroughtheirpriordistributionsp(θ)andp˜(θ). To
distinguishbetweenbothsituations,weusethe˜·accenttorepresenttheneworreduced[17]situation.
Supposethatwestartoutwiththeprobabilisticmodelof(13a)andhavecomputedthecorrespondingposteriordistri-
butionp(θ|Y)andmodelevidencep(Y). Itmightbepossiblethattheinitialpriordistributionp(θ)isnotoptimaland
thatwewouldliketoreevaluatethecomputedtermsforanalternativepriordistributionp˜(θ). However,performing
allcomputationsagaincanbeacostlyoperation.Thereforeitwouldbeidealtoleveragetheinformationfromthepast
toefficientlycomputetheposteriordistributionp˜(θ|Y)andmodelevidencep˜(Y)ofthenewsituation.
Basedontheposteriordistributionp(θ|Y)andevidencep(Y)wecancomputethenewposteriordistributionp˜(θ|Y)
andnewevidencep˜(Y)forthenewpriordistributionp˜(θ)asfollows. First,weobtainthedefinitionsfortheoriginal
andnewposteriordistributionsusingBayes’ruleas
p(Y |θ)p(θ)
p(θ|Y)= , (14)
p(Y)
and similarly for p˜(θ|Y). Both terms share the same likelihood p(Y |θ). Rearranging both p(θ|Y) and p˜(θ|Y)
gives
p(θ|Y)p(Y) p˜(θ|Y)p˜(Y)
=p(Y |θ)= . (15)
p(θ) p˜(θ)
Herewecanrearrangetheexpressiontogivetheexpressionofthenewposteriordistribution
p˜(θ)p(Y)
p˜(θ|Y)=p(θ|Y) , (16)
p(θ)p˜(Y)
whereonlythetermp˜(Y)isunknown.Thistermcanbeobtainedbyintegratingbothsidesoverθweobtain
p(Y) p˜(θ)
1= p(θ|Y) dθ, (17)
p˜(Y) p(θ)
Z
fromwhichwecanobtain(9b)byrearrangingandbytakingthelogarithmofbothsidesoftheequality.Bysubstituting
thisresultinto(16)leadstothenewposteriordistribution.
B Closed-form expressionofEq. (10b)fornormals
Given the prior distribution p(θ) = N(θ|µ ,σ2), posterior q(θ) = N(θ|µ ,σ2) and reduced prior p˜(θ) =
p p q q
N(θ|µ˜ ,σ˜2). Thereducedposteriordistributionin(10a)canbederivedasq˜(θ)=N(θ|µ˜ ,σ˜2),where
p p q q
−1
1 1 1
σ˜2 = + − , (18a)
q σ2 σ˜2 σ2
(cid:18) q p p(cid:19)
µ µ˜ µ
µ˜ =σ˜2 q + p − p . (18b)
q q σ2 σ˜2 σ2
(cid:18) q p p(cid:19)
ThechangeinVFE∆F canbederivedas
θ
1 σ˜2σ2 1 µ2 µ˜2 µ2 µ˜2
∆F = ln q p − q + p − p − q . (19)
θ 2 σ˜2σ2 2 σ2 σ˜2 σ2 σ˜2
p q! q p p q!
12