ISC-POMDPs: Partially Observed Markov
Decision Processes with Initial-State
Dependent Costs
Timothy L. Molloy
©2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works.
Abstract— We introduce a class of partially observed
Markov decision processes (POMDPs) with costs that can
depend on both the value and (future) uncertainty associ-
ated with the initial state. These Initial-State Cost POMDPs
(ISC-POMDPs) enable the specification of objectives rela-
tive to a priori unknown initial states, which is useful in
applications such as robot navigation, controlled sensing,
and active perception, that can involve controlling systems
to revisit, remain near, or actively infer their initial states.
By developing a recursive Bayesian fixed-point smoother
to estimate the initial state that resembles the standard
recursive Bayesian filter, we show that ISC-POMDPs can
be treated as POMDPs with (potentially) belief-dependent
costs. We demonstrate the utility of ISC-POMDPs, including
their ability to select controls that resolve (future) uncer-
tainty about (past) initial states, in simulation.
Index Terms — Markov processes, optimal control,
stochastic systems.
I. INTRODUCTION
T
HE initial state of a dynamical system often has important
practical significance [1]–[4]. For example, the initial
position of a vehicle often corresponds to its owner’s residence
[1]; the initial pose of a robot has enabled safe or recoverable
navigation, path planning, and mapping [2], [3]; and, the initial
configuration of teams of agents can enable recognition of their
roles or intent [4]. This significance has given rise to partially
observed stochastic optimal control problems with objectives
tied directly to initial states, such as the problem of controlling
a system to hinder inference of its initial state to preserve
privacy in networked control systems [1], or the problem of
controlling a system to improve inference of its initial state for
active sensing and perception in target tracking and robotics
[2]–[4]. However, a general framework for solving initial-
state objective problems is lacking. We therefore introduce
and investigate Initial-State Cost Partially Observed Markov
Decision Processes (ISC-POMDPs).
Partially Observed Markov Decision Processes (POMDPs)
have costs that depend on the values of their (current) par-
tially observed state, and are typically solved by constructing
equivalent Markov decision processes (MDPs) with the belief
(i.e., the conditional distribution of the current state given
observed measurements computed via a recursive Bayesian
The author is with the CIICADA Lab, School of Engineering, The
Australian National University (ANU), Canberra, ACT 2601, Australia (e-
mail: timothy.molloy@anu.edu.au)
filter) as their “state” process. POMDPs have been generalized
to encompass cost functions (denoted ρ by convention) that
are explicit functions of the current belief, not just state value,
leading to ρ-POMDPs that can also be solved by constructing
belief MDPs (cf. [5], [6]). ρ-POMDPs have proved important
when controlling state uncertainty is explicitly an objective,
such as in active perception, controlled sensing, or privacy-
based applications (cf. [4]–[7] and [8, Chapter 8]). Never-
theless, both POMDPs and ρ-POMDPs, and their associated
belief MDPs, have Markovian dynamics and costs in the sense
that the current state (or belief) determines the current cost
and evolution of the state (or belief). Initial-state costs are,
however, non-Markovian.
Limited progress has been made in incorporating initial-
state costs into ( ρ-)POMDPs. Notably, [4] aimed to minimize
the entropy of the conditional distribution of the initial state
given (all) measurements at a terminal time by redefining
the belief to be this conditional distribution. However, it is
well-known from Bayesian smoothing that the conditional
distribution of the initial state does not have a Markovian
form computable via a Bayesian filter recursion (cf. [9, Section
4.1.1]). The approach of [4] therefore does not lead to refor-
mulations of initial-state cost problems as belief MDPs. More
generally, [10] considered non-Markovian costs in partially
observed problems by generalizing the approach of [11] devel-
oped for fully observed problems that involves augmenting the
underlying state so the dynamics and costs become Markovian.
This approach suffers from the fact that even if the state
can be modified so that it is Markovian, it may not have
a corresponding Markovian belief. For example, if the costs
only depend on the initial state (or its uncertainty, as in [4]),
then taking this original initial state as a new (static) modified
state leads to a trivial constant Markovian state process but the
conditional distribution of this modified state (i.e., the original
initial state) does not itself have a Markovian form computable
via a Bayesian filter recursion (cf. [9, Section 4.1.1]).
The key contribution of this paper is the introduction of ISC-
POMDPs with costs that can depend on both the value and
(future) uncertainty associated with initial states. We establish
that ISC-POMDPs with (arbitrary) initial-state dependent costs
admit reformulations and solutions as standard ( ρ-)POMDPs
with augmented state processes consisting of the both initial
and current state, and that their associated belief can be
computed with a recursive (fixed-point) Bayesian smoother
that resembles the standard Bayesian filter. Surprisingly, state
arXiv:2503.05030v1  [eess.SY]  6 Mar 2025
augmentation and recursive Bayesian smoothing have not
previously been used to solve initial-state cost problems.
This paper is structured as follows. We introduce ISC-
POMDPs in Section II. We reformulate and analyse ISC-
POMDPs as ( ρ-)POMDPs in Section III. We provide simu-
lations in Section IV, and conclusions in Section V.
Notation: Random variables are denoted by capital letters
(e.g., X), and their realizations by lower-case letters (e.g., x).
The probability mass function (pmf) of X is p(x), the joint
pmf of X and Y is p(x, y), and the conditional pmf of X
given Y = y is p(x|y) or p(x|Y = y). The expectation of a
function f of X is E[f(X)], and the conditional expectation
of f under p(x|y) is E[f(X)|y] or E[f(X)|Y = y]. For a
finite set S, the set of all probability distributions (or pmfs)
over S is ∆(S).
II. P RELIMINARIES AND PROBLEM FORMULATION
We first revisit ( ρ-)POMDPs and introduce ISC-POMDPs.
A. POMDP andρ-POMDP Preliminaries
Let Xk for k ≥ 0 be a discrete-time first-order Markov
chain with finite state space X ≜ {1, 2, . . . , Nx}. Let the initial
state X0 be distributed according to the pmf π0 ∈ ∆(X) with
components π0(x0) ≜ P(X0 = x0).1 Let the state Xk evolve
according to the state-transition probabilities:
Ax,¯x(u) ≜ p(Xk+1 = x|Xk = ¯x, Uk = u) (1)
for k ≥ 0 and x, ¯x ∈ X, with the controls Uk = u belonging
to the finite set U ≜ {1, 2, . . . , Nu}. The state Xk is (partially)
observed through stochastic observations Yk for k ≥ 1 taking
values in the finite set Y ≜ {1, 2, . . . , Ny}. The measurements
Yk are mutually conditionally independent given the states Xk
and controls Uk−1, and distributed according to the measure-
ment probabilities:
Bx(y, u) ≜ p(Yk = y|Xk = x, Uk−1 = u) (2)
for k ≥ 1, x ∈ X, y ∈ Y, and u ∈ U.
In (standard infinite-horizon discounted) POMDPs or ρ-
POMDPs, we may consider the controls Uk to be given by
a policy µπ : ∆(X) → Udependent on the belief πk about
the state Xk given the measurements Y k ≜ {Y1, Y2, . . . , Yk}
and controls Uk−1 ≜ {U0, U1, . . . , Uk−1} (cf. [12, Section
5.4.1] and [8, Chapter 7]). Specifically, let Uk = µπ(πk) for
k ≥ 0 where the belief πk ∈ ∆(X) is a conditional pmf, or
Nx-dimensional vector, with components πk(x) ≜ p(Xk =
x|yk, uk−1) satisfying the Bayesian filter recursion
πk+1(x) = Bx(yk+1, uk) P
¯x∈X Ax,¯x(uk)πk(¯x)P
¯x,˜x∈X B˜x(yk+1, uk)A˜x,¯x(uk)πk(¯x) (3)
for x ∈ X and k ≥ 0 given π0. We use πk+1 =
Π(πk, uk, yk+1) to denote the filter (3), and note that
p(yk+1|πk, uk) is the denominator in (3). We denote the set
of all (deterministic belief) policies µπ as Pπ, with pµ,π being
1Note that π0 ∈ ∆(X) can be viewed as a Nx-dimensional probability
vector (a vector with nonnegative components that sum to 1), and thus ∆(X)
can be viewed as the (Nx − 1)-dimensional simplex.
the probability law induced by µπ ∈ Pπ and its corresponding
expectation being Eµ,π[·]. A POMDP (formulated as a belief
MDP) then involves finding a policy that solves
inf
µπ∈Pπ
Eµ,π
" ∞X
k=0
γkC (πk, Uk)
#
s.t. π k+1 = Π(πk, Uk, Yk+1), π 0 ∈ ∆(X)
Yk+1|πk, Uk ∼ p(yk+1|πk, uk)
Uk = µπ(πk) ∈ U
(4)
given a discount factor γ ∈ (0, 1) and a cost function
C : ∆( X) × U →R that is the conditional expectation
of an underlying (current) state-control cost function κ :
X × U →R; i.e., C(πk, uk) ≜ E[κ(Xk, Uk)|πk, Uk = uk] =P
x∈X πk(x)κ(x, uk). In contrast, a ρ-POMDP (formulated as
a belief MDP) involves finding a policy solving [5]
inf
µπ∈Pπ
Eµ,π
" ∞X
k=0
γkρ (πk, Uk)
#
s.t. π k+1 = Π(πk, Uk, Yk+1), π 0 ∈ ∆(X)
Yk+1|πk, Uk ∼ p(yk+1|πk, uk)
Uk = µπ(πk) ∈ U
(5)
given a discount factor γ ∈ (0, 1) and an arbitrary belief-
dependent cost function ρ : ∆( X) × U →R. ρ-POMDPs
(5) generalize POMDPs since the cost function ρ can be any
(potentially nonlinear) function of the belief πk whilst the
POMDP cost function C is limited to the linear form implied
by conditional expectation. We propose ISC-POMDPs as an
extension of (ρ-)POMDPs with costs that can depend on both
the value and uncertainty associated with the initial state X0.
B. ISC-POMDPs
To introduce ISC-POMDPs, let us consider the possibility
of the controls Uk for k ≥ 0 being given by a policy µ that
is a (deterministic) function of the measurements and controls
(Y k, Uk−1) directly, namely, Uk = µ(Y k, Uk−1). Let the set
of all such policies be P, and let the probability law induced
by a policy µ ∈ Pbe pµ with corresponding expectation
Eµ[·]. Let us also define the (joint posterior) conditional pmf
of the state Xk and the initial state X0 given the information
(yk, uk−1) available at time k ≥ 0 as ξk ∈ ∆(X × X), where
ξk(x0, xk) ≜ p(X0 = x0, Xk = xk|yk, uk−1) (6)
for x0, xk ∈ X. We introduce an ISC-POMDP as the problem
of finding a policy that solves
inf
µ∈P
Eµ
" ∞X
k=0
γk [c(X0, Xk, Uk) + ψ(ξk, Uk)]
#
s.t. Xk+1|Xk, Uk ∼ Axk+1,xk (uk), X0 ∼ π0 ∈ ∆(X)
Yk+1|Xk+1, Uk ∼ Bxk+1 (yk+1, uk),
Uk = µ(Y k, Uk−1) ∈ U
(7)
for a given discount factor γ ∈ (0, 1) where c : X×X×U →R
is an arbitrary cost function dependent on the values of the
current state Xk, initial state X0, and controls Uk, and where
ψ : ∆(X ×X) ×U →R is an arbitrary function of the (joint)
posterior pmf ξk and the controls Uk.
ISC-POMDPs (7) generalize POMDPs (4) by introducing
costs c(X0, Xk, Uk) that can depend on the value of the
initial state X0, not just on the value of the current state Xk.
This dependence enables objectives to specified with respect
to the initial state; for example, the cost c(x0, x, u) = |x0 −x|
for x0, x∈ Xand u ∈ Uspecifies the objective of keeping
(future) states Xk close to the (potentially a priori unknown)
initial state X0. ISC-POMDPs (7) also generalize ρ-POMDPs
(5) by introducing costs ψ(ξk, Uk) that can depend on the joint
posterior ξk of the initial X0 and current state Xk at any time
k ≥ 0 (and hence also their marginals). This generalization
enables the optimization of uncertainty measures associated
with the initial and current states, and exploits that (future)
measurements Yk for k ≥ 1 can contain information about
the (past) initial state X0. For example, Bayesian (fixed-point
smoother) estimates of the initial state at future times k >0
can be improved by solving the ISC-POMDP (7) with the
initial-state entropy cost ψ(ξk, Uk) = H(X0|yk, uk−1) ≜
−P
x0∈X p(x0|yk, uk−1) logp(x0|yk, uk−1) where
p(x0|yk, uk−1) = P
xk∈X ξk(x0, xk).
We propose solving ISC-POMDPs (7) by reformulating
them as ( ρ-)POMDPs with augmented state processes con-
sisting of both the original initial state X0 and the orig-
inal current state Xk. We shall show that this choice of
augmented state leads to an associated (augmented) belief,
equivalent to the joint posterior pmf p(x0, xk|yk, uk−1), that
is Markovian and given by a Bayesian filter recursion, thus
enabling the solution of ISC-POMDPs using standard ( ρ-
)POMDP techniques (cf. [5], [6], [13], [14]). We note that
our choice of this augmented state is necessary to reformulate
ISC-POMDPs as ( ρ-)POMDPs since insight from recursive
Bayesian smoothing implies that only the joint posterior pmf
p(x0, xk|yk, uk−1) has a recursive form whilst the marginal
posterior pmf p(x0|yk, uk−1) does not (cf. [9, Section 4.1.1]).
Interestingly, this insight implies that the joint posterior
pmf p(x0, xk|yk, uk−1) must be used as the belief for ISC-
POMDPs, even when their costs only depend on the initial
state X0 or its posterior pmf p(x0|yk, uk−1), as in the case of
the initial-state entropy H(X0|yk, uk−1).
III. R EFORMULATION AND SOLUTION OF ISC-POMDP S
In this section, we reformulate ISC-POMDPs as ( ρ-
)POMDPs with an augmented state and a Markovian belief.
A. Augmented State and Belief Construction
Let us introduce the augmented state
Sk ≜ X0 + Nx(Xk − 1) ∈ S (8)
for k ≥ 0, with corresponding augmented state space S ≜
{1, 2, . . . , Ns} where Ns ≜ Nx × Nx. The augmented state
Sk provides an invertible representation of the pair (X0, Xk)
in the sense that given (X0, Xk), we can compute Sk via
(8), and given Sk we can compute (X0, Xk) via X0 = Sk −
Nx ⌊(Sk − 1)/Nx⌋ and Xk = (Sk − X0)/Nx + 1 where ⌊·⌋
denotes the floor function. Let L(x0, xk) ≜ x0 +Nx(xk −1) ∈
S for x0, xk ∈ Xbe the mapping implied by (8). 2
To derive the probabilistic structure of the augmented states
Sk, let the pmf describing the initial augmented state S0 be
ξ0 ∈ ∆(S) with ξ0(s) ≜ p(S0 = s) for s ∈ S. Similarly,
let the transition probabilities for the augmented states be
A
s,¯s
(u) ≜ p(Sk+1 = s|Sk = ¯s, Uk = u) for k ≥ 0, s, ¯s ∈ S,
and u ∈ U. Finally, let the measurement probabilities for the
augmented states be B
s
(y, u) ≜ p(Yk = y|Sk = s, Uk−1 = u)
for k ≥ 1, s ∈ S, y ∈ Yand u ∈ U. These probabilities are
developed in the following lemma.
Lemma 3.1: Under the constraints in the ISC-POMDP (7),
the initial augmented-state probabilities satisfy
ξ0(s) =
(
π0(x) if s = L(x, x),
0 otherwise
(9)
for s ∈ S and x ∈ X; the augmented state-transition
probabilities satisfy
A
s,¯s
(u) =
(
Ax,¯x(u) if s = L(x0, x) & ¯s = L(x0, ¯x),
0 otherwise
(10)
for s, ¯s ∈ S, x, ¯x, x0 ∈ X, and u ∈ U; and, the augmented-
state measurement probabilities satisfy
B
s
(y, u) = Bx(y, u) (11)
for s = L(x0, x) ∈ S, y ∈ Y, and u ∈ Uwhere x0, x∈ X.
Proof: By definition S0 = L(X0, X0), and thus for any
s = L(x, ¯x) ∈ Swhere x, ¯x ∈ Xwe have that
p(S0 = s) = p(X0 = x, X0 = ¯x) =
(
π0(x) if x = ¯x
0 otherwise,
proving (9). Consider any k ≥ 0, u ∈ U, s = L(x0, x) ∈ S
and ¯s = L(¯x0, ¯x) ∈ Swhere x, ¯x, x0, ¯x0 ∈ X, then
p(Sk+1 = s|Sk = ¯s, Uk = u)
= p(Xk+1 = x, X0 = x0|Xk = ¯x, X0 = ¯x0, Uk = u)
=
(
p(Xk+1 = x|Xk = ¯x, X0 = x0, Uk = u) if x0 = ¯x0
0 otherwise,
proving (10) since p(Xk+1 = x|Xk = ¯x, X0 = x0, Uk =
u) = p(Xk+1 = x|Xk = ¯x, Uk = u) = A
s,¯s
(u) due to Xk+1
and X0 being conditionally independent given Xk and Uk.
Finally, consider any k ≥ 1, s = L(x0, x0) ∈ S, y ∈ Y, and
u ∈ Uwhere x0, x∈ X, then p(Yk = y|Sk = s, Uk−1 =
u) = p(Yk = y|Xk = x, Uk−1 = u) since Yk and X0 are
conditionally independent given Xk and Uk−1, proving (11).
The proof is complete.
The conditional pmf ξk in (6) corresponds to the (aug-
mented) belief for the augmented state Sk. In a slight abuse
of notation, we shall therefore denote the conditional pmf
ξk introduced in (6) as ξk ∈ ∆(S) with ξk(s) ≜ p(Sk =
2The mapping L is analogous to those used to construct linear, vectorized,
or “flattened” indices of matrices (or tensors), with Sk being the linear
index of the pair (X0, Xk). More generally, the augmented state could be
constructed with any invertible (i.e., bijective) mapping L : X × X → S.
s|yk, uk−1) for s ∈ Sand k ≥ 0. The augmented state in (8)
enables us to treat ξk as an Ns-dimensional probability vector
and ∆(S) as the (Ns −1)-dimensional simplex. The following
lemma establishes that the augmented belief ξk evolves via a
simple recursion resembling the Bayesian filter (3).
Lemma 3.2: Under the constraints in the ISC-POMDP (7),
the augmented belief ξk evolves via the recursion
ξk+1(s) = B
s
(yk+1, uk) P
¯s∈S A
s,¯s
(uk)ξk(¯s)
P
¯s,˜s∈S B
˜s
(yk+1, uk)A
˜s,¯s
(uk)ξk(¯s)
(12)
for s ∈ Sand k ≥ 0 given the initial augmented belief ξ0 ∈
∆(S), the augmented state-transition probabilities (10) and the
augmented measurement probabilities (11). Furthermore,
p(yk+1|ξk, uk) =
X
¯s,˜s∈S
B
˜s
(yk+1, uk)A
˜s,¯s
(uk)ξk(¯s) (13)
for yk+1 ∈ Y, ξk ∈ ∆(S), and uk ∈ U.
Proof: Consider any k ≥ 0. To establish (12), we show
p(sk+1|yk+1, uk) = p(yk+1|sk+1, uk)p(sk+1|yk, uk)
p(yk+1|yk, uk) (14)
where
p(sk+1|yk, uk) =
X
sk∈S
p(sk+1|sk, uk)p(sk|yk, uk−1) (15)
since (12) follows from (14) and (15) via the definitions of
the augmented state-transition probabilities (10), augmented
likelihoods (11), and ξk+1(sk+1) = p(sk+1|yk+1, uk).
To see that (14) holds, note that Bayes’ rule gives
p(sk+1|yk+1, uk) = p(yk+1|sk+1, yk, uk)p(sk+1|yk, uk)
p(yk+1|yk, uk)
= p(yk+1|sk+1, uk)p(sk+1|yk, uk)
p(yk+1|yk, uk)
where the last line holds because Yk+1 and (Y k, Uk) are con-
ditionally independent given Xk+1 with Uk = µ(Y k, Uk−1),
and thus Yk+1 and (Y k, Uk) are conditionally independent
given Sk+1 = L(X0, Xk+1). Next, (15) holds since
p(sk+1|yk, uk) =
X
sk∈S
p(sk+1|sk, yk, uk)p(sk|yk, uk)
=
X
sk∈S
p(sk+1|sk, uk)p(sk|yk, uk−1)
where: i) Xk+1 and (Y k, Uk−1) are conditionally independent
given Xk, and thus Sk+1 = L(X0, Xk+1) and (Y k, Uk−1) are
conditionally independent given Sk = L(X0, Xk); and ii) Sk
and Uk are conditionally independent given (Y k, Uk−1) since
Uk = µ(Y k, Uk−1). Thus (14) and (15) hold, implying (12),
with Bayes’ rule and the law of total probability giving (13).
The proof is complete.
We shall use the shorthand ξk+1 = Ξ( ξk, uk, yk+1) to
denote the recursion in (12) since it (surprisingly) resem-
bles the Bayesian filter (3) but for the augmented state Sk
with measurements Yk and controls Uk. Interestingly, (12) is
equivalently a recursive fixed-point Bayesian smoother for the
initial state X0, with similar recursive smoothers explored in
[9, Section 4.1.1] and references therein, but not exploited in
POMDPs (with controls). We next show that (12) enables the
reformulation of ISC-POMDPs (7) as ( ρ-)POMDPs.
B. Augmented (ρ-)POMDP Reformulation
Our main result reformulating ISC-POMDPs follows.
Theorem 3.1: Consider the ISC-POMDP (7). Define the
augmented-belief cost function
ρ(ξ, u) ≜ ψ(ξ, u) +
X
s∈S
ξ(s)c(s, u) (16)
for ξ ∈ ∆(S) and u ∈ U, where in a slight abuse of notation
we define the augmented cost function c(s, u) ≜ c(x0, x, u)
for u ∈ Uand s = L(x0, x) ∈ S. Then the ISC-POMDP (7)
is equivalent to the augmented-belief ρ-POMDP:
inf
µ∈P
Eµ
" ∞X
k=0
γkρ(ξk, Uk)
#
s.t. ξ k+1 = Ξ (ξk, Uk, Yk+1) , ξ 0 ∈ ∆(S)
Yk+1|ξk, Uk ∼ p(yk+1|ξk, uk)
Uk = µ(ξk) ∈ U
(17)
where the optimization is over (deterministic) policies µ :
∆(S) → Uthat are functions of the augmented belief ξ, with
P being the set of all such policies.
Proof: For µ ∈ P, the cost functional in (7) satisfies
Eµ
" ∞X
k=0
γk [c(X0, Xk, Uk) + ψ(ξk, Uk)]
#
= Eµ
" ∞X
k=0
γkE[c(X0, Xk, Uk) + ψ(ξk, Uk)|Y k, Uk]
#
= Eµ
" ∞X
k=0
γkρ(ξk, Uk)
#
where the first equality is due to the tower property of expec-
tations; and, the second equality holds due to the (augmented)
belief ξk being a sufficient statistic for (Y k, Uk−1) since ξk
is a function of (Y k, Uk−1) and so
E[c(X0, Xk, Uk) + ψ(ξk, Uk)|Y k, Uk]
= ψ(ξk, Uk) +
X
s∈X
ξk(s)c(s, Uk) = ρ(ξk, Uk).
Since ξk is a controlled Markov process via Lemma 3.2,
POMDP (or belief MDP) results imply that this expectation
can be minimized over µ ∈ Punder the constraints in (7) by
(deterministic) functions µ ∈ P of ξk (cf. [12, Section 5.4.1]
and [8, Theorem 6.2.2]). The proof is complete.
A special case of Theorem 3.1 is that when there is no
(explicit) belief-dependent cost ψ, ISC-POMDPs (7) reduce
to POMDPs with states S, measurements Y, controls U, and
transition and observations probabilities (10) and (11).
Corollary 3.1: If ψ(ξ, u) = 0 for ξ ∈ ∆(S) and u ∈ Uin
(7), then (7) is equivalent to the (augmented-state) POMDP
inf
µ∈P
Eµ
" ∞X
k=0
γkC (ξk, Uk)
#
s.t. ξ k+1 = Ξ(ξk, Uk, Yk+1), ξ 0 ∈ ∆(S)
Yk+1|ξk, Uk ∼ p(yk+1|ξk, uk)
Uk = µ(ξk) ∈ U
(18)
where C(ξ, u) ≜ P
s∈S ξ(s)c(s, u) for ξ ∈ ∆(S) and u ∈ U.
Corollary 3.1 implies that all techniques for solving or
analyzing POMDPs of the form (4) apply directly to ISC-
POMDPs (7) that do not have a belief-dependent cost function
ψ. Theorem 3.1, more generally, implies that an optimal policy
µ∗ : ∆(S) → Uand value function V : ∆(S) → R solving an
ISC-POMDP (7) with arbitrary belief-dependent cost function
ψ can be found via Bellman’s equation
V (ξ) = min
u∈U
{ρ(ξ, u) + γE [V (Ξ(ξ, u, Y))|ξ, u]} (19)
for all ξ ∈ ∆(S), with µ∗(ξ) being a minimizing argument
in (19) (cf. [8, Theorem 6.2.2]). We next discuss structural
results useful for finding solutions to ISC-POMDPs via (19).
C. Structural Results and Approximate Solutions
The structural result of foremost utility is that the value
function V is concave when ψ is concave (or constant) in ξ.
Theorem 3.2: Consider the ISC-POMDP (7) reformulated
as the ρ-POMDP (17). If ψ(·, u) is concave in ξ ∈ ∆(S) for
u ∈ U, then ρ(·, u) is concave in ξ ∈ ∆(S) for u ∈ Uand the
value function V given by (19) is concave in ξ ∈ ∆(S).
Proof: Given (16), that ρ(·, u) is concave in ξ ∈ ∆(S) for
u ∈ Uwhen ψ(·, u) is concave in ξ ∈ ∆(S) for u ∈ Uholds
since it is the sum of concave functions. With this concavity,
the theorem follows via [5, Theorem 3.1].
Theorem 3.2 implies that the reformulation in (17) of ISC-
POMDPs (7) with concave belief-dependent cost functions
ψ(·, u) is a ρ-POMDP amenable to approximate solution via
the approach developed in [5]. Indeed, following [5] and
using the concavity of ρ(·, u) established in Theorem 3.2,
we can first construct a piecewise-linear concave (PWLC)
approximation of ρ(·, u) for u ∈ U, before then using standard
POMDP solvers to compute PWLC approximations of the
value function V (see [5, Section 4] for details). The approx-
imation errors are bounded if ρ satisfies the H ¨older-continuity
conditions of [5, Theorem 4.3], and can, in principle, be made
arbitrarily small (see [5, Section 4.2] for details). Many popu-
lar uncertainty costs are concave and satisfy the conditions
of [5, Theorem 4.3]. For example, the initial-state entropy
H(X0|yk, uk−1) is concave in ξk, and entropy functionals
satisfy the conditions of [5, Theorem 4.3] (cf. [5]). However,
if ψ is not concave but is Lipschitz in ξ, then recent Lipschitz-
based approximations can be used (see [6], [15]).
Finally, the reformulations of ISC-POMDPs in Theorem
3.1 and Corollary 3.1 have state and belief spaces S and
∆(S) that scale quadratically with the state space X. However,
they enable the solution of ISC-POMDPs with state-of-the-
art offline and online POMDP solvers capable of handling
very large state spaces (cf. [6], [8], [13], [14]). In contrast,
approaches tailored to specific initial-state costs (such as that
of [4] for the entropy H(X0|yk, uk−1)) have computational
and memory requirements that must be carefully managed via
parameters such as memory length and number of samples.
IV. SIMULATION EXPERIMENT
We now illustrate using ISC-POMDPs to optimize costs
defined with respect to an a priori unknown initial state X0.
Q4 Goal
Q1 Goal
Q3 Goal
Q2 Goal
Agent
(a)
Q4 GoalISC-POMDPPOMDP (b)
Fig. 1. Simulation Experiment: (a) Agent must move to goal in corner
of quadrant of initial stateX0 (agent shown must move to Q2 Goal).
(b) Realizations with POMDP moving to corner closet to locationXk for
k = 2 but ISC-POMDP taking steps to estimateX0 then moving to
correct goal (Q4 Goal).
Consider an agent moving in the grid shown in Fig. 1a that
seeks to move to the corner closest to its initial position. Each
cell in the grid is a state in the state space X = {1, . . . ,16}
(enumerated top-to-bottom, left-to-right). The agent’s initial
position is a priori unknown and distributed uniformly (i.e., π0
is uniform). The agent’s controls U = {1, . . . ,5} correspond
to moving one cell in each of the four compass directions,
or staying still. The controls fail (and the agent remains
still) with probability 0.2, and the walls (bold black lines in
Fig. 1a) block movement, with the agent staying still if it
attempts to move into them. The agent receives measurements
Y = {1, . . . ,16} corresponding to whether or not a wall is
immediately adjacent to its current cell in each of the four
compass directions. A wall is detected when it is present (resp.
not present) with probability 0.8 (resp. 0.2).
We encode the agent’s problem as an ISC-POMDP (7) with
no belief-dependent cost function (i.e., with ψ(ξ, u) = 0 for
ξ ∈ ∆(S) and u ∈ U) and with initial-state cost function
c(x0, x, u) =



1(x ̸= 1) if x0 ∈ {1, 2, 5, 6}
1(x ̸= 4) if x0 ∈ {3, 4, 7, 8}
1(x ̸= 13) if x0 ∈ {9, 10, 13, 14}
1(x ̸= 16) if x0 ∈ {11, 12, 15, 16}
(20)
for x ∈ Xand u ∈ U, where 1(·) denotes the indicator func-
tion. Encoding the agent’s objective of moving to the corner
closest to its initial position is not directly possible using a
(standard) POMDP (4) since they are limited to current-state
dependent costs. For the purpose of comparison, we therefore
encode an approximation of the agent’s objective within a
POMDP (4) with cost κ(x, u) = 1(x ̸∈ {1, 4, 13, 16}) for
u ∈ U. This cost is only an approximation since it encourages
the agent to move to the corner closest to its current location
Xk, rather than to that closest to X0. We use SARSOP [13]
and γ = 0.95 to solve the POMDP and ISC-POMDP (as (18)).
Being an offline anytime algorithm, SARSOP had 5 minutes
prior to deployment to compute each policy (and their use
online was dominated by belief computation).
The results of 10, 000 Monte Carlo simulations of the
ISC-POMDP and POMDP over T = 10 time steps with
TABLE I
MONTE CARLO SIMULATION RESULTS . BEST VALUES IN BOLD .
Criteria ISC-POMDP POMDP
Discounted Cost 6.26 7.91
No. Goals Reached 8031 4116
Final Initial-State Entropy 1.54 1.72
Final Initial-State Prob. 0.296 0.245
X0 ∼ π0 are summarized in Table I and Fig. 2. We report
the: (average) total discounted cost under the ISC-POMDP
cost function in (7) with (20) ( Discounted Cost ); number of
times the agent reaches the correct goal, i.e., the corner closest
to its initial position X0 (No. Goals Reached ); (average)
entropy H(X0|yT , uT−1) of the final initial-state posterior
pmf p(x0|yT , uT−1) (Final Initial-State Entropy ); and, the
(average) probability at the (true) initial state X0 in the
final (marginal) posterior pmf p(x0|yT , uT−1) (Final Initial-
State Prob.). Fig. 2 shows the (average) initial-state entropy
H(X0|yk, uk−1) and (average) probability at the initial-state
in the posterior pmf p(x0|yk, uk−1) at other times. Example
realizations of the agent’s position are shown in Fig. 1b.
Table I shows that the ISC-POMDP outperforms the
POMDP in terms of the discounted cost and the number
of times the agent successfully reaches the corner closest
to X0 (with failures occurring when the measurements do
not enable unambiguous estimation of X0). The superior
performance of the ISC-POMDP is due to it encoding the
agent’s exact objective with the initial-state costs (20) rather
than approximating it with the cost κ(x, u). Furthermore, the
challenge that the ISC-POMDP overcomes (that the POMDP
cannot) is that in order for the agent to move to the correct
goal, it must first determine its initial state X0. The lower
initial-state entropy and higher posterior probability in Table
I and Figs. 2a and 2b for the ISC-POMDP compared to the
POMDP show that the ISC-POMDP selects controls that help
to estimate X0, and hence determine the correct goal to move
to. The realizations shown in Fig. 1b illustrate that the ISC-
POMDP can take extra steps to estimate the initial state X0
and the correct goal, whilst the POMDP will simply move
to the corner closest to its current location Xk when it first
becomes confident of its current location. This experiment
illustrates that ISC-POMDPs enable the optimization of costs
dependent on an a priori unknown initial state X0, which is
important since the optimal policy must select controls Uk that
resolve uncertainty about the initial state X0, rather than just
the current state Xk as in the case of standard ( ρ-)POMDPs.
V. CONCLUSIONS AND FUTURE WORK
We propose ISC-POMDPs as a class of ( ρ-)POMDPs with
costs dependent on the values and/or uncertainty of initial
states. We use recursive Bayesian smoothing to show that they
admit reformulations and solutions as ( ρ-)POMDPs with aug-
mented states and beliefs. Future work will consider problems
with continuous state, control, and measurement spaces.
REFERENCES
[1] L. Wang, I. R. Manchester, J. Trumpf, and G. Shi, “Differential initial-
value privacy and observability of linear dynamical systems,” Automat-
ica, vol. 148, p. 110722, 2023.
0 2 4 6 8 10
Time k
1.5
2.0
2.5
Initial-State Entropy
H(X0|yk, uk−1) ISC-POMDP
POMDP
(a)
0 2 4 6 8 10
Time k
0.0
0.2
0.4
Initial-State Prob.
p(x0|yk, uk−1) ISC-POMDP
POMDP
(b)
Fig. 2. Simulation Results: (a) Entropy H(X0|yk, uk−1) of initial-
state posterior pmfp(x0|yk, uk−1) . (b) Probability at (true) initial state
X0 of posterior pmfp(x0|yk, uk−1).
[2] G. L. Mariottini and S. I. Roumeliotis, “Active vision-based robot
localization and navigation in a visual memory,” in IEEE International
Conference on Robotics and Automation , 2011, pp. 6192–6198.
[3] W. Xue, R. Ying, F. Wen, Y . Chen, and P. Liu, “Active SLAM With Prior
Topo-Metric Graph Starting At Uncertain Position,” IEEE Robotics and
Automation Letters, vol. 7, no. 2, pp. 1134–1141, 2022.
[4] C. Shi, S. Han, M. Dorothy, and J. Fu, “Active perception with initial-
state uncertainty: A policy gradient method,” IEEE Control Systems
Letters, vol. 8, pp. 3147–3152, 2024.
[5] M. Araya, O. Buffet, V . Thomas, and F. Charpillet, “A POMDP extension
with belief-dependent rewards,” in Advances in Neural Information
Processing Systems, vol. 23. Curran Associates, Inc., 2010, pp. 64–72.
[6] M. Fehr, O. Buffet, V . Thomas, and J. Dibangoye, “rho-POMDPs have
Lipschitz-Continuous epsilon-Optimal Value Functions,” in Advances in
Neural Information Processing Systems , vol. 31. Curran Associates,
Inc., 2018.
[7] T. L. Molloy and G. N. Nair, “Smoother Entropy for Active State
Trajectory Estimation and Obfuscation in POMDPs,” IEEE Transactions
on Automatic Control , vol. 68, no. 6, pp. 3557–3572, 2023.
[8] V . Krishnamurthy, Partially observed Markov decision processes. Cam-
bridge University Press, 2016.
[9] O. Capp ´e, E. Moulines, and T. Ryd ´en, Inference In Hidden Markov
Models. New York, NY: Springer, 2005.
[10] F. Belardinelli, B. G. Le ´on, and V . Malvone, “Enabling Markovian
Representations under Imperfect Information,” in Proceedings of the
14th International Conference on Agents and Artificial Intelligence ,
2022, pp. 450–457.
[11] F. Bacchus, C. Boutilier, and A. Grove, “Rewarding behaviors,” in Pro-
ceedings of the Thirteenth National Conference on Artificial Intelligence.
AAAI Press, 1996, p. 1160–1167.
[12] D. P. Bertsekas, Dynamic programming and optimal control , 3rd ed.
Belmont, MA: Athena Scientific, 2005, vol. 1.
[13] H. Kurniawati, D. Hsu, and W. S. Lee, “SARSOP: Efficient point-based
POMDP planning by approximating optimally reachable belief spaces.”
in Robotics: Science and Systems , 2008.
[14] W. Zheng and H. Lin, “Provable-correct partitioning approach for
continuous-observation POMDPs with special observation distributions,”
IEEE Control Systems Letters , vol. 7, pp. 1135–1140, 2023.
[15] Y . E. Demirci, A. D. Kara, and S. Y ¨uksel, “Average cost optimality of
partially observed MDPs: Contraction of nonlinear filters and existence
of optimal solutions and approximations,” SIAM Journal on Control and
Optimization, vol. 62, no. 6, pp. 2859–2883, 2024.