1
Boosting MCTS with Free Energy Minimization
Mawaba Pascal Dao1, Adrian M. Peter1
1Florida Institute of Technology
Keywords: Free Energy, Monte Carlo Tree Search, Cross-Entropy Method, Infor-
mation Gain, Intrinsic Exploration, Continuous Action Spaces
Abstract
Active Inference, grounded in the Free Energy Principle, provides a powerful lens
for understanding how agents balance exploration and goal-directed behavior in un-
certain environments. Here, we propose a new planning framework, that integrates
Monte Carlo Tree Search (MCTS) with active inference objectives to systematically
reduce epistemic uncertainty while pursuing extrinsic rewards. Our key insight is that
MCTS—already renowned for its search efficiency—can be naturally extended to incor-
porate free energy minimization by blending expected rewards with information gain.
Concretely, the Cross-Entropy Method (CEM) is used to optimize action proposals at
arXiv:2501.13083v1  [cs.AI]  22 Jan 2025
the root node, while tree expansions leverage reward modeling alongside intrinsic ex-
ploration bonuses. This synergy allows our planner to maintain coherent estimates of
value and uncertainty throughout planning, without sacrificing computational tractabil-
ity. Empirically, we benchmark our planner on a diverse set of continuous control tasks,
where it demonstrates performance gains over both stand-alone CEM and MCTS with
random rollouts.
1 Introduction
The integration of search mechanisms into decision-making frameworks has consis-
tently led to significant performance improvements across various domains. Monte
Carlo Tree Search (MCTS), a powerful search-based planning method, has been partic-
ularly successful in discrete domains such as game-playing, with notable applications
like AlphaGo combining MCTS with deep neural networks to achieve superhuman per-
formance in the game of Go (Silver et al., 2016, 2017). However, extending MCTS
to more general settings, particularly within the Active Inference framework, presents
both challenges and opportunities.
Active Inference, rooted in the Free Energy Principle (Friston, 2010), provides a
unifying framework for understanding action and perception as processes of minimiz-
ing free energy. Recent advancements have explored the integration of Active Infer-
ence with MCTS to enable sophisticated planning under uncertainty in both discrete
and continuous state-action spaces (Fountas et al., 2020; Tschantz et al., 2020). These
methods demonstrate the potential to balance exploitation and exploration naturally by
2
incorporating free energy minimization as a criterion for action selection. However,
key challenges remain, including the computational demands of planning in continuous
spaces, ensuring reliable value estimation during tree-based search, and extending these
methods to practical applications and benchmarks beyond example problems.
In this paper, we propose a novel framework that integrates MCTS with Active In-
ference to address these challenges. Our approach introduces mechanisms for efficient
planning in continuous state-action spaces while aligning the generative model of Ac-
tive Inference with the tree search process.
Our contributions can be summarized as follows:
• Root Action Distribution Planning: We propose a novel mechanism where
a single Gaussian action distribution is fitted at the root node using the Cross-
Entropy Method (CEM). This root action distribution is utilized consistently through-
out the tree traversal and simulation phases, significantly reducing computational
complexity while ensuring value estimation remains aligned with actual action
selection. By constraining the tree size, we maintain the validity of the root ac-
tion distribution, enabling efficient and reliable planning.
• Enhanced Exploration through Information Gain: Our method incorporates
intrinsic exploration by integrating epistemic value (Information Gain) into the
planning process. This dual exploration mechanism, achieved through both the
expected free energy criterion and MCTS exploration, improves the agent’s abil-
ity to navigate high-dimensional continuous domains.
The remainder of this paper is organized as follows. In Section 2, we discuss related
3
work, providing a comprehensive overview of existing methods that integrate MCTS
with Active Inference. Section 3 provides a background on MCTS, the Free Energy
Principle, and Active Inference. Section 4 describes our proposed planner in detail,
explaining each component and its mathematical grounding. Section 5 presents exper-
imental results demonstrating the effectiveness of our approach. Finally, in the con-
clusion, we discuss the broader implications of our findings and highlight promising
directions for future research.
2 Related Work
Active Inference, rooted in the Free Energy Principle (Friston, 2010), offers a uni-
fied framework for understanding perception and action as inference processes. It has
demonstrated broad applicability across neuroscience and machine learning, modeling
phenomena such as curiosity (Schwartenbeck et al., 2018), dopaminergic discharges
(FitzGerald et al., 2015), and animal navigation. However, a significant challenge lies
in the computational complexity of evaluating all possible policies, which grows expo-
nentially with the planning horizon.
Model-based Reinforcement Learning (RL) methods aim to learn a model of the en-
vironment’s dynamics and use it for planning (Moerland et al., 2023). These methods
can be more sample-efficient than model-free approaches, as they can simulate expe-
riences without interacting with the environment. Chua et al. (2018) proposed PETS
(Probabilistic Ensembles with Trajectory Sampling), which uses an ensemble of proba-
bilistic models for planning in continuous action spaces. Similarly, Hafner et al. (2019)
4
introduced PlaNet, a model-based RL method that learns a latent dynamics model for
planning. By leveraging probabilistic models, these methods provide robust uncertainty
quantification, which is critical for exploration and planning under uncertainty.
Tschantz et al. (2020) proposed the Free Energy of Expected Future (FEEF) as a
tractable objective for decision-making in RL environments. Their method incorpo-
rates a model-based Cross-Entropy Method (CEM) for policy optimization, achieving
a balance between exploration and exploitation in sparse and continuous control tasks.
Monte Carlo Tree Search (MCTS), a decision-making framework, has proven valu-
able in addressing the computational complexity of evaluating an exponentially growing
number of possible policies as the planning horizon increases. MCTS achieves this by
sampling a subset of possible policies. Early applications of MCTS focused on discrete
domains, such as game playing (Coulom, 2006), with significant successes in AlphaGo
(Silver et al., 2016, 2017). While extensions to continuous action spaces, such as pro-
gressive widening (Coulom, 2006) and hierarchical optimization (Bubeck et al., 2011),
have broadened its scope, these approaches are typically employed in Reinforcement
Learning (RL) contexts.
Recent advancements have combined MCTS with Active Inference to address chal-
lenges in planning under uncertainty. For instance, Fountas et al. (2020) proposed an
MCTS-based Active Inference framework that replaces traditional selection criteria,
such as the Upper Confidence Bounds applied to Trees (UCT) (Kocsis and Szepesv´ari,
2006), with an expected free energy (EFE)-based criterion. Their approach employs
a deep neural network to approximate posterior distributions and utilizes Monte Carlo
sampling to evaluate free energy terms efficiently. This integration demonstrated im-
5
proved performance in tasks requiring sophisticated planning, such as the dSprites
dataset and the Animal-AI environment.
Branching Time Active Inference (BTAI), introduced by Champion et al. (2022),
further unified MCTS and Active Inference by framing planning as Bayesian model
expansion. BTAI treats the tree structure as part of the generative model itself, dy-
namically expanding the model to incorporate future observations and latent variables.
This approach reduced the computational overhead associated with traditional Active
Inference models, allowing applications in graph navigation and other complex tasks.
Despite these advances, the integration of MCTS with the Free Energy Principle in
practical applications remains underexplored. Most implementations adopt MCTS as a
planning tool within RL frameworks, leaving the potential of Active Inference—particularly
its capacity for intrinsic motivation and uncertainty minimization—relatively untapped.
In this work, we address these gaps by integrating MCTS with Active Inference in a
novel way. Our framework employs a model-based approach, using MCTS for planning
EFE-optimal paths in continuous state-action spaces. We build upon methods such as
progressive widening and ensemble modeling to extend MCTS to continuous domains
while maintaining compatibility with the generative model of Active Inference. This
approach enables efficient exploration and exploitation in environments characterized
by high uncertainty and sparse rewards.
6
3 Background
3.1 Active Inference and the Free Energy Principle
The Free Energy Principle, originating in neuroscience, posits that systems act to min-
imize a quantity called free energy, which measures how well an internal generative
model predicts observations (Friston, 2010). This principle unifies perception and ac-
tion under the framework of probabilistic inference, where agents aim to align their
beliefs with observed data and predict future states.
Free energy is defined as:
F(Q, y) = DKL[Q(x)∥P(x|y)] − ln P(y), (1)
where:
• Q(x): The approximate posterior distribution over hidden states x.
• P(x|y): The true posterior distribution over hidden states, given observations y.
• ln P(y): The log evidence (marginal likelihood) of the observations.
• DKL[Q(x)∥P(x|y)]: The Kullback-Leibler divergence between the approximate
and true posterior distributions.
Minimizing free energy involves two components:
1. Reducing the Kullback-Leibler (KL) divergence, which aligns the approximate
posterior Q(x) with the true posterior P(x|y).
7
2. Maximizing the log evidence ln P(y), which ensures that the generative model
predicts observations accurately.
Building on this principle, Active Inference provides a framework for decision-
making, where agents minimize variational free energy by simultaneously improving
their beliefs about the environment (perception) and selecting actions that shape future
observations (action). This process naturally balances exploration (uncertainty reduc-
tion) and exploitation (goal achievement).
Key processes in Active Inference include:
• Perception: Updating beliefs about the hidden states of the environment using
variational inference to align the approximate posterior Q(x) with the true poste-
rior P(x|y).
• Action: Selecting actions that minimize free energy, shaping future observations
to conform to the agent’s generative model.
Unlike traditional reinforcement learning, which separates reward maximization
and exploration into distinct objectives, Active Inference unifies these objectives by in-
corporating uncertainty reduction as an intrinsic component of decision-making. This
intrinsic motivation encourages agents to explore uncertain states while also achieving
extrinsic goals.
3.2 Monte Carlo Tree Search (MCTS)
Monte Carlo Tree Search (MCTS) is a heuristic search algorithm designed for decision-
making in large and complex state spaces (Browne et al., 2012). It incrementally builds
8
a search tree by iteratively simulating playouts, balancing exploration and exploitation
to identify promising actions. Each node in the tree represents a state, and edges repre-
sent actions.
The MCTS process is typically divided into four key phases:
• Selection: Starting from the root node, the algorithm traverses the tree by select-
ing child nodes based on a specific policy until a leaf node is reached. The most
commonly used selection policy is based on Upper Confidence Bounds (UCB),
described below.
• Expansion: If the leaf node does not represent a terminal state, one or more child
nodes are added to the tree, representing unexplored actions.
• Simulation: A simulation, or playout, is performed from the expanded node,
where actions are sampled according to a default policy (e.g., random actions)
until a terminal state is reached. The return from this simulation provides an
estimate of the value of the expanded node.
• Backpropagation: The results of the simulation are propagated back up the tree,
updating the values and visit counts of all nodes along the path.
By iteratively performing these steps, MCTS progressively refines its estimates of
action values, focusing computational resources on the most promising parts of the
search space. This property makes MCTS highly effective in problems with large state
spaces and uncertain outcomes.
9
3.2.1 Upper Confidence Bound 1 (UCB1)
The Upper Confidence Bound 1 (UCB1) algorithm is a widely used technique in MCTS
to address the exploration-exploitation tradeoff during tree traversal (Auer et al., 2002).
UCB1 assigns a score to each child node, balancing the average reward observed (ex-
ploitation) with the uncertainty of the node (exploration).
The UCB1 value for selecting a child node i is computed as:
UCB1i = Qi + Cucb
r
ln N
Ni
, (2)
where:
• Qi is the average reward (mean value) of child i.
• N is the total number of times the parent node has been visited.
• Ni is the number of times child i has been visited.
• Cucb is the exploration constant that controls the degree of exploration.
The first term, Qi, promotes exploitation by preferring actions that have yielded
higher rewards on average. The second term, Cucb
q
ln N
Ni
, encourages exploration by
assigning a higher bonus to actions that have been selected fewer times, thus having
higher uncertainty. The logarithmic factor ln N ensures that as the number of visits N
to the parent node increases, the exploration bonus decreases, allowing the algorithm to
focus more on exploitation over time.
By integrating UCB1 into MCTS, the algorithm effectively balances the need to ex-
plore new actions that might lead to better rewards with the need to exploit actions that
10
have already shown promising results. This balance is crucial for converging towards
optimal policies in decision-making tasks.
4 Proposed Planner
We now present our MCTS-CEM planning framework, which integrates Monte Carlo
Tree Search with the Cross-Entropy Method (CEM) at the root node to handle continu-
ous actions effectively. Figure 1 provides a high-level overview, highlighting three main
components:
1. (A) The root node, initialized with the agent’s current state s0.
2. (B) The process of fitting a single Gaussian action distribution using CEM at the
root node.
3. (C) The subsequent tree-based planning (MCTS) stage, which uses the fitted root
action distribution for exploration, rollouts, and leaf-node simulations.
4.1 Root Action Distribution Planning
The key idea is to learn one Gaussian distribution over actions at the root node, then
reuse it throughout tree-based planning and simulation, thereby ensuring consistent es-
timates of value and reward. We denote this root action distribution bya ∼ N
 
µ, Σ

.
where µ and Σ are optimized via CEM to maximize expected returns plus any
epistemic (information gain) terms.
11
Figure 1: MCTS-CEM Diagram.
A: Initialize the MCTS tree with the agent’s current state s0.
B: Fit the root node’s action distribution using CEM. Actions are evaluated by min-
imizing expected free energy ( Gi), with next states sampled using the current action
Gaussian. The epistemic value EV i
t is computed as the KL divergence, and rewards (ri
t)
approximate ln P(yi
t). The top-performing actions refine the distribution iteratively.
C: Use the fitted action distribution for action sampling during MCTS exploration, bal-
ancing exploitation and exploration with UCB-like selection and consistent simulations
at the leaves.
12
4.1.1 Fitting the Root Action Distribution via CEM
At the beginning of planning (from root state s0), we perform Cross-Entropy Method
optimization to obtain µ and Σ. Figure 2 illustrates this process (labeled “3. Evaluation
of Each Candidate”):
1. Initialize a Gaussian action distribution with mean µ0 = 0 and covariance Σ0 =
I.
2. Sample candidates {a(i)}ncandidates
i=1 from the current Gaussian. Each a(i) is an H-
step action sequence for the planning horizon.
3. Evaluate each candidate via short model-based rollouts, scoring it by the sum
of extrinsic reward and the epistemic value (an information-gain term) across the
horizon.
4. Refit the distribution to the top-k performers:
µnew = 1
k
X
i∈S
a(i), Σnew = 1
k
X
i∈S
(a(i) − µnew)(a(i) − µnew)⊤, (3)
where S is the index set of top-k candidates.
5. Repeat for a fixed number of CEM iterations until convergence.
The output is a single, optimized action distributionN(µ, Σ) centered on promising
actions from the model’s perspective.
13
Figure 2: Root Action Distribution Fitting Using CEM: This diagram focuses
on subsection 3 of component B in the MCTS-CEM process. Candidate actions
{a(1), . . . , a(ncandidates)} are sampled from a Gaussian distribution. Their evaluations,
based on the expected free energy objectiveGi, approximate extrinsic value lnP (yi
t) ≈
ri
t and epistemic value EV i
t from KL divergence. The top k candidates refine the distri-
bution, optimizing exploration and exploitation.
14
4.1.2 MCTS-CEM Planning with the Fitted Root Action Distribution
Once the root action distribution has been fit via CEM, we keep it fixed for the duration
of the tree-based planning. Figure 3 depicts this stage:
Figure 3: MCTS-CEM Planning (Component C). After fitting the root action distri-
bution, MCTS uses it to drive action sampling at each expansion step and for leaf-node
simulations.
MCTS Expansion and Action Selection. When selecting actions at each decision
node, we sample from N(µ, Σ) rather than optimizing new distributions at non-root
nodes. This design assumes that states near the root are sufficiently representative of
15
what we will encounter deeper in the tree; hence, a single Gaussian can remain effective
as the agent expands its lookahead. IfNchildren new actions must be considered, we draw
Nchildren distinct samples from the root distribution to populate child nodes.
Simulation/Rollout. When a leaf node is reached and we need to estimate its value
(i.e., to “roll out” until horizon), we again sample future actions from the same root
distribution. This ensures that the value estimates at leaf nodes reflect outcomes under
the same policy used to expand the tree, resulting in consistent planning. In practice,
short rollouts can be performed, or we can simply evaluate a truncated horizon to keep
computations manageable. A recognized limitation is that as states become farther
from the root, the originally fitted action distribution may become suboptimal for these
distant states. This can lead to rollouts that underestimate the true value if the root
distribution does not align well with deeper states. Nonetheless, because we use the
same distribution throughout, our value estimates remain consistent with the policy we
have committed to at the root.
By maintaining one fitted action distribution at the root and using it consistently
throughout expansions and rollouts, MCTS-CEM ties together the policy used to score
states with the policy used to explore them. This avoids needless re-optimization of ac-
tions at every node. While this approach may yield suboptimal action choices in states
that significantly diverge from the root, it provides self-consistent planning in terms of
policy and value estimation. Addressing this limitation to enable broader applicability
to larger search horizons is an exciting area for future work. Empirical results in Sec-
tion 5 demonstrate that this strategy—even with a truncated horizon—leads to robust
16
performance gains, especially in sparse or high-dimensional continuous tasks.
4.2 Epistemic Value as Information Gain Bonus
The expected free energy G(π) for a policy π can be decomposed into two key com-
ponents: the extrinsic value (preferences over observations) and the epistemic value
(information gain). Formally, it is defined as (Friston et al., 2015):
G(π) = Eq(s1:H|π)


HX
t=1
−ln P(ot|st)| {z }
Extrinsic Value
+ DKL [q(st|π) ∥p(st|st−1, at−1)]| {z }
Epistemic Value

, (4)
where q(s1:H|π) is the variational posterior over states given policy π, P(ot|st) is
the likelihood of observations given states, DKL[·∥· ] is the Kullback-Leibler (KL) di-
vergence, and p(st|st−1, at−1) is the prior predictive distribution of states under the
policy. The extrinsic value encodes the agent’s preferences over observations, while
the epistemic value quantifies the expected information gain about the environment’s
dynamics.
In reinforcement learning, the agent’s goal is to maximize cumulative rewards. To
incorporate the Free Energy Principle, we approximate the extrinsic value−ln P(ot|st)
with the negative reward function −r(st, at). This is a standard practice when aligning
reinforcement learning with the Free Energy framework, as rewards are viewed as rep-
resenting the agent’s preferences over states or outcomes (Friston et al., 2009; Tschantz
et al., 2020; Millidge, 2020). This approximation allows us to interpret reward maxi-
mization as a form of free energy minimization, reframing the agent’s extrinsic motiva-
tion in terms of the principle.
17
The epistemic value measures how much an agent’s belief about the next state
changes when new information is available, capturing the expected information gain
from taking an action. For a candidate action sequence ai, the epistemic value at time t
is defined as the expected KL divergence between the approximate posterior and prior
predictive distributions.
EVi
t = Eq(st+1|si
t,ai
t)

ln q(st+1|si
t, ai
t) − ln p(st+1|si
t, ai
t)

. (5)
To compute the epistemic value practically, we approximate the expected KL diver-
gence as the difference between the entropy of the aggregated predictive distribution
and the average entropy of the individual model predictions. This approximation is in-
spired by the Bayesian Active Learning by Disagreement (BALD) framework (Houlsby
et al., 2011; Gal et al., 2017), where mutual information is used to quantify epistemic
uncertainty.
Starting from Equation (5) and with some minor abuse of notation, we observe that
the log-ratio ln q(st+1 | si
t, ai
t) − ln p(st+1 | si
t, ai
t) = lnq(s)
p(s) . Taking the expectation
under the posterior q(·), we have
DKL[q(s)∥p(s)] =
Z
q(s) ln q(s)
p(s) ds (6)
=
Z
q(s) lnq(s) ds −
Z
q(s) lnp(s) ds (7)
= −H(q) −
Z
q(s) lnp(s) ds, (8)
where H(q) = −
R
q(s) lnq(s) ds is the entropy of q(s). The term
R
q(s) lnp(s) ds
can be challenging to compute directly. However, if q(s) and p(s) are both Gaussian
18
distributions or mixtures of Gaussians, and they are close to each other (i.e., q(s) ≈
p(s)), we can approximate this term.
Since q(s) is the aggregated predictive distribution from the ensemble, and p(s)
represents the individual model predictions, we can approximate:
Z
q(s) lnp(s) ds ≈ 1
M
MX
m=1
Z
q(s) lnpm(s) ds. (9)
The cross-entropy between q(s) and p(s) is defined as:
H(q, p) = −
Z
q(s) lnp(s) ds. (10)
Therefore, we have:
Z
q(s) lnp(s) ds = −H(q, p). (11)
Assuming that q(s) is close to p(s), the cross-entropy H(q, p) can be approximated
by the entropy of p(s):
H(q, p) ≈ H(p). (12)
Similarly, for each ensemble model pm(s):
Z
q(s) lnpm(s) ds = −H(q, pm) ≈ −H(pm). (13)
Substituting back into the expression for the KL divergence, we obtain:
DKL[q∥p] ≈ −H(q) − (−H(p)) = −H(q) + H(p). (14)
19
Similarly, using Equation (9), we have:
DKL[q∥p] ≈ −H(q) + 1
M
MX
m=1
H(pm). (15)
Therefore, the epistemic value becomes:
EVt,i = DKL[q(st+1|si
t, ai
t)∥p(st+1|si
t, ai
t)] (16)
≈ −H
 
q(st+1|si
t, ai
t)

+ 1
M
MX
m=1
H
 
pm(st+1|si
t, ai
t)

. (17)
Since q(st+1|si
t, ai
t) is the aggregated predictive distribution p(st+1|si
t, ai
t), we can
write:
EVt,i ≈ H
 
p(st+1|si
t, ai
t)

− 1
M
MX
m=1
H
 
pm(st+1|si
t, ai
t)

. (18)
This approximation relies on the assumptions that the individual model distribu-
tions from the ensemble pm(st+1|si
t, ai
t) and the aggregated distribution p(st+1|si
t, ai
t)
are approximately Gaussian, and that the ensemble disagreement reflects the epistemic
uncertainty about the environment’s dynamics.
In the BALD framework (Houlsby et al., 2011; Gal et al., 2017), the mutual infor-
mation between predictions and model parameters is expressed as:
I[y; θ|x] = H
 
Ep(θ)[p(y|x, θ)]

− Ep(θ) [H (p(y|x, θ))] , (19)
where y is the prediction, x is the input, andθ represents the model parameters. This
parallels our expression for the epistemic value, where the first term is the entropy of
the aggregated predictions, and the second term is the average entropy over the models.
20
Calculating the entropy of the aggregated predictive distribution H (p(st+1|si
t, ai
t))
directly is challenging since it is a mixture of Gaussians without a closed-form entropy
expression. To estimate this entropy, we employ the Kozachenko–Leonenko entropy
estimator (Kozachenko and Leonenko, 1987), a non-parametric method based on k-
nearest neighbor distances among samples. The Kozachenko–Leonenko estimator for a
set of N samples {xi}N
i=1 in Rd is given by:
HKozachenko-Leonenko ≈ ψ(N) − ψ(k) + lncd + d
N
NX
i=1
ln ϵi, (20)
where ψ(·) is the digamma function, cd is the volume of the d-dimensional unit
ball, ϵi is twice the distance from sample xi to its k-th nearest neighbor, and d is the
dimensionality of the state space. In our implementation, we generate samples from
the aggregated predictive distribution by sampling from each ensemble model’s out-
put and aggregating the results. We set k = 3 for the k-nearest neighbors and com-
pute pairwise distances between samples to find ϵi. Using Equation (20), we estimate
H (p(st+1|si
t, ai
t)).
For the individual model entropies H (pm(st+1|si
t, ai
t)), we use the closed-form ex-
pression for the entropy of a Gaussian distribution:
H(N(µ, Σ)) = 1
2 ln

(2πe)d det Σ

, (21)
where Σ is the covariance matrix (assumed diagonal in our case).
Our planner aims to minimize the expected free energy by balancing the trade-off
between exploiting known rewarding actions and exploring uncertain regions. We de-
fine the combined objective function for candidate i as:
21
Gi =
H−1X
t=0

−r(si
t, ai
t) + λ · EVi
t

, (22)
where r(si
t, ai
t) is the reward function, EVi
t is the epistemic value (Information Gain)
at time t, and λ is a weighting factor balancing exploitation and exploration.
During planning, we generate a set of candidate action sequences {ai} and evalu-
ate them using the combined objective function Gi. We use the Cross-Entropy Method
(CEM) to iteratively refine the action distribution by selecting the top-performing can-
didates and fitting a Gaussian distribution over them. The process involves initializing
the action distribution’s mean and standard deviation, sampling candidate action se-
quences, evaluating them using Equation (22), selecting the top performers, updating
the action distribution based on these candidates, and repeating the process for a fixed
number of iterations. By minimizing Gi, the planner selects actions that are expected to
yield high rewards while also providing valuable information about the environment.
Our approach ensures a principled balance between exploration and exploitation.
The negative reward term −r(si
t, ai
t) encourages the agent to select actions that maxi-
mize expected rewards, aligning with the goal of reward maximization in reinforcement
learning. The epistemic valueλ·EVi
t incentivizes the agent to choose actions that reduce
uncertainty, leading to better knowledge of the environment’s dynamics. This balance
is crucial for efficient learning, as it prevents the agent from overly exploiting known
rewarding actions without improving its understanding of the environment.
22
4.3 Algorithmic Details and Pseudocode
Below, we provide the pseudocode for MCTS-CEM (Algorithm 1), which illustrates
how the root action distribution is used during planning. This algorithm reflects the
approach described in Sections 4.1–4.1.2, including how reward and epistemic value
are integrated during the rollouts.
23
Algorithm 1: MCTS-CEM with Root Action Distribution
Input: State s0, fitted Gaussian N(µ, Σ), ensemble {pm}, reward r(·, ·), horizon H, # sims Nsim, c, γ
Output: Selected action a∗
1: Create root node v0 with state s0.
2: for sim = 1. . . Nsim do
3: Selection: v ← v0
4: while v not terminal do
5: if v not fully expanded then
6: break
7: else
8: Select child v′ via UCB:
v′ = arg max
c∈Children(v)
[Q(c) +c
q
ln Nv
Nc
]
9: v ← v′
10: end if
11: end while
12: Expansion:
13: if v not fully expanded and not terminal then
14: Sample a ∼ N(µ, Σ)
15: snew ← f(sv, a), create child node vnew
16: Add vnew to Children(v); v ← vnew
17: end if
18: Simulation (Rollout): G ← 0, s← sv
19: for t = 0. . .rolloutH − 1 do
20: a ∼ N(µ, Σ)
21: rt ← r(s, a); G ← G + γtrt
22: s ← mean{pm(s | s, a)} {(or sample)}
23: end for
24: Backpropagation: Propagate G up the tree
25: end for
26: Action Selection:
27: a∗ ← arg maxac Nc (children of v0)
28: return a∗
24
5 Experiments
In this section, we present experiments conducted to evaluate our proposed MCTS-
CEM. We compare MCTS-CEM to two other planners:
1. CEM Planner : A regular Cross Entropy Method planner that uses the same
model-based rollout used by Tschantz et al. (2020) during the simulate phase
of our MCTS-CEM algorithm.
2. MCTS-Random: An MCTS planner that employs a random policy during roll-
outs. Critically, the simulate phase of MCTS-Random does not use any Free
Energy Minimization for action selection. Instead, at each step of the planning
horizon, it samples a random action from a uniform distribution bounded by the
environment’s action space.
We compare these three planners across five different environments. Specifically,
in the Pendulum and Sparse Mountain Car environments, we run each planner for 10
episodes per trial, while in HalfCheetah-Run and HalfCheetah-Flip we run each for
1000 episodes. We repeat every trial five times with different random seeds to account
for variability in performance. All planners are configured with the same planning
horizon and number of simulations per planning step within each environment to ensure
a fair comparison. The experiments use the same model dynamics and reward function
across all planners.
25
5.1 Pendulum
Figure 4: The Pendulum environment, where the agent applies torque to swing the
pendulum to its upright position.
Figure 5: Performance comparison of MCTS-CEM, CEM, and MCTS-Random on the
Pendulum environment, showing the cumulative reward over episodes averaged over
five trials with different random seeds. Error bars represent the standard deviation
across trials. In this well-shaped, deterministic setting, the additional exploration pro-
vided by MCTS-CEM results in performance comparable to CEM, indicating limited
added benefit beyond a straightforward Cross-Entropy Method approach.
In the Pendulum environment (Figure 4), we observe that while MCTS-CEM consis-
tently outperforms MCTS-Random, it does not provide a significant improvement over
26
CEM. This can be attributed to the nature of the environment, which features a one-
dimensional action space representing the torque applied to the pendulum’s free end.
The reward function is well-shaped and continuous:
r = −(θ2 + 0.1 · ˙θ2 + 0.001 · torque2), (23)
where θ is the pendulum’s angle, normalized between [−π, π], with 0 being the
upright position. The minimum reward is −16.27 when the pendulum is fully displaced
with maximum velocity, and the maximum reward is 0 when the pendulum is perfectly
upright with no torque applied. Given the simple nature of this reward structure and the
deterministic dynamics of the environment, even random action selection by MCTS-
Random can achieve maximum reward in the early episodes.
The comparable performance between CEM and MCTS-CEM suggests that the
additional exploration performed by MCTS-CEM might be unnecessary in this well-
defined and deterministic environment. Once the agent achieves the maximum reward
state, further exploration does not add value, as MCTS-CEM continues to search novel
state-action pairs even when the optimal policy is already known. The exploration term
in MCTS encourages novelty, which, while beneficial in more complex environments,
may be counterproductive here where a known deterministic policy is sufficient to con-
sistently achieve optimal performance. Our findings align with those of Bellemare et al.
(2016), who demonstrated that in environments with sparse rewards, methods incor-
porating intrinsic motivation—such as count-based exploration strategies—are particu-
larly effective. In contrast, in simpler, well-shaped environments, relying on established
optimal policies, as CEM does, might be more efficient than the continual planning that
27
MCTS-CEM performs.
5.2 Sparse Mountain Car
Figure 6: The Sparse Mountain Car environment, where the agent must navigate a car
to the flag by building momentum.
Figure 7: Performance comparison of MCTS-CEM, CEM, and MCTS-Random on the
Sparse Mountain Car environment, showing the cumulative reward over episodes av-
eraged over five trials with different random seeds. Error bars represent the standard
deviation across trials. Although CEM initially outperforms MCTS-CEM due to its re-
liance on an approximate reward model, MCTS-CEM surpasses both baselines in later
episodes by refining reward estimates and leveraging broader UCB-based exploration
to uncover high-reward trajectories.
28
In the Sparse Mountain Car environment (Figure 6), the goal is to generate controls that
drive a car to the top of a hill marked by a flag, representing the goal state. The car lacks
sufficient acceleration to climb the hill directly. Instead, it must first move backward up
a smaller hill to gain enough momentum to ascend the larger hill in front. In this sparse
reward setting, the agent receives a positive reward of +1 only upon reaching the goal
state, while incurring a negative reward at every time step it does not reach the goal.
Results on the Sparse Mountain Car environment (Figure 7) highlight the strengths
of MCTS-CEM, particularly in later episodes. Initially, CEM outperforms MCTS-
CEM, but as more episodes are played, MCTS-CEM improves significantly, ultimately
achieving higher maximum rewards than both CEM and MCTS-Random. This im-
provement can be attributed to the synergy between CEM’s root distribution optimiza-
tion and the UCB-based tree search within MCTS-CEM. The UCB formula for child
selection in our planner is given by:
UCBi = Qi + c ·
r
ln Nv
Ni
, (24)
where Qi is the estimated return of childi, Nv is the visit count of the parent nodev,
and Ni is the visit count of the child itself. Initially, MCTS-CEM’s double reliance on
an approximate reward model can cause performance to lag behind CEM, since reward
models may overestimate certain state-action pairs (Pathak et al., 2017). However, as
more data is gathered over multiple episodes, the reward model becomes more accurate.
Combined with the broader search from UCB expansions, MCTS-CEM discovers high-
reward trajectories more effectively than CEM in this sparse environment.
The ability of MCTS-CEM to tap into a wider range of potential future states with
29
more accurate reward estimates allows it to excel in environments like Mountain Car,
where momentum and long-term planning are key to achieving success. This demon-
strates the potential of MCTS-CEM in sparse reward environments, where effective
planning from multiple future states is crucial for overcoming challenges like sparse
rewards and delayed returns (Sutton & Barto, 2018).
5.3 HalfCheetah-Run
Figure 8: The HalfCheetah-Run environment, where the agent controls a two-
dimensional cheetah to maximize forward velocity.
30
Figure 9: Performance comparison of MCTS-CEM, CEM, and MCTS-Random on the
HalfCheetah-Run environment. Each curve shows the reward over episodes averaged
across five trials with different random seeds. The shaded regions around each curve
represent the standard deviation across those trials. MCTS-CEM consistently achieves
higher returns by leveraging both UCB-based exploration and CEM’s optimization in
continuous action spaces; however, occasional performance dips, or “policy collapse”
(Millidge, 2019), (Friston et al., 2009), highlight the challenge of managing exploration
and exploitation in practice.
In the HalfCheetah-Run environment (Figure 8), the objective is for the agent, control-
ling a simulated two-dimensional cheetah, to run as quickly as possible in the forward
direction. The agent receives dense rewards based on its forward velocity, offering
frequent feedback that helps MCTS-CEM correct suboptimal trajectories during plan-
31
ning. As illustrated in Figure 9, MCTS-CEM consistently outperforms both the CEM
Planner and MCTS-Random by leveraging both upper confidence bound (UCB)-based
exploration and CEM’s strength in continuous action optimization.
Despite this strong overall performance, both MCTS-CEM and CEM occasionally
exhibit abrupt dips in reward—an effect we refer to as “policy collapse,” following
Millidge (2019) and Friston et al. (2009). Here, an imbalance between exploration
and exploitation, coupled with potential inaccuracies in the planner’s value or reward
estimates, can cause the agent to momentarily overcommit to suboptimal actions. Nev-
ertheless, MCTS-CEM significantly mitigates these collapses compared to plain CEM,
likely due to the “double exploration” stemming from both the Monte Carlo tree search
(via UCB) and the CEM-based optimization in continuous action space. This dual layer
of exploration makes the planner more robust to episodic failures, as it can more quickly
identify and correct suboptimal trajectories. In practice, policy collapse can be further
alleviated by tuning the weight of intrinsic exploration terms and improving model fi-
delity (e.g., by additional training data or larger ensembles), thereby helping to maintain
a more reliable balance between exploration and exploitation.
32
5.4 HalfCheetah-Flip
Figure 10: The HalfCheetah-Flip environment, where the agent controls a two-
dimensional cheetah to perform front flips.
33
Figure 11: Performance comparison of MCTS-CEM, CEM, and MCTS-Random on the
HalfCheetah-Flip environment. Each curve shows the reward over episodes averaged
across five trials with different random seeds. The shaded regions around each curve
represent the standard deviation across those trials. MCTS-CEM and CEM exhibit
nearly identical performance, highlighting the relative simplicity of the task for more
advanced planners.
In the HalfCheetah-Flip environment (Figure 10), the objective is for the agent, control-
ling a simulated two-dimensional cheetah, to perform front flips rather than maximizing
forward velocity. The agent receives dense rewards based on the quality and frequency
of its flips. Unlike HalfCheetah-Run, the task requires simple motor coordination and
involves coarser control over the cheetah’s dynamics.
As illustrated in Figure 11, MCTS-CEM and CEM exhibit nearly identical perfor-
34
mance, with no significant advantage observed for the additional planning provided by
MCTS-CEM. This is in stark contrast to environments like HalfCheetah-Run, where
MCTS-CEM demonstrated a clear performance advantage. The similarity in perfor-
mance could be attributed to the nature of the task: flipping requires coarser motor
control than running, and the additional computational effort of MCTS planning does
not yield substantial improvements. Instead, simpler optimization through CEM alone
appears sufficient for achieving near-optimal results in this environment.
This result emphasizes the importance of task characteristics in determining the
utility of MCTS-CEM. While the algorithm excels in sparse reward environments or
well-shaped tasks involving complex, high-dimensional controls, its benefits are less
apparent in tasks requiring coarser motor coordination and less intricate action plan-
ning. This insight highlights the nuanced trade-offs between computational overhead
and planning efficacy in different types of continuous control tasks.
6 Conclusion
In this work, we introduced MCTS-CEM, a model-based planner that unifies Monte
Carlo Tree Search (MCTS) with the Free Energy Principle’s uncertainty minimization
objective. By adopting ensemble-based rollouts and incorporating epistemic value es-
timates as an intrinsic exploration bonus, our planner naturally balances the drive to
maximize extrinsic rewards with the need to reduce uncertainty about the environment’s
dynamics.
A key element of our approach is fitting a single Gaussian action distribution at the
35
root node using the Cross-Entropy Method (CEM). We then use this root distribution
consistently throughout the expansion and simulation phases, ensuring that the policy
underlying tree search remains coherent with the policy used for value estimation in
leaf-node rollouts. This strategy avoids redundant re-optimization at deeper nodes and
lends stability to planning. Moreover, the integration of information gain as part of the
Free Energy minimization criterion enables a principled exploration mechanism that
boosts performance in environments featuring sparse and delayed rewards.
Empirically, we validated MCTS-CEM on a variety of continuous control tasks, in-
cluding Pendulum, Sparse Mountain Car, and HalfCheetah. Across these benchmarks,
our method consistently outperformed or matched baseline planners that either rely
solely on CEM or combine MCTS with simplistic (random) policies. Notably, MCTS-
CEM demonstrated superior robustness to random seed variability and scaled favorably
with increasing amounts of environment interaction, suggesting that its exploration-
driven planning is well-suited to long-horizon tasks with limited feedback signals.
Overall, this work highlights the promise of coupling MCTS with Free Energy min-
imization for active inference in high-dimensional control problems. By merging a
powerful search paradigm with an epistemic drive to reduce model uncertainty, we
bring a unified view of planning, exploration, and policy refinement to reinforcement
learning. However, our results also reveal challenges related to balancing exploration
and exploitation, particularly when intrinsic exploration bonuses dominate extrinsic re-
ward optimization, leading to episodic policy collapse. Future research should focus on
mitigating this issue by exploring methods such as adaptive regularization of intrinsic
exploration bonuses, incorporating uncertainty-aware thresholds, and improving the fi-
36
delity of reward models through ensemble methods or additional training. Furthermore,
addressing the current limitation of truncated search horizons by extending the planner
to deeper or more flexible expansions represents an exciting direction for broadening the
applicability of our approach. Addressing these challenges will solidify the application
of active inference principles in scalable, model-based reinforcement learning.
Acknowledgments
This research was conducted as part of the requirements for my PhD. My PhD is sup-
ported by funding from Air Force Research Laboratory (AFRL) grant No. FA8650-21-
C-1147. Any opinions, findings, conclusions, or recommendations contained herein are
those of the authors and do not necessarily represent the official policies or endorse-
ments, either expressed or implied, of the AFRL or the U.S. Government.
References
Coulom, R. (2006). Efficient Selectivity and Backup Operators in Monte-Carlo Tree
Search. International Conference on Computers and Games, 72–83.
Browne, C. B., Powley, E., Whitehouse, D., Lucas, S. M., Cowling, P. I., Rohlfshagen,
P., Tavener, S., Perez, D., Samothrakis, S., & Colton, S. (2012). A survey of Monte
Carlo Tree Search methods. IEEE Transactions on Computational Intelligence and
AI in Games, 4(1), 1–43.
Houthooft, R., Chen, X., Duan, Y ., Schulman, J., De Turck, F., & Abbeel, P. (2016).
37
VIME: Variational Information Maximizing Exploration. Advances in Neural Infor-
mation Processing Systems, 1109–1117.
Lindley, D. V . (1956). On a Measure of the Information Provided by an Experiment.
The Annals of Mathematical Statistics, 27(4), 986–1005.
Rosin, C. D. (2011). Multi-armed bandits with episode context. Annals of Mathematics
and Artificial Intelligence, 61(3), 203–230.
Rubinstein, R. Y . (1999). The Cross-Entropy Method for Combinatorial and Continuous
Optimization. Methodology and Computing in Applied Probability, 1(2), 127–190.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G.,
Schrittwieser, J., Antonoglou, I., Panneershelvam, V ., Lanctot, M., et al. (2016). Mas-
tering the game of Go with deep neural networks and tree search.Nature, 529(7587),
484–489.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert,
T., Baker, L., Lai, M., Bolton, A., et al. (2017). Mastering the game of Go without
human knowledge. Nature, 550(7676), 354–359.
Cou¨etoux, A., Hoock, J.-B., Sokolovska, N., Teytaud, O., & Bonnard, N. (2011). Con-
tinuous Upper Confidence Trees. International Conference on Learning and Intelli-
gent Optimization, 433–445.
Friston, K. J. (2010). The free-energy principle: a unified brain theory? Nature Reviews
Neuroscience, 11(2), 127–138.
38
Gelly, S., & Silver, D. (2007). Combining online and offline knowledge in UCT. Pro-
ceedings of the 24th International Conference on Machine Learning, 273–280.
Bubeck, S., Munos, R., Stoltz, G., & Szepesv´ari, C. (2011). X-Armed Bandits. Journal
of Machine Learning Research, 12, 1655–1695.
Mansley, C. R., Weinstein, A., & Littman, M. L. (2011). Sample-Based Planning for
Continuous Action Markov Decision Processes. Proceedings of the 21st Interna-
tional Conference on Automated Planning and Scheduling, 335–338.
Moerland, T. M., Broekens, J., Plaat, A., & Jonker, C. M. (2023). Model-based rein-
forcement learning: A survey. Foundations and Trends in Machine Learning, 16(1),
1–118.
Chua, K., Calandra, R., McAllister, R., & Levine, S. (2018). Deep Reinforcement
Learning in a Handful of Trials using Probabilistic Dynamics Models. Advances in
Neural Information Processing Systems, 4754–4765.
Hafner, D., Lillicrap, T., Norouzi, M., & Ba, J. (2019). Learning Latent Dynamics for
Planning from Pixels. Proceedings of the 36th International Conference on Machine
Learning, 2555–2565.
Schmidhuber, J. (2010). Formal Theory of Creativity, Fun, and Intrinsic Motivation
(1990–2010). IEEE Transactions on Autonomous Mental Development , 2(3), 230–
247.
Nagabandi, A., Kahn, G., Fearing, R. S., & Levine, S. (2018). Neural Network Dynam-
39
ics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning.
IEEE International Conference on Robotics and Automation (ICRA), 7559–7566.
Lakshminarayanan, B., Pritzel, A., & Blundell, C. (2017). Simple and Scalable Predic-
tive Uncertainty Estimation using Deep Ensembles. Advances in Neural Information
Processing Systems, 6402–6413.
De Boer, P.-T., Kroese, D. P., Mannor, S., & Rubinstein, R. Y . (2005). A Tutorial on the
Cross-Entropy Method. Annals of Operations Research, 134(1), 19–67.
Bellemare, M. G., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., & Munos, R.
(2016). Unifying Count-Based Exploration and Intrinsic Motivation. Advances in
Neural Information Processing Systems, 1471–1479.
Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T. (2017). Curiosity-Driven Explo-
ration by Self-Supervised Prediction. Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition Workshops, 16–17.
Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT
Press.
Guez, A., Silver, D., & Dayan, P. (2013). Scalable and Efficient Bayes-Adaptive Rein-
forcement Learning Based on Monte-Carlo Tree Search. Journal of Artificial Intelli-
gence Research, 48, 841–883.
Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time Analysis of the Multiarmed
Bandit Problem. Machine Learning, 47(2-3), 235–256.
40
Champion, T., Da Costa, L., Bowman, H., & Grze´s, M. (2022). Branching Time Active
Inference: The theory and its generality. Neural Networks, 151, 295–316.
Fountas, Z., Sajid, N., Mediano, P. A., & Friston, K. (2020). Deep active inference
agents using Monte-Carlo methods. Proceedings of the 34th Conference on Neural
Information Processing Systems (NeurIPS).
Schwartenbeck, P., Passecker, J., Hauser, T. U., FitzGerald, T., Kronbichler, M., &
Friston, K. (2018). Computational mechanisms of curiosity and goal-directed explo-
ration. eLife, 7, e41703.
FitzGerald, T. H. B., Dolan, R. J., & Friston, K. (2015). Dopamine, reward learning,
and active inference. Frontiers in Computational Neuroscience, 9, 136.
Kocsis, L., & Szepesv ´ari, C. (2006). Bandit Based Monte-Carlo Planning. In J.
F¨urnkranz, T. Scheffer, & M. Spiliopoulou (Eds.), Machine Learning: ECML 2006.
Lecture Notes in Computer Science (V ol. 4212, pp. 282–293). Springer, Berlin, Hei-
delberg.
Tschantz, A., Millidge, B., Seth, A. K., & Buckley, C. L. (2020). Reinforcement Learn-
ing through Active Inference. Proceedings of the Workshop on Bridging AI and
Cognitive Science (ICLR 2020).
Friston, K., Rigoli, F., Ognibene, D., Mathys, C., Fitzgerald, T., & Pezzulo, G. (2015).
Active inference and epistemic value. Cognitive Neuroscience, 6(4), 187–214.
Tschantz, A., Baltieri, M., Seth, A. K., & Buckley, C. L. (2020). Scaling Active Infer-
41
ence. In Proceedings of the 2020 International Joint Conference on Neural Networks
(IJCNN) (pp. 1–8). IEEE.
Millidge, B. (2020). Deep active inference as variational policy gradients. Journal of
Mathematical Psychology, 96, 102348.
Friston, K. J., Daunizeau, J., & Kiebel, S. (2009). Reinforcement Learning or Active
Inference? PLoS ONE, 4(7), e6421.
Houlsby, N., Husz ´ar, F., Ghahramani, Z., & Lengyel, M. (2011). Bayesian
Active Learning for Classification and Preference Learning. arXiv preprint
arXiv:1112.5745.
Gal, Y ., Islam, R., & Ghahramani, Z. (2017). Deep Bayesian Active Learning with
Image Data. Proceedings of the 34th International Conference on Machine Learning,
1183–1192.
Kozachenko, L. F., & Leonenko, N. N. (1987). Sample Estimate of the Entropy of a
Random Vector. Problems of Information Transmission, 23(2), 95–101.
Millidge, B. (2019). Deep active inference as variational policy gradients. Journal of
Mathematical Psychology, 96, 102348.
42