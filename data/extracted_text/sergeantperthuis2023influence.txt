Influence of the Geometry of the world model on Curiosity
Based Exploration
Gr´ egoire Sergeant-Perthuis gregoireserper@gmail.com
LCQB Sorbonne Universit´ e & OURAGAN team, Inria Paris Paris, France.
Nils Ruet
CIAMS, Universit´ e Paris-Saclay, Orsay & Universit´ e d’Orl´ eans, Orl´ eans, France.
David Rudrauf
CIAMS, Universit´ e Paris-Saclay, Orsay & Universit´ e d’Orl´ eans, Orl´ eans, France.
Dimitri Ognibene
Dipartimento di Psicologia, Universit` a Milano-Bicocca, Milan, Italy.
Yvain Tisserand
CISA, University of Geneva, Geneva, Switzerland.
Editor: *
Abstract
In human spatial awareness, 3-D projective geometry structures information integration
and action planning through perspective taking within an internal representation space.
The way different perspectives are related and transform a world model defines a specific
perception and imagination scheme. In mathematics, such collection of transformations
corresponds to a ‘group’, whose ‘actions’ characterize the geometry of a space. Imbuing
world models with a group structure may capture different agents’ spatial awareness and
affordance schemes. We used group action as a special class of policies for perspective-
dependent control. We explored how such geometric structure impacts agents’ behavior,
comparing how the Euclidean versus projective groups act on epistemic value in active
inference, drive curiosity, and exploration behaviors. We formally demonstrate and sim-
ulate how the groups induce distinct behaviors in a simple search task. The projective
group’s nonlinear magnification of information transformed epistemic value according to
the choice of frame, generating behaviors of approach toward an object of interest. The
projective group structure within the agent’s world model contains the Projective Con-
sciousness Model, which is know to capture key features of consciousness. On the other
hand, the Euclidean group had no effect on epistemic value : no action was better than the
initial idle state. In structuring a priori an agent’s internal representation, we show how
geometry can play a key role in information integration and action planning.
Keywords: Geometric world model; Exploration; Embodied Cognitive Science; Cognitive
Modeling; Perception-Action Coupling
1 Introduction
In artificial agent learning and control, intrinsic and extrinsic rewards can be combined to
optimize the balance between exploration and exploitation. Intrinsic rewards in Reinforce-
ment Learning (RL) (Hester and Stone, 2017; Merckling et al., 2022; Oudeyer et al., 2007)
or terms of epistemic value in active inference (Friston et al., 2015) have been brought forth
arXiv:2304.00188v2  [cs.AI]  18 Oct 2023
as mechanisms mimicking curiosity and driving exploration, e.g. by integrating prediction
error or uncertainty to drive actions favoring their reduction. However, efficient exploration
is a computationally hard task. Recent neural planning models have increased planning
flexibility and generality (Sekar et al., 2020). Yet, it is well-known that models’ structures
heavily impact planning performance and tractability (Geffner and Bonet, 2013) as well as
learning complexity (Goyal and Bengio, 2022). A good representation of information may
improve learning and search efficiency.
These issues are particularly salient for computation-heavy, highly recursive machine
learning algorithms and applications, e.g. reinforcement learning (RL) in artificial agents
(Bonet and Geffner, 2019) or recursive modeling method (RMM) in multi agent systems
(MAS) and partially observable stochastic games (POSG) (Geffner and Bonet, 2013). Al-
though generic neural world models can support exploration-related processes, incorporating
prior knowledge that shapes internal representations to more effectively support exploration
across a broad range of environments, such as 3-D environments, may enable autonomous
agents to explore more complex and realistic settings on a larger scale (Goyal and Bengio,
2022). The exploration planning problem can thus be approached by considering how the
structure of representation impacts exploration behaviors. In this article, the structure of
representations is encoded into the geometry of the state space of an agent; we quantify the
impact of changing this geometry on the behavior of the agent.
Here, we do not consider mechanisms of representation learning, e.g. in which world
dynamics and action effects need to be learned and represented, as typically done in RL. We
specifically consider control and execution when object locations, world states, and maps
may not be known but dynamics, rewards, and action effects already are. We focus on
action selection for environment exploration and mapping.
We adopt the active inference framework, i.e. an implementation of the Bayesian Brain
Hypothesis aimed at generating adaptive behaviors in agents (Friston et al., 2006), that
has found applications in neuroscience (Da Costa et al., 2020) and proposed for modeling
molecular machines (Timsit and Sergeant-Perthuis, 2021; Timsit et al., 2021). It relies on an
internal representation of the environment that an agent is driven to explore and exploit.
The agent continually updates its beliefs about plausible competing internal hypotheses
on the environment state. Under common sensory limitations, active inference relates to
Partially Observable Markov Decision Process (POMDP) (Kaelbling et al., 1998; Ognibene
et al., 2019). The epistemic value of states is a quantity that arises in active inference
(Friston et al., 2015). Its maximization drives the agent’s curiosity and actions.
For exploration or search in 3-D space, it is warranted to consider how geometrical
principles could be embedded into efficient control mechanisms, to regularize the internal
representation of information and mediate exploration under a drive of uncertainty reduction
or information maximization. Geometrical considerations have previously been integrated
into a variety of optimization and machine learning approaches, such as RL, active inference,
and Bayesian inference (See Related Works below), but not in the specific perspective we
introduce herein.
We build upon the hypothesis that 3-D internal representations of space in agents per-
forming active inference may correspond to specific geometries, with properties that can
be exactly analysed. More specifically, we consider how different first person perspec-
tives may relate to each other, through transformations of a world model, as a specific
perception and imagination scheme for agents. This entails considering the action of ge-
ometrical groups of transformations (in the mathematical sense of the concept in Group
theory; see Section 3.1) on the spatial distribution of information experienced and en-
coded by agents internally. The question is whether such group action could contribute
to information gain estimation and maximization, as an internal planning or perspective-
dependent control mechanism. Certain geometrical groups might imply internal repre-
sentations, policies, value functions and principles of action that are particularly relevant
for search and exploration. More specifically, we wish to compare how different groups
impact the quantification of epistemic value. We then wish to characterize how the opti-
mization of action from those different groups may yield different exploration behaviors.
We contrasted two separate toy models of an agent performing a simple search task us-
ing active inference based solely on epistemic drives. One model used Euclidean geome-
try and the other projective geometry for the agent’s internal space. We compared the
two models in terms of resulting exploration behaviors and effects on epistemic value.
We chose to compare Euclidean versus projective geometry based on previous work,
leveraging psychological research on the phenomenology of spatial consciousness and its role
in the control of behaviors (Rudrauf et al., 2017, 2020, 2022, 2023). This research suggested
that 3-D projective geometry plays a central role in human cognition and decision-making
by shaping information representation and subsequent drives. It also offers a mechanism
of changes of points of view and perspectives on a world model, including for perspective
taking in social cognition, which is critical for the development of strategies of action plan-
ning in humans (see Rudrauf et al. 2022, 2023). We used Euclidean geometry as a standard
baseline geometry for comparison (Ognibene and Demiris, 2013). Our geometrical rationale
implies a different understanding of how agents’ actions in their environment (here in the
behavioral sense of the term) are implemented and selected compared to usual active infer-
ence. Agents’ actions, such as navigation and approach-avoidance behaviors, can naturally
be seen as dual to internal changes of perspective, i.e. group actions, in their representa-
tion space. We thus used group actions as a predictive model of actual behavioral actions.
The approach allowed us to formally study and demonstrate how the geometry gov-
erning the internal representation space may directly impact the computation of epistemic
value and ensuing exploratory behaviors. Projective geometry versus Euclidean geometry
demonstrated remarkable properties of information integration for motion planning under
epistemic drive.
2 Related Works
2.1 Representation of Space and Exploration, in the Context of Machine
Learning and Control
The integration of geometrical mapping in machine learning has been proposed to reduce
the high-dimensionality of input spaces and provide efficient solutions for action selection
and navigation. Seminal neurally inspired models used projections on 2-D manifolds for
representation learning of complex spatial information and self-motion effects (Arleo et al.,
2004). The impact of changes of perspective in exploration has long been of interest (Og-
nibene and Demiris, 2013). Ferreira et col. (Ferreira et al., 2013) proposed an internal
3-D egocentric, spherical representation of space, to modulate information sampling and
uncertainty as a function of distance, and control a robot attention through Bayesian infer-
ence. This was a seminal example of how geometrical rationale could suggest solutions to
integrate perception and action planning.
Exploration methods must often maintain high-resolution representations of space to
maximize information gain following action. This may hinder exploration efficiency, in par-
ticular in large-scale environments. 3-D topological representations of ambient space have
been proposed as part of an abstract planning scheme, showing promising improvements of
exploration efficiency (Yang et al., 2021).
Active vision principles, combined with curiosity-based algorithms and RL, were ap-
plied to the learning of saliency maps in the context of autonomous robots’ navigation
(Craye et al., 2016). The approach yielded promising optimization solutions to both adap-
tive learning of task-independent, spatial representations, and efficient exploration policies,
which could serve as prior to support long-term, task-oriented, utility-driven RL mechanisms
(Craye et al., 2016) (see also Ognibene and Baldassare, 2014; Sperati and Baldassarre, 2017).
Complex control tasks with continuous state and action spaces have been solved using
deep reinforcement learning (DRL) with joint learning of representations and predictions.
Such approach may entail non-stationarity, risks of instability and slow convergence, in par-
ticular in control tasks with active vision. Separating representation learning and policies’
computations may mitigate the issues, but may also lead to inefficient information repre-
sentations. Merckling et col. (Merckling et al., 2022) have sought to build compact and
meaningful representations based on task-agnostic and reward-free agent-environment in-
teractions. They used (recursive) state representation learning (SRL) while jointly learning
a state transition estimator with near-future prediction objective, to contextually remove
distracting information and reduce the exploration problem complexity. Positive outcomes
were maximized through inverse predictive modeling, and prediction error was used to fa-
vor actions reducing uncertainty, which improved subsequent performance in RL tasks. The
authors emphasized that dealing with partial observability through memory and active vi-
sion may require new solutions to both representation learning of hidden information and
exploration strategies.
Uncertainty-based methods using intrinsic reward and exploration bonuses to plan tra-
jectories have been criticized for inducing non-stationary decaying surprise, and for being
hard to structure and optimize (Guo et al., 2021). Maximum State-Visitation Entropy
(MSVE) was introduced to maximize state exploration uniformity, but optimization has
been often challenging for large state spaces. Guo et col. (Guo et al., 2021) have intro-
duced Geometric Entropy Maximization, which leverages geometry-aware entropy based on
Adjacency Regularization (AR) and a similarity function, in order to optimize the MSVE
problem at scale.
Geometrical constraints considered across these related works were not integrated into
a global model, and were somewhat ad hoc. They pertained to a lower level of processing
than the one we are concerned with here. However, they emphasize the current needs and
challenges for integrating geometry in learning, control and navigation.
Methods and algorithms combining computer vision, machine learning and optimiza-
tion, e.g. for robotic planning, have integrated group theoretic concepts to obtain, for
instance, invariance to rotation and translation in image processing (Lee and Moore, 2004;
Qin et al., 2019; Meng et al., 2017). Likewise, the leveraging of geometrical concepts, in
Deep learning, e.g. for learning manifolds and graphs, has been growing in recent years,
demonstrating very promising results for representation learning (Gerken et al., 2021; Cao
et al., 2022). The approach introduces combinatorial structures to leverage prior knowledge
of geometry on the data of interest, e.g. applying ‘convolutional Neural Networks’ to non-
Euclidean space. However, the Euclidean group E3, or more specifically SE3 (see Lee and
Moore, 2004), which includes translations and rotations, but excludes reflections, or simply
SO3, the 3-dimensional rotation group (Gerken et al., 2021), are the groups being typically
considered.
Here, in addition to the Euclidean group, we also consider P GL(R3), the projective
general linear group in 3-D, which acts on a projective space through projective trans-
formations. The projective group is central to computer vision, for instance to generate
2-D images from 3-D information, but is used in such context in a restricted manner. We
sought approaches based on cognitive science, considering spatial cognition and its relations
to action at a higher level of integration, which does not reduce to the visual modality, but
instead assume the mapping of multimodal information on a supramodal internal space of
representation.
2.2 Projective Consciousness Model (PCM) and Active Inference
It has been shown that geometrically constrained active inference can be used as a frame-
work to understand and model central aspects of human spatial consciousness, through
the Projective Consciousness Model (PCM) (Rudrauf et al., 2017, 2022). According to
this model, consciousness accesses and represents multimodal information through a Global
Workspace (Dehaene et al., 2017) within which subjective perspectives on an internal world
model can be taken. The process contributes to appraise possible actions based on their ex-
pected utility and epistemic value (Rudrauf et al., 2022). In publications on PCM (Rudrauf
et al., 2017; Williford et al., 2018; Rudrauf et al., 2022, 2023; Williford et al., 2022), it was
hypothesized that such internal representation space is geometrically structured as a 3 − D
projective space, denoted P3(R). Changes of perspective then correspond to the choice of
a projective transformation ψ, i.e. an action from P GL3. A projective transformation can
also be modeled as a linear isomorphism Mψ ∈ GL4(R) up to a multiplicative constant.
The model yieled an explanation for the Moon illusion (Rudrauf et al., 2020) with falsifi-
able predictions on how strong the effect should be depending on context; as well as for
the generation of adaptive and maladaptive behaviors, consistent with developmental and
clinical psychology (see Rudrauf et al., 2022). Though essential in integrative spatial cogni-
tion, notably for understanding multi-agent social interactions, perspective taking is rarely
integral to existing models of consciousness or formally implemented (Koch et al., 2016;
Kleiner and Tull, 2021; Mashour et al., 2020; Dehaene et al., 2017; Merker et al., 2022).
The PCM assumes that projective mechanisms of perspective changes are integral to the
global workspace of consciousness, both in non-social and social contexts. The advantages
of mechanisms of perspective taking for cybernetics remains to be fully formulated (see
Rudrauf et al., 2022).
3 Methodology
The experiment we considered is that of an agent, denoted as a, which is looking for an
object O in the ‘real world’, the 3-D Euclidean space E3 := R3. The set of moves of
the agent is denoted M. The position of O is denoted o ∈ E3. The agent ‘represents’
the position of the object O inside its ‘internal world model’. We consider ‘internal world
models’, spaces denoted W, that are such that there is a group acting on them; we call such
spaces, group structured world models. This group accounts for the change of coordinates
that each movement of the agent induces when the positions of the object are expressed in
the agents reference frame. We consider two spaces in particular:
1. Euclidean case: W is the 3-D vector space, W = R3
2. Projective case: W is the 3-D projective space, denoted as P3(R)
We will denote B(W) the Borel σ-algebra of the respective topological spaces.
In Section 3.2, we explain how the ‘real world’ and the ‘internal world model’ are related
to one another in both the Euclidean and Projective case. Figure 1 illustrates the setup of
the toy model and main transformations considered. The agent’s internal beliefs about the
position of the object are encoded by a probability measure on W that the agent updates
through observations. The agent explores its environment through the computation of an
epistemic value, the maximization of which captures curiosity-based exploration. In Section
3.3, we explain how epistemic value is defined for group structured internal representations.
In Section 3.4 we give the details of the exploration algorithm.
3.1 Group Structured World Model
Let us first recall what a group is.
Definition 1 (Group, §2 Chapter 1 Lang, 2012) . A group is a set G with an operation . :
G×G → G that is associative, such that there is an element e ∈ G for which e.g = g for any
g ∈ G, and any g ∈ G has an inverse denoted g−1 defined as satisfying, g.g−1 = g−1.g = e.
We call a group structured world model, a world model provided with a group action;
we now make this statement formal.
Definition 2 (Group structured world model) . W is a group structured world model for
the group G when there is a map h : G × W → W denoted as h(g, x) = g.x for g ∈ G and
x ∈ X, such that,
1. ( g.g1).x = g.(g1.x) for all g, g1 ∈ G, x ∈ W
2. e.x = x, for all x ∈ W
In the Euclidean case the group structured world model, W, is the 3-D vector space
R3; it is structured by the group of invertible matrices GL3(R). In the Projective case, the
group structured world model, W, is the projective space P3(R); it is structured by the
group of projective linear transformations P GL(R3).
Figure 1: Toy model setup and main transformations
Upper-tier. Agent a simulates move m in Euclidean space E. R0 and R are its frames in E before
and after the move, oriented toward object O. Vertical arrows indicate transformations ϕ from the
external to the internal space. Lower-tier. Rendering of the effect of the internal group action
ψ(m) corresponding to move m in the Euclidean versus projective case. (Made with Unity).
3.2 Relating the ‘Real World’ to the ‘Internal World Model’
We assume that the ‘real world’ is the 3-D Euclidean space, E3. We assume that the ‘real
world’ comes with with an Euclidean frameRE, i.e. a point C and three independent vectors
e0, e1, e2. This frame is used to set up the experiment: the configurations of the object and
agent across time are encoded in this frame; it is fixed once and for all before starting
the experiment. Therefore we now identify E3 with R3, C with (0, 0, 0) and e0, e1, e2 with
the respective basis vectors, (1 , 0, 0), (0, 1, 0), (0, 0, 1). The agent, denoted as a, is modeled
as a solid in the ‘real world’; it has its own Euclidean frame (the solid reference frame),
R := (P, u0, u1, u2), with P the center of a and u0, u1, u2 three unitary vectors that form a
basis.
In the Euclidean case, the map that relates E3 and its group structured world model,
W, is the affine map, ϕR, that changes the coordinate in RE to coordinates in R.
In the Projective case, this map is a projective transformation. The choice of such a
projective transformation is dictated by Proposition A.1 (Rudrauf et al., 2022). Let us now
recall some facts about that transformation.
Let for any (x, y, z) ∈ R3,
ρ(x, y, z) =
 x
γz + 1, y
γz + 1, z
γz + 1

(1)
with γ ∈ R+ a strictly positive parameter.
The (projective) transformation ϕp
R, from E3 to W, which relates the ‘real world’ to the
‘internal world model’ in the projective case, is posed to be ϕp
R := ρ ◦ ϕR.
Proposition 1. When the agent a makes the move m ∈ M, its solid reference frame
changes from R to Rm. In the Euclidean case this move induces an invertible affine map,
from the ‘internal world model’ to itself. In the Projective case it induces a projective
transformation, ψm ∈ P GL(R3).
We denote 1U or x → 1[x ∈ U] the characteristic function of subset U, i.e. the functions
that is equal to 1 for x ∈ U and 0 elsewhere.
Remark 1. In both cases there is a dense open subset, U, of W which is in continuous
bijection with R3. From the Lebesgue measure dx on R3, we define the following measure
on W, dλ := 1U dx. In what follows we do not raise this technical point anymore and simply
refer to dλ as the Lebesgue measure on W.
3.3 Beliefs, Policies and Epistemic Value
3.3.1 Beliefs
The agent a keeps internal beliefs about the position of the object represented in its ‘internal
world model’; these beliefs are encoded by a probability measure QX ∈ P(W), where P(W)
denotes the set of probability measures on W. Probability measures will be denoted with
upper case letters and their densities with lower case letters. These beliefs are updated
according to noisy sensory observations of the position of O. ‘Markov Kernels’ can be used
to formalize noisy sensors. Let us recall their definition.
A ‘Markov Kernel’ Π from Ω 1 to Ω is a map Π : Ω × Ω1 → [0, 1] such that for any
ω1 ∈ Ω1, P
ω∈Ω Π(ω|ω1) = 1, i.e. a map that sends any ω1 ∈ Ω1 to a probability measure
Π|ω1 ∈ P(Ω).
The uncertainty on the sensors of a is captured by a Markov kernel PY |X from W to
W. It is a parameter of the experiment: it is fixed before the agent starts looking for O.
The couple ( PY |X, QX) defines the following probability density, PX,Y ∈ P(W × W): for
any x, y∈ W,
PX,Y (dx, dy) := pY |X(y|x)qX(x)dxdy (2)
where dx is the Lebesgue measure on W. An observation of the position of the object
yo ∈ W triggers an update of the belief QX to the belief with density
QX|yo = pX,Y (x|yo)qX(x)dxR
x∈W pX,Y (x|yo)qX(x)dx (3)
3.3.2 Policies
Recall that the agent has a set of moves it can make M; moves m ∈ M are associated
with the group action ψm : W → W (Proposition 1). The agent plans the consequence of
its moves on its internal world model one step ahead: each change of frame induces the
following Markov Kernel, for any m ∈ M, A ∈ B(W), and x0 ∈ W,
pX1|X0,m(A|x0, m) = 1[ψm(x0) ∈ A] (4)
Each move m spreads a prior QX on X0 into the following prior on X1: ∀A ∈ B(W),
ψm,∗QX(A) :=
Z
1[ψm(x0) ∈ A]qX(x0)dx (5)
= QX(ψ−1
m A) (6)
We chose to denote this probability measure as ψm,∗QX, because it is the standard
mathematical notation for the ‘pushforward measure’ by ψm. The generative model the
agent uses to plan its future actions is summarized in Figure 2.
3.3.3 Epistemic Value
The following definition is a restatement of the epistemic value introduced in (Friston et al.,
2015) in the case of the kernel PY |X : W → W.
Definition 3 (Epistemic Value). For any probability measure QX ∈ P(W), the epistemic
value of this measure is:
C(QX) :=EPY

H(PX|Y |QX)

(7)
=
Z
pY (y)dy
Z
pX|Y (x|y) ln pX|Y (x|y)
qX(x) dx (8)
H is the relative entropy, also called Kullback-Leibler divergence.
X0 = WϕRm
o ∈ E3
X1
Y = W ∋ yo Y ∋ yo
m
1[ψm(x0) ∈ A]
PY |X
ϕR
Figure 2: m ∈ M is a move of the agent a, 1[ψm(x0) ∈ A] defines the kernel induced by
move m, PY |X is the noisy sensor. The diagram constituted of solid arrows defines
the generative model the agent uses to plan its actions. o is the position of the
object in the ‘real world’, yo ∈ W is the representation of o in the ‘internal world
model’ of a with respect to the solid reference frame R, yo
m is the same for the
reference frame Rm after move m.
Reexpressing Equation 7, it becomes apparent that epistemic value is simply a mutual
information:
C(QX) =
Z
pX,Y ln pX,Y (x, y)
pY (y)qX(x)dxdy (9)
We propose to define the epistemic value of movem as the epistemic value of the induced
prior on X1,
C(m) := C(ψm,∗QX) (10)
3.4 Exploration Algorithm
Let us now put the previous elements together to describe the exploration behavior pro-
grammed in our agent. The agent a is initialized in a configuration of the ‘real world’, with
solid reference frame R0; the object O is positioned at o ∈ E3. a starts with an initial belief
Q0
X ∈ P(W) on the position of O. It plans one step ahead the consequence of move m; move
m induces a group action ψm : W → W that pushes forward the belief Q0
X to ψm,∗Q0
X. The
agent then evaluates the epistemic value of ( PY |X, ψm,∗Q0
X) for each move m and chooses
the move that maximizes this value, m∗. a executes the move m∗ which transforms its solid
reference frame R0 to R. It can then observe (with its ’noisy sensors’) the position of O
which is yo := ϕR(o) in its internal world model, which triggers the update of prior ψm,∗Q0
X
to the distribution conditioned on the observation:
 
ψm,∗Q0
X

|yom
. The process is iterated
with this new prior. The exploration algorithm is summarized in Algorithm 1.
4 Theoretical Predictions
We wish to understand how the group by which the internal world model is structured
influences the exploration behavior of the agent. The Euclidean case serves as the reference
model; in this case the world model shares the same structure as the real world: it is the
‘classical’ way of modeling this exploration problem. The Projective case corresponds to the
Algorithm 1: Curiosity based Exploration for agent a
Data: Initialization: Q0
X initial belief, R0 initial solid reference frame of a
1 QX ← Q0
X;
2 while True do
3 m∗ ← argmaxm∈M C(ψm,∗QX);
4 R ←solid reference frame of a after move m∗;
5 QX ← ψm,∗QX;
6 yo ← ϕR(o);
7 QX ← QX|yo;
8 end
hypothesis underlying the PCM. The following Theorem states that this experiment allows
us to discriminate when the behavior of the agent is dictated by ‘objective’ perspectives
(Euclidean change of frame) versus by ‘subjective’ perspectives (projective change of frame)
on its environment.
We consider the following noisy sensor, for any x, y∈ R3,
PY |X(y|x) = 3
4πϵ3 1[∥x − y∥ ≤ϵ] (11)
where ∥.∥ designates the Euclidean norm on R3, i.e. ∥x∥2 = x2
0 + x2
1 + x2
2; ϵ >0 is a
strictly positive real number.
Theorem (Discrimination of behavior with respect to internal representations) . Let us
assume that staying still is always a possible move for the agent.
Euclidean case: when the agent has an objective representation of its environment, given
by an affine map, the agent stays still.
Projective case: Assume now that the set of moves M is finite; assume furthermore that
after any possible move, the agent faces O, in other words, we assume that the agent knows
in which direction to look in order to find the object but is still uncertain on where the
object is exactly. If it has a ‘subjective’ perspectives, i.e. its representation is given through
a projective transformation, it will choose the moves that allows it to approach O (for any
ϵ small enough).
Proof. The details of the proof are given in Appendix A.2. Let us here sketch the proof.
The agent circumscribes a region of space in which it believes it is likely to find the object.
This region corresponds to the error the agent tolerates on the measurement it makes of
the position of O; we can also see it as the precision up to which the agent measures the
position of O. In the Euclidean case, the region in which the agent circumscribes the object
appears to always be of the same size, irrespective of the agent’s configuration with respect
to the object. Therefore not moving ends up being an optimal option and the agent will not
approach the object without additional extrinsic reward. In the Projective case, the agent
can ‘zoom’ on this region in order to gain more precision in measuring o; the configurations
of the agent in which this region is magnified are more informative regarding the position
of O and therefore preferred by the agent. The only way for the agent to actually zoom
onto this area is to approach the location it believes O is likely to be, therefore the agent
will end up approaching O.
Remark 2. This particular choice of Markov kernel (Equation 11) allows for an explicit
expression of epistemic value which simplifies the proof of the result; however we expect the
result to hold for a larger class of kernels.
In the next section we present an implementation of this experiment and simulation
results.
5 Simulations
5.1 Methods
Algorithm 1 is implemented in the following manner (source code available at https://
github.com/NilsRuet/effect-of-geometry-on-exploration ). Beliefs and the Markov
kernel corresponding to sensors were considered to be multivariate normal distributions, that
is PY |X ∼ N(µY |X, ΣY |X) and QX ∼ N(µX, ΣX). Belief update through the action of a
group was approximated using a Gaussian distribution; a projective transformation changes
a Gaussian distribution into a non Gaussian one which is difficult to describe. Therefore we
replace this non-Gaussian distribution with a Gaussian distribution with same mean and
variance.
We assumed µY |X = x (which implies µy = µx) and ΣY |X = ϵ2I where I is the identity
matrix and ϵ >0 a positive real number. As a result, for a given observation yo, QX|yo and
C(ψm,∗QX) can be computed efficiently. The joint distribution P on X, Y is a Gaussian
distribution:
P(x, y) = PY |X(y|x)QX(x)
P(x, y) ∼ N(µX,Y , ΣX,Y )
with µX,Y = (µX, µX) and
ΣX,Y =
ΣXX ΣXX
ΣXX ϵ2I + ΣXX

(12)
The variance of Y is ΣY Y = ϵ2I + ΣXX .
The joint distribution being Gaussian entails that the distribution of X conditioned on
y = yo is also Gaussian, thus QX|yo ∼ N(µX|yo, ΣX|yo). Applying Proposition 3.13.(Eaton,
2007) to our setting, the mean and covariance of the conditioned distribution are given by:
µX|yo = µX + ΣXX Σ−1
Y Y(yo − µX) (13)
ΣX|yo = ΣXX − ΣXX Σ−1
Y YΣXX (14)
Epistemic value is computed using the Kullback-Leibler divergence. With full knowl-
edge of the joint distribution, in the Gaussian case, following the expression of entropy for
gaussian vectors (Chapter 12 Equation (12.39) Cover and Thomas, 2006) it is computed as:
C(QX) = I(X; Y ) = 1
2 ln (det ΣXX )(det ΣY Y)
det ΣX,Y
(15)
The set of moves that can be selected by the agent is restricted to translations as the
agent must always face the object. This constraint, as well as the choice of a simple model
of noisy sensor (with homogeneous precision and resolution), was motivated by the aim of
making formal demonstrations of theorems tractable, and implementations tightly related
to the theoretical predictions. This is a departure from how spatial sampling typically
operates in perception, e.g. decreasing resolution of the visual field with eccentricity (see
(Rudrauf et al., 2022, 2023) for more realistic but less formally analyzable applications of a
broader version of the PCM). However, the specific aim herein of demonstrating fundamental
properties of the action of different geometrical groups on epistemic value and ensuing
behaviors of approach motivated such restrictions.
The set of possible translations is composed of eight translations with the same norm,
with evenly distributed angles (one of them being oriented toward the object irrespective
of the position of the agent), and also contained an idle state, i.e. no translation. Here
the angles correspond to the angles of the translation and not a rotation angle of the solid
frame of the agent as the agent must always face the object.
We approximated the belief after the action m of a given group using a Gaussian distri-
bution, ψm,∗QX ∼ N(µm, Σm). The mean and covariance matrix are approximated using
numerical integration:
µm =
Z
xp(ψ−1
m (x)) 1
|det Jψm(ψ−1m (x))|dx (16)
Σm =
Z
(x − µm)(x − µm)T p(ψ−1
m (x)) 1
|det Jψm(ψ−1m (x))|dx (17)
We ran two sets of simulations. In the first one (Figure 3 left tier), the agent started
from an initial position with the object always located at a fixed position, and the algorithm
was applied for 20 iterations, for both the Euclidean and Projective internal spaces. The
agent started at (0, 0) and the object was located at (0 , 2) in the world frame E2 spanning
the agent’s displacement floor. If all translations were associated with epistemic values that
only varied within a small range ( ±1e − 4) as compared to the epistemic value of the idle
state, reflecting numerical imprecision, the idle state was selected (the agent did not move).
The aim of this set of simulations was to compare trajectories of agents displacements
through time across the two geometries. In the second set of simulations (Figure 3 right
tier), the agent started at the center of a 20 × 20 grid of possible positions of the object.
The positions that are too close to the agent are excluded so that a sufficient effect can be
observed. For each object position, we considered only the first set of translations that the
agent could envision from its initial position. We computed epistemic value across the set
of possible translations of the agent for each object position and for both the Euclidean and
projective internal spaces. The aim of this set of simulations was to be able to systematically
compare epistemic values across the two geometries for all possible positions of objects.
5.2 Results
Figure 3 left tier shows a representative example of trajectories obtained in the Euclidean
versus projective cases. In the projective case, the agent always approached the object. In
the Euclidean case, the agent always stayed idle. Figure 3 right tier shows epistemic value as
Figure 3: Simulation results
Left-tier. Trajectory of the agent for the projective versus Euclidean internal spaces. Right-tier.
Epistemic value as a function of directions of translation with respect to the object direction, for
the projective versus Euclidean internal spaces. Points are average values across comparable
directions, and error-bars are standard errors.
a function of translation direction expressed in radians for both the projective and Euclidean
cases. The direction of 0 radian corresponds to the object direction. We averaged epistemic
values across object positions within comparable directions. In the Projective case, average
epistemic value was maximal for the direction of the object, and decreased with directions
farther away from it. In the Euclidean case, average epistemic value was identical across
directions. Maximum average epistemic value was much higher for the projective case than
for the Euclidean case.
6 Discussion
In this article, we introduced a generative model for environment exploration based on
a first-person perspective in which actions are encoded as changes in perspective. The
families of geometry for the world model encode possible ‘kinds’ of perspective-taking on
the environment and structures the representation of sensory evidence within the world
model of the agent. In other words, each family corresponds to a specific perception and
imagination scheme for the agent. We encoded two such families, namely the Euclidean
versus projective group as acting on the internal world model of the agent, i.e., within the
geometric properties of this internal world model. We showed that different geometries
induce different behaviors, focusing on the two cases: when the internal world model of
the agent followed Euclidean geometry versus projective geometry. This result contributes
to understanding how integrative geometrical processing and principles can play a central
role in cybernetics. In our approach, the geometry of the world model links perceptual and
imaginary representations with actions and behaviors.
Although beyond the scope of this article, we are interested in generalizing the approach
to compare the results obtained with those that could be obtained with other models in
which geometry plays a central integrative role, e.g. as in (Ferreira et al., 2013) who used
an internal 3-D egocentric, spherical representation of space. Such an approach would need
to be expressed in terms of group action to make it formally comparable to our approach.
Likewise, it would be interesting to compare other groups, and other more realistic models
of sensors, and what types of behaviors they would induce. This would be useful to refine
predictions and envision experimental designs for empirical validation in humans. We would
also like to use more sophisticated settings (see for instance Rudrauf et al., 2022, 2023),
even though it may be incompatible with the derivation of analytical solutions, but could
lead to richer simulations and induction of behaviors.
We also wish to further examine how the geometry of a latent space intertwines with
information processing. One motivation is theoretical, as we would like to assess how ge-
ometry changes learning behavior (Goyal and Bengio, 2022). In this contribution, we have
discarded representation learning per se, as it was beyond the scope of this study focusing
on planning. In future work, we intend to use deep learning to learn group structured
representations. It is important to note that such approach differs from geometric deep
learning (Bronstein et al., 2021; Sergeant-Perthuis et al., 2022) as we do not seek to learn
equivariant representations: a group structure will only be considered for the internal world
model but none will be presupposed on the observation side. Likewise, we are interested in
examining how geometry may play a role in overt and covert attention.
Our contribution can simplify the design of novel agent architectures where exploratory
and sensory choices of actions naturally emerge as a consequence of the internal representa-
tion and the reflection of the perceptual mechanism of the agent’s embodiment. The study of
world models with projective geometries was motivated by ongoing work in computational
psychology aimed at reproducing features of consciousness. Projective geometry induces
effects of magnification and focalization on information that appear immediately relevant
for spatial attention, and more generally for contextual salience. Another motivation for
this research is more practical, as we would like to use such principles to design virtual
and robotic artificial agents mimicking human cognition and behaviors following (Rudrauf
et al., 2022, 2023).
References
A. Arleo, F. Smeraldi, and W. Gerstner. Cognitive navigation based on nonuniform ga-
bor space sampling, unsupervised growing networks, and reinforcement learning. IEEE
Transactions on Neural Networks, 15(3):639–652, 2004. doi: 10.1109/TNN.2004.826221.
Blai Bonet and Hector Geffner. Learning first-order symbolic representations for planning
from the structure of the state space. arXiv preprint arXiv:1909.05546 , 2019.
Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Veliˇ ckovi´ c. Geometric deep
learning: Grids, groups, graphs, geodesics, and gauges, 2021. URL https://arxiv.org/
abs/2104.13478.
Wenming Cao, Canta Zheng, Zhiyue Yan, and Weixin Xie. Geometric deep learning:
progress, applications and challenges. Science China Information Sciences, 65(2):126101,
2022.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series
in Telecommunications and Signal Processing) . Wiley-Interscience, USA, 2006. ISBN
0471241954.
C´ eline Craye, David Filliat, and Jean-Fran¸ cois Goudou. Rl-iac: An exploration policy for
online saliency learning on an autonomous mobile robot. In2016 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS) , pages 4877–4884, 2016. doi: 10.
1109/IROS.2016.7759716.
Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and
Karl Friston. Active inference on discrete state-spaces: A synthesis. Journal of
Mathematical Psychology, 99:102447, 2020. ISSN 0022-2496. doi: https://doi.org/10.
1016/j.jmp.2020.102447. URL https://www.sciencedirect.com/science/article/
pii/S0022249620300857.
Stanislas Dehaene, Hakwan Lau, and Sid Kouider. What is consciousness, and could ma-
chines have it? Science, 358(6362):486–492, 2017.
Morris L. Eaton. Multivariate statistics: A vector space approach.Lecture Notes-Monograph
Series, 53:i–512, 2007. ISSN 07492170. URL http://www.jstor.org/stable/20461449.
Jo˜ ao Ferreira, Jorge Lobo, Pierre Bessiere, Miguel Castelo-Branco, and Jorge Dias. A
bayesian framework for active artificial perception. IEEE transactions on cybernetics, 43
(2):699–711, 2013.
Karl Friston, James Kilner, and Lee Harrison. A free energy principle for the brain. Journal
of Physiology-Paris, 100(1):70–87, 2006. ISSN 0928-4257. doi: https://doi.org/10.1016/
j.jphysparis.2006.10.001. URL https://www.sciencedirect.com/science/article/
pii/S092842570600060X. Theoretical and Computational Neuroscience: Understand-
ing Brain Functions.
Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas Fitzgerald,
and Giovanni Pezzulo. Active inference and epistemic value. Cognitive Neuroscience, 6
(4):187–214, 2015. ISSN 17588936. doi: 10.1080/17588928.2015.1020053.
Hector Geffner and Blai Bonet. A concise introduction to models and methods for auto-
mated planning. Synthesis Lectures on Artificial Intelligence and Machine Learning , 8
(1):1–141, 2013.
Jan E Gerken, Jimmy Aronsson, Oscar Carlsson, Hampus Linander, Fredrik Ohlsson,
Christoffer Petersson, and Daniel Persson. Geometric deep learning and equivariant
neural networks. arXiv preprint arXiv:2105.13926 , 2021.
Anirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cog-
nition. Proceedings of the Royal Society A , 478(2266):20210068, 2022.
Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Alaa Saade, Shantanu Thakoor, Bilal
Piot, Bernardo Avila Pires, Michal Valko, Thomas Mesnard, Tor Lattimore, and R´ emi
Munos. Geometric entropic exploration. arXiv preprint arXiv:2101.02055 , 2021.
Todd Hester and Peter Stone. Intrinsically motivated model learning for developing curious
robots. Artificial Intelligence, 247:170–186, 2017.
Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting
in partially observable stochastic domains. Artificial intelligence, 101(1-2):99–134, 1998.
Johannes Kleiner and Sean Tull. The mathematical structure of integrated information
theory. Frontiers in Applied Mathematics and Statistics , 6, 2021. ISSN 2297-4687. doi:
10.3389/fams.2020.602973. URL https://www.frontiersin.org/article/10.3389/
fams.2020.602973.
Christof Koch, Marcello Massimini, Melanie Boly, and Giulio Tononi. Neural correlates of
consciousness: progress and problems. Nature Reviews Neuroscience, 2016.
Serge Lang. Algebra, volume 211. Springer Science & Business Media, 2012.
Pei Yean Lee and John B Moore. Geometric optimization for 3d pose estimation of quadratic
surfaces. In Conference Record of the Thirty-Eighth Asilomar Conference on Signals,
Systems and Computers, 2004. , volume 1, pages 131–135. IEEE, 2004.
George A. Mashour, Pieter Roelfsema, Jean-Pierre Changeux, and Stanislas Dehaene. Con-
scious processing and the global neuronal workspace hypothesis. Neuron, 105(5):776–
798, 2020. ISSN 0896-6273. doi: https://doi.org/10.1016/j.neuron.2020.01.026. URL
https://www.sciencedirect.com/science/article/pii/S0896627320300520.
Zehui Meng, Hao Sun, Hailong Qin, Ziyue Chen, Cihang Zhou, and Marcelo H Ang. Intel-
ligent robotic system for autonomous exploration and active slam in unknown environ-
ments. In 2017 IEEE/SICE International Symposium on System Integration (SII) , pages
651–656. IEEE, 2017.
Astrid Merckling, Nicolas Perrin-Gilbert, Alex Coninx, and St´ ephane Doncieux. Ex-
ploratory state representation learning. Frontiers in Robotics and AI , 9, 2022.
Bjorn Merker, Kenneth Williford, and David Rudrauf. The integrated information theory
of consciousness: a case of mistaken identity. Behavioral and Brain Sciences , 45, 2022.
Dimitri Ognibene and Gianluca Baldassare. Ecological active vision: four bioinspired prin-
ciples to integrate bottom–up and adaptive top–down attention tested with a simple
camera-arm robot. IEEE transactions on autonomous mental development , 7(1):3–25,
2014.
Dimitri Ognibene and Yiannis Demiris. Towards active event recognition. In Twenty-Third
International Joint Conference on Artificial Intelligence , pages 2495–2501, 2013.
Dimitri Ognibene, Lorenzo Mirante, and Letizia Marchegiani. Proactive intention recogni-
tion for joint human-robot search and rescue missions through monte-carlo planning in
pomdp environments. In Miguel A. Salichs, Shuzhi Sam Ge, Emilia Ivanova Barakova,
John-John Cabibihan, Alan R. Wagner, ´Alvaro Castro-Gonz´ alez, and Hongsheng He,
editors, Social Robotics, pages 332–343, Cham, 2019. Springer International Publishing.
ISBN 978-3-030-35888-4.
Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems
for autonomous mental development. IEEE transactions on evolutionary computation ,
11(2):265–286, 2007.
Hailong Qin, Zehui Meng, Wei Meng, Xudong Chen, Hao Sun, Feng Lin, and Marcelo H
Ang. Autonomous exploration and mapping system using heterogeneous uavs and ugvs in
gps-denied environments. IEEE Transactions on Vehicular Technology, 68(2):1339–1350,
2019.
D. Rudrauf, G. Sergeant-Perthuis, O. Belli, Y. Tisserand, and G. Di Marzo Serugendo.
Modeling the subjective perspective of consciousness and its role in the control of be-
haviours. Journal of Theoretical Biology , 534:110957, 2022. ISSN 0022-5193. doi:
https://doi.org/10.1016/j.jtbi.2021.110957. URL https://www.sciencedirect.com/
science/article/pii/S0022519321003763.
D Rudrauf, G Sergeant-Perhtuis, Y Tisserand, T Monnor, V De Gevigney, and O Belli.
Combining the Projective Consciousness Model and Virtual Humans for immersive psy-
chological research: a proof-of-concept simulating a ToM assessment. ACM Transactions
on Interactive Intelligent Systems , 2023.
David Rudrauf, Daniel Bennequin, Isabela Granic, Gregory Landini, Karl Friston, and Ken-
neth Williford. A mathematical model of embodied consciousness. Journal of theoretical
biology, 428:106–131, 2017.
David Rudrauf, Daniel Bennequin, and Kenneth Williford. The moon illusion explained by
the projective consciousness model. Journal of Theoretical Biology, 507:110455, 2020.
Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and
Deepak Pathak. Planning to explore via self-supervised world models. In International
Conference on Machine Learning, pages 8583–8592. PMLR, 2020.
Gr´ egoire Sergeant-Perthuis, Jakob Maier, Joan Bruna, and Edouard Oyallon. On
non-linear operators for geometric deep learning. In S. Koyejo, S. Mohamed,
A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural In-
formation Processing Systems , volume 35, pages 10984–10995. Curran Associates,
Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
474815daf1d4096ff78b7e4fdd2086a5-Paper-Conference.pdf.
Valerio Sperati and Gianluca Baldassarre. Bio-inspired model learning visual goals and
attention skills through contingencies and intrinsic motivations. IEEE Transactions on
Cognitive and Developmental Systems , 10(2):326–344, 2017.
Youri Timsit and Gr´ egoire Sergeant-Perthuis. Towards the idea of molecular brains.
International Journal of Molecular Sciences , 22(21), 2021. ISSN 1422-0067. doi:
10.3390/ijms222111868. URL https://www.mdpi.com/1422-0067/22/21/11868.
Youri Timsit, Gr´ egoire Sergeant-Perthuis, and Daniel Bennequin. Evolution of ribo-
somal protein network architectures. Scientific Reports , 11(1):625, January 2021.
ISSN 2045-2322. doi: 10.1038/s41598-020-80194-4. URL https://doi.org/10.1038/
s41598-020-80194-4 .
Kenneth Williford, Daniel Bennequin, Karl Friston, and David Rudrauf. The projective
consciousness model and phenomenal selfhood. Frontiers in Psychology, 9:2571, 2018.
Kenneth Williford, Daniel Bennequin, and David Rudrauf. Pre-reflective self-consciousness
& projective geometry. Review of Philosophy and Psychology , 13(2):365–396, 2022.
Fan Yang, Dung-Han Lee, John Keller, and Sebastian Scherer. Graph-based topo-
logical exploration planning in large-scale 3d environments. In 2021 IEEE Interna-
tional Conference on Robotics and Automation (ICRA) , pages 12730–12736, 2021. doi:
10.1109/ICRA48506.2021.9561830.
Appendix A. Proof of Proposition and Theorem
A.1 Proof of Proposition 1
Any 3-D affine transformation is encoded by a matrix M = (mi,j; i, j= 1..3) and a vector
(mj,4; j = 1..3); let (mR
i,j; i, j= 1..3) be the matrix associated to ϕR and (mR
4,j; j = 1..3) its
vector.
Projective case: ϕp
R = ρ ◦ ϕR is the projective map with expression in homogeneous
coordinates given by the matrix,


m1,1 m1,2 m1,3 m1,4
m2,1 m2,2 m2,3 m2,4
m3,1 m3,2 m3,3 m3,4
0 0 γ 1


By construction, the transition map in the projective case, ψp
m, is ϕp
Rm ◦ ϕp
R
−1; it is the
composition of two projective transformations, therefore it is a projective transformation.
A.2 Proof of Theorem 1
We will denote Bϵ
y the Euclidean ball of radius 1 around y ∈ R3,
i.e. Bϵ
y = {x ∈ R3| ∥x − y∥ ≤ϵ}.
Lemma 1. For any Q ∈ P(W), both in Euclidean and Projective cases, for any affine map
or projective transformation ψ : W → W,
C(ψ∗Q) = −
Z
dyQ(ψ−1(Bϵ
y)) lnQ(ψ−1(Bϵ
y)) (18)
Proof.
C(ψ∗Q) = 3
4πϵ3 ×
Z
ψ∗Q(dx1)
Z
dy 1[x1 ∈ Bϵ
y] ln 1[x1 ∈ Bϵ
y]R
ψ∗Q(dx1) 1[x1 ∈ Bϵy]
= − 3
4πϵ3
Z
dy ln Q(ψ−1(Bϵ
y)
Z
ψ∗Q(dx1) 1[x1 ∈ Bϵ
y]
= − 3
4πϵ3
Z
dyQ(ψ−1(Bϵ
y)) lnQ(ψ−1(Bϵ
y)) (19)
Proof of Theorem:
Euclidean case: for any set of moves M, and for any m ∈ M, ψm is a rotation; therefore
for any y ∈ W, ψ−1
m (Bϵ
y) = Bϵ
ψ−1
m (y). Then, for any prior Q ∈ P(W),
C(ψ∗,mQ) = − 3
4πϵ3
Z
dyQ(Bϵ
ψ−1
m (y)) lnQ(Bϵ
ψ−1
m (y))
= − 3
4πϵ3
Z
dyQ(Bϵ
y) lnQ(Bϵ
y) (20)
In this case, the epistemic value is independent from the change of Euclidean frame, and
not moving is a perfectly valid choice for the agent to maximize it, at each time step of the
exploration algorithm (Algorithm 1).
Remark 3. The fact that staying still is a valid strategy arises as the agent assumes (or
believes) that it has access to the whole configuration space of O. If it knew it had limited
access to it, through for example limited sight, we expect the agent would look around until
the object O would be in sight, and then stop moving.
Projective case:
Consider two projective transformations ψ, ψ1 : W → W, if for any y ∈ W,
ψ−1(Bϵ
y) ⊆ ψ−1
1 (Bϵ
y) (21)
then,
−Q(ψ−1(Bϵ
y)) lnQ(ψ−1(Bϵ
y)) (22)
≥ −Q(ψ−1
1 (Bϵ
y)) lnQ(ψ−1
1 (Bϵ
y)) (23)
This suggests that the moves that maximize epistemic value are those whereψ−1
m shrinks
the zone around yo = ρ(ϕR(o)), which is the representation of O in the internal world of the
agent. In particular, it means magnifying the zone around ρ(ϕRm(o)) in the agent’s new
frame, Rm, after move m. The only way to do so is to select moves that bring the agent
closer to O. Let us denote yo
m := ρ(ϕRm(o)). Let us now make the previous argument more
formal. We assume that the set of moves M is finite. Let Q0 = q0dλ be any initial prior
on W = P3(R), at stating time t = 0. After one step, move m1 is chosen and the agent
updates its prior as,
q1(x) ∼= 1[x ∈ Bϵ
yom1
]q0(x) (24)
where ∼= means proportional to. The prior we now consider is Q1 denoted simply as Q.
One shows that there is α >0, such that for all m ∈ M, and ϵ >0 small enough,
C(ψ∗,mQ) = − 3
4πϵ3 ×
Z
dy 1[y ∈ Bαϵ
yom]Q(ψ−1
m (Bϵ
y)) lnQ(ψ−1
m (Bϵ
y)) (25)
(26)
Let ≈ stand for ‘approximately equal to’ (equal at first order in expansion in powers of
ϵ). Then from the previous statement the summand can be approximated by its value in
yo
m:
C(ψ∗,mQ) ≈ −α3Q(ψ−1
m (Bϵ
yom)) lnQ(ψ−1
m (Bϵ
yom)) (27)
Furthermore, Q(ψ−1
m (Bϵ
yom)) ≈ 4πϵ3
3
q1(yo)
|det ∇ψm|(yo) , where |det ∇ψm|(yo) is the absolute
value of the Jacobian determinant of ψm at yo. The epistemic value is maximized when
|det ∇ψm|(yo) is maximized. By definition, ψm = ρ ◦ ϕRm ◦ ϕ−1
R ◦ ρ−1, therefore, by the
chain rule of differentiation
|det ∇ψm|(yo)
= |det ∇ρ|(ϕRm(o)).|det ϕRm|(o).|det ∇[ϕ−1
R ◦ ρ−1]|(yo) (28)
Let us make explicit each terms in the previous equation. ϕRm is a rigid movement
therefore, |det ϕRm|(o) = 1. |det ∇[ϕ−1
R ◦ρ−1]|(yo) does not depend on m so we can label it
as a constant C. ϕRm(o) is the coordinate of O in the Euclidean frame Rm; let us denote
(xm, ym, zm) these coordinates, i.e. ( xm, ym, zm) := ϕRm(o). Then,
|det ∇ρ|(xm, ym, zm) = 1
(γzm + 1)4 (29)
Therefore, |det ∇ψm|(yo) = C 1
(γzm+1)4 .
As we assumed that for any move m ∈ M, the object O is always in front of the agent,
then zm ≥ 0; in this case, zm is also the distance of the agent to the object. Epistemic value
is maximized when zm is minimized and therefore the agent selects moves that reduce its
distance to the object. Denote one of such move m∗; the argument then loops back with
the new reference frame Rm∗
and updated belief q ← ψm,∗q|yom.