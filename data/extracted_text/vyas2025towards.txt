Towards smart and adaptive agents for active
sensing on edge devices
Devendra Vyas*
VERSES
Nikola Pi ˇzurica*
Computer Science Center, U. of Montenegro
Nikola Milovi ´c
Fain Tech
Igor Jovanˇcevi´c
Computer Science Center, University of Montenegro
Miguel de Prado †
VERSES
Tim Verbelen†
VERSES
Abstract—TinyML has made deploying deep learning models
on low-power edge devices feasible, creating new opportunities
for real-time perception in constrained environments. However,
the adaptability of such deep learning methods remains limited to
data drift adaptation, lacking broader capabilities that account
for the environment’s underlying dynamics and inherent uncer-
tainty. Deep learning’s scaling laws, which counterbalance this
limitation by massively up-scaling data and model size, cannot
be applied when deploying on theEdge, where deep learning
limitations are further amplified as models are scaled down for
deployment on resource-constrained devices.
This paper presents an innovative agentic system capable of
performing on-device perception and planning, enabling active
sensing on the edge. By incorporating active inference into our
solution, our approach extends beyond deep learning capabilities,
allowing the system to plan in dynamic environments while
operating in real-time with a compact memory footprint of as
little as 300 MB. We showcase our proposed system by creating
and deploying a saccade agent connected to an IoT camera with
pan and tilt capabilities on an NVIDIA Jetson embedded device.
The saccade agent controls the camera’s field of view following
optimal policies derived from the active inference principles,
simulating human-like saccadic motion for surveillance and
robotics applications.
Index Terms—smart agents, edgeAI, dynamic planning
I. INTRODUCTION
The human visual system has a unique ability to focus on
key details within complex surroundings, a process known as
saccading [1]. This quick and dynamic scanning allows us
to gather essential information. Saccading is part of a larger
concept known as active (visual) sensing [2], [3], an innate
capability that enables organisms to forage for information
and dynamically adapt to an evolving environment [4].
Active sensing is critical in various applications, particularly
when the information is unavailable or too vast to process. For
instance, in remote sensing for Earth observation [5] or aerial
search-and-rescue operations [6], the system must parse vast,
detailed scenes, focusing only on critical features, e.g., ice-
sea or missing person. Similarly, in sports events, tracking
dynamic scenes requires a system to zoom in on players,
capturing players’ faces or gestures while not missing the play.
* Co-first authors. † Co-senior authors.
{devendra.vyas, miguel.deprado, tim.verbelen}@verses.ai
This work was partly supported by Horizon Europe dAIEdge under grant
No. 101120726.
Fig. 1.Conceptual Framework for Smart Edge Agents, composed of a
deep-learning perception module and an active inference planning module
for active (visual) sensing. The camera frames are processed by the object
detector, which forwards the detected results to the active inference module.
Our agent plans its next action, minimizing free energy, and dynamically
adapting to the environment.
Other areas, such as smart cities and surveillance systems [7],
demand robust monitoring solutions for crowded areas to track
movement, anticipate potential issues, and enhance safety.
Active sensing becomes even more apparent in robotics, where
the agent’s actions determine the next observations for the
system, driving exploration [8].
Recent advances in machine learning (ML), particularly in
deep learning, have substantially improved sensing accuracy
and complexity. However, state-of-the-art deep learning mod-
els show limitations in their adaptability [9], i.e., the ongoing
accumulation and refinement of knowledge over time. These
limitations are further amplified when these models are scaled
down for deployment on resource-constrained edge devices,
where memory, computational power, and energy efficiency
are limited. As a result, true active sensing—requiring both
perception and planning—remains challenging to implement
on embedded systems.
Active inference, an approach rooted in the first principles
of physics, offers a promising alternative to address these
limitations [10]. Emerging as a viable paradigm, active infer-
ence grounds learning within probabilistic principles, enabling
smart systems, or agents, to model the uncertainty and variabil-
arXiv:2501.06262v2  [cs.RO]  16 Oct 2025
ity inherent in dynamic environments, making it well-suited
for continual learning and adaptive decision-making [11]. This
shift represents a move beyond perception-focused AI toward
adaptive systems capable of adjusting their actions based on
environmental feedback. Thus, agents on the edge provide
a powerful framework for real-time perception and planning
without dependence on cloud resources, ensuring low-latency
responses and enhanced data privacy.
This work presents an integrated system that combines deep
learning and active inference to realize an adaptive, memory-
efficient, real-time saccade agent for edge devices. Our system
leverages a deep learning-based object detection module for
initial perception and an active inference planning module to
actively sense and adapt to the environment. The saccade agent
can observe, plan, and control a camera for strategic informa-
tion gathering, demonstrating adaptive decision-making and
exploration. Our deployment on an Nvidia Jetson platform
showcases the potential for responsive applications in robotics
and smart city environments, highlighting the feasibility of
edge-based adaptive systems for complex, real-world tasks.
II. RELATED WORK
We categorize the related work in three main areas:
A. Active Sensing
Active sensing is an essential building block across diverse
fields where efficient scanning is needed to locate and focus on
critical details. In Earth observation applications, the AutoICE
Challenge [5] addresses sea ice detection, where maximiz-
ing area coverage and detection through adaptive zooming
is indispensable for safe navigation. Similarly, active search
strategies are also explored in rescue operations [6], stressing
techniques like saccading to enhance search efficiency over
large areas. For public safety in smart cities, deep learning
methods are proposed for people tracking and counting [12],
enabling security, crowd management, and urban analytics
applications.
B. TinyML
TinyML has made deploying ML models on low-power
edge devices feasible, bringing opportunities for real-time
perception in constrained embedded devices. The edge de-
ployment pipeline is summarized in [13], streamlining end-
to-end model deployment on embedded platforms to enhance
the accessibility of edgeAI applications. A popular example
of this process is YOLO [14], an efficient architecture for
deploying object detection and tracking at the edge, improving
the responsiveness of embedded applications. Recent works
have made progress in enabling on-device domain adaptation,
adjusting deployed applications to account for data distribution
shifts between training and target environments [15], [16].
However, these adaptations remain limited to addressing data
drifts and lack broader capabilities for behavioral changes.
Our approach overcomes this limitation by integrating active
inference on top of a deep learning module, allowing the
system to plan and adapt to the environment accordingly.
C. Probabilistic computing
Probabilistic computing has shown promise for active sens-
ing by optimizing information acquisition in dynamic envi-
ronments. Probabilistic principles can be used to maximize
information gain through camera adjustment [17], which is
valuable in applications like surveillance, sports analysis, and
patient monitoring. This is extended to maximize mutual
information gain in multi-camera setups, combining objectives
like exploration and tracking to enable adaptive, informed
scene monitoring [18]. Unlike these methods, we base our
probabilistic agent on active inference, grounding our ap-
proach in the Free Energy Principle.
III. METHODOLOGY
To develop an effective active sensing solution, it is essential
to consider the unpredictable and dynamic nature of real-
world environments. Smart sensors must be able to handle
uncertainty and adapt to constant changes. Therefore, any
change in the observed environment must influence the policy
selection for the following action. For a system to operate
autonomously and intelligently, it must be able to adjust its
perception and actions in real time without relying on cloud
processing. This requirement for on-device adaptation supports
faster decision-making and enhances data privacy.
In this work, we propose an efficient active sensing agent
composed of two modules: i) a deep learning-based perception
module and ii) an active inference module that enables plan-
ning and control. This architecture combines deep learning’s
feature extraction performance with active inference’s Bayes-
optimal control, presenting an adaptable and scalable solution
for various resource-constrained edge applications.
A. Perception
Deep learning techniques have achieved remarkable success
in detecting features of interest in images, audio, or textual
data [19]. Convolutional neural networks (CNNs) have demon-
strated exceptional performance in visual tasks like object
detection and segmentation [20]. By employing deep learning
for perception, active sensing agents can rapidly process high-
dimensional data and identify patterns that provide relevant
spatial information.
Recent advances in transformer architectures and large
language models (LLMs) have further expanded the scope
of deep learning. Transformers excel in capturing complex
dependencies and long-range relationships in data, making
them very powerful for tasks requiring a deeper contextual
understanding. The self-attention mechanism, being a core
component of transformers, enables the models to focus selec-
tively on the most relevant aspects of the data, enhancing the
agent’s ability to perform active sensing by prioritizing critical
visual cues.
However, while deep learning and LLMs offer powerful
feature extraction, their static nature limits adaptability when
deployed in uncertain or changing environments, which can
only be counteracted by massively scaling data or model size.
Thus, to address this challenge, our approach integrates deep
learning for perception but relies on active inference as a much
lighter, sample- and parameter-efficient higher-level module,
enabling the agent to plan and dynamically adapt based on
ongoing observations in real-time.
B. Planning
Active inference builds on the Free Energy principle, a
theoretical framework stating that intelligent agents minimize
the discrepancy between their internal (generative) model of
the environment and incoming sensory data. By reducing this
discrepancy, or ”free energy”, the agent maintains an updated
and accurate representation of its surroundings, allowing the
agent to make predictions about its environment and actively
take actions to reduce uncertainty.
Concretely, the agent’s generative model is a joint proba-
bility distribution over statessand observationso. As infer-
ring hidden statessgiven some observationsois typically
intractable, an approximate posteriorQ(s|o)is introduced and
optimized by minimizing the free energyF[10]:
min
Q(s|o)
F=D KL[Q(s|o)||P(s)]| {z }
complexity
−EQ(s|o)[logP(o|s)]| {z }
accuracy
(1)
Hence, the agent strives to give the most accurate predic-
tionsP(o|s)while minimizing the complexity of the model
with respect to the priorP(s). To select actions or policiesπ
for the futureτ, an active inference agent will evaluate and
minimize the expected free energyG[10]:
G(π) =EQ(oτ |π)

DKL[Q(sτ |oτ , π)||Q(sτ |π)]

| {z }
(negative) information gain
−E Q(oτ |π)

logP(o τ |C)

| {z }
expected utility
(2)
The agent now averages across expected future outcomeso τ
and balances the expected information gain (i.e., exploration)
with the expected utility (i.e., reward) encoded in prior pref-
erencesC. Both the observationsoand hidden statesscan be
modeled as discrete variables with Categorical distributions,
whereas optimizingFandGcan be done using tractable
update rules [11]. Therefore, we convert the outputs of the
deep learning perception module into a discrete observation
space and use active inference for the action selection of our
agent.
IV. SMART EDGE AGENT
Our smart edge agent, the saccade agent, comprises two
modules, as introduced in the previous section, combining
deep learning perception capabilities with active inference
planning, as shown in Fig. 1. Specifically, we employ i) a
deep-learning object detection model, offering efficient object
and human detection capabilities directly at the edge, and ii) an
active inference module that enables adaptive motion control
(pan and tilt), allowing the agentic system to dynamically
adjust its field of view or track detected entities autonomously.
This adaptive behavior enhances the camera’s utility as an
intelligent surveillance IoT tool or for scene exploration and
information foraging in robotic applications.
A. Object detection using YOLOv10
We chose YOLOv10 [21] from the YOLO family for our
perception module due to its strong balance of detection accu-
racy and computational efficiency. YOLO models are single-
pass object detectors that predict object categories and loca-
tions, making them ideal for real-time applications. YOLOv10
consistently demonstrates state-of-the-art performance and re-
duced latency across various model scales (N/S/M/L/X). We
employ the Nano variant, with 2.3 million parameters, as
it offers an efficient trade-off between accuracy, speed, and
memory for efficient edge deployment.
To deploy the YOLOv10n network as efficiently as possible,
we export it to ONNX [22]. ONNX has become a standard
for neural network representation and exchange and is widely
supported by hardware vendors’ software stacks, i.e., inference
engines. This positions ONNX as a strong candidate for
deployment space exploration and optimizations on a range
of embedded devices. Thus, we create an edge-deployment
workflow to find the most suitable deployment for YOLOv10n.
The edge-deployment workflow contains several edge-
oriented inference engines, namely ONNX-runtime [23],
TFlite [24], and TensorRT [25] that can input an ONNX
model and generate an optimized implementation for a given
target hardware platform. These frameworks apply several
optimizations across the network’s graph, e.g., operator fusion,
quantization, on the software stack, e.g., algorithm optimiza-
tion, and leverage parallel hardware acceleration, vectoriza-
tion, and optimized memory scheduling. The process results
in a bespoke network description and a runtime that is ready
for deployment on the hardware platform.
B. Planning using Active Inference
To enable an efficient saccade agent with active inference
planning, we define a discrete action, observation, and hidden
state space. To this end, we divide the full area the camera can
pan and tilt into a discrete grid ofK×Lblocks, see Fig. 2.
Given a particular fixation point, the camera’s field of view
will only spanW×Hblocks, highlighted in blue. For each
block, at each timestep, an observationo w,h is a Categorical
distribution with three bins, i.e., the block can have no object
detected (0 - blue), an object detected (1 - red) or not visible
(2 - gray). The confidence of the bounding box outputs of the
object detection neural network provides the probability of
an object being detected. As state space, we similarly have a
state variables k,l per block, which is Bernoulli, i.e., object not
present (0) or present (1). In addition, we also equip the agent
with a proprioceptive states p and observation, i.e., it observes
the fixation point it is currently looking at. We specify the
likelihood mappingAwhich predicts observationo w,h given
states k,l and fixation points p:
Fig. 2.Action space:We discretize the action space intoK×Lfixation
points. Given a fixation point, the field of view of the camera spansW×H
blocks (in blue). Object detections are translated into discrete bins (in red).
Aw,h,k,l,p =



0ifs k,l = 0, k→p w, l→p h
1ifs k,l = 1, k→p w, l→p h
2otherwise
(3)
wherek→ p wmeans “blockkmaps to observation
wgiven the agent looks at fixation pointp”. We currently
also assume that objects don’t move considerably between
timesteps and use the previous timestep posterior as the current
prior, i.e.,P(s t) =Q(s t−1|ot−1), but we could expand this
with dynamics modeling of the objects as future work.
The saccade agent uses the object detections received to
perform inference, updating its beliefs about the hidden states.
For example: Detecting a “person” with high confidence would
increase the probability assigned to the hidden state “person
present” corresponding to the particular block. The absence of
detection, on the other hand, would decrease the probability
of that object being present.
After each observation, the agent evaluates possible actions,
i.e., the next fixation points, based on their predicted outcomes.
Actions are chosen to minimize expected free energy, which
combines two key factors:
1)Expected observations matching prior preferences:
This essentially means choosing the most likely actions
expected to lead to desired outcomes. For example,
if the agent’s goal is to locate a specific object, we
set a preferenceC w,h = 1, which favors actions that
increase the probability of detecting that object in the
field of view. Similarly, if we only setC cw,ch = 1, with
(cw, ch)the center coordinates of the camera, this yields
a “tracking” agent that pans/tilts to keep the object of
interest in the center.
2)Epistemic value:The agent also aims to reduce un-
certainty about the hidden states. Actions expected to
provide more informative observations about the envi-
ronment have higher epistemic value (i.e., information
gain in eq. 2). In our model, moving the field of view
to previously unobserved blocks will result in a high
amount of information gain. Hence, in the absence of
objects of interest, the camera will pan and tilt to cover
the whole area with as few moves as possible.
We provide a qualitative demonstration of our saccade agent
at [26].
Fig. 3.Applications:Our agentic system enables active sensing solutions
for edge robotics and surveillance IoT cameras. On the left, the Tapo IoT
camera [27] used for surveillance applications. On the right, the Locobot
robot WX250 [28] for information gathering and scene discovery.
V. EXPERIMENTAL RESULTS
A. Experimental Set up
Our experimental setup comprises the following compo-
nents, as shown in Fig. 3 :
1)Tapo Camera:We use an IoT Tapo camera equipped
with pan and tilt capabilities, which serves both as the
input for the observations, i.e., images/frames, forwarded
to the object detector, and the actuator for our saccade
agent. The agent commands the camera to perform dy-
namic adjustments in its field of view based on optimal
policies derived by minimizing free energy, simulating a
human-like saccadic motion to maintain focus on areas
of interest.
2)Locobot WX250:We also deploy our agent on the Lo-
cobot WX250, a mobile robot platform for autonomous
mobility. Equipped with a 6-DOF manipulator and
pan/tilt camera, the Locobot enables active exploration
of the environment. This capability complements the
Tapo camera’s pan and tilt actions with navigation. The
robot’s mobility enhances the agent’s capacity for active
sensing in new environments, allowing it to reposition
itself and collect additional perspectives to refine per-
ception in complex settings.
3)Nvidia Jetson Orin NX:Our setup leverages the Nvidia
Jetson Orin NX. Equipped with an 8-core ARM Cortex-
A78 CPU and a 1024-core Ampere GPU, the Jetson Orin
NX represents a powerful edge AI platform designed for
accelerated machine learning tasks with up to 100 TOPs
of processing capability and 16 GB of memory, running
at 25W.
A pre-trained YOLOv10n model on the COCO dataset [29]
is deployed on the Nvidia Jetson Orin NX for real-time
detection. The active inference module receives bounding
boxes as observations and returns the optimal pan and tilt
actions for the IoT or robotic camera. Both perception and
planning modules employ the edge-deployment workflow to
find the most suitable deployment engine, yielding the highest
performance.
B. Results
Next, we summarize our deployment experimental results:
Fig. 4.Perception module performance:Optimization achieved by export-
ing the YOLOv10n Torch model from Ultralytics to ONNX and compiling it
with different deployment inference engines on the Nvidia Jetson Orin NX’s
CPU and GPU.
1) Perception:We optimized the YOLOv10n model, as
introduced in Section IV, by exporting the original Torch
model from the originalUltralitycslibrary to ONNX. Then,
the model gets compiled with one of the various deployment
inference engines. Figure 4 depicts the average results of 1000-
inference runs, with one warm-up sample, while deploying the
different implementations on the Nvidia Jetson’s CPU, single-
and multi-core, and GPU.
When evaluating the various inference engines using single-
floating-point precision (FP32) operations on a single-core
CPU, Ultralitics comes out as the slowest inference, with
over 600 ms. When the model is exported to ONNX and
compiled with ONNX-runtime (ORT) and TensorFlow Lite
(TFLite), the latency decreases considerably by 23% 35%,
respectively. We obtain higher gains when parallelizing across
all available 8 CPU cores, achieving 427 ms when using
Ultralytics. The performance can be notably increased if we
deploy with ORT (intra-operator parallelization) and TFLite,
with a 3.75x and 4.5x speed-up over Ultralytics, respectively.
Finally, when offloading inference to the GPU, we see a boost
in performance for all GPU-available frameworks by leverag-
ing the native parallel compute units of the device, achieving
an inference in under 45 ms. When using TensorRT (TRT),
designed explicitly for Nvidia Jetson devices, we accomplish
an optimized deployment through reduced numerical precision
(FP16), with as little as 22 ms, i.e., 45 FPS.
Figure 5 shows the memory profile of the YOLOv10n
deployments with the various inference engines. It can be
observed that while TFLite provides the fastest deployment
on the single-CPU setup, it also consumes the most memory,
with over 50% more than Ultralytics. On the other hand,
ORT provides a good balance between latency and memory
utilization, being slightly slower than TFLite, but achieving
inference with as little as 168 MB. This pattern is also
repeated in the multi-CPU deployment setup. On the GPU
side. Ultralytics, being a training-oriented framework, is the
Fig. 5.Perception module memory profileof the YoloV10n when executed
with different deployment inference engines on the Nvidia Jetson Orin NX’s
CPU and GPU.
one that consumes the most memory, with up to 1.2 GB. As
we discussed earlier, TRT is designed explicitly for Nvidia
Jetson devices. As such, it accomplishes a very performant
deployment with an optimized memory usage under 400 MB.
Overall, the results demonstrate that the predominant con-
tribution to overall latency stems from the neural network
inference stage. In contrast, the pre-processing phase exhibits
minimal computational overhead, and the post-processing
step remains negligible — a characteristic intrinsic to the
YOLOv10 architecture. Moreover, the comparative analysis
across inference engines and hardware configurations re-
veals distinct performance and memory usage behaviors. For
instance, TFLite achieves favorable latency on single-core
execution, although with the highest memory usage, while
ORT offers an excellent performance-memory tradeoff. On
the GPU side, TRT is the absolute winner with excellent
latency-memory performance. These observations highlight
the importance of a flexible edge-deployment workflow that
systematically identifies the most suitable software–hardware
pairing for a given operational context.
2) Planning:Next, we deploy the active inference planning
module, which only contains 1.34K parameters, on the Nvidia
Jetson NX. The original active inference model was designed
with thePymdplibrary [30], which contains a JAX backend.
We then follow a similar workflow as with YOLOv10n and
export the active inference model to ONNX and deploy it with
several inference engines.
Figure 6 depicts the latency of the active inference model
when planning the following position to look at. The pre-
processing step accounts for decoding and mapping the object
detection bounding boxes to the generative model. The infer-
ence part involves theinfer statesmethod, which updates the
agent’s beliefs about the hidden states given the observations,
and theinfer policiesmethod, which defines the set of policies
from which the agent will pick the best action. When deployed
on a single CPU, the planning process executes fast, achieving
6 ms using JAX and 4 ms (250 FPS) when compiled with
Fig. 6.Planning module performance: Performance optimization by
exporting the active inference model (saccade agent) from JAX to ONNX
and compiling it with different deployment inference engines on the Nvidia
Jetson Orin NX’s CPU and GPU.
TFLite. The saccade agent does not benefit from parallel pro-
cessing, showing similar or worse performance when deployed
on multiple CPUs and considerably worse when offloaded
to the GPU. We argue this is due to the small size of the
agent, paying a high penalty for thread synchronization, which
dominates over the benefit from parallel computation.
Figure 7 illustrates a trend consistent with that observed for
the YOLOv10n model, revealing a trade-off between inference
performance and memory consumption. For example, while
TFLite delivers the fastest execution, it may consume up
to four times more memory than ORT. This behavior may
be related to internal optimization mechanisms in TFLite,
potentially involving intermediate data caching or buffer reuse
strategies to reduce computation time.
Overall, these results highlight the lightweight deployment
of our active inference model, achieving real-time planning
and adaptation to the environment for IoT and robotics sac-
cading applications.
3) System:Integrating the perception and planning modules
produces a highly efficient saccade agent capable of real-time
operation with only 2.3 million parameters. Table 1.1 sum-
marizes key configurations, illustrating the trade-offs between
latency and memory consumption. For example, in config-
urationA, the perception module optimized with TensorRT-
FP16 achieves a throughput of 45 FPS, enabling rapid feature
extraction. In parallel, the active inference module using
TFLite can perform planning and decision-making at up to
250 FPS, with a total memory footprint under 1 GB. The
decision-making rate can be reduced to 108 FPS using ORT,
lowering memory usage to 533 MB, as shown in configuration
B. Finally, configurationCemploys the ORT (intra-operator
parallelization) for perception, decreasing the perception rate
to 9 FPS while providing a compact memory footprint of
304 MB, including all required libraries and runtimes. This
configuration effectively balances perception and decision-
making’s latency and memory, enabling dynamic adaptation
and efficient operation on resource-constrained edge devices.
Fig. 7.Planning module memory profileof the active inference model
when executed with different deployment inference engines on the Nvidia
Jetson Orin NX CPU and GPU.
# Params (M) Memory (GB) Perception
Rate (Hz)
Decision-making
Rate (Hz)
A 2.3 947 45250
B 2.3 53345108
C 2.33049 108
TABLE I
SYSTEM PARAMETERS: SACCADE AGENT DEPLOYED ON THENVIDIA
JETSONORINNX
C. Discussion
These results highlight the effectiveness of combining deep
learning for perception with active inference for planning in
edge-based intelligent agents. With only 1.34K parameters,
the active inference module achieves real-time planning while
maintaining minimal computational and memory demands.
In contrast to large-scale foundation models that rely on
billions of parameters to achieve general-purpose intelligence,
our domain-oriented approach demonstrates how compact,
task-specific active inference models can complement deep
learning perception. The adaptability of our edge-deployment
workflow illustrates deployment interoperability, facilitating
the exploration of diverse configurations to achieve a suitable
balance between latency, accuracy, and resource efficiency.
VI. CONCLUSIONS ANDFUTUREWORK
This work has introduced a smart edge agent composed of a
deep learning-based perception module and an active inference
planning module for active sensing. The system demonstrates
the feasibility of adaptable, on-device surveillance and robotic
solutions and their potential to handle real-world challenges
effectively. Our study highlights the potential of active infer-
ence in edge-based systems. Active inference offers a robust
framework that accounts for the environment’s underlying
dynamics and inherent uncertainty. It allows the agent to
actively reduce ambiguity or explore the environment for
new information. This approach establishes a foundation for
efficient edge agents for adaptive, resource-efficient decision-
making in IoT and edge computing applications.
In future work, we aim to compare the proposed system
against other state-of-the-art works, showing a quantitative
functional comparison on popular challenges, such as the
Habitat Navigation Challenge. Finally, we envision extending
this concept to other applications, which could unlock a new
generation of adaptive edge devices capable of context-driven
interactions in complex environments.
REFERENCES
[1] S. Grossberg, K. Roberts, M. Aguilar, and D. Bullock, “A neural model
of multimodal adaptive saccadic eye movement control by superior
colliculus,”Journal of Neuroscience, vol. 17, no. 24, pp. 9706–9725,
1997.
[2] M. Ferro, D. Ognibene, G. Pezzulo, and V . Pirrelli, “Reading as
active sensing: a computational model of gaze planning during word
recognition,”Frontiers in Neurorobotics, vol. 4, p. 1262, 2010.
[3] T. Parr, N. Sajid, L. Da Costa, M. B. Mirza, and K. J. Friston, “Gener-
ative models for active vision,”Frontiers in Neurorobotics, vol. 15, p.
651432, 2021.
[4] M. B. Mirza, R. A. Adams, C. D. Mathys, and K. J. Friston, “Scene
construction, visual foraging, and active inference,”Frontiers in compu-
tational neuroscience, vol. 10, p. 56, 2016.
[5] A. Stokholm, J. Buus-Hinkler, T. Wulf, A. Korosov, R. Saldo, L. T.
Pedersen, D. Arthurs, I. Dragan, I. Modica, J. Pedroet al., “The autoice
challenge,”The Cryosphere, vol. 18, no. 8, pp. 3471–3494, 2024.
[6] S. McClanahan, “Object recognition and detection: Potential implica-
tions from vision science for wilderness searching,”J Search Rescue,
vol. 5, no. 1, pp. 1–17, 2021.
[7] M. Kashef, A. Visvizi, and O. Troisi, “Smart city as a smart service sys-
tem: Human-computer interaction and smart city surveillance systems,”
Computers in Human Behavior, vol. 124, p. 106923, 2021.
[8] L. Da Costa, P. Lanillos, N. Sajid, K. Friston, and S. Khan, “How Active
Inference Could Help Revolutionise Robotics,”Entropy, vol. 24, no. 3,
p. 361, Mar. 2022.
[9] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, “Continual
lifelong learning with neural networks: A review,”Neural networks, vol.
113, pp. 54–71, 2019.
[10] T. Parr, G. Pezzulo, and K. J. Friston,Active Inference: The Free Energy
Principle in Mind, Brain, and Behavior. The MIT Press, 03 2022.
[Online]. Available: https://doi.org/10.7551/mitpress/12441.001.0001
[11] K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, G. Pezzuloet al.,
“Active inference and learning,”Neuroscience & Biobehavioral Reviews,
vol. 68, pp. 862–879, 2016.
[12] N. Krishnachaithanya, G. Singh, S. Sharma, R. Dinesh, S. R. Sihag,
K. Solanki, A. Agarwal, M. Rana, and U. Makkar, “People counting in
public spaces using deep learning-based object detection and tracking
techniques,” in2023 International Conference on Computational Intel-
ligence and Sustainable Engineering Solutions (CISES). IEEE, 2023,
pp. 784–788.
[13] M. D. Prado, J. Su, R. Saeed, L. Keller, N. Vallez, A. Anderson,
D. Gregg, L. Benini, T. Llewellynn, N. Ouerhaniet al., “Bonseyes ai
pipeline—bringing ai to you: End-to-end integration of data, algorithms,
and deployment tools,”ACM Transactions on Internet of Things, vol. 1,
no. 4, pp. 1–25, 2020.
[14] J. Redmon, “You only look once: Unified, real-time object detection,”
inProceedings of the IEEE conference on computer vision and pattern
recognition, 2016.
[15] Q. Zhang, R. Han, C. H. Liu, G. Wang, and L. Y . Chen, “Elasticdnn: On-
device neural network remodeling for adapting evolving vision domains
at edge,”IEEE Transactions on Computers, 2024.
[16] C. Cioflan, L. Cavigelli, M. Rusci, M. de Prado, and L. Benini, “On-
device domain learning for keyword spotting on low-power extreme edge
embedded systems,”arXiv preprint arXiv:2403.10549, 2024.
[17] E. Sommerlade and I. Reid, “Information-theoretic active scene ex-
ploration,” in2008 IEEE Conference on Computer Vision and Pattern
Recognition. IEEE, 2008, pp. 1–7.
[18] E. Sommerlade and I. Reid, “Probabilistic surveillance with multiple
active cameras,” in2010 IEEE International Conference on Robotics
and Automation. IEEE, 2010, pp. 440–445.
[19] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,”nature, vol. 521,
no. 7553, pp. 436–444, 2015.
[20] Z.-Q. Zhao, P. Zheng, S.-T. Xu, and X. Wu, “Object detection with
deep learning: A review,”IEEE Transactions on Neural Networks and
Learning Systems, vol. 30, no. 11, pp. 3212–3232, 2019.
[21] A. Wang, H. Chen, L. Liu, K. Chen, Z. Lin, J. Han, and G. Ding,
“Yolov10: Real-time end-to-end object detection,”arXiv preprint
arXiv:2405.14458, 2024.
[22] J. Bai, F. Lu, K. Zhanget al., “Onnx: Open neural network exchange,”
https://github.com/onnx/onnx, 2019.
[23] O. R. developers, “Onnx runtime,” https://onnxruntime.ai/, 2021, ver-
sion: x.y.z.
[24] R. David, J. Duke, A. Jain, V . Janapa Reddi, N. Jeffries, J. Li, N. Kreeger,
I. Nappier, M. Natraj, T. Wanget al., “Tensorflow lite micro: Embedded
machine learning for tinyml systems,”Proceedings of Machine Learning
and Systems, vol. 3, pp. 800–811, 2021.
[25] E. Jeong, J. Kim, and S. Ha, “Tensorrt-based framework and optimiza-
tion methodology for deep learning inference on jetson boards,”ACM
Transactions on Embedded Computing Systems (TECS), vol. 21, no. 5,
pp. 1–26, 2022.
[26] “Saccade demonstration,” https://drive.google.com/drive/folders/1VCN8MAMnsdbj-
xY5qudjn7vLFlB5mSfi?usp=sharing, accessed: 2025-10-16.
[27] “Tapo surveillance camera,” https://www.tapo.com/us/product/smart-
camera/tapo-c210/, accessed: 2024-11-12.
[28] “Locobot robot,” http://www.locobot.org/, accessed: 2024-11-12.
[29] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll ´ar, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” inComputer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13.
Springer, 2014, pp. 740–755.
[30] C. Heins, B. Millidge, D. Demekas, B. Klein, K. Friston, I. Couzin, and
A. Tschantz, “pymdp: A python library for active inference in discrete
state spaces,”arXiv preprint arXiv:2201.03904, 2022.