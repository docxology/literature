Toward Ownership Understanding of Objects: Active Question Generation
with Large Language Model and Probabilistic Generative Model
Saki Hashimoto1‚Ä† , Shoichi Hasegawa1, Tomochika Ishikawa1, Akira Taniguchi2,
Yoshinobu Hagiwara3,4, Lotfi El Hafi4, and Tadahiro Taniguchi4,5
1Graduate School of Information Science and Engineering, Ritsumeikan University, Osaka, Japan
(E-mail: { hashimoto.saki, hasegawa.shoichi, ishikawa.tomochika } @em.ci.ritsumei.ac.jp)
2College of Information Science and Engineering, Ritsumeikan University, Osaka, Japan
(E-mail: a.taniguchi@em.ci.ritsumei.ac.jp)
3Faculty of Science and Engineering, Soka University, Tokyo, Japan
(E-mail: hagiwara@soka.ac.jp)
4Research Organization of Science and Technology, Ritsumeikan University, Shiga, Japan
(E-mail: lotfi.elhafi@em.ci.ritsumei.ac.jp)
5Graduate School of Informatics, Kyoto University, Kyoto, Japan
(E-mail: taniguchi@i.kyoto-u.ac.jp)
Abstract:Robots operating in domestic and office environments must understand object ownership to correctly execute
instructions such as ‚ÄúBring me my cup.‚Äù However, ownership cannot be reliably inferred from visual features alone. To
address this gap, we propose Active Ownership Learning (ActOwL), a framework that enables robots to actively generate and
ask ownership-related questions to users. ActOwL employs a probabilistic generative model to select questions that maximize
information gain, thereby acquiring ownership knowledge efficiently to improve learning efficiency. Additionally, by leveraging
commonsense knowledge from Large Language Models (LLM), objects are pre-classified as either shared or owned, and only
owned objects are targeted for questioning. Through experiments in a simulated home environment and a real-world laboratory
setting, ActOwL achieved significantly higher ownership clustering accuracy with fewer questions than baseline methods. These
findings demonstrate the effectiveness of combining active inference with LLM-guided commonsense reasoning, advancing the
capability of robots to acquire ownership knowledge for practical and socially appropriate task execution.
Keywords:Object Ownership Inference, Active Inference, Information Gain, Large Language Models, Question Generation,
Probabilistic Generative Models
1. INTRODUCTION
Robots operating in daily life environments must under-
stand object ownership to carry out instructions naturally
given by users, such as ‚ÄúBring me my cup.‚Äù Without own-
ership knowledge, a robot cannot determine which object is
being referred to when multiple similar objects exist. This
problem is especially evident in kitchens, offices, or labora-
tories, where objects with similar appearances may belong
to different individuals. Relying solely on perceptual fea-
tures such as location or appearance is insufficient because
ownership is inherently context-dependent and often deter-
mined by social conventions. Therefore, enabling robots to
acquireownershipknowledgeisacrucialsteptowardsocially
appropriate human-robot interaction.
Toenablerobotstolearnobjectownershipindailylifeen-
vironments,itisessentialtoimplementaquestion-generation
mechanism that efficiently acquires necessary information.
However, in real-world environments with large numbers of
objects, this is impractical and imposes a heavy burden on
users. Although robots can explore the environment to col-
lect visual features of objects, it remains difficult to obtain
ownership knowledge because it depends on users and con-
text. Therefore,allowingrobotstoaskquestionsbasedonthe
current situation enables them to acquire ownership knowl-
‚Ä†Saki Hashimoto is the presenter of this paper.
Owner 1
Owner 2
Shared
Owner 1
Owner 2
Shared
Owner 1
Owner 2
Shared
Bring me 
my cup.
Which one is owned 
by the Bob?
Bob
Whose cup is this?
It‚Äôs Mine.
(a) (b)
(c)
Fig. 1. Overview of this study: (a) Without ownership
knowledge, the robot cannot follow instructions contain-
ing owner names. (b) The robot generates questions for
the user, and (c) The probabilistic generative model is
updated to predict object ownership based on the user‚Äôs
answers, enabling ownership learning
edge through interaction with users.
Efficiency can be improved by first distinguishing shared
from owned objects. In real-world environments, many ob-
jects, such as tissue boxes, are typically shared rather than
arXiv:2509.12754v1  [cs.RO]  16 Sep 2025
individuallyowned. Askingaboutsuchobjectsincreasescog-
nitiveloadandlowerslearningefficiency. Bypre-classifying
objectsassharedorownedandexcludingsharedobjectsfrom
questioning, the robot can minimize interactions and acquire
ownership knowledge more effectively.
In learning ownership knowledge, it is important for the
robot to integrate perceptual features‚Äîsuch as object loca-
tion and visual attributes‚Äîwith ownership labels provided
by users, and to store this information as structured, linked
knowledge. In this study, we hypothesize that objects owned
by the same person tend to be placed in close proximity and
sharesimilarattributes. Basedonthishypothesis,weassume
thattherobotcanachievemoreaccurateownershipestimation
by associating spatial and visual features with owner identi-
ties. Forexample,byassociatingspatialandvisualcharacter-
istics(e.g.,‚Äúasmall,round,browncuplocatedatcoordinates
(1,2)‚Äù)withthecorrespondingowner(e.g.,‚ÄúownedbyTaro‚Äù),
therobotcanaccuratelylearnobjectownershiprelationships.
Our challenge in this study is to improve the efficiency of
learning object ownership knowledge in daily life environ-
ments through robot-generated questions. To overcome this
challenge, this study introduces Active Ownership Learn-
ing (ActOwL), a framework that enables robots to efficiently
learn ownership knowledge through active question genera-
tion. AnoverviewofthisstudyisshowninFig.2. ActOwLin-
tegrates a probabilistic generative model with commonsense
reasoningfromLargeLanguageModels(LLM).First,objects
are pre-classified as shared or owned based on LLM-guided
commonsenseknowledge,allowingtherobottoavoidasking
questions about shared objects. Then, for candidate-owned
objects, ActOwL strategically selects the most informative
question by maximizing information gain (IG) and generates
human-like questions via LLM. By incorporating the princi-
ple of active exploration into the question generation process
andselectingobjectsthatmaximizeIG[1‚Äì3],ActOwLeffec-
tively reduces model uncertainty during ownership learning.
The user‚Äôs answers are subsequently incorporated into the
probabilistic generative model, enabling the robot to incre-
mentally refine its ownership knowledge.
ToevaluatetheeffectivenessoftheActOwL,weconducted
experiments in simulated and real-world environments that
mimic home and laboratory settings. In addition, we quan-
titatively compared ownership clustering accuracy and the
number of questions required, and further performed abla-
tion studies on the probabilistic generative model to analyze
the impact of multimodal attributes on learning efficiency.
The main contributions of this study are as follows:
1. We show a probabilistic generative model that integrates
object location, attribute information, and user answers to
learn ownership distributions, demonstrating that combining
these multimodal inputs is effective for acquiring ownership
knowledge.
2. We show an active object selection method based on IG,
andclarifyhowtheselectionstrategyinfluencestheefficiency
of ownership learning.
3. We show through comparative experiments with baseline
andablationmethodsinbothsimulationandreal-worldenvi-
ronments that the ActOwL achieves higher ownership clus-
tering accuracy with fewer questions.
Thestructureofthispaperisasfollows. Section2clarifies
the problem setting and key challenges of this study. Sec-
tion 3 reviews related work. Section 4 describes the details
of the ActOwL. Sections 5, 6, and 7 present experimental
results that evaluate ownership learning from the perspec-
tives of clustering accuracy, quantified by ARI, and learning
efficiency,assessedbythenumberofquestionsrequired. Sec-
tion8presentsthelimitationsofthisstudy. Finally,Section9
concludes the paper and discusses future directions.
2. PROBLEM STATEMENT
This study focuses on the following two key approaches:
1. Improving ownership learning by constructing a proba-
bilistic generative model that integrates object location, at-
tributes, and user answers.
2. Enhancing ownership learning efficiency by actively se-
lecting objects for questioning based on IG.
2.1. Challenges in Handling ownership knowledge
For robots to behave appropriately in daily life environ-
ments, they must accurately understand object ownership.
However,learningownershipknowledgebasedsolelyonspa-
tial location or visual attributes has inherent limitations. For
instance, in real-world settings, even objects owned by the
same person may be placed in different locations due to their
intended use or storage convenience, leading to inconsisten-
cies in spatial placement. As such, observable information
such as location and appearance alone is often insufficient
to uniquely identify the owner of an object. Therefore, in
additiontoexploringtheenvironment,robotsmustengagein
interaction with users to obtain supplementary information,
thereby accurately acquiring ownership knowledge.
2.2. Challengesin AcquiringInformation throughQues-
tioning
To learn object ownership in daily life environments, it is
effective for a robot to obtain necessary information through
interaction with users. However, asking users to manually
provideownershipknowledgeforeveryobjectintheenviron-
ment imposes a significant burden and is impractical in real-
worldsettings. Moreover,iftherobotgeneratesquestionsran-
domly, the efficiency of learning ownership knowledge may
be substantially reduced. Therefore, a strategic approach is
required, in which the robot selectively asks questions about
objects that are likely to yield informative answers, while
minimizing the number of interactions.
3. RELATED WORK
3.1. Learning Object Ownership
Understanding object ownership is essential for assistive
robots,andseveralapproacheshavebeenproposedtoaddress
this challenge. Prior work has estimated ownership from ob-
ject attributes, spatial features, or user interactions [4‚Äì6].
While these methods provide useful cues, they typically lack
integration of multimodal information and do not address
efficientquestiongeneration,commonsensereasoning,orau-
tonomous decision-making in real-world environments.
In contrast, our approach enables robots to autonomously
acquireownershipknowledgethroughinteractionwithusers.
By leveraging commonsense knowledge from LLM and a
probabilisticgenerativemodelthatintegratesobjectlocation,
attributes, and user answers, our method selects informative
questionsefficiently. Thisallowsrobotstominimizeuserbur-
denwhileacquiringownershipknowledgedirectlyapplicable
to task execution in daily environments.
3.2. Knowledge Acquisition through Active Exploration
Beyond passive observation, robots must actively explore
their environments to reduce uncertainty and acquire useful
knowledge under real-world constraints. Prior studies have
introduced IG-based exploration strategies, including multi-
modalperceptionwiththeMultimodalHierarchicalDirichlet
Process (MHDP) [7,8] and spatial concept learning frame-
works [2,3]. These approaches integrate multimodal infor-
mation and exploit IG to guide efficient exploration, yet they
primarily aim to understand the environment from observ-
able cues such as appearance, sound, and location. However,
they do not consider ownership relations between users and
objects, which are crucial for executing user-specific com-
mands.
In contrast, our study incorporates the uncertainty reduc-
tion principle of active exploration into the question genera-
tionprocess,applyingIG-basedselectiontoidentifythemost
informative objects. By combining this with a probabilistic
generative model that integrates object position, attributes,
and user answers, our approach enables robots to minimize
user burden while acquiring ownership knowledge directly
applicable to task execution.
3.3. Resolving Uncertainty during Task Execution
Recent work has explored methods for resolving uncer-
tainty in task execution from natural language instructions.
Systemspromptclarificationquestionstodisambiguatecom-
mands [9,10], while other approaches combine perceptual
cues,contextualreasoning,orlanguagemodelstoidentifytar-
getsunderuncertainty[11‚Äì15]. Anotherlineofworkfurther
incorporatesuserpreferencesthroughactivequestioning[16].
These methods demonstrate that robots can flexibly address
ambiguity through interaction and inference, enabling robust
task execution in complex environments.
However,ownershiprelationsaretypicallytreatedastem-
poraryandtask-specific,withoutbeingstructuredorretained
as reusable knowledge. As a result, repeated instructions re-
quirethere-acquisitionofthesameinformation,andsequen-
tialquestioningoftenincreasestheuser‚Äôsburden. Incontrast,
our study enables robots to autonomously acquire and retain
structured ownership knowledge by integrating object loca-
tion,attributes,anduseranswersinaprobabilisticgenerative
model. This allows efficient interpretation of ambiguous in-
structions while minimizing redundant user interactions.
4. PROPOSED METHOD
Inthisstudy,weproposeActiveOwnershipLearning(Ac-
tOwL),amethodthatenablesrobotstoefficientlylearnobject
ownershipindailylifeenvironments. ActOwLselectsobjects
with the highest IG and asks targeted ownership questions,
reducing the need for extensive manual instruction. Owner-
shipknowledgeisthenacquiredatthecategorylevelthrough
a probabilistic generative model that integrates multimodal
information, including object location, attributes, and user
answers.
An overview of the proposed method is shown in Fig. 2.
The procedure consists of the following steps:
1. Trainaprobabilisticgenerativemodelofownershipusing
thepositionandattributeinformationofeachobjectobtained
through prior exploration (Fig. 2(a)).
2. Classify each object as owned or shared based on com-
monsense knowledge from the LLM (Fig. 2(b)).
3. Treattheclassificationresultsaspseudo-useranswersand
use them to update the generative model (Fig. 2(c)).
4. Forobjectspredictedasowned,computetheIGregarding
ownership and identify the object expected to reduce uncer-
tainty most effectively (Fig. 2(d)).
5. Generateanaturalquestionabouttheselectedobjectusing
the LLM and ask the user (Fig. 2(d)).
6. Update the generative model with the user‚Äôs answer
(Fig. 2(c)).
7. RecalculateIGbasedontheupdatedmodelandselectthe
next object for questioning (Fig. 2(d)).
These steps are repeated until the ownership of all objects
is identified. Details of the prior environmental exploration
are described in Section 4.1, the classification of objects as
sharedorownedinSection4.2,thelearningofownershipdis-
tributionsinSection4.3,andtheobjectselectionandquestion
generation process in Section 4.4.
4.1. Pre-exploration of Objects in the Environment
Inthisstudy,weassumethattherobothaspriorknowledge
of the spatial layout and categories of objects in the environ-
ment to efficiently learn ownership knowledge. The robot is
provided with an environment map and object-related data,
including class names, 2D coordinates, visual attributes, and
a list of candidate owners. Object positions are represented
as coordinates on the robot‚Äôs 2D map, and visual attributes
are encoded as feature vectors that represent the appearance.
Such representations can be obtained using vision‚Äìlanguage
models, such as CLIP [17], while spatial and attribute in-
formation can be embedded through semantic mapping tech-
niques, such as NLMap [18]. The candidate owner list spec-
ifies potential owners with concrete user names. Using this
information,therobotstrategicallygeneratesquestionsbased
on spatial configuration and inter-object relationships to effi-
ciently acquire ownership knowledge.
4.2. Classification of Objects as Owned or Shared using
Commonsense Knowledge from LLM
Toimprovetheefficiencyofownershiplearning,weintro-
duce a filtering mechanism that pre-selects objects for ques-
tioning. Objects are classified as either owned or shared
(a) Pre-exploration of Objects in the Environment
(c) Learning a Probabilistic Model of Object Ownership (d) Acquiring ownership knowledge 
via User-Directed Question Generation
(b) Classification of Objects as Ownedor Shared 
using Commonsense Knowledge from LLM
Object List
„ÉªClass name
„ÉªPosition ùë•ùëõ
„ÉªAttributes ùëúùëõ Classify 
prompt
Class name
„Éª Phone
„Éª Bottle
„Éª Cup
Shared Object List
„Éª Phone
Owned Object List
„Éª Bottle
„Éª Cup
Whose red cup is it?
Mine
Estimate the latent 
variables in the 
owner‚Äòs index ùê∂ùëõ
Calculate IG
User‚Äôs 
answer ùë§ùëõ
ùê∂1 ùê∂2 ùê∂3 ùê∂4
Considered as the User's answer ùë§ùëõ
Next step
ùúè ‚Üê ùúè + 1
Fig. 2. Overview of the proposed method: (a) The robot explores the environment to obtain object positions and attributes,
training an initial ownership model. (b) Objects are classified as owned or shared using commonsense knowledge from the
LLM. (c) Classification results are treated as pseudo-answers to update the ownership distribution. (d) For owned objects,
the robot computes IG, selects the most informative one, generates a question via LLM, and updates the model based on the
user‚Äôs answer.
Prompt. 4.1. Object Ownership Classification
1: You are an excellent household robot.
2: Please classify the list of objects that we give you as either ‚Äúowned‚Äù or
‚Äúshared‚Äù by someone in the living environment.
3: Let‚Äôs think step by step.
4:
5: A list of the objects you observed in a home environment:
6: existing_object = [OBJECT_LIST]
7:
8: However, the environment is a family household.
9:
10: Output only those objects that are property as a result of classification.
11:
12: Follow the instructions below to answer the questions.
13: Do not output anything other than the answer.
14:
15: Example: If the list of objects is
16: [book, pen, desk, window].
17:
18: Answer:
19: Owned_object = [book, pen]
OTHER_OBJECT_LIST: List of Objects in the Environment
(Class, Attribute, Point.X, Point.Y, Observed User)
using commonsense knowledge from an LLM. As illustrated
in Prompt. 4.1, the robot provides the object‚Äôs class name as
input to the LLM, which determines whether the object is
typically shared or individually owned. Objects classified as
shared are assumed not to have a specific owner and are ex-
cluded from question generation and probabilistic modeling,
whereasobjectsclassifiedasownedareretainedascandidates
for ownership identification.
4.3. Learning a probabilistic generative model of Object
Ownership
The robot learns a probabilistic generative model to esti-
mate object ownership from the position and attribute infor-
mationobtainedduringpriorexploration. Anoverviewofthe
model is illustrated in Fig. 3, and the variables are defined in
Table 1. The generative process is formalized as follows.
œÄ‚àºDir(Œ≥)(1)
Œ£k ‚àº IW(V0, ŒΩ0)k= 1,2, . . . , K(2)
¬µk ‚àº N(m0,Œ£ k/Œ∫0)(3)
œïl ‚àºDP(Œª)l= 1,2, . . . , L(4)
œÜl ‚àºDir(Œ±)(5)
Œ∑l ‚àºDir(Œ≤)(6)
Cn ‚àºCat(œÄ)n= 1,2, . . . , N(7)
in ‚àºCat(œï Cn)(8)
xn ‚àº N(¬µin,Œ£ in)(9)
on ‚àºMult(œÜ Cn)(10)
wn ‚àºMult(Œ∑ Cn)(11)
Dir()denotestheDirichletdistribution,DP()theDirichlet
process,Mult()the multinomial distribution, andCat()the
categorical distribution.IW()denotes the inverse-Wishart
distribution, andN()denotes the multivariate Gaussian dis-
tribution.
The position distribution assumes that a person‚Äôs belong-
ingsaremorelikelytobefoundintheirownroom. Tohandle
sharedspacessuchaskitchensandlivingroomsusedbymul-
tiple individuals, it is modeled as a Gaussian mixture. The
ùõæ
ùëö0,ùúÖ0
ùëâ0,ùúà0
ùúã
ùë•ùëõ
ùë§ùëõ
ùëúùëõ
Œ£ùëò
ùúáùëò
ùúëùëô
ùúÇùëô
ùõº
ùõΩ
ùê∂ùëõ
ùëÅ
ùêø
ùëñùëõ ùúôùëô ùúÜ
ùêæ
 Position distribution
User's Answer
It's mine.
Index of 
Ownership Concepts
1
3
 4
2
Object Attributes
{classÔºåcolorÔºåsizeÔºåshape}
Fig. 3. Graphical model of the proposed method
attribute vector includes not only the object‚Äôs class but also
features such as color, size, and shape, allowing the model to
capture individual user preferences. For example, if a user
typicallyprefersredobjects,redobjectsaremorelikelytobe
inferred as belonging to that person.
User answers obtained through question generation do
not always correspond directly to predefined owner indices.
For instance, instead of explicitly naming an owner (e.g.,
‚ÄúTaro‚Äôs‚Äù), users may reply with pronouns such as ‚Äúmine‚Äù or
expressions like ‚Äúmy father‚Äôs.‚Äù To address this variability,
thesystemmapsuserresponsestoownershipindicesthrough
semantic interpretation.
To incrementally update the ownership distribution in
answer to sequential observations, we adopt an online
learning approach using a Rao-Blackwellized Particle Fil-
ter (RBPF) [19], following the framework of SpCoAE [2].
Therobottreatstheobject‚Äôsposition,attributes,andtheuser‚Äôs
Table 1. Definitions of variables used in the graphical
model of the proposed method
Cn Latentvariableindicatingtheownership
index
xn 2D coordinates of the object
on Attribute vector of the object
wn User answer to the robot‚Äôs question
in Latent variable indicating the index of
the position distribution
œÄ, œÜl, Œ∑l, œïl Parameters of multinomial distributions
Œ£k, ¬µk Mean vector and covariance matrix of
the position distribution
Œ≥,m 0,Œ∫ 0,V 0,
ŒΩ0,Œª,Œ±,Œ≤ Hyperparameters of the model
n Index of an object
N Totalnumberofobjectsobtainedduring
prior exploration
l Index of an ownership concept
L Total number of ownership concepts
k Index of a position distribution compo-
nent
K Total number of position distribution
components
answer as multimodal observations and performs sequential
inference of the posterior distribution over ownership.
Thejointposteriordistributionofallparametersestimated
during ownership learning is factorized using Bayes‚Äô theo-
rem. Following the derivation presented in SpCoAE [2], the
particle weight update equation can be expressed as follows:
p(xn, on, wn|C1:n‚àí1, i1:n‚àí1, x1:n‚àí1, o1:n‚àí1, w1:n‚àí1, h)
=
X
Cn
X
in
p(xn|x1:n‚àí1, i1:n, h)p(on|o1:n‚àí1, C1:n, Œ±)
√óp(w n|w1:n‚àí1, C1:n, Œ≤)
√óp(C 1:n, i1:n|C1:n‚àí1, i1:n‚àí1, Œ≥, Œª)
(12)
Based on this equation, particles are resampled according to
their weightsœâ n. During the initial exploration phase, the
robot does not ask any questions; therefore,wn is treated as
uninformative. After question generation begins, the prob-
abilistic generative model is updated by incorporating the
user‚Äôsanswerw n,enablingthelearningofownershipknowl-
edge.
4.4. Acquiring ownership knowledge via User-Directed
Question Generation
To efficiently acquire ownership knowledge, the proposed
method calculates IG for each object and selects the one ex-
pectedtomosteffectivelyreduceuncertainty. Fortheselected
object, a natural language question is generated using LLM,
and an answer is obtained from the user.
4.4.1. Object Selection Based on IG
Forobjectsidentifiedasownedbelongings,thenextobject
toqueryisselectedbasedonitsexpectedIG.Theoverallalgo-
rithm for the proposed method is presented in Algorithm 1.
Here,UPDATE-MODELdenotes the RBPF-based update
step,whichtakesthecurrentsetofobservations(x n, on, wn)
and sequentially updates the posterior distribution of owner-
ship concepts and model parameters.
In this study, the full set of parameters related to owner-
ship is denoted byŒò ={{¬µ k},{Œ£ k},{œï l},{œÜ l},{Œ∑ l}, œÄ},
and the set of hyperparameters is denoted byh=
{Œ±, Œ≤, Œ≥, Œª, m0, Œ∫0, V0, ŒΩ0}. The latent variables are repre-
sentedasZ={C 1:N , i1:N ,Œò},andSetofobservedinforma-
tionisW n0 ={x 1:N , o1:N , wn0 , h}. Here,a‚àà {1 :N} \n0
denotes the index of a candidate object for the next question,
whilen 0 representsthesetofindicescorrespondingtoobjects
that have already been observed.
IG is defined to reduce the uncertainty in the ownership
distribution before and after a question is asked.
a‚àó =argmax
a
IG(Z;W a|Wn0 ) (13)
IGisapproximatedusingtheparticlesZ [r] andtheircorre-
spondingweightsœâ [r]
n0 obtainedfromtheRBPF,asexpressed
by the following equation:
Algorithm 1Active Exploration Algorithm for Ownership
Acquisition
1:Z ‚àí1 =UPDATE-MODEL
 
{wa =unknown| ‚àÄa}

2:Identify shared objects via LLM; setwa =Shared
3:Z 0 =UPDATE-MODEL
 
{wa ‚àà {unknown, Shared} |
‚àÄa}

4:n 0 ={a|w a =Shared},W n0 ={w a |a‚ààn 0}
5:forœÑ= 1toTdo
6:for alldetected objectsx a, oa ‚ààR D do
7:forr= 1toRdo
8:forj= 1toJdo
9:W [r,j]
a ‚àºMultinomial(p= ÀÜp(wa|Z[r], Wn0 ))
10:end for
11:end for
12:Compute IG a using sampledW [r,j]
a and particle
weightsœâ [r]
n0
13:end for
14:a ‚ãÜ ‚Üêarg max a IGa
15:q a‚àó =QUESTION-GENERATION(W a‚àó)
16:w a‚ãÜ ‚ÜêUNDERSTAND-ANSWER(a ‚ãÜ)
17:n 0 ‚Üên 0 ‚à™ {a‚ãÜ},W n0 ‚ÜêW n0 ‚à™ {wa‚ãÜ}
18:Z œÑ ‚ÜêUPDATE-MODEL(W n0 )
19:end for
IG(Z;W a|Wn0 )
‚âà
RX
r=1
JX
j=1
"
œâ[r]
n0 log p(W[j]
a |Z[r], Wn0 )
PR
r‚Ä≤=1[p(W[j]
a |Z[r‚Ä≤], Wn0 )]œâ[r‚Ä≤]
n0 ]
#
(14)
W[j]
a ‚àºp(W a|Z[r], Wn0 )(15)
W[j]
a denotes a pseudo-observation (i.e., a simulated user
answerw a)forobjectaobtainedfromthenextpossibleques-
tion,andJrepresentsthenumberofpseudo-observationsam-
ples. The termp(W [j]
a |Z [r], Wn0 )is sampled from the
predictive distribution based on particler, and is computed
using the following equation:
W[j]
a = (wa|xa, oa)‚àºp(w a|Z[r], xa, oa, wn0 , a, h)
=
X
Ca,ia
p(wa|Ca, wn0 , h)p(Ca, ia|Z[r], xa, oa, a, h)
(16)
The positionx a and attributeo a of the object are held
fixed, and sampling is performed only over the user answer
wa. Byleveragingtheparticlesandweightsobtainedthrough
online learning, IG can be efficiently approximated.
4.4.2. Question Generation using LLM
For the object with the highest IG, the robot generates a
questionaboutownership. Conventionalapproachestoques-
tion generation have relied on predefined templates [12] or
Prompt.4.2.ObjectOwnershipQuestionGeneration
1: You are an excellent household robot.
2: Generate a question asking the owner of the object we provide.
3: In addition, provide information about other objects observed in the home
environment.
4: Note that similar objects may have been placed nearby, or objects near one
with a known owner may share the same owner.
5: Instead of simply asking ‚ÄúWhose is this?‚Äù, add brief object information to
help the listener understand the question.
6: Let‚Äôs consider them in this order.
7:
8: Belowisinformationabouteachobject. Usethefollowingformattodescribe
it:
9: [class, attribute, point.x, point.y, observed user]
10: The description of each variable is as follows
11: class: Class name of the object
12: attribute: Feature vector of an object
13: The vector concatenates: Object label (18D) +
14: Attribute [color: red, blue, yellow, green, black, white] (6D) +
15: Attribute [size: large, medium, small] (3D) +
16: Attribute [shape: round, square, triangle] (3D).
17: The object labels are:
18: [Backpack, Bed, Book, Bottle, Chair, Clock, Cup, Desk, Dining Table,
Handbag/Satchel, Laptop, Monitor/TV, Mouse, Pillow, Potted Plant, Printer,
Refrigerator, Trash bin Can].
19: answer: owner name of the object
20: point.x: x-coordinate of object
21: point.y: y-coordinate of object
22: observeduser: Nameoftheowneroftheobject(or‚Äúunknown‚Äùifunknown)
23:
24: Please also consider the following information on other objects.
25: Use only other information related to the object being presented.
26:
27: other object:[OTHER_OBJECT_LIST]
28:
29: Do not output anything other than the question. Keep your question brief.
30: Do not include coordinate information in the question text.
31:
32: object in question:[TARGET_OBJECT_LIST]
OTHER_OBJECT_LIST: List of Objects in the Environment
(Class, Attribute, Point.X, Point.Y, Observed User)
TARGET_OBJECT_LIST: List of Objects Targeted for Questioning
(Class, Attribute, Point.X, Point.Y, Observed User)
Example ofOTHER_OBJECT_LISTandTARGET_OBJECT_LIST
-Refrigerator, [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,1,0],
0.736902236, 3.799175403, unknown
-Backpack, [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0],
6.325937769, 2.471300272, hashimoto
-Book, [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0],
6.517469798, 2.656295813, unknown
on masking portions of object datasets to elicit answers [20].
However,thesemethodsareofteninflexibleinnovelenviron-
ments with unknown object configurations, and the resulting
questionstendtobeoverlyformalandrestrictive,limitingthe
naturalness and diversity of user responses.
Toaddressthisissue,ourapproachemploysanLLM,asin
priorstudies[9,10,13],togeneratequestionsinamorenatu-
ralandconversationalmanner. Thisenablesflexiblequestion
generation and intuitive interaction with users. When gen-
erating questions, the LLM receives not only the attributes
of the target object but also its coordinates, features, and
ownershipknowledgealreadylearnedforotherobjectsinthe
environment. The prompt used for this process is shown in
Prompt. 4.2, which allows the model to incorporate spatial
relationships and prior knowledge to produce contextually
appropriate questions.
Additionally, when interpreting user answers, the name of
the respondent is included in the prompt. The prompt used
for this process is shown in Prompt. 4.3, which enables the
Prompt. 4.3. Ownership Identification
1: You are an excellent household robot.
2: Ask the following question about the owner of an object.
3:
4: Question text : ‚ÄúQUESTION_TEXT‚Äù
5: To this question, the user gives the following answer.
6: User answer : ‚ÄúUSER_ANSWER‚Äù
7:
8: Identify the name of the owner of the object from this content.
9: However, if the user‚Äôs answer indicates that the object is shared by multiple
people, rather than a specific person, please output ‚ÄúShared‚Äù.
10:
11: The list of users is as follows:
12: user_list =USER_LIST.
13:
14: However, the name of the user who is responding is shown below.
15: Responding user : ‚ÄúRESPONDING_USER‚Äù
16:
17: Extracttheansweraccordingtotheuser‚Äôsinputandoutputtheowner‚Äôsname
as follows.
18:
19: Example
20: answer_output = ‚Äúhashimoto‚Äù
21:
22: User answers are subject to typos, in which case please choose the closest
match.
23:
24: Do not output anything other than ‚Äúanswer_output‚Äù.
25: Let‚Äôs consider them in this order.
QUESTION_TEXT: The question asked by the robot.
USER_ANSWER: The natural language answer provided by the user.
USER_LIST: The list of owners in the environment.
RESPONDING_USER:Theowner‚Äôsnameasreceivedfromtheuser‚Äôsanswer,
including the shared label ‚ÄúShared‚Äù.
model to correctly handle both possessive expressions (e.g.,
‚ÄúIt‚Äôs mine‚Äù) and explicit references to individuals (e.g., ‚ÄúIt‚Äôs
Taro‚Äôs‚Äù).
5. EXPERIMENT 1: OWNERSHIP LEARN-
ING UNDER SIMPLIFIED SIMULATION
CONDITIONS
5.1. Objective
Theobjectiveofthisexperimentistoevaluatewhetherthe
proposed method can efficiently learn object ownership in a
simple simulated environment. In addition, we conduct an
ablationstudytoassessthecontributionofeachfeature‚Äîsuch
as object position and attributes‚Äîto the accuracy of owner-
ship estimation.
5.2. Conditions
The experiments were conducted in a Gazebo simulation
environment using the aws-robomaker-small-house-world1.
We set up an environment with three users and 12 objects,
each assigned an ownership label. The robot was assumed
to have prior knowledge of all objects‚Äô 2D coordinates and
categorylabels. TheobjectlayoutisshowninFig.4. Shared
objects were labeled as ‚ÄúShared,‚Äù while objects belonging
to the same user were given consistent color attributes and
placed in close proximity. To simulate realistic conditions,
the environment included both owned and shared objects, as
well as categories with multiple instances and those with a
single instance.
1https://github.com/aws-robotics/aws-robomaker-small-house-world
Computer
Phone
Bag
Bag
Shoes
Toy
Table
Chair
Chair
Book Cup
Phone
Fig.4. Objectlayout(Experiment1): Classname,color,size,
shape, and position distribution of objects are shown
Object positions were represented in a local two-
dimensional coordinate system(x, y). Each object was de-
scribedbya21-dimensionalattributevectorconstructedfrom
four types of features:
‚Ä¢Class: one-hot vector corresponding to the number of ob-
ject types (9 classes in this experiment).
‚Ä¢Color: 6-dimensional vector (red, blue, yellow, green,
black, white).
‚Ä¢Size: 3-dimensional vector (large, medium, small).
‚Ä¢Shape: 3-dimensional vector (circle, square, triangle).
Attributes were manually assigned to each object. User
answers were collected in textual form. This included ex-
plicitownernames(e.g.,‚ÄúTaro‚Äôs‚Äù),possessivepronouns(e.g.,
‚Äúmine‚Äù), and referential expressions (e.g., ‚Äúmy father‚Äôs‚Äù).
During the learning process, the number of particles was
set to 100, and the number of pseudo-observations (i.e., an-
swer samples per question) was set to 10. The hyperparam-
eters were set as follows: Attribute distribution parameter:
Œ±= 1.0, User answer distribution parameter:Œ≤= 0.01,
Ownership concept prior:Œ≥= 5.0, Precision parameter for
the position distribution:Œ∫ 0 = 1.0, Prior mean vector for
the position distribution:m0 = [0,0,0,0], Prior covariance
matrix:V 0 = diag(0.1,0.1)Degrees of freedom for the po-
sitiondistribution:ŒΩ 0 = 5.0. Wealsosetthetotalnumberof
ownership concepts:L= 4and the total number of position
distributions:K= 4Tostabilizeestimation,thetrueposition
distributionindexcorrespondingtoeachobservedcoordinate
xn was fixed.
When integrating multimodal information‚Äîobject posi-
tion, attributes, and user answers‚Äîwe treated user answers
as the most reliable modality. A weighting coefficient
œâanswer = 5.0wasappliedtouseranswers,whileothermodal-
ities were equally weighted. For the LLM, we used GPT-4
(gpt-4-0613) [21].
5.3. Comparison Methods
To evaluate the performance of our approach (ActOwL),
we compare it with the following baseline methods:
1.IG-min: A method that selects the object with the lowest
IG for question generation and learns ownership knowledge
based on the user‚Äôs answer.
2.Random: A method that randomly selects an object for
question generation and learns ownership knowledge based
on the user‚Äôs answer.
3.No-LLM:Amethodthatselectsobjectsforquestiongen-
eration based solely on the probabilistic generative model
and IG, without using LLM, and learns ownership knowl-
edge from the user‚Äôs answers.
4.LLM-only: Amethodthatpredictsownershipknowledge
bypromptingLLMwithoutusinganyprobabilisticgenerative
model.
In addition, to examine the contribution of each feature
to ownership learning, we conducted the following ablation
studies:
1. Onlycolorinformationfromtheobject‚Äôsattributesisused.
2. Onlytheobject‚Äôspositionalcoordinatesareusedforlearn-
ing, without incorporating attribute information.
3. Onlytheobject‚Äôsattributeinformationisusedforlearning,
without incorporating positional coordinates.
5.4. Evaluation Metrics
This study evaluates (1) the accuracy of ownership learn-
ing and (2) the efficiency of learning in terms of the number
of user questions required. The latter focuses on how ac-
tiveexplorationbasedonIGmaximizationimproveslearning
efficiency in the probabilistic generative model.
5.4.1. Adjusted Rand Index
Ownership clustering accuracy was measured using the
Adjusted Rand Index (ARI), a standard metric for cluster-
ing performance. ARI quantifies the agreement between
predicted ownership clusters and ground-truth labels, with
higher values indicating more accurate ownership concepts.
5.4.2. Number of Questions
Learning efficiency was evaluated by the number of ques-
tions posed by the robot to the user until a given ARI was
achieved. Thismeasuredirectlyindicateshoweffectivelythe
proposed method acquires ownership knowledge compared
to baseline methods.
5.5. Results
Fig. 5 shows ARI and its standard deviation forCn across
20 trials at each step. The proposed method consistently
achieved the highest ARI values, indicating that the robot
accurately learned ownership knowledge at an early step.
ThisimprovementstemsfromIG-basedselectionofinforma-
tiveobjects,whichenabledefficientacquisitionofownership
knowledge. The No-LLM method also improved steadily
but required more questions to reach similar accuracy, high-
lightingthebenefitofexcludingsharedobjectsinadvance. In
contrast,theLLM-onlymethodshowedlimitedARIimprove-
ment,despiteaskingfewerquestions,suggestingthatreliance
oncommonsensereasoningalonecanleadtoovergeneraliza-
tionmisalignedwithactualownership. ByintegratingLLM-
basedcommonsenseknowledgewithprobabilisticgenerative
modeling, the proposed method achieved more robust and
efficient ownership learning.
Fig.6showsthetransitionofIGvaluesforselectedobjects
at each step across 20 trials. In the early steps, objects with
high IG values were consistently chosen, demonstrating that
question generation was strategic and efficient. From Step 6
1
 0 1 2 3 4 5 6 7 8 9 10 11 12
Step
0.0
0.2
0.4
0.6
0.8
1.0Average ARI of index C
Proposed Method
IG-min
Random
No-LLM
LLM-only
Fig. 5. ARI ofCn per step (Experiment 1): Step‚àí1shows
thepost-explorationbaseline,andStep0showsresultsaf-
ter LLM-based shared/owned classification. Subsequent
stepsindicateperformanceaftereachadditionalquestion.
Since shared objects are excluded from querying (except
in No-LLM), learning typically converges by Step 9
1 2 3 4 5 6 7 8 9
Step
0.0
0.5
1.0
1.5
2.0
2.5Information Gain
Mean of Each Step IG
Std of Each Step IG
Max IG
Min IG
Fig. 6. Trends in IG values per step (Experiment 1)
onward,IGvariationdecreased,suggestingthattheremaining
candidates contributed less new information. This indicates
that the robot had already acquired sufficient knowledge by
this point and could maintain high ownership prediction per-
formance without additional questions.
Fig.7showstheaverageARIforC n ateachstepacross20
trials in the ablation experiments, with error bars indicating
standard deviation. Among the ablation settings, using only
color information yielded the highest ARI, reflecting consis-
tentcolorpreferencesamongownersinthesimulatedenviron-
ment. However, since real-world owners do not necessarily
use objects of uniform color, relying solely on color lacks
generalizability. In contrast, the proposed method, which
integrates multiple attribute types, shows greater robustness
and adaptability to diverse environments. Settings that used
onlypositionalinformationoronlyattributeinformationpro-
ducedlowerARIthantheproposedmethod,indicatingthata
single modality is insufficient and that combining positional
and attribute information is essential for accurate ownership
inference.
1
 0 1 2 3 4 5 6 7 8 9
Step
0.0
0.2
0.4
0.6
0.8
1.0Average ARI of index C
Proposed Method
Proposed Method(Color Attributes) 
Proposed Method w/o Attributes
Proposed Method w/o Position
Fig. 7. ARI ofCn per step (Experiment 1: ablation study)
Table
Chair
Computer
Phone
Bag
Bag
Shoes
Toy
Book Cup
Phone
Shoes
Cup
Box
Chair
Chair
Box
Box
Toy
Cup
Fig.8. Objectlayout(Experiment2): Classname,color,size,
shape, and position distribution of objects are shown
6. EXPERIMENT 2: OWNERSHIP LEARN-
INGINACOMPLEXSIMULATEDENVI-
RONMENT
6.1. Objective
The objective of this experiment is to evaluate the effec-
tiveness of the proposed method in a more complex environ-
ment. In particular, we examine whether it remains effective
for ownership learning when both the number of objects and
users are increased, simulating a more realistic household
setting.
6.2. Conditions
TheexperimentwasconductedinthesameGazebosimula-
tionenvironmentasExperiment1,usingtheaws-robomaker-
small-house-world. We set up an environment with three
users and 20 objects, each assigned an ownership label. The
robotwasassumedtohavepriorknowledgeofallobjects‚Äô2D
coordinates and category labels through prior exploration.
TheobjectlayoutisshowninFig.8. AsinExperiment1,the
environment included both owned and shared objects, with
categories appearing multiple times as well as only once. To
evaluate robustness, most objects owned by the same user
were grouped by consistent color attributes, while a subset
was intentionally assigned inconsistent color patterns to in-
troduce contradictory cues for ownership.
In this experiment, the number of object categories was
set to 10, resulting in a 22-dimensional attribute vector con-
1
 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Step
0.0
0.2
0.4
0.6
0.8
1.0Average ARI of index C
Proposed Method
IG-min
Random
No-LLM
LLM-only
Fig. 9. ARI ofCn per step (Experiment 2): Step‚àí1shows
thepost-explorationbaseline,andStep0showsresultsaf-
ter LLM-based shared/owned classification. Subsequent
stepsindicateperformanceaftereachadditionalquestion.
Since shared objects are excluded from querying (except
in No-LLM), learning typically converges by Step 16
structed in the same manner as in Experiment 1. The repre-
sentation of object positions and the format of user answers
werealsothesameasinExperiment1. Allotherexperimental
settings, including hyperparameters, multimodal integration,
andLLM-basedprocessing,wereidenticaltothosedescribed
in Section 5.2.
To evaluate performance, we employed the same com-
parison methods as in Experiment 1. The same evaluation
metricswerealsousedtoassessownershiplearningaccuracy
and learning efficiency.
6.3. Results
Fig. 9 shows the average ARI and standard deviation of
Cn over 20 trials at each step. In this experiment, the pro-
posed method underperformed compared to No-LLM. This
was mainly because the object class ‚ÄúBox,‚Äù although owned,
was incorrectly categorized as shared and excluded from
questioning, which resulted in missing information. This re-
vealsalimitationoftheproposedstrategy: relianceoninitial
classificationcancausecriticalinformationlossandsuppress
learning performance. By contrast, No-LLM, which relies
solely on the probabilistic generative model, achieved high
ARI, indicating the robustness of the model even when po-
sitional and attribute consistency across owners is limited.
Nevertheless, the proposed method outperformed the other
baselines such as Random and IG-min, confirming that IG-
based question selection contributes to efficient ownership
learning.
7. EXPERIMENT 3: EFFECTIVENESS IN A
COMPLEX REAL ENVIRONMENT
7.1. Objective
The objective of this experiment is to evaluate whether
the proposed method can efficiently learn object ownership
in a complex real-world environment. In particular, labora-
tory settings are often shared by multiple users with similar
workspaces, and object attributes are not always clearly dis-
Dining 
Table
Bed
Pillow
Trash 
bin 
Can
Chair
Chair
Chair
Cup
Potted 
Plant
Monitor
/TV 
Desk
Desk
Clock
LaptopBook
Book
Handbag
/Satchel
Refrigerator
Trash 
bin 
Can
Printer
Bottle
Bottle
Bottle 
Bottle
Mouse
Backpack
Monitor/
TV
Fig. 10. Object layout (Experiment 3): Class name, color,
size and shape of objects are shown
tinguishable. This experiment examines whether the method
can still acquire ownership knowledge effectively under such
challenging conditions.
7.2. Conditions
The experiment was conducted in a real laboratory at
Ritsumeikan University using the Human Support Robot
(HSR) [22]. We set up a scenario with seven users and 48
objects,eachassignedanownershiplabel. Therobotwasas-
sumedtohavepriorknowledgeofallobjects‚Äô2Dcoordinates
and category labels through prior exploration. The object
layout is shown in Fig. 10, and the corresponding position
distribution in Fig. 11.
In this real-world environment, it is common for multiple
userstosharethesameworkspace,andobjectsusedbydiffer-
entindividualsoftenlackdistinguishingvisualfeatures. Asa
result, object attributes are less effective cues for ownership,
making spatial location and user answers more critical. To
ensure realistic evaluation, we used actual laboratory objects
with known ownership assignments.
The number of object categories was set to 18, resulting
in a 30-dimensional attribute vector constructed in the same
mannerasdescribedinSection5.2forExperiment1. Object
positions and user answers were handled in the same way as
in Experiment 1.
Most hyperparameter settings were identical to those in
Experiment1,exceptthatthetotalnumberofownershipcon-
cepts was set toL= 8and the total number of position
distributions toK= 8. As before, the indexin of the spatial
distribution corresponding to each observed coordinatexn
was fixed for stability, and multimodal integration applied a
weight ofœâanswer = 5.0to user answers. Furthermore, since
no significant differences were observed in object attributes
in the experimental environment, the attribute modality was
down-weightedwithacoefficientofœâ attribute = 0.1,whilethe
Fig. 11. Position distribution (Experiment 3): Large green
circles on left show the distribution of shared objects,
whereas the others show the distributions of owned ob-
jects
Prompt. 7.1. Prompt added in Experiment 3.
In Prompt. 4.1
8: However, the environment shall be a university laboratory.
9: Eachstudenthashisorherowndeskinoneroom,andthereisasharedspace
for meetings.
other modalities were equally weighted.
For LLM-based processing, we used GPT-4 (gpt-4-0613),
as in the previous experiments. The prompt used for classi-
fying objects as shared or owned (Prompt. 4.1) was slightly
modified to reflect the contextual characteristics of the labo-
ratory environment, as shown in Prompt. 7.1.
To evaluate performance, we employed the same com-
parison methods as in Experiment 1. The same evaluation
metricswerealsousedtoassessownershiplearningaccuracy
and learning efficiency.
7.3. Preparation before experiment
Inthisexperiment,therobotfirstconstructedamapofthe
domestic environment using gmapping, a ROS-based map-
pingpackage. Forobjectdetection,itemployedDetic[23]to
recognize objects and obtain their category labels.
7.4. Results
7.4.1. Qualitative evaluation
Table 2 presents examples of object classes classified as
shared or owned by the LLM. Incorporating the laboratory
context into the prompt enabled appropriate classification
based on usage and placement, showing that the LLM‚Äôs
commonsense knowledge functioned effectively. By pre-
classifying shared objects, the robot excluded them from
question generation, avoiding unnecessary queries. Conse-
quently, ownership knowledge was learned more efficiently
with fewer user questions.
Fig. 12 presents examples of questions generated in this
experiment. The proposed method produced clear and flexi-
ble questions by leveraging spatial relationships with nearby
objects. However, in some cases, objects not actually close
Table 2. Examples of object classes classified as Shared
and Owned objects (Experiment 3)
Object Type Examples
Shared Clock,DiningTable,Printer,Potted
Plant, Refrigerator, Trash bin Can
Owned Backpack, Bed, Book, Bot-
tle, Chair, Cup, Desk, Hand-
bag/Satchel, Laptop, Monitor/TV,
Mouse, Pillow
Learning a Probabilistic Generative Model of Ownership
Who does this small, square, 
black handbag belong to?
It‚Äôs User B‚Äôs.
Learning a Probabilistic Generative Model of Ownership
Is this black, middle, square-shaped laptop
yours, considering there is a similar one 
nearby with an unknown owner?
It‚Äôs Mine.
User answer = ‚ÄúUser B‚Äú
User answer = ‚ÄúUser A‚Äú
Fig. 12. Sample questions generated by LLM
were incorrectly described as ‚Äúnearby,‚Äù indicating that the
LLMcouldnotfullycapturespatialrelationships. Thishigh-
lights a remaining challenge in precise spatial reasoning.
7.4.2. Quantitative evaluation
Fig. 13 shows the average ARI and standard deviation
ofC n over 10 trials at each step. In the laboratory envi-
ronment, where object types and appearances were highly
similar across users, the discriminative power of attribute in-
formation was limited, making IG-based question selection
less effective. Even so, by lowering the weight of the at-
tribute modality, our method achieved slightly higher ARI
thanIG-minandRandom,suggestingthatadjustingmodality
contributions can improve adaptability in challenging envi-
ronments. Moreover, the proposed method outperformed
both No-LLM and LLM-only in clustering consistency, indi-
cating that the combination of strategic question generation
and the probabilistic generative model remained effective.
It also achieved accuracy comparable to or higher than No-
LLM with fewer questions, showing that pre-classifying ob-
jectsintosharedandindividuallyownedcategoriesenhances
learningefficiencybeyondconventionalapproaches. Overall,
these results demonstrate both the potential of our approach
andtheinherentchallengesofownershiplearningincomplex
real-worldsettings,whereusersoftenpossessvisuallysimilar
objects and ownership boundaries are ambiguous.
1 3 5 7 9 11131517192123252729313335373941434547
Step
0.0
0.2
0.4
0.6
0.8
1.0Average ARI of index C
Proposed Method
IG-min
Random
No-LLM
LLM-only
Fig.13. ARIofC n perstep(Experiment3): Step‚àí1shows
the post-exploration baseline, and Step 0 shows results
after LLM-based shared/owned classification. Subse-
quent steps indicate performance after each additional
question. Sincesharedobjectsareexcludedfromquery-
ing(exceptinNo-LLM),learningtypicallyconvergesby
Step 40
8. LIMITATIONS
The method employed commonsense-based classification
ofobjectsintosharedandownedcategoriesusingLLM;how-
ever, such interpretations can vary across environments, cul-
tures,andcontexts,leavingariskofmisclassification. Amore
flexible classification framework is needed that accounts for
uncertainty in sharedness.
Thesystemassumedthatuserresponseswerealwaysaccu-
rateandconsistent;however,itwasunabletohandleambigu-
ous expressions or subjective variability. To ensure robust
dialogue in real-world scenarios, response processing must
incorporate uncertainty in user utterances.
Object location, class, and attribute information were en-
tirely annotated by hand, and the robot was not able to au-
tonomouslyexploreorperceivetheenvironment. Inaddition,
featureweightsweremanuallyset; therefore, amechanismis
necessary that allows the robot to autonomously select and
utilize relevant features based on context.
Ownershipwastreatedasabinaryrelation‚Äîeitherindivid-
uallyownedorsharedbyall‚Äîandpartialorgroupownership
could not be represented. To reflect realistic social contexts,
a more flexible framework is needed for representing and
inferring complex ownership relationships.
9. CONCLUSION
9.1. Summary
In this study, we proposed ActOwL, which enables robots
to efficiently learn object ownership in domestic environ-
ments. The approach selects objects based on IG and gen-
erates targeted questions to acquire ownership knowledge.
By leveraging commonsense reasoning from an LLM, the
robotcandistinguishsharedfromownedobjects,reducingthe
scopeofuserinteraction. WeevaluatedActOwLinbothsim-
ulatedandreal-worldsettings,whereitoutperformedbaseline
approaches by achieving higher ownership clustering accu-
racy with fewer interactions. These results suggest that ef-
ficient acquisition of ownership knowledge through question
generation can enhance a robot‚Äôs ability to perform flexible
and socially appropriate tasks in daily life environments.
9.2. Future Work
By leveraging background information about users in the
environment, such as their roles and occupations, it becomes
possible to narrow down candidate objects that are likely to
belong to them. This can improve the precision of ques-
tion generation by the LLM and is expected to enhance the
efficiency of ownership learning.
The future framework will incorporate temporary owner-
shipordynamicchangesinownershipovertime. Byintegrat-
ingobservationsofuserbehaviorandactivitypatternswithin
the environment, robots will be able to update ownership
knowledge more flexibly and adaptively.
Once a robot understands ownership knowledge, it can
immediately identify the target object when the user gives
a command, enabling prompt and appropriate actions. This
capability can support personalized and socially appropriate
assistanceineverydaytaskssuchastidyingupandmanaging
personal belongings.
ACKNOWLEDGMENTS
This work was supported by JSPS KAKENHI Grants-in-
Aid for Scientific Research (Grant Numbers JP23K16975,
JP25K15292) and JST Moonshot Research & Development
Program (Grant Number JPMJMS2011).
REFERENCES
[1] Friston, K.: The Free-Energy Principle: A Unified
Brain Theory? Nat. Rev. Neurosci.11(2), 127‚Äì138
(2010)
[2] Taniguchi,A.,etal.: ActiveExplorationbasedonInfor-
mationGainbyParticleFilterforEfficientSpatialCon-
cept Formation. Adv. Robot.37(13), 840‚Äì870 (2023)
[3] Ishikawa, T., et al.: Active Semantic Mapping for
Household Robots: Rapid Indoor Adaptation and Re-
ducedUserBurden. In: IEEEInternationalConference
on Systems, Man, and Cybernetics (SMC), pp. 3116‚Äì
3123 (2023)
[4] Tan, Z.X., et al.: That‚Äôs Mine! Learning Ownership
Relations and Norms for Robots. In: The Association
for the Advancement of Artificial Intelligence (AAAI)
onArtificialIntelligence,vol.33,pp.8058‚Äì8065(2019)
[5] Wu, H., et al.: Item Ownership Relationship Semantic
Learning Strategy for Personalized Service Robot. Int.
J. Autom. Comput.17(3), 390‚Äì402 (2020)
[6] Hu, Y., et al.: An Interactive Learning Framework for
Item Ownership Relationship in Service Robots. Hum.
Factors Robots Drones Unmanned Syst. p. 29 (2023)
[7] Taniguchi, T., et al.: Multimodal Hierarchical Dirichlet
Process-based Active Perception by a Robot. Front.
Neurorobot.12, 22 (2018)
[8] Nakamura, T., et al.: Multimodal Categorization by
Hierarchical Dirichlet Process. In: IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems
(IROS), pp. 1520‚Äì1525 (2011)
[9] Ren, A.Z., et al.: Robots That Ask For Help: Uncer-
tainty Alignment for Large Language Model Planners.
In: The Conference on Robot Learning (CoRL) (2023)
[10] Park, J., et al.: CLARA: Classifying and Disambiguat-
ing User Commands for Reliable Interactive Robotic
Agents. IEEE Robot. Autom. Lett.9(2), 1059‚Äì1066
(2023)
[11] Majumdar, A., et al.: FindThis: Language-Driven Ob-
ject Disambiguation in Indoor Environments. In: The
Conference on Robot Learning (CoRL) (2023)
[12] Pramanick, P., et al.: Talk-to-Resolve: Combining
Scene Understanding and Spatial Dialogue to Resolve
GranularTaskAmbiguityforaCollocatedRobot.Robot
Auton. Syst.155, 104,183 (2022)
[13] Dai, Y., et al.: Think, Act, and Ask: Open-World
Interactive Personalized Robot Navigation. In: IEEE
International Conference on Robotics and Automation
(ICRA), pp. 3296‚Äì3303 (2024)
[14] Kuang, Y., et al.: OpenFMNav: Towards Open-
Set Zero-Shot Object Navigation via Vision-Language
Foundation Models. In: Annual Conference of the Na-
tions of the Americas Chapter of the Association for
Computational Linguistics (NAACL) (2024)
[15] Oyama, A., et al.: Take That for Me: Multimodal
Exophora Resolution with Interactive Questioning for
Ambiguous Out-of-View Instructions. In: IEEE Inter-
national Conference on Robot and Human Interactive
Communication (RO-MAN) (2025)
[16] Wang,H.,etal.: APRICOT:ActivePreferenceLearning
and Constraint-Aware Task Planning with LLMs. In:
The Conference on Robot Learning (CoRL) (2024)
[17] Radford, A., et al.: Learning Transferable Visual Mod-
els from Natural Language Supervision. In: Interna-
tional Conference on Machine Learning (ICML), pp.
8748‚Äì8763 (2021)
[18] Chen, B., et al.: Open-Vocabulary Queryable Scene
Representations for Real World Planning. In: IEEE
International Conference on Robotics and Automation
(ICRA), pp. 11,509‚Äì11,522 (2023)
[19] Murphy, K., et al.: Rao-Blackwellised Particle Fil-
tering for Dynamic Bayesian Networks. In: Sequen-
tial Monte Carlo Methods in Practice, pp. 499‚Äì515.
Springer (2001)
[20] Uehara, K., et al.: K-VQG: Knowledge-Aware Visual
Question Generation for Common-Sense Acquisition.
In: IEEE/CVF Winter Conference on Applications of
Computer Vision (WACV), pp. 4401‚Äì4409 (2023)
[21] Achiam, J., et al.: GPT-4 Technical Report. arXiv
preprint arXiv:2303.08774 (2023)
[22] Yamamoto, T., et al.: Development of Human Support
Robot as the Research Platform of a Domestic Mobile
Manipulator. ROBOMECH J.6(1), 1‚Äì15 (2019)
[23] Zhou, X., et al.: Detecting Twenty-Thousand Classes
using Image-Level Supervision. In: European Confer-
enceonComputerVision(ECCV),pp.350‚Äì368(2022)