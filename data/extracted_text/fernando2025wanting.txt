arxiv.org/abs/123
April 2025
Wanting to be Understood
Chrisantha Fernando1, Dylan Banarse1 and Simon Osindero1
1Google DeepMind
This paper explores an intrinsic motivation for mutual awareness, hypothesizing that humans possess
a fundamental drive to understandand to be understoodeven in the absence of extrinsic rewards.
Through simulations of the perceptual crossing paradigm, we explore the effect of various internal
reward functions in reinforcement learning agents. The drive to understand is implemented as an
active inference type artificial curiosity reward, whereas the drive to be understood is implemented
through intrinsic rewards for imitation, influence/impressionability, and sub-reaction time anticipation
of the other. Results indicate that while artificial curiosity alone does not lead to a preference for
social interaction, rewards emphasizing reciprocal understanding successfully drive agents to prioritize
interaction. We demonstrate that this intrinsic motivation can facilitate cooperation in tasks where only
one agent receives extrinsic reward for the behaviour of the other.
Keywords: Perceptual Crossing Paradigm, Intrinsic Motivation, Mutual Awareness, Primary Intersubjec-
tivity
Introduction
Evidence from developmental psychology and
primatology suggests that humans (and possibly
other primates) have a drive to establish new
forms ofshared intentionality(Tomasello, 2022).
Here, ‚Äúshared intentionality‚Äù refers to the capac-
ityfortwoormoreindividualstoaligntheirinten-
tions, goals, values, policies, and world models
in a mutually recognizable way.
This drive aims to achievecommon knowledgeof
each other, eventually resulting in understand-
ing the other through the intentional stance Den-
nett (1989) and in more bodily ways (Di Paolo
et al., 2008)(Clark and Chalmers, 1998)(Varela
et al., 1991). In other words, we struggle afresh
in each lifetime to develop techniques ofprimary
intersubjectivity (Trevarthen et al., 1979), which
are basic methods for relating to one another.
Primary intersubjectivity typically appears in in-
fancy and includes face-to-face interactions, mu-
tual gazes, and the early emotional attunement
between caregiver and child.
Later, we developsecondary intersubjectivity(Vy-
gotsky, 2012), expanding our interactions to in-
clude a third entity in a shared context. This
step involves joint attention, referential gestures
or performances, and eventually language. Cru-
cially, these activities are often pursuedpurely
for the sake of establishing shared understanding,
rather than for any immediate extrinsic task re-
wards (Moeller et al., 2023) or for conforming
to externally imposed ‚Äúlanguage games‚Äù (Lewis,
1969).
We hypothesise that through the Baldwin ef-
fect1 (Baldwin, 1896) operating during the Pale-
olithic in the context of diverse extrinsically re-
warded social coordination tasks (Fernando et al.,
2024) a genetic intrinsic motivation evolved that
rewarded the mutual creation of social coordina-
tion‚Äúforthesakeofit‚Äùinanewwaywhichhadnot
been an intrinsic motivation for apes (Tomasello,
2022). Such minimal pre-Gricean2(Scott-Phillips,
2015) ‚Äúcognitive gadgets‚Äù (Heyes, 2018) allowed
the bootstrapping of language, art, and open-
ended culture. We propose humans evolved a
motivation not merely to understand, but also to
1The Baldwin effect describes a mechanism by which
traits initially acquired through individual learning within
a lifetime can eventually become genetically encoded over
evolutionary time through selection for mutations that facil-
itate faster learning, and eventually leading to their innate
emergence.
2Gricean communication involves inferences about speak-
ers‚Äô intentions (hidden mental states) and shared assump-
tions (conversational maxims). Pre-Gricean systems rep-
resent an earlier evolutionary stage, involving more basic
signaling (Moore, 2017)
arXiv:2504.06611v2  [cs.LG]  10 Apr 2025
Wanting to be Understood INTRODUCTION
be understood by the other. That is, to want the
other to understand one‚Äôs values, goals, policies,
and world models, in short, it brought about an
innate struggle for mutual awareness, promoting
the establishment of primary and secondary in-
tersubjectivity (Trevarthen et al., 1979) which
allowed shared intentionality (Tomasello et al.,
1993)(Tomaselloetal.,2005)manifestedinturn-
taking (Sacks et al., 1974), imitation (Meltzoff
and Moore, 1989), cultural evolution (Boyd and
Richerson, 1985), language (Tomasello, 2008),
and other varieties of external representation
making such as art and music (No√´, 2023).
In fact, such a core set of cognitive gadgets may
explain why "the hard problem of consciousness"
is a problem at all (Chalmers, 1997). The hard
problem of consciousness refers to the challenge
of explaining to another why and how physical
processes in my brain give rise to subjective,
qualitative experiences (qualia) ‚Äì the feeling of
what it‚Äôs like for me to be a conscious organism.
We are troubled by the fact that due to bandwidth
restrictions (McGregor and Fernando, 2005), our
external representations of our raw experience
of red,qualia, will never be able to convey the
way it truly feels, or why it feels that way to me.
There is ana prioriimpossibility of satisfactory
explanation either to myself or to another why
red feels as it does to me (Jackson, 1982), but
why do we keep on trying to make the other
understand?
Exactly how, if such a drive for wanting
one‚Äôs raw experience to be understood, can
being-understood-by be measured by the sender
in the absence of extrinsic reward of the type
provided in language games specified top-down
(Lewis, 1969)? In Lewis language games of the
type introduced by Wittgenstein (Wittgenstein,
1953), communicative success is externally
defined, and reward is given to both agents
symmetrically (Steels, 1997) (Steels, 1999). We
would like such an intrinsic motivation to explain
the open-ended unsupervised invention of new
language games, for the sheer joy of it.
Any such intrinsic motivation must be biologi-
cally plausible; it cannot depend on direct access
to the actions and brain states of another agent
but must use internal models of the other agent,
these being a function only of one‚Äôs observations,
actions, predictions, and one‚Äôs own brain states
(Jaques et al., 2019). But unlike earlier work,
we propose that 1. such a drive should not
require sophisticated counterfactual reasoning,
as it appears to exist shortly after birth (Hobson,
2004), and we explore the possibility 2. that
the drive is not only a drive to influence, but
a drive to be influenced by another. In Jaques
et al. (2019) whilst communication protocols
emerged, they did so because of an extrinsic
reward shared by all agents, i.e. the need for
shared intensionality was pre-supposed by the
task, the task was not established for the sake of
shared intensionality per se.
A necessary (but not sufficient) condition for
the drive we seek is that when incorporated into
an agent it results in the agent establishing a high
interaction equilibrium with other agents with
the reciprocal drive in the absence of extrinsic
reward (Lerique, 2022). This corresponds to
the concept of primary intersubjectivity defined
by Trevarthen et al. (1979). Another much
more stringent requirement is that this intrinsic
motivation results in agents co-inventing a set of
external representations such as performances,
signs or symbols that areabout other entities in
the world (Fernando et al., 2024), i.e. secondary
intersubjectivity utilizing techniques for joint
attention such as pointing and other forms of
reference (Trevarthen and Hubley, 1978) which
allows shared intentionality, e.g. joint goals.
We focus in this paper purely on potential
drives for primary intersubjectivity, and study
them within a simulation of a minimal experimen-
tal paradigm which has been used extensively
in humans, namely, the perceptual crossing
paradigm (PCP) (Auvray et al., 2009), see Figure
1. In the PCP, two humans interact using only a
mouse which controls an avatar which can move
around on a circle. When their avatars cross, both
perceive a buzz. If an avatar crosses their private
stationary object on the circle they also feel a
buzz. In addition, each avatar has a ‚Äòshadow‚Äô
2
Wanting to be Understood EMPIRICAL EVIDENCE
Figure 1|The perceptual crossing paradigm, Figure from Auvray et al. (2009). Two agents (avatars) live on a line. They
can cross each other, each other‚Äôs shadows, or their own private stationary object.
which is situated a fixed displacement from them-
selves. If an avatar crosses the other‚Äôs shadow,
they feel a buzz but the other does not feel a buzz.
In the original experiment, subjects were
asked to press a button when they thought
they were interacting with the other. Human
pairs spend most of their time interacting
with each other in preference to the fixed
object or each other‚Äôs shadows. They develop
turn-taking strategies to ident ify each other
reliably, and some pairs of humans play games of
dance-like anticipation with each other (Declerck
et al., 2009) involving passive (following) vs
active(leading)turn-taking(Kojimaetal.,2017).
Previous simulations of the PCP evolved a
policy within an evolutionary robotics framework
(Iizuka and Di Paolo, 2007), (Di Paolo et al.,
2008), the fitness function being to be close to
the other agent or to click when close (Froese
and Di Paolo, 2010), (Auvray and Rohde,
2012). Whilst this resulted in the number of
self-other crossings increasing over evolutionary
generations, the resulting policy does not
constitute a valid intrinsic motivation because it
presupposes an oracle which can determine that
one is interacting with another agent. In this
paper it is the nature of such an ‚Äòother‚Äô detecting
‚Äòoracle‚Äô which interests us. The policies that
evolutionary robotics approaches produce in such
simulations are instead exquisitely over-fitted to
this particular task, and will not generalize to
other tasks. Instead, we wish to propose intrinsic
motivation functions which do have the potential
to show preference for self-other interaction
without a priori being told that an interaction
with another agent is occurring.
Empirical Evidence
Imagine placing a handful of newborn infants on
a desert island with no access to humans, their
life support provided by soulless robots. Would
they develop language? If so then some genetic
information (plus information from the desert
island) must account for why humans do so (and
chimpanzees do not). If they do not develop
language within one generation, then some
information transmitted, not genetically, but
culturally, is needed to bootstrap language, such
as a tradition of prior language use. For example,
a transformer trained with a supervised next
token prediction loss such as ChatGPT (OpenAI,
2023) will not reinvent language if not exposed
to language data, but it will admirably pass on
the tradition of at least the most effortless aspects
of language use. In this sense ChatGPT may be
said to understand language, but to not want
to be understood enough to go to the trouble
of inventing new language (Rita et al., 2022)
(Lazaridou and Baroni, 2020).
Tomasello has proposed that humans differ
from apes in that humans are intrinsically
motivated to achieve shared intentionality,
3
Wanting to be Understood EMPIRICAL EVIDENCE
i.e. to form joint goals, joint intentions, joint
attention, create mutual knowledge and form
shared beliefs (p7 (Tomasello, 2008)), using
prosocial motivations and norms for helping and
sharing. Collaboration is intrinsically rewarding
(p178, ibid) sometimes more rewarding than the
instrumental goal of the task itself (Warneken
et al., 2006). In short, children collaborate for
the sake of collaborating. But as described by
Kaplan and Hafner ‚ÄúTomasello remains relatively
elusive on what exactly this ‚Äúsharing motivation‚Äù
consists of. How does the brain recognize ‚Äúshared
experiences‚Äù? What is special in situations of
‚Äújoint intentionality‚Äù? (Sect. 3.4 (Kaplan and
Hafner, 2006)).
A drive for primary intersubjectivity is sup-
ported by Trevarthen‚Äôs observations of infant
coordinative gesture use, vocalizations, and
affect, in face-to-face protoconversations with
caregivers (Trevarthen et al., 1979). Primary
intersubjectivity involves dyadic interactions
without reference to a third object or person.
Travarthen was concerned with how 2-3 month
infants and mothers came to share control of
joint sensorimotor contingencies (SMCs) and
how they could predict what the other would
do and know, in quite a different way to how
SMCs are established with inanimate objects. For
example, intersubjective behavioural dynamics
include synchronization and alternation between
infant and mother. The mother makes attempts
to attract the infant‚Äôs attention, principally
to obtain eye contact, if the baby makes an
expression the mother mirrors it, and when the
baby is ready to make an utterance the mother
‚Äòsteps down‚Äô. Watching, admitting, withdrawing,
makes the mother subordinate to the infant‚Äôs
communicative intentions. A complex form of
mutual understanding develops.
Stern describes how during the first months
of life infants orient more to highly predictable
SMCs from self-generated events, but after 3
months to more imperfect SMCs (other-generated
events) (Stern, 2018). More recent work shows
that children establish dyadic SMCs even earlier,
aligning their gaze with the timing of adults‚Äô
vocal behaviors (Crown et al., 2002), and by
2 months raising their arms in anticipation of
being picked up (Reddy et al., 2013). Infants
of only 45 minutes of age make strenuous
efforts to copy an adult‚Äôs tongue protrusion
and match other facial expressions and vocal
rhythms, and at later ages seem to try to infer
the goals of adult demonstrators (Hobson, 2004)
Kugiumutzakis and Trevarthen (2015) (Meltzoff
and Moore, 1989). The ‚Äústill-face-paradigm‚Äù
shows infants become extremely distressed when
their caregiver suddenly holds a neutral face
(Tronick et al., 1978) or the caregivers responses
are delayed in video feedback by only 30 seconds
(Murray, 1985)3.
Whilst the desert island baby experiment is a
Gedankenexperiment, some poor approximations
to it do exist, and suggest that there is an
intrinsic drive for establishing communicative
relationship. Firstly, profoundly deaf children
raised without sign language, out of desperation,
will invent a language together called homesign
(Sacks, 2009). Confounders here are that these
children can observe full language users, and
their external representations, and be scaffolded
also by the pre-linguistic interactional drives of
existing language users.
Finally, in autism, mutual engagement with
neurotypicals my fail (Milton, 2012). From the
3We do not consider secondary intersubjectivity here.
A drive for secondary intersubjectivity may be needed to
explain the special kinds of triadic interaction between two
humans and an object, e.g. joint attention involving infor-
mational helpful pointing, giving of spontaneous help such
as picking up dropped objects, involving alignment with
others‚Äô goals and attitudes to objects without direct external
rewards (Warneken and Tomasello, 2006), organizing col-
laborative activity with role taking and reversal (Warneken
and Tomasello, 2007), and perspective taking during coop-
eration. Chimp gestures are limited to imperatives (asking
for things) and getting attention, and although they can be
learned in pairs, they are not transmitted culturally. They do
not ‚Äôget it‚Äô that anyone would want to help them by giving
themcooperativeinformationdeclaringknowledge. Theydo
not ask questions to get answers (p72, (Tomasello, 2008)).
In the perceptual crossing experiment there is absolutely no
scope for secondary intersubjectivity (by design) because
there is no third object that both agents can interact with,
and therefore, secondary intersubjectivity cannot be studied
with this task
4
Wanting to be Understood METHODS
neurotypical perspective, there can sometimes be
a sense that one is being treated as an inanimate
object rather than as a person Shanahan (2024),
and from the autistic perspective one may be
able to coordinate better with other autistic
individuals than neurotypicals (Heasman and
Gillespie, 2019). One reason this could be is
that there is misalignment in methods that are
being used to establish mutual awareness, and
therefore expectations of what an engagement
should constitute (Bolis et al., 2018). But
precisely what is responsible for this mismatch of
engagement styles?
Methods
We examine a multi-agent interaction scenario
in a 1D ‚ÄúPerceptual Crossing‚Äù environment in
which two agents each occupy a position on a
circular or modular 1D line (0 to 1, wrapped)
Auvray et al. (2009). Each agent can move left,
move right, or remain still, attempting to elicit
and anticipate ‚Äúcrossing‚Äù events. A crossing
occurs whenever an agent‚Äôs position coincides
with a salient stimulus within a predefined
distance threshold‚Äîsuch as another agent, the
other agent‚Äôs ‚Äúshadow‚Äù (the position offset by
0.2 modulo 1), or a fixed (stationary) object.
Each agent receives a step-based observation
that includes its current position, the previous
step‚Äôs crossing value, its last action, and its last
received reward. Both agents select discrete
actions from the set left,right,no-op, which affect
their positions by a fixed increment of 0.05 or no
movement. We train these agents jointly using a
reinforcement learning (RL) framework (Liang
et al., 2018).
Separately from the main RL policy, each
agent also runs a small Long-Short Term Memory
(LSTM) recurrent neural network (Hochreiter
and Schmidhuber, 1997) that predicts whether
the next time step will yield a crossing or not.
This recurrent predictor is trained offline from
trajectories collected over episodes: after each
episode, the agent‚Äôs sequence of observations
and actual crossing values is appended to a
dataset, which is then used to update the
LSTM model with supervised learning. At each
environment step, the trained LSTM outputs
a predicted crossing and an associated confi-
dence, which can be used to provide additional
shaping rewards or to detect ‚Äúuncertainty‚Äù states.
We use Proximal Policy Optimization (PPO)
(Schulman et al., 2017) via the Ray RLlib library
to train both agents simultaneously (Moritz et al.,
2018). Each agent‚Äôs policy network is an LSTM-
based model receiving the 4D observation de-
scribed above; training proceeds in episodic form,
with one episode lasting a fixed number of 100
steps. We now describe the various reward func-
tions we investigate.
A Drive For Understanding
Intrinsic rewards are mechanisms to encourage
specific behaviors from each agent. As described
above, each agent maintains an LSTM which pre-
dicts whether a crossing will occur in the next
timestep and outputs a confidenceùëù(ùëê).The drive
for understanding is modelled in the form of the
reward function shown in Figure 2.
ùëü(ùëí, ùëù)=
(
1 ‚àíùëù if ùëí = 0
ùëùùëõ if ùëí = 1 (1)
Figure 2|The artificial curiosity reward rewards policies
that minimize errors when certainty is low, but maximizes
errors when certainty is high. This is analogous to conduct-
ing scientific experiments by setting up a situation in which
a prediction is strong, and attempting to contradict that
prediction. Reward is shown on the y-axis, and certainty on
the x-axis.
5
Wanting to be Understood METHODS
If an agent‚Äôs predictor makes no error then the
policy is rewarded in proportion to the uncer-
tainty of that prediction. If the agent‚Äôs predictor
makes an error however, then there is an expo-
nentially increasing reward for this error as the
certainty of the prediction reaches a probability
of 1. This means the policy is rewarded, once the
predictor becomes quite good, for spending time
in regions of the environment which are expected
tobepredictablebutcancontinuetobesurprising.
Is such surprise sufficient to make agents prefer
to interact with each other as proposed elsewhere
(Lenay et al., 2011) (Friston et al., 2024)?
A Drive For Being Understood
A drive for being understood can be implemented
in various ways. We consider three interpreta-
tions of what it means to be understood.
To Imitate and Be Imitated-By
This intrinsic motivation mechanism encourages
imitation and being imitated by another. This
has been previously studied in the context of
simultaneous extrinsic rewards (Eccles et al.,
2019) in which the intrinsic motivation was to
minimize the difference between the objectively
determined behavioural trajectories of both
agents. Using such a measure of what it means
to imitate, but without a concomitant extrinsic
reward as they used, has the unfortunate null
behavioural attractor where both agents can
maximize reward by doing nothing. To avoid
this, we implement a reward signal based on the
observation of reciprocal action patterns within
the agents‚Äô recent interaction history. Unlike
Eccles et al. (2019) two distinct and reciprocal
imitation reward functions are utilized: promote
imitation and imitate.
Promoting Imitation: This function rewards an
agent when it actively creates a certain pattern
of crossings by its own active actions (actions 1
or 2) in the initial segment of its recent history,
which is then followed by a passively observed set
of crossings which are the same as it produced,
consistent with the possibility that the other
agent has imitated its recent behavior. This
encourages an agent to initiate actions that will
then be followed by the other agent‚Äôs imitation
of them.
Imitation Reward: Conversely, the imitation
function rewards an agent for passively observing
a pattern in the initial segment of its recent
history (i.e. whilst doing action 0) followed by
actively replicating that pattern using actions 1
or 2 in the subsequent segment. This function
rewards an agent for following the other agent‚Äôs
lead.
Both functions analyze the recent interaction
history of each agent, tracking actions and
crossing events. A reward is generated propor-
tionally to the frequency of observed reciprocal
action patterns, normalized by the number of
opportunities to observe those patterns. The
history array is 10 steps long, and the first 5
steps are compared to the second 5 steps. Only
crossing transitions (0->1, 1->0) are compared
in these two windows, not absolute crossing
states.
Influence and Impressionability
In a more general version of the above, we
give each agent a reward for maximizing the
recent contribution of the mutual information
between their recent actions and the subsequent
observations of the other, plus the mutual
information between their observations of the
other and their subsequent actions, see Figure 3.
Agents both want to be controlled and to control.
In other words, agents want to both influence
and be influenced by each other. This is similar to
the imitation reward, except instead of requiring
perfect imitation and imitation promotion, we
relax the requirement and only require high
mutual information between passively observed
crossings and actively generated crossings, and
vice versa.
To calculate their recent contribution to
mutual information, each agent maintains a
rolling buffer of the last 20 4-timestep chunks. At
6
Wanting to be Understood METHODS
(a) Agent0 is passive for two steps followed by
actively causing a crossing transition; recreating
the crossing transitions observed in the passive
phase.
(b) Agent1 is active and then passive, observing transi-
tions caused by agent0.
Figure 3|Mutual information is calculated (between two
pairs of crossing transitions) when either of two interac-
tion conditions are met. For passive‚Äì>active interactions an
agent must do two no-ops, then at least one op with an asso-
ciated crossing transition. For active‚Äì>passive interactions
an agent must do at least one op associated with a crossing
transition followed by two no-ops.
each timestep the observed crossings and action
is stored. The mutual information between
passively observed crossing events in the first 2
timesteps of the chunk and the actively produced
crossing events in the last 2 timesteps of the
chunk is calculated, over the whole rolling
buffer of 20 chunks. Dirichlet priors are added
(pseudo-counts in the frequency table of events
usedtocalculateMI)inordertopreventexquisite
sensitivity of reward to low counts, a factor
which can considerably influence the shaping of
the policy. This same calculation is then made
over just the first 15 chunks in the rolling history.
The Impressionability reward is calculated as the
difference between the MI calculated over the
full buffer and the MI calculated over a first 15
chunks of the buffer, to encourage the maximi-
sation of change in mutual information. This is
repeated in the reciprocal case of active crossings
followed by passively observed crossings, and
this is the Influence reward. Impressionability
and Influence rewards are summed and this
constitutes the full reward. This encourages the
agents to influence each other, and be influenced
by each other. Related work has rewarded agents
for having causal influence over the environment
(Seitzer et al., 2021), but does not consider a sym-
metric influence of the environment on the agent.
Finally, we investigate the effect of removing
one half of this drive, the drive to be impression-
able, and leaving only the drive to influence. In
this setting, both agents are trying to influence
as in Jaques et al. (2019) but have no intrinsic
reward to be influenced by another.
Sub-Reaction Time Anticipation
One sense in which I can know that you have
understood me is that your expectations of
what I will do next are correct, i.e that you can
anticipate my actions before you could have had
time to respond to them reactively. For example
if there is a delay of 2 timesteps between your
observation and your action, and you are able to
maximize the mutual information between your
observations of me and your responses to me,
faster than in 2 timesteps, then this must mean
your policy had, in an implicit sense, anticipated
the observations two timesteps before your
action, and produced your actions in anticipation
to fit with those events before they could have
been actually observed.
For example, if I say "Would you like to come
... " and you complete my sentence "for dinner",
starting just after I say "co.." then I know that
you have anticipated me asking you for dinner.
Therefore I know what you know about me, by
virtue of the fact that you have demonstrated
this knowledge of what I was to say. If I am
also rewarded for producing such anticipations
in you, then I will act so as to allow you to
anticipate me better. Therefore, we simply add a
2 timestep action delay to both agents between
their observation and their action, keeping the
7
Wanting to be Understood RESULTS
same ‚ÄúInfluence and Impressionability‚Äù intrinsic
motivation function above.
Benefit of Intrinsic Drive to be Understood
Finally we address the question, if the agents are
required to complete a task in which cooperation
is necessary, but only one agent gets an extrinsic
reward, can the existence of intrinsic motivations
for coordination help? Such a task is harder than
a standard Lewis Language Game because there
is no top down reward for both agents on comple-
tion. The task is as follows. Agent0 is given a new
observation +1 or -1 randomly determined for
each episode. If this signal bit is +1 then agent0
gets +1 units of reward for each timestep during
that episode when agent1 is in the top half of
the space (0.5 to 1.0) but gets punished by -1 for
each timestep agent1 is in the bottom half (0.0
to 0.5) of the space. The opposite is true if the
signal bit = -1. However, agent1 gets no reward
for being anywhere, irrespective of the signal bit
setting. Finally, we remove the Impressionability
component of the mutual information reward and
observe whether this interferes with the extrinsic
reward task.
Results
A Drive to Understand
In this control we consider only the drive to un-
derstand (artificial curiosity) and not the drive to
be understood, see Figure 2. Figure 4 shows the
mean and standard deviation of results averaged
over 5 independent runs for each reward setting.
The returns and crossing proportions over a typi-
cal developmental trajectory over 2000 episodes
of experience is shown in Figure 5.
There is no preference for interaction with
the other (solid blue line on the lower graph).
Rewards are initially dominated by reducing
errors, but quickly become dominated by creating
errors in situations of certainty. There is a brief
period around 200 iterations where self-other
interactions are strongly preferred. Three typical
episodes are shown in Figure 6 which shows
that cumulative rewards increase more rapidly
when agents are interacting with the other or the
other‚Äôs shadow. In all cases, agents like to make
predictable crossings with an object, moving
away from that object, and coming back to the
object for repeated interactions with it.
A Drive to Be Understood
To Imitate and Be Imitated By
We consider here only the drive to explicitly
imitate and be imitated-by. Figure 7 shows
the returns and crossing proportions over a
typical developmental trajectory, notably that
self-other crossings predominate. Figure 8 shows
a sample of 3 episodes from the end of the
run. Self-other association consists of periods
of repeated crossing and uncrossing with very
stereotyped patterns. Limited reward can be
achieved by crossing a shadow. Interacting with a
stationary object gives no opportunity for reward.
Influence and Impressionability
We consider here only the drive for influence
and impressionability. Figure 9 shows the
returns and crossing proportions over a typical
developmental trajectory, notably that self-other
crossings predominate. Figure 11 shows a
sample of 3 episodes progressing through the
run. Self-other association consists of periods
of repeated crossing and uncrossing but with
much less stereotyped patterns than before.
Again, limited reward can be achieved by
crossing a shadow, whilst interacting with a
stationary object gives no opportunity for reward.
Reward does not monotonically increase with the
number of self-other crossings however; it is the
mutual information that is critical, and so more
systematic self-other interactions are preferred
to just any random self-other interaction.
Surprisingly, Figure 12 shows that removing
half of this drive, the drive to be impressionable,
and leaving only the drive to influence does
not in this case reduce the number of self-other
crossings compared to where both influence
8
Wanting to be Understood RESULTS
Figure 4|Mean and standard deviation of rewards and crossing frequencies averaged over both agents over 5 runs.
0 250 500 750 1000 1250 1500 1750 2000
Training Iteration
0
10
20
30
40
50Mean Episode Return
Mean Returns Over Training
Agent 0 Return
Agent 1 Return
Agent 0 Uncert
Agent 1 Uncert
Agent 0 Error
Agent 1 Error
0 250 500 750 1000 1250 1500 1750 2000
Training Iteration
0
10
20
30
40Count of Crossings
Crossing Proportions
A0 other_agent
A0 other_shadow
A0 fixed_object
A1 other_agent
A1 other_shadow
A1 fixed_object
Figure 5|Artificial Curiosity Reward: Artificial curiosity
does not produce an overall preference for interaction with
each other. Agents actively explore both
and impressionability terms contribute to the
reward as in Figure 4. This is because agents
learn to take it in turns to obtain reward, first
agent0 influences (or is influenced by) agent1,
and then agent1 influences (or is influenced by)
agent0. Also, in the PCP, agent0 influencing 1
makes it more likely that agent1 can influence
agent0, because they are closer together, and so
a separate drive to be influenced does not seem
to be needed4
Sub-Reaction Time Anticipation
Even when a delay of 2 timesteps is introduced
for both agents between observation and action,
the same reward function can be optimized, with
agents preferring self-other crossings, see Figure
10 and Figure 13. Both agents learn to move
around the circle together, occasionally stopping,
and catching up with the other, intermittently.
Impact on an Extrinsically Motivated Task
Figure 14 demonstrates that when the mutual
information reward is coupled with an extrinsic
4However, one might imagine that in a single shot non-
repeated communication, or where byx influencing y , y is
somehow prevented from influencingx, then both compo-
nents of the drive would seem to be needed, but we leave
these speculations for further work, although see results for
the extrinsic reward task in Figure 15.
9
Wanting to be Understood DISCUSSION AND CONCLUSION
Figure 6|Artificial Curiosity Reward: Three typical episodes from iteration 2100. Self-other crossing (red), self-shadow
crossing (green), self-stationary crossing (blue). Cumulative reward (dotted lines on bottom graph) increases more when
agents are in more unpredictable interactions e.g. with the shadow or with the other directly.
reward given to only agent0 for signalling to
agent1 that they should be in one of the two
halves of the environment for a given episode,
that agent0 can learn to reward agent1 through
interaction, in order to manipulate/guide agent1
to the correct region. On the other hand, without
intrinsic reward capability, agent1 does not
cooperate.
While impressionability rewards were not
needed to establish self-other interactions (Figure
12), removing them reduces performance in the
extrinsic reward task, see Figure 15 compared to
Figure 16 and 14. If both agents seek only influ-
ence, then agent0 finds it harder to preferentially
influence agent1 to go to the correct half of the
line based on agent0‚Äôs private information. So
in short, this investigation supports the principle
that to be impressionable is useful when some
extrinsic reward is to be had by being influenced
by the sender (in this case the benefit is to the
sender), but where influencing the sender is not
necessary or desirable. Wanting to be impression-
able is useful when gaining extrinsic reward de-
pends on an asymmetric interaction where there
is a learner and a teacher.
Discussion and Conclusion
All three examples of intrinsic motivations for
coordinative engagement successfully resulted
in agents spending more time interacting with
each other than with inanimate objects or with
each other‚Äôs shadows. Artificial curiosity alone
did not result in agents preferring to interact
with each other. This is because it is possible to
find policies for interacting with the environment
that can provide surprising outcomes due to
the fact that one‚Äôs predictor is always limited
in some way, such that a policy can always
uncover its inadequacies at the boundaries of its
effectiveness. However, rewards for wanting to
be understood depend on reciprocal interactions
which can only be achieved by interacting with
another agent who has the reciprocal intention,
e.g. imitating and wanting to be imitated, or
influencing and wanting to be influenced, or
controlling and wanting to be controlled, and
this cannot be achieved typically with inanimate
objects.
The two agents learn an other-specific policy,
which is not expected to generalize to other
agents. To achieve an agent-general policy we
could choose 2-of-N agents to play with each
10
Wanting to be Understood DISCUSSION AND CONCLUSION
0 100 200 300 400
Training Iteration
0
10
20
30
40
50
60Mean Episode Return
Mean Returns Over Training
Agent 0 Return
Agent 1 Return
Agent 0 Uncert
Agent 1 Uncert
Agent 0 Error
Agent 1 Error
0 100 200 300 400
Training Iteration
5
10
15
20
25
30Count of Crossings
Crossing Proportions
A0 other_agent
A0 other_shadow
A0 fixed_object
A1 other_agent
A1 other_shadow
A1 fixed_object
Figure 7|Imitation: The thick blue line shows the
frequency of self-other crossings far exceeds that
of all other kinds of crossing with this intrinsic
motivation.
other randomly at the start of each episode. This
is left for further work. if this is done, it would
be interesting to test the learned general policies
in interaction with real humans on the PCP,
if the algorithm can be implemented without
significant lag.
The mutual information calculation used to
determine influence and impressionability is
somewhat contrived in this setup, meaning
that for example, causal influence must be
temporally tightly sequential, whereas we are
happy if someone imitates us with a considerable
delay. Also the features that count towards
the mutual information calculation are hard
coded. An alternative would be to calculate
the mutual information present in the hidden
layers of an inverse model of the type used in
self-supervised prediction (Pathak et al., 2017).
The current setup assumes agents have already
worked out how to tell which observations are
caused by them, and which are caused by the
other. MI calculation does not take into account
observations which could only be caused by
both agents acting together, e.g. crossing state
1 persisting although one is moving right for
>2 timesteps, can only be possible if the other
agent is moving with you. Whilst identifying
a behavioural trajectory of this kind would be
sufficient to infer a self-other type of interaction,
the existing reward functions cannot detect the
significance of such behaviour.
All the rewards considered result in the
emergent property that behaviours of agent x
can reward those of agent y and vice versa. This
provides a rich set of ways in which agents can
influence each other. When this is achieved,
extrinsic tasks in which the other is not rewarded
can come within the cooperative domain,
because one can give the other a reward for
acting in a way which would be beneficial to only
oneself. In other words a new language game
can be created. We have not explored the effects
of mismatch between how mutual information
is calculated between the two agents, but leave
this for further work.
So far we have not allowed the intrinsic reward
function to have direct access to hidden neural
states. However, a relaxation of this constraint,
which is entirely biologically plausible, allows
for example, attractors in embeddings created
in the hidden layers of a neural network by
unsupervised learning processes to be available
for behavioural expression, e.g. as utterances
which may help the other to gain access to
the insights of the discoverer. This much more
direct way of communicating understanding
for the benefit of the learning systems of the
other will be studied in a subsequent paper,
and is expected to involve mutual information
calculations between performances or markings
produced in the world, their referents, and the
other‚Äôs response to both. In the meantime we
have shown a plausible outline of an intrinsic
motivation function for primary intersubjectivity.
11
Wanting to be Understood DISCUSSION AND CONCLUSION
Figure 8|Imitation: Over the course of the run, agents learn that most reward can be achieved by imitating and being
imitated by with the other.
0 500 1000 1500 2000
Training Iteration
0
10
20
30
40
50Mean Episode Return
Mean Returns Over Training
Agent 0 Return
Agent 1 Return
Agent 0 Uncert
Agent 1 Uncert
Agent 0 Error
Agent 1 Error
0 500 1000 1500 2000
Training Iteration
5
10
15
20
25
30Count of Crossings
Crossing Proportions
A0 other_agent
A0 other_shadow
A0 fixed_object
A1 other_agent
A1 other_shadow
A1 fixed_object
Figure 9|Influence and Impressionability: The thick blue
line shows the frequency of self-other crossings far exceeds
that of all other kinds of crossing with this intrinsic motiva-
tion. However, the increase in reward is not monotonically
related to the number of self-other crossings which actu-
ally decreases by 1500 iterations as the episode returns are
increasing for both agents.
0 200 400 600 800 1000 1200 1400 1600
Training Iteration
0
10
20
30
40
50
60Mean Episode Return
Mean Returns Over Training
Agent 0 Return
Agent 1 Return
Agent 0 Uncert
Agent 1 Uncert
Agent 0 Error
Agent 1 Error
0 200 400 600 800 1000 1200 1400 1600
Training Iteration
5
10
15
20
25
30Count of Crossings
Crossing Proportions
A0 other_agent
A0 other_shadow
A0 fixed_object
A1 other_agent
A1 other_shadow
A1 fixed_object
Figure 10 |Sub-reaction time anticipation: Self-other
crossings are strongly preferred in this case.
12
Wanting to be Understood DISCUSSION AND CONCLUSION
Figure 11|Influence and Impressionability: Over the course of the run, agents learn that most reward can be achieved
by imitating and being imitated by with the other.
Figure 12|In the absence of a drive for being impressionable, leaving only the drive to be imitated, self-other interaction
is as prevalent as it was before.
13
Wanting to be Understood DISCUSSION AND CONCLUSION
Figure 13|Sub-reaction time anticipation: Reward is less than in with Influence and Impressionability reward without
delay, however, the number of self-other crossings is greater.
Figure 14|The top two rows shows the training curves and 4 example episodes for the extrinsic reward task without
intrinsic reward. The bottom two rows show the same, but with intrinsic reward as well. With intrinsic reward agent1
successfully goes to the top as desired by agent0 in the green (+1) tasks and to the bottom in the red (-1) tasks. No such
influence is possible without the mutual information reward, and so in the top two rows agent1 does not cooperate.
14
Wanting to be Understood DISCUSSION AND CONCLUSION
Figure 15|Agent0‚Äôs reward is in red, and Agent1‚Äôs reward is in blue, averaged over 5 independent runs. Agents do not
spend much time associating with each other (green line in graph on right) compared to when both components of the
mutual information reward are present.
Figure 16|Agent0‚Äôs reward is in red, and Agent1‚Äôs reward is in blue, averaged over 5 independent runs. In this case, the
full paired influence and impressionability rewards are given.
15
Wanting to be Understood REFERENCES
Acknowledgements
Thanks to Tom Froese, Ben Moran, Tim Rock-
t√§schel, Wil Cunningham, Igor Mordatch, Emily
Nicholls, and Vera Vasas.
References
M.AuvrayandM.Rohde. Perceptualcrossing: the
simplest online paradigm.Frontiers in human
neuroscience, 6:181, 2012.
M.Auvray, C.Lenay, andJ.Stewart. Perceptualin-
teractions in a minimalist virtual environment.
New ideas in psychology, 27(1):32‚Äì47, 2009.
J. M. Baldwin. A new factor in evolution.The
American Naturalist, 30(354):441‚Äì451, 1896.
D. Bolis, J. Balsters, N. Wenderoth, C. Becchio,
and L. Schilbach. Beyond autism: Introducing
the dialectical misattunement hypothesis and
a bayesian account of intersubjectivity. Psy-
chopathology, 50(6):355‚Äì372, 2018.
R. Boyd and P. J. Richerson.Culture and the Evo-
lutionary Process. University of Chicago Press,
Chicago, IL, 1985.
D. J. Chalmers.The conscious mind: In search of a
fundamental theory. Oxford Paperbacks, 1997.
A. Clark and D. J. Chalmers. The extended mind.
Analysis, 58(1):7‚Äì19, 1998.
C. L. Crown, S. Feldstein, M. D. Jasnow, B. Beebe,
andJ.Jaffe. Thecross-modalcoordinationofin-
terpersonal timing: Six-week-olds infants‚Äô gaze
withadults‚Äôvocalbehavior. Journalofpsycholin-
guistic research, 31:1‚Äì23, 2002.
G. Declerck, C. Lenay, and A. Khatchatourov. Ren-
dre tangible le visible.Irbm, 30(5-6):252‚Äì257,
2009.
D. C. Dennett.The intentional stance. MIT press,
1989.
E. A. Di Paolo, M. Rohde, and H. Iizuka. Sensi-
tivity to social contingency or stability of inter-
action? modelling the dynamics of perceptual
crossing. New Ideas in Psychology, 26(2):278‚Äì
294, 2008.
T. Eccles, E. Hughes, J. Kram√°r, S. Wheelwright,
andJ.Z.Leibo. Learningreciprocityincomplex
sequential social dilemmas, 2019. URLhttps:
//arxiv.org/abs/1903.08082.
C. Fernando, S. Osindero, and D. Banarse. The
origin and function of external representations.
Adaptive Behavior, page 10597123241262534,
2024.
K. J. Friston, L. Da Costa, A. Tschantz, A. Kiefer,
T. Salvatori, V. Neacsu, M. Koudahl, C. Heins,
N. Sajid, D. Markovic, et al. Supervised
structure learning.Biological Psychology, 193:
108891, 2024.
T. Froese and E. A. Di Paolo. Modelling social
interaction as perceptual crossing: an investi-
gation into the dynamics of the interaction pro-
cess. Connection Science, 22(1):43‚Äì68, 2010.
B. Heasman and A. Gillespie. Neurodivergent
intersubjectivity: Distinctive features of how
autistic people create shared understanding.
Autism, 23(4):910‚Äì921, 2019.
C. Heyes. Cognitive Gadgets: The Cultural Evo-
lution of Thinking. Harvard University Press,
2018.
P. Hobson.The cradle of thought: Exploring the
origins of thinking. Pan Macmillan, 2004.
S. Hochreiter and J. Schmidhuber. Long short-
term memory.Neural Computation, 9(8):1735‚Äì
1780, 1997. doi: 10.1162/neco.1997.9.8.
1735.
H. Iizuka and E. A. Di Paolo. Minimal agency
detection of embodied agents.Lecture Notes in
Computer Science, 4648:485‚Äì494, 2007.
F. Jackson. Epiphenomenal qualia.The Philosoph-
ical Quarterly, 32(127):127‚Äì136, 1982. doi:
10.2307/2960077.
N. Jaques, A. Lazaridou, E. Hughes, C. Gul-
cehre, P. A. Ortega, D. Strouse, J. Z. Leibo,
and N. de Freitas. Social influence as intrinsic
motivation for multi-agent deep reinforcement
learning, 2019. URL https://arxiv.org/
abs/1810.08647.
16
Wanting to be Understood REFERENCES
F. Kaplan and V. V. Hafner. The challenges of joint
attention. Interaction Studies, 7(2):135‚Äì169,
2006.
H. Kojima, T. Froese, M. Oka, H. Iizuka, and
T. Ikegami. A sensorimotor signature of the
transition to conscious social perception: co-
regulationofactiveandpassivetouch. Frontiers
in Psychology, 8:1778, 2017.
G. Kugiumutzakis and C. Trevarthen. Neonatal
imitation. International encyclopedia of the so-
cial & behavioral sciences, 16:481‚Äì488, 2015.
A. Lazaridou and M. Baroni. Emergent multi-
agent communication in the deep learning era.
arXiv preprint arXiv:2006.02419, 2020.
C. Lenay, J. Stewart, M. Rohde, and A. A. Amar.
‚Äúyou never fail to surprise me‚Äù: The hallmark of
the other: Experimental study and simulations
of perceptual crossing.Interaction Studies, 12
(3):373‚Äì396, 2011.
S. Lerique. Embodied rationality through game
theoretic glasses: An empirical point of contact.
Frontiers in Psychology, 13:815691, 2022.
D. Lewis.Convention: A Philosophical Study. Har-
vard University Press, Cambridge, MA, 1969.
E. Liang, R. Liaw, R. Nishihara, P. Moritz, R. Fox,
K. Goldberg, J. E. Gonzalez, M. I. Jordan, and
I. Stoica. Rllib: Abstractions for distributed re-
inforcement learning.Proceedings of the 35th
International Conference on Machine Learning
(ICML), 2018. URL https://arxiv.org/
abs/1712.09381.
S. McGregor and C. Fernando. Levels of descrip-
tion: A novel approach to dynamical hierar-
chies. Artificial life, 11(4):459‚Äì472, 2005.
A. N. Meltzoff and M. K. Moore. Imitation in
newborn infants: Exploring the range of ges-
tures imitated and the underlying mechanisms.
Developmental psychology, 25(6):954, 1989.
D. E. Milton. On the ontological status of autism:
The ‚Äòdouble empathy problem‚Äô. Disability &
Society, 27(6):883‚Äì887, 2012. doi: 10.1080/
09687599.2012.710008.
S. Moeller, A. M. Unakafov, J. Fischer, A. Gail,
S. Treue, and I. Kagan. Human and macaque
pairs employ different coordination strategies
in a transparent decision game. Elife, 12:
e81641, 2023.
R. Moore. Gricean communication and cognitive
development. The Philosophical Quarterly, 67
(267):303‚Äì326, 2017.
P. Moritz, R. Nishihara, S. Wang, A. Tu-
manov, R. Liaw, E. Liang, M. Elibol, Z. Yang,
W. Paul, M. I. Jordan, and I. Stoica. Ray:
A distributed framework for emerging AI
applications. In 13th USENIX Symposium on
Operating Systems Design and Implementation
(OSDI 18), pages 561‚Äì577, 2018. URL
https://www.usenix.org/conference/
osdi18/presentation/moritz.
L.Murray. Emotionalregulationofinteractionsbe-
tween two-month-olds and their mothers.So-
cial perception in infants, pages 177‚Äì197, 1985.
A. No√´.The Entanglement: How Art and Philoso-
phy Make Us What We Are. Princeton University
Press, 2023. ISBN 9780691188812. URL
https://books.google.co.uk/books?
id=fJCkEAAAQBAJ.
OpenAI. Gpt-4 technical report, 2023.
D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell.
Curiosity-driven exploration by self-supervised
prediction. In Proceedings of the IEEE Confer-
enceonComputerVisionandPatternRecognition
Workshops (CVPRW), 2017.
V. Reddy, G. Markova, and S. Wallot. Antici-
patory adjustments to being picked up in in-
fancy. PLOS ONE, 8(6):e65289, 2013. doi:
10.1371/journal.pone.0065289.
M. Rita, C. Tallec, P. Michel, J.-B. Grill,
O. Pietquin, E. Dupoux, and F. Strub. Emergent
communication: Generalization and overfitting
in lewis games.Advances in neural information
processing systems, 35:1389‚Äì1404, 2022.
H. Sacks, E. A. Schegloff, and G. Jefferson. A
simplest systematics for the organization of
turn-taking for conversation.Language, 50(4):
696‚Äì735, 1974.
17
Wanting to be Understood REFERENCES
O. Sacks.Seeing voices: A journey into the world
of the deaf. Pan Macmillan, 2009.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford,
and O. Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347,
2017. URLhttps://arxiv.org/abs/1707.
06347.
T. Scott-Phillips. Speaking Our Minds: Why
Human Communication is Different, and how
Language Evolved to Make it Special. Macmillan
Education UK, 2015. ISBN 9781137334572.
URL https://books.google.co.uk/
books?id=d09GEAAAQBAJ.
M. Seitzer, B. Sch√∂lkopf, and G. Martius. Causal
influence detection for improving efficiency in
reinforcement learning.Advances in Neural In-
formationProcessingSystems ,34:22905‚Äì22918,
2021.
M. Shanahan. Simulacra as conscious exotica.
arXiv preprint arXiv:2402.12422, 2024.
L. Steels. The synthetic modeling of language
origins. Evolution of Communication, 1(1):1‚Äì
34, 1997.
L. Steels. The talking heads experiment. 1999.
D. N. Stern.The interpersonal world of the infant:
A view from psychoanalysis and developmental
psychology. Routledge, 2018.
M. Tomasello.Origins of Human Communication.
MIT Press, 2008.
M. Tomasello. The evolution of agency, 2022.
M. Tomasello, A. C. Kruger, and H. H. Ratner.
Culturallearning. BehavioralandBrainSciences ,
16(3):495‚Äì511, 1993.
M. Tomasello, M. Carpenter, J. Call, T. Behne, and
H. Moll. Understanding and sharing intentions:
The origins of cultural cognition.Behavioral
and Brain Sciences, 28(5):675‚Äì691, 2005.
C. Trevarthen and P. Hubley. Secondary inter-
subjectivity: Confidence, confiding and acts
of meaning in the first year. In A. Lock, edi-
tor,Action, gesture and symbol: The emergence
of language, pages 183‚Äì229. Academic Press,
London, UK, 1978.
C. Trevarthen et al. Communication and coop-
eration in early infancy: A description of pri-
mary intersubjectivity. Before speech: The be-
ginning of interpersonal communication, 1:530‚Äì
571, 1979.
E. Tronick, H. Als, L. Adamson, S. Wise, and T. B.
Brazelton. The infant‚Äôs response to entrapment
betweencontradictorymessagesinface-to-face
interaction. Journal of the American Academy
of Child psychiatry, 17(1):1‚Äì13, 1978.
F. J. Varela, E. Thompson, and E. Rosch.The
embodied mind: Cognitive science and human
experience. MIT press, 1991.
L. S. Vygotsky.Thought and language. MIT press,
2012.
F. Warneken and M. Tomasello. Altruistic help-
ing in human infants and young chimpanzees.
science, 311(5765):1301‚Äì1303, 2006.
F. Warneken and M. Tomasello. Helping and
cooperation at 14 months of age.Infancy, 11
(3):271‚Äì294, 2007.
F. Warneken, F. Chen, and M. Tomasello. Coop-
erative activities in young children and chim-
panzees. Child development, 77(3):640‚Äì663,
2006.
L. Wittgenstein. Philosophical Investigations.
Blackwell, 1953.
18