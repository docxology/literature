bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
Transferring ac(cid:12)ve inference to newly encountered prism-
1
shi(cid:25)ed environments
2
3
4 Megumi Yoshihara1,2*, Takuya Isomura2*
5
6 1 Graduate School of Informa!cs, Kyoto University, 36-1 Yoshidahonmachi, Sakyo-ku, Kyoto 606-
7 8501, Japan
8 2 Brain Intelligence Theory Unit, RIKEN Center for Brain Science, 2-1 Hirosawa, Wako, Saitama 351-
9 0198, Japan
10 * Corresponding authors emails: megumi.yoshihara@riken.jp, takuya.isomura@riken.jp
11
12
1
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
13 Abstract
14 Ac!ve inference has been proposed as a unified explana!on of percep!on and ac!on. Previous
15 work has shown that canonical neural networks that minimize shared Helmholtz energy are cast as
16 performing ac!ve inference of the external environment. However, how animals flexibly adapt to
17 newly encountered environments remains to be fully addressed. To inves!gate the brain’s
18 generalizability and adaptability to new environments, this work develops canonical neural
19 networks that employ mul!ple policy matrices in parallel. We demonstrate that the proposed
20 model can recapitulate the prism adapta!on—a form of visuomotor adapta!on—under an arm-
21 reaching task. Using policy matrices pretrained under various target posi!ons, these networks
22 could transfer previous experiences and exhibit faster adapta!on to the prism-shiBed environment
23 than the naive networks. Furthermore, aBer-effects were observed following the removal of
24 simulated prism glasses. These results suggest the biological plausibility and u!lity of the proposed
25 model, providing insights into the adap!ve capabili!es of the brain.
26
27 Keywords: Free-energy principle, Prism adapta!on, Ac!ve inference, POMDP, Transfer learning
28
29 INTRODUCTION
30 The brain exhibits great adaptability to newly encountered environments, and modeling it using
31 neural networks remains a significant challenge in neuroscience. The free-energy principle (FEP)
32 (Friston et al., 2006; Friston, 2010) has been proposed to account for percep!on (Lanillos et al.,
2
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
33 2021; Mirza et al., 2021), learning, and ac!on(Klar et al., 2025; Lanillos et al., 2021) of biological
34 organisms in terms of Bayesian inference and the minimiza!on of varia!onal free energy. Biological
35 organisms are considered to develop the genera!ve model—a hypothesis about the dependence
36 structure between hidden states and sensory inputs—in the brain by minimizing varia!onal free
37 energy as a tractable proxy for minimizing the sensory surprise. By doing so, they perceive hidden
38 environmental states and op!mize their ac!ons accordingly to minimize the risk associated with
39 future outcomes, a process referred to as ac!ve inference (Da Costa et al., 2020; Friston et al.,
40 2017).
41 Notably, recent works have shown that canonical neural networks that minimize Helmholtz
42 energy can be read as following the FEP under a class of par!ally observable Markov decision
43 process (POMDP) models (Isomura et al., 2022). These networks can exhibit ac!ve inference under
44 various task seKngs, including causal inference (Isomura & Friston, 2020) rule learning (Tazawa &
45 Isomura, 2024), predic!on, and planning (Paul et al., 2024) These proper!es suggest that
46 recapitula!ng the external environment in the network's internal states is an inherent property of
47 neural networks.
48 Despite these successes, when encountering novel environments, these networks basically need
49 to learn a genera!ve model for new environments from scratch. This requires a considerable
50 training cost, and the generaliza!on error usually declines with a widely observed power-low order
51 of !me (i.e., ) (Has!e et al., 2009). By contrast, recent advances in machine learning suggest
(cid:4)(cid:5)
(cid:1)(cid:2)(cid:3) (cid:6)
52 that transferring previously learned models may facilitate these networks to adapta!on to new
3
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
53 environments (Kouw & Loog, 2018; Zhuang et al., 2020). However, whether canonical neural
54 networks can exhibit such transfer learning remains largely unexplored.
55 To inves!gate the neuronal mechanisms of adaptability and generalizability, prism adapta!on
56 serves as a representa!ve visuomotor phenomenon (Hermann & CaPell, 1898; Prablanc et al.,
57 2020). This can be observed, for instance, when subjects perform a reaching task while wearing
58 the prism glasses that shiB their visual field. Ini!ally, subjects wearing prism glasses struggle to
59 complete reaching tasks as predic!on misalignment causes an increase in reaching error; aBer a
60 few trials, however, they gradually learn to accurately point to targets with minimal errors,
61 demonstra!ng the adaptability to new environments. Upon removal of the prism glasses, ini!al
62 errors are made but are immediately recovered. This task requires the readjustment and
63 integra!on of visual and somatosensory informa!on to novel sensory-motor con!ngencies, making
64 it suited to model the brain’s generalizability using canonical neural networks.
65 In doing so, some observa!ons can be considered as a generalizability criterion: In healthy
66 human prism exposure, the adapta!on occurs within approximately 10 trials, and the reaching
67 error approaches approximately zero (Luauté et al., 2009; RosseK et al., 1993). This is a
68 remarkably low number compared to the number of !mes a person has ever reached, indica!ng
69 that biological brains can immediately correct their predic!ve discrepancies. However, previous
70 modeling works have yet to fully explain the neuronal mechanisms underlying this significant gap
71 (Sakaguchi et al., 2001; Smith et al., 2006; Inoue et al., 2015). Although Bayesian frameworks have
72 been proposed as a promising avenue for addressing this issue (Pe!tet et al., 2018), concrete
73 models that capture the underlying processes are s!ll lacking.
4
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
74 To address this issue, this work developed canonical neural networks that exhibit transfer
75 learning based on the FEP and ac!ve inference and applied them to explain the prism adapta!on.
76 We demonstrated that these neural networks can transfer previously learned policy matrices—
77 pretrained to generate ac!ons to reach various target posi!ons depending on observa!ons—to
78 facilitate adapta!on to a newly encountered environment. These networks exhibited the rapid
79 adapta!on in the presence of simulated prism glasses and the aBer-effects following the removal
80 of the prism glasses, as observed in empirical prism adapta!on. Mathema!cal analyses
81 demonstrated that the predic!on error for selec!ng the op!mal mixture of policy matrices
82 converges with an exponen!al order decay (i.e., with a coefficient ), which is
(cid:4)(cid:8)(cid:9)
(cid:1)(cid:2)(cid:7) (cid:6) (cid:10) > 0
83 considerably faster than the widely known order decay of generaliza!on errors for
(cid:4)(cid:5)
(cid:3)
84 conven!onal models. We conclude by discussing possible biological implementa!ons of these
85 learning mechanisms.
86
87 METHODS
88 Canonical neural networks
89 We began by outlining canonical neural networks (Isomura et al., 2022; Isomura & Friston, 2020)
90 with a par!cular focus on modeling arm reaching tasks. We defined a two-layer canonical neural
91 network (Fig. 1) of rate-coding models comprising middle (cid:21) and output
(cid:13)(cid:2)(cid:3)(cid:6) ≔ (cid:15)(cid:13)(cid:5)(cid:2)(cid:3)(cid:6),…,(cid:13)(cid:18)(cid:19) (cid:2)(cid:3)(cid:6)(cid:20)
92 (cid:21) layers. On receiving sensory inputs (cid:21),
(cid:22)(cid:2)(cid:3)(cid:6) ≔ (cid:23)(cid:22)(cid:5)(cid:2)(cid:3)(cid:6),…,(cid:22)(cid:18)(cid:24) (cid:2)(cid:3)(cid:6)(cid:25) (cid:26)(cid:2)(cid:3)(cid:6) ≔ (cid:15)(cid:26)(cid:5)(cid:2)(cid:3)(cid:6),…,(cid:26)(cid:18)(cid:27) (cid:2)(cid:3)(cid:6)(cid:20) (cid:13)(cid:2)(cid:3)(cid:6)
93 yields the neural dynamics and propagates it to ; then, generates ac!ons of the agent,
(cid:22)(cid:2)(cid:3)(cid:6) (cid:22)(cid:2)(cid:3)(cid:6)
5
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
94 that is, a mo!on in either up, down, leB, or right direc!on in a grid world. These neural ac!vi!es
95 are provided as follows:
96
(cid:4)(cid:5)
(cid:13)(cid:28)(cid:2)(cid:3)(cid:6) ∝ (cid:30)sig "(cid:13)(cid:2)(cid:3)(cid:6)#$(cid:2)%(cid:5) (cid:30)%&(cid:6)(cid:26)(cid:2)(cid:3)(cid:6)$(cid:2)’(cid:5) (cid:30)’&(cid:6)(cid:13)(cid:2)(cid:3)(cid:30)((cid:3)(cid:6)$ℎ(cid:5) (cid:30)ℎ& (cid:2)1(cid:6)
97
(cid:4)(cid:5)
(cid:22)(cid:28)(cid:2)(cid:3)(cid:6) ∝ (cid:30)sig "(cid:22)(cid:2)(cid:3)(cid:6)#$(cid:2)+(cid:5) (cid:30)+&(cid:6)(cid:2)(cid:13)(cid:2)(cid:3)(cid:30)((cid:3)(cid:6)⊗-(cid:6)$.(cid:5) (cid:30).& (cid:2)2(cid:6)
98 where (cid:21) denotes firing intensi!es of modulator neurons encoding mixture
- ≔ "-(cid:5),…,-(cid:18)0 #
99 parameters; and , are synap!c weight
(cid:18)(cid:19)3(cid:18)(cid:27) (cid:18)(cid:19)3(cid:18)(cid:19) (cid:18)(cid:19)3(cid:18)(cid:19)(cid:18)0
%(cid:5),%& ∈ ℝ ’(cid:5),’& ∈ ℝ +(cid:5),+& ∈ ℝ
100 matrices; and are the adap!ve
ℎ(cid:5) ≔ ℎ(cid:5)(cid:2)%(cid:5),’(cid:5)(cid:6),ℎ& ≔ ℎ&(cid:2)%&,’&(cid:6),.(cid:5) 4 .(cid:5)(cid:2)+(cid:5)(cid:6),.& 4 .&(cid:2)+&(cid:6)
101 firing thresholds that depend on synap!c strengths. Here, represents
(cid:2)+(cid:5) (cid:30)+&(cid:6)(cid:2)(cid:13)(cid:2)(cid:3) (cid:30)((cid:3)(cid:6)⊗-(cid:6)
102 synap!c inputs from the middle layer that are modulated by .
-
103 Without loss of generality, Equa!ons (1) and (2) can be derived as a gradient descent on
104 Helmholtz energy . The func!onal form of can be iden!fied by compu!ng the integral of the
5 5
105 right-hand side of Equa!ons (1) and (2), as follows:
106 (cid:9) T
(cid:13)(cid:2)7(cid:6) (cid:13)(cid:2)7(cid:6) %(cid:5) ’(cid:5) ℎ(cid:5)
5 4 6 (cid:23) (cid:25) :ln(cid:23) (cid:25)(cid:30)(cid:23) (cid:25)(cid:26)(cid:2)7(cid:6)(cid:30)(cid:23) (cid:25)(cid:13)(cid:2)7(cid:30)((cid:3)(cid:6)(cid:30)(cid:23) (cid:25)=>7
&
(cid:13)̅(cid:2)7(cid:6) (cid:13)̅(cid:2)7(cid:6) %& ’& ℎ&
107 (cid:9) T
(cid:22)(cid:2)7(cid:6) (cid:22)(cid:2)7(cid:6) +(cid:5) .(cid:5)
$6 (cid:23) (cid:25) :ln(cid:23) (cid:25)(cid:30)"1(cid:30)2A(cid:2)7(cid:6)#(cid:23) (cid:25)(cid:2)(cid:13)(cid:2)7(cid:30)((cid:3)(cid:6)⊗-(cid:6)(cid:30)(cid:15) (cid:20)=>7$B (cid:2)3(cid:6)
&
(cid:22)@(cid:2)7(cid:6) (cid:22)@(cid:2)7(cid:6) +& .&
108 The form of is equivalent to the varia!onal free energy under a class of POMDPs (Isomura et al.,
5
109 2022). This indicates that the dynamics of canonical neural networks are equivalent to Bayesian
110 Belief upda!ng under a class of POMDPs, suppor!ng the validity of the free energy principle as a
111 universal characteriza!on of neural dynamics and self-organiza!on. Notably, a gradient descent on
112 the shared Helmholtz energy derives the synap!c plas!city in the output-layer synap!c weights V,
113 which is expressed as neuromodula!on of Hebbian plas!city in the form of a three-factor learning
114 rule (Frémaux & Gerstner, 2016; Kuśmierz et al., 2017; Pawlak et al., 2010) (Fig. 1A). This formally
6
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
115 corresponds to the update of policy matrix to minimize future risks (Isomura et al., 2022). Based on
116 this founda!on, hereaBer we formulated prism adapta!on in terms of POMDPs (that correspond
117 to canonical neural networks) following the FEP.
118
119
120 Fig. 1. Schema(cid:12)c of canonical neural networks performing ac(cid:12)ve inference. A. Canonical neural
121 network architecture for performing an arm reaching task (top) and possible neuronal
122 implementa!on of policy learning in terms of the three-factor learning rule (boPom). Middle layer
123 ac!vity is generated by sensory inputs, and that carry informa!on about target and hand
(cid:9) E
(cid:13)D (cid:26)D (cid:26)D
124 posi!ons. Ac!on is generated by using the tensor of policy matrices, yielding a hand
FGHI (cid:13)D
125 mo!on in the grid world. B. A POMDP model for reaching tasks expressed in the form of a
7
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
126 graphical model. The lower part formally corresponds to the canonical neural network shown in
127 (A). This architecture enables transfer learning by selec!ng C from the pool via aPen!onal switch
128 . Network weights and are updated by using , , and risk . C. Generaliza!on errors in and .
J J K F A L J
129 D. Engaged view of (C) with semi-log scale.
130
131 Corresponding genera(cid:12)ve models
132 The behavior of an agent interac!ng with a discrete state space is expressed in the form of a
133 POMDP as follows: The environment comprises binary sensory inputs (observa!ons)
(cid:26)D 4
134 (cid:5) (cid:18)(cid:27) (cid:21) (cid:18)(cid:27) and hidden states (cid:5) (cid:18)P (cid:21) (cid:18)(cid:27) , and their sequences are
"(cid:26)D,…,(cid:26)D # ∈ M0,,1N OD 4 "OD,…,OD # ∈ M0,,1N
135 denoted as and . Here, hidden states involve a factorial structure
(cid:26)(cid:5):(cid:9) 4 M(cid:26)(cid:5),…,(cid:26)(cid:9)N O(cid:5):(cid:9) 4 MO(cid:5),…,O(cid:9)N
136 comprising two one-hot vectors of hand ( ) and target ( ) posi!ons, which is denoted as
E (cid:9)
OD OD OD 4
137 using the Kronecker product operator . Similarly, sensory inputs comprise
E (cid:9) E (cid:9)
OD ⊗OD ⊗ (cid:26)D 4 (cid:26)D ⊗(cid:26)D
138 a product of somatosensory input for hand posi!on and visual input for target posi!on .
E (cid:9)
(cid:26)D (cid:26)D
139 Ac!on or decision describes the mo!on of the agent’s hand, which can move up, down, leB, or
RD
140 right at each !me step. This makes the agent’s hand posi!on (sh) move one grid, but it will stay if it
141 reaches the end of the grid field.
142 Sensory inputs are generated based on hidden states in the form of a categorical
(cid:26)D OD
143 distribu!on through the likelihood matrix . The dynamics of hidden
S(cid:2)(cid:26)D|OD,U(cid:6) 4 Cat(cid:2)UOD(cid:6) U
144 states depend on the previous state and ac!on, expressed as
S(cid:2)ODH(cid:5)|OD,RD,Y(cid:6) 4 Cat"Y(cid:2)RD ⊗OD(cid:6)#
145 using transi!on matrix . The agent’s ac!on is determined based on the policy matrix that
Y Z
146 defines the selec!on probability of ac!on depending on hidden states, .
S(cid:2)RDH(cid:5)|OD,Z(cid:6) 4 Cat(cid:2)ZOD(cid:6)
8
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
147 Here, a counterfactual learning of the policy matrix is adopted following the previous work
148 (Isomura et al., 2022). Under this seKng, the agent adopts a risk-dependent policy model
149
\]^_ ‘
\@@]@@^@@_@ to update , in which the agent learns to
S(cid:2)RDH(cid:5)|OD,[DH(cid:5),Z(cid:6) 4 Cat(cid:2)ZOD(cid:6) Cat"(cid:2)Z ⊘Z(cid:6)OD# Z
150 retain or enhance the current strategy if the risk is 0, whereas the agent forgets it if the risk is
[DH(cid:5)
151 1 (where denotes the sign-flipped ). Hence, the genera!ve model is defined
@[@D@@H@(cid:5)@ 4 1(cid:30)[DH(cid:5) [DH(cid:5)
152 as follows:
(cid:9)
153
S(cid:2)R(cid:5):(cid:9),O(cid:5):(cid:9),(cid:26)(cid:5):(cid:9),[(cid:5):(cid:9),b(cid:6) 4 S(cid:2)b(cid:6)S(cid:2)[(cid:9)(cid:6)cS(cid:2)(cid:26)D|OD,U(cid:6)S(cid:2)OD|OD(cid:4)(cid:5),RD(cid:4)(cid:5),Y(cid:6)S(cid:2)RD|OD(cid:4)(cid:5),[D,Z(cid:6) (cid:2)4(cid:6)
Dd(cid:5)
154 where is a set of parameters, . Prior beliefs about parameter matrices
b b 4 MU,Y,ZN S(cid:2)b(cid:6) 4
155 follow Dirichlet distribu!ons , , and
S(cid:2)U(cid:6)S(cid:2)Y(cid:6)S(cid:2)Z(cid:6) S(cid:2)U(cid:6) 4 Dir(cid:2)h(cid:6) i(cid:2)Y(cid:6) 4 Dir(cid:2)j(cid:6) i(cid:2)Z(cid:6) 4 Dir(cid:2)k(cid:6)
156 parameterized by Dirichlet parameters and .
h,j, k
157
158 Implementa(cid:12)on of transfer learning
159 In contrast to naive ac!ve inference agents defined above (i.e., naive learners), here we define
160 agent exhibi!ng transfer learning (i.e., transfer learners). Transfer learning (Zhuang et al., 2020)
161 enables the deployment of previously learned policy matrices to quickly adapt to newly
162 encountered environments, including the presence of prism exposure. In this work, we model
163 policy matrix using a mixture of policy matrices as a func!on of mixture parameter , as follows:
Z -
164
lnZ(cid:2)-(cid:6) 4 -(cid:5)lnZ(cid:5) $-llnZl $⋯$-(cid:18)n lnZ(cid:18)n (cid:2)5(cid:6)
165 Each fixed independent policy matrix defines the selec!on probability of hand ac!on in the k-th
Zp
166 context, and is a mixture weight parameter or aPen!onal switch that follows a categorical
-
9
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
167 distribu!on , analogous to a mixture of genera!ve models(Isomura et al., 2019). A set of
-~Cat(cid:2)Λ(cid:6)
168 parameter matrices are generated by pretraining using naive learners. In this formula!on,
MZpN
169 transfers learners employ the parameter set and update them instead of upda!ng
b 4 MU,Y,-N
170 .
bstuvw 4 MU,Y,ZN
171
172 Varia(cid:12)onal Bayesian inference
173 An approximate posterior distribu!on is defined using a mean-field approxima!on as
(cid:9)
174
i(cid:2)R(cid:5):(cid:9),O(cid:5):(cid:9),bnaive(cid:6) 4 i(cid:2)U(cid:6)i(cid:2)Y(cid:6)i(cid:2)Z(cid:6)ci(cid:2)OD(cid:6)i(cid:2)RD(cid:6) (cid:2)6(cid:6)
Dd(cid:5)
175 for naive learners and
(cid:9)
176
i(cid:2)R(cid:5):(cid:9),O(cid:5):(cid:9),b(cid:6) 4 i(cid:2)U(cid:6)i(cid:2)Y(cid:6)i(cid:2)-(cid:6)ci(cid:2)OD(cid:6)i(cid:2)RD(cid:6) (cid:2)7(cid:6)
Dd(cid:5)
177 for transfer learners. Approximate posterior beliefs about hidden states, ac!on, and mixture
178 parameter follow categorical distribu!ons , and
i(cid:2)OD(cid:6) 4 Cat(cid:2)KD(cid:6) i(cid:2)RD(cid:6) 4 Cat(cid:2)FD(cid:6) i(cid:2)-(cid:6) 4 Cat(cid:2)J(cid:6)
179 parameterized by the posterior expecta!ons , , and . Moreover, approximate posterior
KD FD J
180 beliefs about parameter matrices follow Dirichlet distribu!ons ,
i(cid:2)U(cid:6) 4 Dir(cid:2)|(cid:6) i(cid:2)Y(cid:6) 4 Dir(cid:2)}(cid:6)
181 and parameterized by Dirichlet parameters and .
i(cid:2)Z(cid:6) 4 Dir(cid:2)~(cid:6) |,}, ~
182 Given the above genera!ve model and posterior distribu!on, the varia!onal free energy for
183 naive learners is provided as follows:
184
(cid:127)(cid:2)(cid:26)(cid:5):(cid:9),K(cid:5):(cid:9),F(cid:5):(cid:9),(cid:128)(cid:6) 4 E(cid:130),(cid:131)(cid:2)\(cid:6)(cid:132)(cid:30)lnS(cid:2)R(cid:5):(cid:9),O(cid:5):(cid:133),(cid:26)(cid:5):(cid:9),[(cid:5):(cid:9),b(cid:6)$lni(cid:2)R(cid:5):(cid:9),O(cid:5):(cid:9),b(cid:6)(cid:134)
(cid:9)
185
4 (cid:135)KD ⋅(cid:2)lnKD (cid:30)ln(cid:137)⋅(cid:26)D (cid:30)ln(cid:138)(cid:2)FD(cid:4)(cid:5) ⊗KD(cid:4)(cid:5)(cid:6)(cid:6)
Dd(cid:5)
10
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
(cid:9)
186
$(cid:135)FD ⋅(cid:2)lnFD (cid:30)(cid:2)1(cid:30)2AD(cid:6)lnLKD(cid:4)(cid:5)(cid:6)
Dd(cid:5)
187
$(cid:2)|(cid:30)h(cid:6)⋅ln(cid:137)(cid:30)ln(cid:139)(cid:2)|(cid:6)$(cid:2)}(cid:30)j(cid:6)⋅ln(cid:138)(cid:30)ln(cid:139)(cid:2)}(cid:6)$(cid:2)~(cid:30)c(cid:6)⋅lnL(cid:30)ln(cid:139)(cid:2)~(cid:6) (cid:2)8(cid:6)
188 For transfer learners, the complexity of in Equa!on (8), , is replaced with
Z (cid:2)~(cid:30)c(cid:6)⋅lnL(cid:30)ln(cid:139)(cid:2)~(cid:6)
189 that of , which is given as . Here, is a risk that
- (cid:142)(cid:143)(cid:144)(cid:132)i(cid:2)-(cid:6)||S(cid:2)-(cid:6)(cid:134) 4 J⋅(cid:2)lnJ(cid:30)ln(cid:145)(cid:6) A(cid:9) ∈ (cid:132)0,1(cid:134)
190 evaluates the goodness of ac!on for each step.
191 The gradient descent on furnishes the Bayesian belief update rules. Posterior beliefs about
(cid:127)
192 hidden states and ac!on are updated for each step. These inference update rules are derived
OD RD
193 by solving the fixed point of implicit gradient descent (i.e., and ):
(cid:146)(cid:127)/(cid:146)K(cid:9) 4 0 (cid:146)(cid:127)/(cid:146)F(cid:9) 4 0
194
s(cid:9) 4 (cid:149)"lnA⋅(cid:26)(cid:9) $lnB(cid:2)δ(cid:9)(cid:4)(cid:5) ⊗s(cid:9)(cid:4)(cid:5)(cid:6)#
(cid:148) (cid:2)9(cid:6)
δ(cid:9) 4 (cid:149)(cid:2)lnCs(cid:9)(cid:4)(cid:5)(cid:6)
195 Dirichlet parameters and mixture parameter are updated at the end of each trial as part
h,j,k, -
196 of the learning. These learning update rules are derived by solving as:
(cid:146)(cid:127)/(cid:146)b 4 0
(cid:9)
197
(cid:21)
| 4 h$(cid:135)(cid:26)DKD
Dd(cid:5)
(cid:9)
198
(cid:21)
} 4 j$(cid:135)KD(cid:2)FD(cid:4)(cid:5) ⊗KD(cid:4)(cid:5)(cid:6)
Dd(cid:5)
(cid:9)
199
(cid:21)
~ 4 k $(cid:135)(cid:2)1(cid:30)2AD(cid:6)FDKD(cid:4)(cid:5) (cid:2)10(cid:6)
Dd(cid:5)
200 for naive learners. For transfer learners, the ’s update is replaced with the ’s update as:
Z -
201
λp 4 (cid:149)(cid:155)ln(cid:145)p $(cid:135)(cid:2)1(cid:30)2AD(cid:6)RD ⋅lnZpOD(cid:156) (cid:2)11(cid:6)
D
202 Through Equa!on (11), the agent learns to select the ac!on that minimizes the risk, while avoiding
203 ac!ons that increase the risk.
11
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
204
205 Simula(cid:12)on se.ngs
206 In the simula!on, a set of 100 policy matrices, (cid:158) , are
(cid:157)3(cid:5)&
Z(cid:2)(cid:5),(cid:5)(cid:6),Z(cid:2)(cid:5),l(cid:6),…,Z(cid:2)(cid:5)&,(cid:5)&(cid:6) ∈ (cid:132)0,1(cid:134)
207 prepared for a mutually different target posi!on, where each denotes the one pretrained to
Z(cid:2)(cid:159),(cid:160)(cid:6)
208 reach the target on the coordinate . During each pretraining, the target posi!on was fixed at
(cid:2)¡,.(cid:6)
209 , and the agent con!nued to randomly move its hand for 10000 steps, which was followed
(cid:2)¡,.(cid:6)
210 by the learning of the posterior belief about based on Equa!on (10). In pre-training, we use
Z(cid:2)(cid:159),(cid:160)(cid:6)
211 an A matrix created based on a uniform distribu!on as the ini!al value so that the same policy can
212 be adopted even if the target moves to a nearby loca!on.
213
214 RESULTS
215 Transferring ac(cid:12)ve inference to novel environments
216 This work aims to mathema!cally model the generalizability and adaptability of the brain to
217 novel environments. An arm reaching task is a widely used visuo-motor paradigm to inves!gate
218 the emergence of sensory-motor con!ngencies, in which subjects move their hands towards the
219 target posi!on based on visual input (Bourgeois et al., 2021). This can be modeled using canonical
220 neural networks (Isomura et al., 2022), which receive visual and somatosensory inputs, infer the
221 latent environmental states via middle-layer neural ac!vity, and generate ac!ons to shiB the arm
222 posi!on (Fig. 1A, top). Risk here is characterized by the distance from the target, and the network
223 learns the op!mal ac!on policy through the plas!city of the output layer synap!c connec!on
224 weights (Fig. 1A, boPom). As previously established (Isomura et al., 2022), canonical neural
12
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
225 networks are read as performing ac!ve inference of the external environment to minimize risks
226 associated with future outcomes, under a POMDP genera!ve model (Fig. 1B). Thus, aBer sufficient
227 exposure to the environment, these networks self-organize to recapitulate the interac!ng
228 environment as a universal property.
229 In doing so, conven!onal models need to relearn the policy matrix (C) from scratch upon
230 encountering a new environment, making it extremely !me-consuming. Since the policy matrix
231 posterior takes con!nuous values following a Dirichlet distribu!on, this method exhibits a widely
232 observed 1/t-order generaliza!on error decay (Ref: Has!e et al., 2009) (Fig. 1C, D, red lines).
233 However, this indicates a large gap from humans and animals that can quickly adapt to moving
234 their arms to a given target posi!on (Luauté et al., 2009; RosseK et al., 1993).
235 To model the quick adapta!on, we constructed canonical neural networks that employ mul!ple
236 policy matrices in parallel, each of which was pretrained to reach different target posi!ons. The key
237 advantage is the incorpora!on of the switching parameter λ that follows a categorical distribu!on.
238 In contrast to the naive model, this network converges the generaliza!on error considerably faster
239 in a simple example seKng (Fig. 1C, D blue lines).
240 Mathema!cal analyses revealed the dis!nct learning mechanisms (Appendix A). Cri!cally,
241 discrete switching variable tends to converge to a one-hot vector as evidence is accumulated.
-
242 This makes the op!mal policy selec!on from a set of policies converge with an exponen!al order, a
243 speed considerably faster than the well-known 1/t order generaliza!on error decay. Intui!vely, this
244 is because upon receiving sensory inputs, the evidence is accumulated linearly with !me (i.e.,
245 )—and the amplitude of the selected value is determined by the difference in evidence for
(cid:1)(cid:2)(cid:3)(cid:6)
13
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
246 each component of . This makes the posterior expecta!on in the form of an exponen!al func!on
-
247 , where denotes the difference in normalized evidence.
exp(cid:2)(cid:30)(cid:3)〈evidence gap〉(cid:6) 〈evidence gap〉
248 Although compu!ng this difference involves the generaliza!on error in the order, this is
1/√(cid:3)
249 negligible compared to the order 1 amplitude of the difference itself. Therefore, the generaliza!on
250 error of is determined by the form as the leading order, where and
- exp"(cid:30)(cid:3)(cid:2)¤(cid:5)'(cid:133) (cid:30)¤ls“(cid:6)# ¤(cid:5)«(cid:9)
251 are the normalized evidences for the most plausible and second most plausible policies,
¤l(cid:159)‹
252 causing it to decrease exponen!ally with !me. Further details are provided in Appendix A.
253 In essence, both mathema!cal analyses and numerical simula!ons (Fig. 1C, D) suggest that the
254 selec!on mechanism of policy matrices enables more rapid adapta!on than naive parameter
255 learning, analogous to empirically observed prism adapta!on. Based on these observa!ons, in the
256 remainder of this paper, we formally model prism adapta!on using the canonical neural networks
257 exhibi!ng transfer learning—and examine whether the rapid adapta!on to a prism-shiBed
258 environment and the aBer-effects following the removal of prism glasses can be reproduced.
259
260 Modeling prism adapta(cid:12)on
261 In this sec!on, we instan!ated transfer of ac!ve inference through modeling a prism-shiB
262 paradigm of arm reaching tasks (Hermann & CaPell, 1898; RosseK et al., 1993) (Fig. 2A). Agents
263 perform a reaching task. For each trial, the agent’s hand is ini!ally located at a random posi!on in
264 a 10 × 10 grid field (Fig. 2B). At each step, the agents receive sensory informa!on about target and
265 hand posi!on and infer hidden states and ac!ons to determine the next mo!on from either up,
266 down, leB, or right (Equa!on (9)). A trial ends when either hand reaches the target or 100 steps
14
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
267 have passed, followed by the updates of parameters (Equa!ons. (10) and (11)). This cycle
268 con!nues for 200 trials. The risk is defined based on the change in ManhaPan distance between
A(cid:9)
269 the hand and target: when the distance decreases for each step (i.e., when the hand
A(cid:9) 4 0.45
270 moved toward the target); otherwise, .
A(cid:9) 4 0.55
271 A simula!on comprises three phases (Fig. 2C): The first 140 trials serves a “baseline”
272 environment without prism glasses, in which the target is placed on the coordinate
(cid:2)(cid:13),(cid:22)(cid:6) 4 (cid:2)7,5(cid:6)
273 (Fig. 2C, Baseline). Star!ng from a flat prior , the agent op!mized the mixture parameter for the
J J
274 baseline condi!on (Fig. 2C). The second 30 trials correspond to the “prism exposure,” in which the
275 target is placed on (4,5) (a white filled grid in Fig. 2C, prism exposure phase) but its
(cid:2)(cid:13),(cid:22)(cid:6) 4
276 apparent posi!on is s!ll observed at (7,5). This prism shiB was modeled by changing the likelihood
277 mapping , which displaces only the target posi!on but not the hand posi!on. While the apparent
U
278 target posi!on is unchanged from the baseline, the agent can no!ce the difference when it
279 receives the risk aBer an ac!on was made. In the last 30-trial “post exposure” phase, the agent
280 returns to the environment in the absence of prism exposure, where the target is placed—and
281 observed—at . In each phase, the agent learns a policy that brings the hand closer to
(cid:2)(cid:13),(cid:22)(cid:6) 4 (cid:2)7,5(cid:6)
282 the true target by minimizing the free energy.
283 We demonstrated that transfer learners that employ pretrained policy matrices can replicate the
284 key empirical proper!es of prism adapta!on. Performance was assessed by the error between the
285 x coordinates of the target and hand aBer the minimum steps to reach the target (Fig. 2C). While
286 large errors were ini!ally observed during both the prism exposure and post-exposure phases,
287 they rapidly diminished to approximately zero. The transi!on in policy matrix shows the
L
15
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
288 occurrence of adapta!on in both phases (Fig. 2C, boPom), which was driven by the update of
289 mixture weights (Fig. 2D). In each phase, transfer learners consistently learn the op!mal
J J
290 values. Specifically, during prism exposure, is adjusted to align the policy with the actual target
J
291 posi!on, while in the post-exposure phase, reverts to the baseline policy.
J
292 We observed dis!nct proper!es between transfer and naive learners. The proposed model
293 (transfer learner) qualita!vely replicated experimental outcomes (Fig. 2E). During prism exposure,
294 ini!ally large dura!on to reach the target decreased aBer approximately 5 trials and converged
295 around 10. In the post-exposure phase, transfer learners ini!ally exhibited an aBer-effect akin to
296 empirical prism adapta!on and re-learn gradually. In contrast, naive learners that learn policy
297 matrix directly via Equa!on (10) could not reproduce the aBer-effect (Fig. 2E, red error bars).
L
298 While the dura!on of naive learners increased during the prism exposure, they failed to exhibit the
299 dura!on increase during post-exposure, arguably because of slow learning during the prism
300 exposure.
301 In summary, the proposed model can learn to perform the arm reaching task both in the
302 absence and presence of prism shiB in the Bayes op!mal manner and replicate the dynamics of
303 empirical prism adapta!on. We confirmed that these outcomes can be observed even with
304 randomly varied target posi!ons (Fig. 3).
305
16
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
306
307 Fig. 2. Simula(cid:12)ng prism adapta(cid:12)on using canonical neural networks. A. The prism adapta!on
308 paradigm: (1) a subject performs an arm reaching task in the baseline phase; (2) during the prism
309 exposure phase, the same task is performed with wearing prism glasses; and (3) during the post-
310 exposure phase, the prism glasses are removed. B. The pretraining of policy matrices. Prism
311 adapta!on is modeled with a discrete state space (POMDP) model. The hand and target are placed
312 in a 10x10 grid world. The colors in each grid represent the hand-moving direc!ons that
313 correspond to the color map besides the heatmap, indica!ng the ac!on selec!on probability for
17
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
314 moving up/down/leB/right. C. Overview of the simula!on seKng. The upper panel shows the
315 trajectory of the x-coordinate difference between the target and hand aBer the minimum steps.
316 The boPom panel shows the corresponding changes in the policy matrix. Heatmaps illustrate how
317 the agent’s policy matrix changes over trials. While the colors are blurred before training
318 (leBmost), the colors become high contrast through training. The fixed point (crosspoint of ver!cal
319 and horizontal dashed lines) approximately pursues the target posi!on (open square) both in the
320 absence and presence of prism shiB. D. Changes in the two policy mixture weights corresponding
321 to original and new target posi!ons. E. The upper and lower panels show the dura!on (the number
322 of steps) for transfer learners and naive learners, respec!vely. F. Comparison of the dura!on at the
323 1 trial aBer prism exposure between the transfer and naive learners. In (C)–(E), the lines and
324 shaded areas indicate the medians and interquar!le ranges.
325
326
327 Fig. 3. Simula(cid:12)ons with randomized target. Same as Fig.2 but the target posi!on is changed
328 randomly over sessions, showing the robustness of the proposed model. A. The x-coordinate
18
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
329 difference between the target and the hand aBer the minimum steps. B. Changes in the two policy
330 mixture weights. C. Dura!on for the transfer (top) and naive learners (boPom). D. Comparison of
331 the dura!on value between the transfer and naïve learners. In (A)–(C), the lines and shaded areas
332 indicate the medians and interquar!le ranges.
333
334 Exponen(cid:12)al-order generaliza(cid:12)on error decay in prism adapta(cid:12)on
335 Lastly, we analy!cally inves!gated the learning speed of transfer and naive learners. For
336 analy!cal tractability, here we assume that the posterior hidden state well approximates the
KD
337 true hidden state , the ac!on follows a random walk, and that there exists an op!mal policy
OD RD
338 matrix in a set of pretrained matrices . Under these condi!ons, the mixture parameter
∗
Z MZpN J
339 and policy matrix converge to the Bayes op!mal values and with the order decay
∗ ∗ (cid:4)fl(cid:9)
L(cid:2)J(cid:6) J L (cid:7)
340 of generaliza!on error, where . Detailed deriva!ons are described in detail in the Appendix.
h > 0
341 This convergence speed is considerably dis!nct from learning speed in the policy matrix of naive
L
342 learners that converges to the Bayes op!mal value with the 1/t order decay.
∗
L
343 To valid the change in generaliza!on error, we simulated the transfer learners under the
344 condi!on in which the ac!on is defined as a random walk and confirmed the order
(cid:4)fl(cid:9)
(cid:7)
345 convergence of policy mixture weights (Fig. 4A) and policy matrix (Fig. 4B). The generaliza!on
J L
346 error of the mixture weight in transfer learners converges in approximately 2400 steps, and its
J
347 change resembles a straight line in a semi-log plot, indica!ng the exponen!al order convergence
348 (Fig. 4A, blue line). ABer 2400 steps, more than half of elements become Bayes op!mal one-hot
J
349 vector; therefore, the median generaliza!on error becomes zero. The gradient of the
19
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
350 generaliza!on error in the semi-log plot closely matches the theore!cal gradient calculated in the
351 Appendix. The generaliza!on error of the policy matrix in the transfer learner also exhibited a
L
352 similar property (Fig. 4B), in which empirical errors resembled a theore!cal line (green line). In
353 contrast, the naive learner requires a longer convergence !me in the order (Fig. 4C), aligning a
(cid:4)(cid:5)
(cid:3)
354 line in a log-log scale aBer about 500 steps.
355 In essence, the error of the transfer learner decreases much faster than that of the naive
356 learner, enabling a faster op!miza!on of the policy matrix. The faster adapta!on is further
357 confirmed under the condi!on where the hidden states are updated following Bayesian belief
358 upda!ng rules (Fig. 4D, E). At the one trial aBer the prism shiB (i.e., trial 142), the majority of
359 transfer learners reached the target significantly faster than the naïve learners (Fig. 2F;
(cid:176) 4
360 , Mann–Whitney U test) and the median dura!on of transfer learners was only
(cid:4)(cid:5)–
4.15310
361 approximately 8.3% of that of naive learners.
362
363
20
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
364 Fig. 4. Generaliza(cid:12)on error in simplified model simula(cid:12)on. A. Generaliza!on error of and the
J
365 theore!cal value in toy model. B. Generaliza!on error of and theore!cal value in semi-log plot in
L
366 toy model. C. Generaliza!on error of , and theore!cal value in log-log plot. Both learners have a
L
367 common ini!al value of the policy matrix in toy model. D. Generaliza!on error of in main
J
368 simula!on. E. Generaliza!on error of and theore!cal value in semi-log plot in main simula!on.
L
369
370
371 DISCUSSION
372 In this work, we developed an ac!ve inference model incorpora!ng transfer learning and
373 applied it to model prism adapta!on. Canonical neural networks employing mul!ple policy
374 matrices can transfer previously learned policies to novel environments, demonstra!ng faster
375 adapta!on than naive learners. This has broader implica!ons for understanding the brain’s
376 generalizability and adaptability, par!cularly in the context of prism adapta!on. The transfer
377 learners involve three different !mescales comprising fast updates of state and ac!on , slow
O R
378 updates of mixture weights , and ultra-slow updates of parameters , and , akin to previous
J (cid:137) (cid:138) L
379 computa!onal models for prism adapta!on (Inoue et al., 2015).
380 Two !mescales in prism adapta!on were suggested by several works (G. M. Redding et al., 2005;
381 G. Redding & Wallace, 2002, 2006). On this view, fast and slow adapta!ons are interpreted as
382 recalibra!on of frames and realignment of the visuo-motor mapping, respec!vely, and the laPer
383 induces aBer effect. Our observa!ons were consistent with this view: only transfer learners that
21
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
384 update , but not naive learners without , exhibited aBer effect, indica!ng the importance of
J J
385 transfer mechanisms to reproduce empirical phenomena.
386 Such correspondences in !mescales are beneficial to iden!fy a precise mapping between the
387 parameters of ac!ve inference models and corresponding brain regions. Specifically, faster
388 !mescale adapta!on is aPenuated by lesions in the posterior parietal cortex (Canavan et al., 1990;
389 Clower et al., 1996; Newport & Jackson, 2006; Pisella et al., 2004; Welch & Goldstein, 1972). This
390 impairs both error reduc!on and aBer effects. Moreover, fast adapta!on involves the primary
391 motor cortex (Danckert et al., 2008), anterior cingulate cortex (Danckert et al., 2008; Luauté et al.,
392 2009), and cerebellum (Danckert et al., 2008; Luauté et al., 2009). In par!cular, cerebellar lesions
393 result in decreased or absent prism aBer-effects (Calzolari et al., 2015; Hanajima et al., 2015;
394 Pisella et al., 2005), sugges!ng its role in slow adapta!on.
395 Furthermore, the underlying neuronal mechanisms can be considered in terms of
396 neuromodula!on of synap!c plas!city. The update rule in Equa!on (11) compares ac!on and
J
397 experience and reinforces op!mal policy, providing a func!onal explana!on of policy adapta!on,
398 which may be associated with aPen!onal switch and working memory mediated by such as
399 dopaminergic neurons. Then, the process of amplifying more similar connec!ons and aPenua!ng
400 less similar connec!ons can be formulated as dopaminergic modula!on of synap!c plas!city.
401 Many tradi!onal models in prism adapta!on have posited mul!-!mescale mechanisms (Inoue
402 et al., 2015; Kim et al., 2015; Kording et al., 2007). While these models can account for behavioral
403 performance, they do not fully explain the underlying neuronal mechanisms and how different
404 !mescales contribute to learning. The proposed canonical neural networks have poten!al to
22
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
405 assimilate empirical data into modelling. Earlier work has established a reverse engineering
406 method to es!mate genera!ve model from empirical neural ac!vity data based on canonical
407 neural networks (Isomura et al., 2023, 2025). This allows us to compare model predic!ons with
408 empirical results (Kitazawa et al., 1995). In future work, we hope to elucidate the func!onal
409 rela!onship between brain regions and the prism adapta!on process by comparing the network
410 proper!es of our model and empirical data.
411
412 CONCLUSION
413 In summary, we developed a transfer learning model of ac!ve inference based on canonical
414 neural networks. Our model selected the Bayes op!mal parameter from pretrained parameters
415 and demonstrated faster adapta!on to a novel prism glasses environment. Further, it successfully
416 recapitulated features of prism adapta!on such as aBer effect. These results support that the
417 mixture policy matrices enable transfer of knowledge learned from experiences to quickly adapt to
418 the environment and have the capacity to reproduce the brain’s generalizability.
419
23
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
420 Appendix
421 Comparison of generaliza(cid:12)on errors
422 Here, we assume that the posterior beliefs about the hidden state and ac!on well approximate
423 the true hidden state and ac!on: . In addi!on, we assume that is a random walk,
K 4 O,F 4 R MRDN
424 indica!ng that any are independent and iden!cally distributed.
‘
RD,RD † (cid:2)7 ≠ 7 (cid:6)
425 We define a Hebbian product as a block vector, and its large-
¤⃗ 4 (cid:5) ∑ (cid:9) Dd(cid:5)(cid:2)1(cid:30)2AD(cid:6)FD ⊗KD(cid:4)(cid:5)
(cid:9)
426 !me limit as . The generaliza!on error of is given by the
¤⃗∗ 4 lim∞¤⃗ 4 ⟨(cid:2)1(cid:30)2AD(cid:6)FD ⊗KD(cid:4)(cid:5)⟩ ¤⃗
(cid:9)→
427 covariance matrix of their difference , where . When the agent obeys
”» 4 …Δ¤⃗Δ¤⃗(cid:21) (cid:190) Δ¤⃗ 4 ¤⃗ (cid:30)¤⃗∗
428 a random work, can be analy!cally computed, showing the order convergence.
(cid:4)(cid:5)
”» (cid:3)
429 The posterior belief about the policy mapping is parameterized by the Dirichlet parameter,
Z
430 which is given as Equa!on (7), or equivalently, as a vector . Then, the posterior belief
~⃗ 4 k⃗$(cid:3)¤⃗
431 about is given as when is negligibly small, where indicates
Z L 4 ~⊘"1¿⃗1¿⃗(cid:21) ~# ⟺ L⃗ 4 ¤⃗ ⊘`¿⃗ k ⊘
432 the Hadamard division operator and . When the agent obeys a
`¿⃗ 4 (cid:5) ∑ (cid:9) Dd(cid:5)(cid:2)1(cid:30)2AD(cid:6)1¿⃗⊗KD(cid:4)(cid:5)
(cid:9)
433 random walk, holds. This provides
`¿⃗∗ 4 …(cid:2)1(cid:30)2AD(cid:6)1¿⃗⊗KD(cid:4)(cid:5)(cid:190) 4 (cid:5) (cid:2)1(cid:30)2⟨AD⟩(cid:6)1¿⃗⊗1¿⃗ L⃗∗ 4
(cid:18)P
434 .
´«(cid:2)1(cid:30)2⟨AD⟩(cid:6)
(cid:4)(cid:5) ¤⃗∗
435 The generaliza!on error of is given as (cid:21) . Then,
L⃗ ”ˆ 4 ˜"L⃗(cid:30)L⃗∗ #"L⃗(cid:30)L⃗∗ # ¯ L⃗ 4 ¤⃗ ⊘`¿⃗ 4
436 is approximated as ⊙l
"¤⃗∗ $Δ¤⃗#⊘"`¿⃗∗ $Δ`¿⃗# L⃗ ≃ ¤⃗∗ ⊘`¿⃗∗ $Δ¤⃗ ⊘`¿⃗∗ (cid:30)¤⃗∗ ⊘`¿⃗∗ ⊙Δ`¿⃗ 4
437 as the first-order approxima!on. Thus, becomes
L⃗∗ $´«(cid:2)1(cid:30)2⟨AD⟩(cid:6) (cid:4)(cid:5) "Δ¤⃗ (cid:30)L⃗∗ ⊙Δ`¿⃗# ”ˆ
438 T
”ˆ 4 ´« l (cid:2)1(cid:30)2⟨AD⟩(cid:6) (cid:4)l ˜"(¤⃗ (cid:30)¿¨¿⃗∗ ⊙(`¿⃗#"(¤⃗ (cid:30)¿¨¿⃗∗ ⊙(`¿⃗# ¯ (cid:2)12(cid:6)
439 Because is expressed as using a block diagonal matrix ,
`¿⃗ `¿⃗ 4 Ω¤⃗ Ω L⃗∗ ⊙Δ`¿⃗ 4 diag˚L⃗∗ ¸"`¿⃗(cid:30)
440 holds. Thus, we obtain
`¿⃗∗ # 4 diag˚L⃗∗ ¸"Ω¤⃗ (cid:30)Ω¤⃗∗ # 4 diag˚L⃗∗ ¸ΩΔ¤⃗
24
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
T
441 ”ˆ 4 ´«
l
(cid:2)1(cid:30)2⟨AD⟩(cid:6)
(cid:4)l
˜"(cid:204)
(cid:30)diag˚¿C⃗∗ ¸˝#(¤⃗(¤⃗T
"(cid:204)
(cid:30)diag˚¿C⃗∗
¸˝# ¯
T
4 ´«
l
(cid:2)1(cid:30)2⟨AD⟩(cid:6)
(cid:4)l
"(cid:204)
(cid:30)diag˚¿C⃗∗
¸˝#”»"(cid:204)
(cid:30)diag˚¿C⃗∗
¸˝# (cid:2)13(cid:6)
442
443 In contrast, the transfer learner that equips with the mixture policies updates the mixing balance
444 instead of upda!ng directly. In this case, the posterior belief about the policy mapping is given
J ~
445 as follows:
446 ¿C⃗ 4 exp˚l¿¿n¿¿¿¿Z¿¿¿ (cid:5) ⃗λ(cid:5) $⋯$l¿¿n¿¿¿¿Z¿¿ (cid:18) ¿¿¿ 0 ⃗λ(cid:18)0 ¸
4 ¿C⃗∗ $¿C⃗∗ ⊙"l¿¿n¿¿¿¿Z¿¿¿ (cid:5) ⃗(λ(cid:5) $⋯$l¿¿n¿¿¿¿Z¿¿ (cid:18) ¿¿¿ 0 ⃗(λ(cid:18)0 # (cid:2)14(cid:6)
447
448 Here, the update rule is expressed as and its large-!me limit as
- Jp 4 σ"lnˇ— $l¿¿n¿¿¿¿Z¿¿¿ p ⃗⋅(cid:3)¤⃗# Jp ∗ 4
449 . The difference between and is computed as
σ"lnˇ— $l¿¿n¿¿¿¿Z¿¿¿ p ⃗⋅(cid:3)¤⃗∗ # L⃗ L⃗∗
450
¿C⃗(cid:30)¿C⃗∗ 4 ¿C⃗∗ ⊙"l¿¿n¿¿¿¿Z¿¿¿ (cid:5) ⃗,…,¿l¿n¿¿¿¿Z¿¿ (cid:18) ¿¿¿ 0 ⃗#(λ (cid:2)15(cid:6)
451 where holds and is assumed to be a flat prior and thus omiPed for simplicity.
∗
ΔJ 4 J(cid:30)J (cid:145)
452 When one of the previously experienced contexts has a considerably higher similarity to the
453 current environment compared to other contexts, an element of that corresponds to the context
J
454 monotonically converges to one. Under this condi!on, (cid:21)
J ∗ 4 (cid:149)(cid:15)(cid:3)"¿l¿n¿¿¿¿Z¿¿¿ (cid:5) ⃗,…,l¿¿n¿¿¿¿Z¿¿ (cid:18) ¿¿¿ 0 ⃗# ¤⃗∗ (cid:20) 4
455 (cid:21) holds owing to the property of soBmax func!on .
(cid:149)(cid:15)(cid:3)"¿l¿n¿¿¿¿Z¿¿¿ (cid:5) ⃗,…,l¿¿n¿¿¿¿Z¿¿ (cid:18) ¿¿¿ 0 ⃗# ¤⃗∗ (cid:30)(cid:3)maxl¿¿n¿¿¿¿Z¿¿⃗ (cid:209) (cid:21) ¤⃗∗ (cid:20) (cid:149)
(cid:209)
456 Thus, by defining (cid:210) 4 argmaxu(cid:2)l¿¿n¿¿¿¿Z¿¿⃗ (cid:211) T ¤⃗∗ (cid:6) and (cid:212)(cid:213) 4 l¿¿n¿¿¿¿Z¿¿⃗ (cid:211)¤⃗∗ , we obtain Jp ∗ 4
∑
(cid:214)
(cid:218)
(cid:215)
(cid:214)
(cid:216) (cid:215) (cid:217) (cid:216)(cid:218) ∝ (cid:7) (cid:9)(cid:2)(cid:219)(cid:217)(cid:4)(cid:219)(cid:220)(cid:6) . All
457 elements of except for the largest element ( ) converge to zero exponen!ally with !me, i.e.,
J (cid:221) 4 (cid:210)
458 as the leading order, because the generaliza!on
(cid:4)(cid:2)(cid:219)(cid:220)(cid:4)(cid:219)(cid:217)(cid:6)(cid:9)
Jp 4 exp(cid:15)(cid:3)(cid:212)p (cid:30)(cid:3)(cid:212)(cid:209) $(cid:1)"√(cid:3)#(cid:20) 4 (cid:1)"(cid:7) #
459 error of is of order and negligibly smaller than . Conversely, the largest
l¿¿n¿¿¿¿Z¿¿⃗ (cid:211)¤⃗ (cid:1)"1/√(cid:3)# (cid:2)(cid:212)(cid:209) (cid:30)(cid:212)p(cid:6)
460 element is computed as , where
Jp 4 1(cid:30)∑(cid:209)(cid:222)pJ(cid:209) 4 1(cid:30)(cid:1)"(cid:7) (cid:9)(cid:2)(cid:219) (cid:220)†(cid:4)(cid:219)(cid:220)(cid:6) # (cid:210) ‘ 4 argmax(cid:213)(cid:222)(cid:209)(cid:2)l¿¿n¿¿¿¿Z¿¿⃗ (cid:211) (cid:223) ¤⃗∗ (cid:6)
461 indices the second largest element.
25
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
462 This provides . Therefore, the generaliza!on error
J(cid:30)J ∗ 4 (cid:1)"(cid:7) (cid:4)(cid:9)(cid:2)(cid:219)(cid:220)(cid:4)(cid:219) (cid:220)†(cid:6) # ”ˆ 4 ˜"L⃗(cid:30)
463 (cid:21) for the transfer learner is in , which indicates that of the transfer
L⃗∗ #"L⃗(cid:30)L⃗∗
# ¯ (cid:1)"(cid:7)
l(cid:9)(cid:2)(cid:219)(cid:220)(cid:4)(cid:219) (cid:220)†(cid:6)
# ”ˆ
464 learner converges to zero much faster than does that of the naive learner. Thus, the generaliza!on
465 error of the transfer learner is determined based on the generaliza!on error of pretrained policy
466 matrices.
467
468
469 Table 1. Glossary of expressions
Expression Defini!on Explana!on
t Current !me
(cid:3) ∈ M0,1,…N
Time index
7 7 ∈ M0,1,…,(cid:3)N
Sensory input
(cid:5)&&&&3(cid:5)
(cid:26)D (cid:26)D ∈ M0,1N
Hidden state
E (cid:9)
OD OD 4 OD ⊗OD
Hidden state of hand
E E (cid:5)&&3(cid:5)
OD posi!on OD ∈ M0,1N
Hidden state of
(cid:9) (cid:9) (cid:5)&&3(cid:5)
OD target posi!on OD ∈ M0,1N
Expected hidden Posterior belief of hidden state ,
KG state OD
(cid:5)&&&&3(cid:5)
KG ∈ (cid:132)0,1(cid:134)
Ac!on (up down leB
(cid:157)3(cid:5)
RD right) RD ∈ M0,1N
Expected ac!on Posterior belief of ac!on ,
FD RD
(cid:157)3(cid:5)
FD ∈ (cid:132)0,1(cid:134)
26
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
parameter matrices
(cid:5)&&&&3(cid:5)&&&& (cid:5)&&&&3(cid:157)&&&&
U,Y,Z U ∈ (cid:132)0,1(cid:134) ,Y ∈ (cid:132)0,1(cid:134) ,
(cid:157)3(cid:5)&&&&
Z ∈ (cid:132)0,1(cid:134)
Expected parameter Posterior belief of parameter matrices,
(cid:137),(cid:138),L matrices U,Y,Z,
(cid:5)&&&&3(cid:5)&&&& (cid:5)&&&&3(cid:157)&&&&
(cid:137) ∈ (cid:132)0,1(cid:134) ,(cid:138) ∈ (cid:132)0,1(cid:134) ,
(cid:157)3(cid:5)&&&&
L ∈ (cid:132)0,1(cid:134)
Mixture weight Mixture weight for policy.
-
Expected mixture Posterior belief of mixture parameter .
J weight -
parameters The set of parameters.
b
Expected Posterior belief of parameter
(cid:128) parameters b.
Dirichlet parameter Dirichlet parameter of matrices,
h,j,k U,Y,Z,
(cid:5)&&&&3(cid:5)&&&& (cid:5)&&&&3(cid:157)&&&&
h ∈ ℝ(cid:224)& ,j ∈ ℝ(cid:224)& ,
(cid:157)3(cid:5)&&&&
k ∈ ℝ(cid:224)&
Dirichlet parameter Dirichlet parameter of matrices,
|,},~ (cid:137),(cid:138),L,
(cid:5)&&&&3(cid:5)&&&& (cid:5)&&&&3(cid:157)&&&&
| ∈ ℝ(cid:224)& ,} ∈ ℝ(cid:224)& ,
(cid:157)3(cid:5)&&&&
~ ∈ ℝ(cid:224)&
risk
Γ Γ ∈ (cid:132)0,1(cid:134)
Beta func!on
: Gamma func!on
(cid:139)(cid:2)⋅(cid:6)
∏(cid:218)ª(cid:2)|(cid:218)(cid:217)(cid:6)
(cid:139)(cid:2)|⋅p(cid:6) 4 , Γ(cid:2)⋅(cid:6)
ª(cid:2)∑(cid:218)|(cid:218)(cid:217)(cid:6)
470
471 Reference
472 Bourgeois, A., Schmid, A., Turri, F., Schnider, A., & Ptak, R. (2021). Visual but not auditory-verbal feedback
473 induces aBereffects following adapta!on to virtual prisms. Fron(cid:13)ers in Neuroscience, Volume 15-
474 2021. hPps://doi.org/10.3389/fnins.2021.658353
27
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
475 Calzolari, E., Bolognini, N., Casa!, C., Marzoli, S. B., & Vallar, G. (2015). Restoring abnormal aBereffects of
476 prisma!c adapta!on through neuromodula!on. Effects of Non-Invasive Brain S(cid:13)mula(cid:13)on on
477 A"en(cid:13)on: Current Debates, Cogni(cid:13)ve Studies and Novel Clinical Applica(cid:13)ons, 74, 162–169.
478 hPps://doi.org/10.1016/j.neuropsychologia.2015.04.022
479 Canavan, A. G. M., Passingham, R. E., Marsden, C. D., Quinn, N., Wyke, M., & Polkey, C. E. (1990). Prism
480 adapta!on and other tasks involving spa!al abili!es in pa!ents with Parkinson’s disease, pa!ents
481 with frontal lobe lesions and pa!ents with unilateral temporal lobectomies. Neuropsychologia,
482 28(9), 969–984. hPps://doi.org/10.1016/0028-3932(90)90112-2
483 Clower, D. M., Hoffman, J. M., Votaw, J. R., Faber, T. L., Woods, R. P., & Alexander, G. E. (1996). Role of
484 posterior parietal cortex in the recalibra!on of visually guided reaching. Nature, 383(6601), 618–
485 621. hPps://doi.org/10.1038/383618a0
486 Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V., & Friston, K. (2020). Ac!ve inference on discrete state-
487 spaces: A synthesis. Journal of Mathema(cid:13)cal Psychology, 99, 102447.
488 hPps://doi.org/10.1016/j.jmp.2020.102447
489 Danckert, J., Ferber, S., & Goodale, M. A. (2008). Direct effects of prisma!c lenses on visuomotor control: An
490 event-related func!onal MRI study. European Journal of Neuroscience, 28(8), 1696–1704.
491 hPps://doi.org/10.1111/j.1460-9568.2008.06460.x
492 Frémaux, N., & Gerstner, W. (2016). Neuromodulated spike-!ming-dependent plas!city, and theory of
493 three-factor learning rules. Fron(cid:13)ers in Neural Circuits, Volume 9-2015.
494 hPps://doi.org/10.3389/fncir.2015.00085
495 Friston, K. (2010). The free-energy principle: A unified brain theory? Nature Reviews Neuroscience, 11(2),
496 127–138. hPps://doi.org/10.1038/nrn2787
497 Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., & Pezzulo, G. (2017). Ac!ve Inference: A Process
498 Theory. Neural Computa(cid:13)on, 29(1), 1–49. hPps://doi.org/10.1162/NECO_a_00912
499 Friston, K., Kilner, J., & Harrison, L. (2006). A free energy principle for the brain. Theore(cid:13)cal and
500 Computa(cid:13)onal Neuroscience: Understanding Brain Func(cid:13)ons, 100(1), 70–87.
501 hPps://doi.org/10.1016/j.jphysparis.2006.10.001
502 Hanajima, R., Shadmehr, R., Ohminami, S., Tsutsumi, R., Shirota, Y., Shimizu, T., Tanaka, N., Terao, Y., Tsuji, S.,
503 Ugawa, Y., Uchimura, M., Inoue, M., & Kitazawa, S. (2015). Modula!on of error-sensi!vity during a
28
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
504 prism adapta!on task in people with cerebellar degenera!on. Journal of Neurophysiology, 114(4),
505 2460–2471. hPps://doi.org/10.1152/jn.00145.2015
506 Has!e, T., Tibshirani, R., & Friedman, J. H. (2009). The elements of sta(cid:13)s(cid:13)cal learning: Data mining,
507 inference, and predic(cid:13)on. Springer. hPps://books.google.co.jp/books?id=eBSgoAEACAAJ
508 Hermann, V. Helmholtz., & CaPell, J. M. (1898). Handbuch der physiologischen Op!k. Science, 8(205), 794–
509 796. hPps://doi.org/10.1126/science.8.205.794.b
510 Inoue, M., Uchimura, M., Karibe, A., O’Shea, J., RosseK, Y., & Kitazawa, S. (2015). Three !mescales in prism
511 adapta!on. Journal of Neurophysiology, 113(1), 328–338. hPps://doi.org/10.1152/jn.00803.2013
512 Isomura, T., & Friston, K. (2020). Reverse-Engineering Neural Networks to Characterize Their Cost Func!ons.
513 Neural Computa(cid:13)on, 32(11), 2085–2121. hPps://doi.org/10.1162/neco_a_01315
514 Isomura, T., Kotani, K., Jimbo, Y., & Friston, K. J. (2023). Experimental valida!on of the free-energy principle
515 with in vitro neural networks. Nature Communica(cid:13)ons, 14(1), 4547.
516 hPps://doi.org/10.1038/s41467-023-40141-z
517 Isomura, T., Parr, T., & Friston, K. (2019). Bayesian Filtering with Mul!ple Internal Models: Toward a Theory
518 of Social Intelligence. Neural Computa(cid:13)on, 31(12), 2390–2431.
519 hPps://doi.org/10.1162/neco_a_01239
520 Isomura, T., Shimazaki, H., & Friston, K. J. (2022). Canonical neural networks perform ac!ve inference.
521 Communica(cid:13)ons Biology, 5(1), 55. hPps://doi.org/10.1038/s42003-021-02994-2
522 Isomura, T., Tanimoto, Y., Torigoe, M., Okamoto, H., & Shimazaki, H. (2025). Predic!ng individual learning
523 trajectories in zebrafish via the free-energy principle. bioRxiv : The Preprint Server for Biology.
524 hPps://doi.org/10.1101/2025.08.06.668947
525 Kim, S., Ogawa, K., Lv, J., Schweighofer, N., & Imamizu, H. (2015). Neural Substrates Related to Motor
526 Memory with Mul!ple Timescales in Sensorimotor Adapta!on. PLOS Biology, 13(12), e1002312.
527 hPps://doi.org/10.1371/journal.pbio.1002312
528 Kitazawa, S., Kohno, T., & Uka, T. (1995). Effects of delayed visual informa!on on the rate and amount of
529 prism adapta!on in the human. The Journal of Neuroscience, 15(11), 7644.
530 hPps://doi.org/10.1523/JNEUROSCI.15-11-07644.1995
531 Klar, M., Stein, S., Paterson, F., Williamson, J. H., & Murray-Smith, R. (2025). An ac!ve inference model of
532 mouse point-and-click behaviour. arXiv Preprint arXiv:2510.14611.
29
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
533 Kording, K. P., Tenenbaum, J. B., & Shadmehr, R. (2007). The dynamics of memory as a consequence of
534 op!mal adapta!on to a changing body. Nature Neuroscience, 10(6), 779–786.
535 hPps://doi.org/10.1038/nn1901
536 Kouw, W. M., & Loog, M. (2018). An introduc!on to domain adapta!on and transfer learning. arXiv Preprint
537 arXiv:1812.11806.
538 Kuśmierz, Ł., Isomura, T., & Toyoizumi, T. (2017). Learning with three factors: Modula!ng Hebbian plas!city
539 with errors. Computa(cid:13)onal Neuroscience, 46, 170–177. hPps://doi.org/10.1016/j.conb.2017.08.020
540 Lanillos, P., Franklin, S., Maselli, A., & Franklin, D. W. (2021). Ac!ve strategies for mul!sensory conflict
541 suppression in the virtual hand illusion. Scien(cid:13)fic Reports, 11(1), 22844.
542 hPps://doi.org/10.1038/s41598-021-02200-7
543 Luauté, J., Schwartz, S., RosseK, Y., Spiridon, M., Rode, G., Boisson, D., & Vuilleumier, P. (2009). Dynamic
544 Changes in Brain Ac!vity during Prism Adapta!on. The Journal of Neuroscience, 29(1), 169.
545 hPps://doi.org/10.1523/JNEUROSCI.3054-08.2009
546 Mirza, M. B., Cullen, M., Parr, T., Shergill, S., & Moran, R. J. (2021). Contextual percep!on under ac!ve
547 inference. Scien(cid:13)fic Reports, 11(1), 16223. hPps://doi.org/10.1038/s41598-021-95510-9
548 Newport, R., & Jackson, S. R. (2006). Posterior parietal cortex and the dissociable components of prism
549 adapta!on. Neuropsychologia, 44(13), 2757–2765.
550 hPps://doi.org/10.1016/j.neuropsychologia.2006.01.007
551 Paul, A., Isomura, T., & Razi, A. (2024). On Predic!ve Planning and Counterfactual Learning in Ac!ve
552 Inference. Entropy, 26(6). hPps://doi.org/10.3390/e26060484
553 Pawlak, V., Wickens, J. R., Kirkwood, A., & Kerr, J. N. D. (2010). Timing is not Everything: Neuromodula!on
554 Opens the STDP Gate. Fron(cid:13)ers in Synap(cid:13)c Neuroscience, 2, 146.
555 hPps://doi.org/10.3389/fnsyn.2010.00146
556 Pe!tet, P., O’Reilly, J. X., & O’Shea, J. (2018). Towards a neuro-computa!onal account of prism adapta!on.
557 Special Issue: Lesions and Brain Mapping, 115, 188–203.
558 hPps://doi.org/10.1016/j.neuropsychologia.2017.12.021
559 Pisella, L., Michel, C., Gréa, H., Tilikete, C., VighePo, A., & RosseK, Y. (2004). Preserved prism adapta!on in
560 bilateral op!c ataxia: Strategic versus adap!ve reac!on to prisms. Experimental Brain Research,
561 156(4), 399–408. hPps://doi.org/10.1007/s00221-003-1746-4
30
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
562 Pisella, L., RosseK, Y., Michel, C., Rode, G., Boisson, D., Pélisson, D., & Tilikete, C. (2005). Ipsidirec!onal
563 impairment of prism adapta!on aBer unilateral lesion of anterior cerebellum. Neurology, 65(1),
564 150–152. hPps://doi.org/10.1212/01.wnl.0000167945.34177.5e
565 Prablanc, C., Panico, F., Fleury, L., Pisella, L., Nijboer, T., Kitazawa, S., & RosseK, Y. (2020). Adap!ng
566 terminology: Clarifying prism adapta!on vocabulary, concepts, and methods. Neuroscience
567 Research, 153, 8–21. hPps://doi.org/10.1016/j.neures.2019.03.003
568 Redding, G. M., RosseK, Y., & Wallace, B. (2005). Applica!ons of prism adapta!on: A tutorial in theory and
569 method. Neuroscience & Biobehavioral Reviews, 29(3), 431–444.
570 hPps://doi.org/10.1016/j.neubiorev.2004.12.004
571 Redding, G., & Wallace, B. (2002). Strategie Calibra!on and Spa!al Alignment: A Model From Prism
572 Adapta!on. Journal of Motor Behavior, 34, 126–138. hPps://doi.org/10.1080/00222890209601935
573 Redding, G., & Wallace, B. (2006). Generaliza!on of Prism Adapta!on. Journal of Experimental Psychology.
574 Human Percep(cid:13)on and Performance, 32, 1006–1022. hPps://doi.org/10.1037/0096-1523.32.4.1006
575 RosseK, Y., Koga, K., & Mano, T. (1993). Prisma!c displacement of vision induces transient changes in the
576 !ming of eye-hand coordina!on. A"en(cid:13)on Percep(cid:13)on & Psychophysics, 54, 355–364.
577 hPps://doi.org/10.3758/BF03205270
578 Sakaguchi, Y., Akashi, Y., & Takano, M. (2001). Visuo-Motor Adapta!on to Stepwise and Gradual Changes in
579 the Environment: Rela!onship between Consciousness and Adapta!on. Journal of Robo(cid:13)cs and
580 Mechatronics, 13(6), 0601.
581 Smith, M. A., Ghazizadeh, A., & Shadmehr, R. (2006). Interac!ng Adap!ve Processes with Different
582 Timescales Underlie Short-Term Motor Learning. PLOS Biology, 4(6), e179.
583 hPps://doi.org/10.1371/journal.pbio.0040179
584 Tazawa, U. T., & Isomura, T. (2024). Synap!c pruning facilitates online Bayesian model selec!on. bioRxiv :
585 The Preprint Server for Biology. hPps://doi.org/10.1101/2024.05.15.593712
586 Welch, R. B., & Goldstein, G. (1972). Prism adapta!on and brain damage. Neuropsychologia, 10(4), 387–
587 394. hPps://doi.org/10.1016/0028-3932(72)90001-2
588 Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H., & He, Q. (2020). A comprehensive survey on
589 transfer learning. Proceedings of the IEEE, 109(1), 43–76.
590
31
bioRxiv preprint doi: https://doi.org/10.1101/2025.11.10.687533; this version posted November 11, 2025. The copyright holder for this
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.
591
592 Acknowledgements
593 T.I. is supported by the Japan Society for the Promo!on of Science (JSPS) KAKENHI under Grant
594 Number JP23H04973, the Japan Science and Technology Agency (JST) CREST under Grant Number
595 JPMJCR22P1, and the Japan Agency for Medical Research and Development (AMED) under Grant
596 Number JP23wm0625001. The funders had no role in study design, data collec!on and analysis,
597 decision to publish, or prepara!on of the manuscript.
598
599 Compe(cid:12)ng interest declara(cid:12)on
600 The authors declare no compe!ng interests.
32