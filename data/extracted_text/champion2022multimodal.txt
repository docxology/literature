Multi-Modal and Multi-Factor Branching Time Active Inference(BTAI 3MF).
Th´ eophile Champion tmac3@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Marek Grze´ s m.grzes@kent.ac.uk
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Howard Bowman H.Bowman@kent.ac.uk
University of Birmingham, School of Psychology,
Birmingham B15 2TT, United Kingdom
University of Kent, School of Computing
Canterbury CT2 7NZ, United Kingdom
Editor: TO BE FILLED
Abstract
Active inference is a state-of-the-art framework for modelling the brain that explains a wide range of mechanisms such as
habit formation, dopaminergic discharge and curiosity. Recently, two versions of branching time active inference (BTAI)
based on Monte-Carlo tree search have been developed to handle the exponential (space and time) complexity class that
occurs when computing the prior over all possible policies up to the time horizon. However, those two versions of BTAI
still suﬀer from an exponential complexity class w.r.t the number of observed and latent variables being modelled. In
the present paper, we resolve this limitation by ﬁrst allowing the modelling of several observations, each of them having
its own likelihood mapping. Similarly, we allow each latent state to have its own transition mapping. The inference
algorithm then exploits the factorisation of the likelihood and transition mappings to accelerate the computation of
the posterior. Those two optimisations were tested on the dSprites environment in which the metadata of the dSprites
dataset was used as input to the model instead of the dSprites images. On this task, BTAIV MP (Champion et al.,
2022b,a) was able to solve 96.9% of the task in 5.1 seconds, and BTAIBF (Champion et al., 2021a) was able to solve
98.6% of the task in 17.5 seconds. Our new approach ( BTAI3MF ) outperformed both of its predecessors by solving the
task completly (100%) in only 2.559 seconds. Finally, BTAI3MF has been implemented in a ﬂexible and easy to use
(python) package, and we developed a graphical user interface to enable the inspection of the model’s beliefs, planning
process and behaviour.
Keywords: Branching Time Active Inference, Monte-Carlo Tree Search, Belief Propagation, Bayesian Prediction,
Temporal Slice
1. Introduction
Active inference extends the free energy principle to generative models with actions (Friston et al., 2016; Costa et al.,
2020; Champion et al., 2021b) and can be regarded as a form of planning as inference (Botvinick and Toussaint,
2012). This framework has successfully explained a wide range of neuro-cognitive phenomena, such as habit formation
(Friston et al., 2016), Bayesian surprise (Itti and Baldi, 2009), curiosity (Schwartenbeck et al., 2018), and dopaminergic
discharges (FitzGerald et al., 2015). It has also been applied to a variety of tasks, such as animal navigation (Fountas
et al., 2020), robotic control (Pezzato et al., 2020; Sancaktar et al., 2020), the mountain car problem (C ¸ atal et al.,
2020), the game DOOM (Cullen et al., 2018) and the cart pole problem (Millidge, 2019).
However, active inference suﬀers from an exponential (space and time) complexity class that occurs when com-
puting the prior over all possible policies up to the time horizon. Recently, two versions of branching time active
inference (BTAI) based on Monte-Carlo tree search (Browne et al., 2012) have been developed to handle this exponen-
tial growth. In the original formulation of the framework (Champion et al., 2022b,a), inference was performed using
the variational message passing (VMP) algorithm (Winn and Bishop, 2005; Champion et al., 2021b). In a follow up
©2020 Th´ eophile Champion and Marek Grze´ s and Howard Bowman.arXiv:2206.12503v1  [cs.AI]  24 Jun 2022
Champion et al.
paper, VMP was then replaced by a Bayesian ﬁltering (Fox et al., 2003) scheme leading to a faster inference process
(Champion et al., 2021a).
In this paper, we develop an extension of Branching Time Active Inference (BTAI), to allow modelling of several
modalities as well as several latent states. Indeed, even if the Bayesian ﬁltering version of Branching Time Active
Inference (BTAIBF) is fast, its modelling capacity is limited to one observation and one hidden state. Consequently,
if one wanted to model nlatent states S1
t,...,S n
t , then those nlatent states would have to be encoded into one latent
state X representing all possible conﬁgurations of the n latent states S1
t,...,S n
t . Unfortunatly, the total number of
conﬁgurations is given by:
#X =
n∏
i=1
#Si
t ≥2n,
where #X is the number of possible values taken by X, and similarly # Si
t is the number of possible values taken by
Si
t. The above inequality is obtained by realizing that #Si
t ≥2, and is problematic in practice because #X is growing
exponentially with the number of latent states nbeing modelled. Also, note that in practice this exponential growth
may be way worse than 2 n. For example, if one were to model the ﬁve modalities of the dSprites environment (c.f.
Section 3.1), the total number of conﬁgurations would be:
#Sy
t ×#Sx
t ×#Sscale
t ×#Sorientation
t ×#Sscale
t = 33 ×32 ×3 ×40 ×6 = 760,320 ≫25 = 32.
A similar exponential explosion also appears when trying to model several modalities O1
t,...,O m
t using a single one
Y, i.e.
#Y =
m∏
i=1
#Oi
t ≥2m,
where #Y is the number of possible values taken by Y, and similarly # Oi
t is the number of possible values taken by
Oi
t. Note, throughout this paper, we will use the term states to refer to the latent states of the model at a specify
time step, e.g., S1
t,...,S n
t for time step t. Additionally, we will use the terms state conﬁgurations or values to refer
to particular values taken by the latent variables.
The present paper aims to remove those two exponential growths, by allowing the modelling of several observations
and latent states, while providing an easy to use framework based on a high-level notation, which allows the user
to create models by simply declaring the variables it contains, and the dependencies between those variables. Then,
the framework performs the inference process automatically. Appendix A shows an example of how to implement
a custom BTAI3MF agent using our framework. In section 2, we describe the theory underlying our approach.
Importantly, BTAI3MF takes advantage of the generative model struture to perform inference eﬃciently using a
mixture of belief propagation (Yedidia, 2011; Friston et al., 2017; Kschischang et al., 2001) and forward predictions
as will be explained in Section 2.3. The name BTAI3MF is an abbreviation for BTAIMMMF that stands for: Multi-
Modal and Multi-Factor Branching Time Active Inference. Next, in Section 2.4, we provide the deﬁnition of the
expected free energy in the context of our new approach, and in Section 2.5, we describe the planning algorithm
used to expand the generative model dynamically. Then, in Section 3, we compare BTAI3MF to BTAIVMP and
BTAIBF, and demonstrate empirically that BTAI3MF outperformed both BTAIVMP and BTAIBF on the dSprites
environment, which requires the modelling of many latent states and modalities. Finally, Section 4 concludes this
paper by summarizing our approach and results.
2. Theory ofBTAI 3MF
In this section, we introduce the mathematical foundation of BTAI3MF. To simplify the graphical representation
of our generative model, we ﬁrst introduce a notion of “temporal slice”. Then, we build on this idea to describe
the generative model of BTAI3MF. Next, we explain how belief updates are performed using a mixture of belief
propagation and forward predictions. Afterwards, we provide the deﬁnition of the expected free energy for this new
generative model. Finally, we describe the planning algorithm used to dynamically expand the generative model, and
the action selection process.
2
Multi-Modal and Multi-Factor BTAI.
2.1 Temporal slice
A temporal slice TSJ = {O1
J,...,O #O
J ,S1
J,...,S #S
J }is a set of random variables indexed by a sequence of actions
J. Each random variable of the temporal slice represents either an observation Oo
J or a latent state Ss
J. The index
of the temporal slice correponds to the sequence of actions that lead to this temporal slice. By deﬁnition, if J is an
empty sequence, i.e., J = ∅, then TSJ is the temporal slice of the present time step t, also denoted TSt. Within a
temporal slice TSJ, an observation Oo
J depends on a number of latent states ρo
J ⊆{Ss
J |s = 1,..., #S}, such that
P(Oo
J|ρo
J) is a factor in the generative model. Given an action a and a sequence of actions J, we let I = J::a be the
sequence of actions obtained by appending the action a at the end of the sequence of actions J. If I = J::a, then
the temporal slice TSJ can be the parent of TSI. This means that a latent state Ss
I in TSI can depend on the latent
states ρs
I ⊆{Ss
J |s = 1,..., #S}in TSJ, such that P(Ss
I|ρs
I) is a factor in the generative model. The concept of
temporal slice is illustrated in Figure 1, and Figure 2 depicts a more compact representation of the content of Figure
1.
TSt
Ss
t
s= 1,..., #S
Oo
t
o= 1,..., #O
TSI
Ss
I
s= 1,..., #S
Oo
I
o= 1,..., #O
Figure 1: This ﬁgure illustrates two temporal slices TSt and TSI, which are depicted by rectangles with thick border.
Within each temporal slice, plate notation is used to generate # S latent states and # O observations. The dashed
lines that connect two random variables from two diﬀerent plates are new to this paper, and represent an arbitrary
connectivity between the two sets of random variables generated by the plates. For example, the dashed line from Ss
t
to Oo
t, means that for each observation Oo
t, the parents of Oo
t denoted ρo
t is a subset of {Ss
t |s= 1,..., #S}, i.e., the
generative model contains the factor P(Oo
t|ρo
t) where ρo
t ⊆{Ss
t |s= 1,..., #S}.
TSt TSI
Figure 2: This ﬁgure illustrates the two temporal slices TSt and TSI from Figure 1 in a more compact fashion. Since
Oo
t is an observed variable for all o∈{1,..., #O}, the square representing TSt has a gray background. In contrast,
the square representing TSI has a white background because Oo
I is a latent variable for all o∈{1,..., #O}.
2.2 Generative model
In this section, we build upon the notion of temporal slice to describe the full generative model. Intuitively, the
probability of the entire generative model is the product of the probability of each temporal slice within the model.
This includes the current temporal slice TSt and the future temporal slices TSI for all I ∈I, where I is the set
of all multi-indices expanded during the tree search (c.f., Section 2.5). Within each temporal slice, there are # O
observations and #S latent states. Each observation depends on a subset of the latent states. Moreover, each latent
state depends on a subset of the latent states of the parent temporal slice. Note, the current temporal slice TSt
does not have any parents, therefore its latent state does not depend on anything. In other words, the model makes
the Markov assumption, i.e., each state only depends on the states at the previous time step. More formally, the
3
Champion et al.
generative model is deﬁned as:
P(Ot,St,OI,SI) = P(TSt)
∏
I∈I
P(TSI)
=
#O∏
o=1
P(Oo
t|ρo
t)
#S∏
s=1
P(Ss
t)
  
current temporal slice TSt
∏
I∈I
[#O∏
o=1
P(Oo
I|ρo
I)
#S∏
s=1
P(Ss
I|ρs
I)
  
future temporal slice TSI
]
where t is the current time step, ρx
τ is the set of parents of Xx
τ, Ot = {Oo
t | o = 1 ,..., #O}is the set of all
observations at time t, OI = {Oo
I |o= 1,..., #O}is the set of all future observations that would be observed after
performing the sequence of actions I, OI = ∪I∈IOI is the set of all future observations contained in the temporal
slices expanded during the tree search (c.f., Section 2.5), St = {Ss
t |s = 1,..., #S}is the set of all latent states at
time t, SI = {Ss
I |s= 1,..., #S}is the set of random variables describing the future latent states after performing
the sequence of actions I, SI = ∪I∈ISI is the set of latent variables representing all future states contained in the
temporal slices expanded during the tree search (c.f., Section 2.5). Importantly, the above generative model has to
satisfy:
• ∀I ∈I,∀o∈{1,..., #O},ρo
I ⊆SI;
• ∀I::a ∈I,∀s∈{1,..., #S},ρs
I::a ⊆SI, also, if I = ∅then by deﬁnition SI =∆ St.
Additionally, we deﬁne the factors of the generative model as:
P(Oo
t|ρo
t) = Cat(Ao), P (Ss
t) = Cat(Ds
t),
P(Oo
I|ρo
I) = Cat(Ao), P (Ss
I|ρs
I) = Cat(Bs
I),
where Ao is the tensor modelling the likelihood mapping of the o-th observation, Ds
t is the vector modelling the prior
over the s-th latent state at time t (see below for details), Bs is the tensor modelling the transition mapping of the
s-th latent state under each possible action, Bs
I is the tensor modelling the transition mapping of the s-th latent state
under the last action Ilast of the sequence I, i.e., Bs
I = Bs( •,..., •,Ilast). Also, note that at the beginning of a trial,
i.e., when t= 0, Ds
t is a vector that encodes the modeller’s understanding of the task. Afterwards, when t> 0, Ds
t is
a vector containing the parameters of the posterior over hidden states according to the observations made and actions
taken so far, i.e., P(Ss
t) =∆ P(Ss
t|O0:t−1,A0:t−1) = Cat(Ds
t) for all s ∈{1,..., #S}. Finally, Figure 3 illustrates the
full generative model using the notion of temporal slices.
TSt
TS(1) TS(2)
TS(11) TS(12) TS(21) TS(22)
I =
{
(1),(2),(11),(12)
}
Figure 3: This ﬁgure illustrates the full generative model of BTAI3MF. The temporal slices depited in light gray
correspond to temporal slices that have not yet been explored by the planning algorithm, c.f., Section 2.5. The
numbers between parentheses correspond to the sequence of actions performed to reach the temporal slice.
4
Multi-Modal and Multi-Factor BTAI.
2.3 Belief updates: the inference and prediction (IP) algorithm
The IP algorithm is composed of two steps, i.e., the inference step (or I-step) and the prediction step (or P-step).
The goal of the I-step is to compute the posterior beliefs over all the latent variables at time t. In other words, the
goal of the I-step is to compute: P(Ss
t|Ot),∀s ∈{1,..., #S}. The P-step takes as inputs the posterior beliefs over
all the latent variables corresponding to the states of the system after performing a sequence of actions I, and an
action a to be performed next. The goal of the P-step is to compute the posterior beliefs over all the latent variables
corresponding to the future states and observations after performing the sequence of actions I::a, where I::a is the
sequence of actions obtained by adding the action a at the end of the sequence of actions I. In other words, given
P(Ss
I|Ot),∀s∈{1,..., #S}and an action a, the goal of the P-step is to compute: P(Ss
I::a|Ot),∀s∈{1,..., #S}and
P(Oo
I::a|Ot),∀o∈{1,..., #O}. Note that by deﬁnition, we let P(Sm
I |Ot) =∆ P(Sm
t |Ot) if I = ∅. To derive the inference
and prediction steps, the following sections make use of the sum-rule, product-rule, and d-separation criterion (c.f.,
Appendix C for details about those properties).
2.3.1 Inference step
As just stated, the goal of the I-step is to compute P(Sm
t |Ot),∀m ∈{1,..., #S}. First, we re-write the posterior
computation to ﬁt the kind of problem that belief propagation — also known as the sum-product algorithm — can
solve:
P(Sm
t |Ot) ∝P(Sm
t ,Ot)
T∑
∼Sm
t
(Bayes theorem)
=
∑
∼Sm
t
P(St,Ot) (sum rule)
=
∑
∼Sm
t
#O∏
o=1
P(Oo
t|ρo
t)
#S∏
s=1
P(Ss
t) (product rule & d-separation)
where St = {Ss
t |s= 1,..., #S}is the set of all latent states at time t, ∼Sm
t = St \Sm
t is the set of all latent states
at time t except Sm
t , and the summation is over all possible conﬁgurations of ∼Sm
t , i.e., we are marginalizing out
all states, apart from one; thus P(St,Ot) has #S+ #O dimensions, while P(Sm
t ,Ot) has 1 + #O dimensions. Since
ρo
t ⊆St, the expression inside the summation is a function g(St) that factorizes as follows:
g(St) =
#O∏
o=1
P(Oo
t|ρo
t)
#S∏
s=1
P(Ss
t)
=∆
N∏
i=1
fi(Xi),
where Xi ⊆St for all i∈{1,..., #O+ #S}, the number of factors is N = #O+ #S, and:
fi(Xi) =∆
{
P(Oi
t|ρi
t) if i∈{1,..., #O}
P(Si−#O
t ) if i∈{#O+ 1,..., #O+ #S}.
Note that, because Oo
t (denoted Oi
t here) are known constants, we do not specify that g(St) depends on Oo
t. To
conclude, by substituting the deﬁnition of g(St) into the formula of the posterior P(Sm
t |Ot) presented above, we get:
P(Sm
t |Ot) ∝
∑
∼Sm
t
g(St),
5
Champion et al.
which means that the posterior P(Sm
t |Ot) can be computed by ﬁrst marginalizing g(St) w.r.t. Sm
t , i.e.,
g(Sm
t ) =
∑
∼Sm
t
g(St),
and then normalizing:
P(Sm
t |Ot) = g(Sm
t )∑
Sm
t
g(Sm
t ).
The marginalization of g(St) can be performed eﬃciently using belief propagation (Kschischang et al., 2001), which
can be understood as a message passing algorithm on a factor graph. The message from a node x to a factor f is
given by:
mx→f(x) =
∏
h∈n(x)\{f}
mh→x(x),
where n(x) are the neighbours of x in the factor graph. Note, in a factor graph the neighbours of a random variable
are factors. Moreover, the message from a factor f to a node x is given by:
mf→x(x) =
∑
Y
(
f(X)
∏
y∈Y
my→f(y)
)
,
where X = n(f) are the neighbours of f in the factor graph, Y = X\{x}are all the neighbours of f except x, and
the summation is over all possible conﬁgurations of the variables in Y. Note, in a factor graph the neighbours of a
factor are random variables. Once all the messages have been computed, the marginalization of g(St) w.r.t. Sm
t is
given by the product of all the incoming messages of the node Sm
t , i.e.,
g(Sm
t ) =
∏
f∈n(Sm
t )
mf→Sm
t (Sm
t ).
2.3.2 Prediction step
The P-step is analogous to the prediction step of Bayesian ﬁltering (Fox et al., 2003). Given P(Ss
I|Ot) for each
s∈{1,..., #S}and an action a, the goal of the P-step is to compute P(Ss
I::a|Ot) for each latent state s∈{1,..., #S}
and P(Oo
I::a|Ot) for each future observation o ∈{1,..., #O}. For the sake of brevity, we let J =∆ I::a. Let’s start
with the computation of P(Ss
I::a|Ot):
P(Ss
I::a|Ot) =∆ P(Ss
J|Ot) =
M∑
ρs
J
P(Ss
J,ρs
J|Ot) (sum rule)
=
M∑
ρs
J
P(Ss
J|ρs
J,Ot)P(ρs
J|Ot) (product rule)
=
M∑
ρs
J
P(Ss
J|ρs
J)P(ρs
J|Ot) (d-separation)
≈
∑
ρs
J
P(Ss
J|ρs
J)
#ρs
J∏
i=1
P(ρs
J,i|Ot) (mean-ﬁeld approximation)
where #ρs
J is the number of parents of Ss
J, and ρs
J,i is the i-th parent of Ss
J. Importantly, P(Ss
J|ρs
J) is known from the
deﬁnition of the generative model. Moreover, since ρs
J,i ∈SI, then P(ρs
J,i|Ot) = P(Sm
I |Ot) for some m∈{1,..., #S}.
6
Multi-Modal and Multi-Factor BTAI.
Thus, P(ρs
J,i|Ot) is given as input to the P-step, i.e., P(ρs
J,i|Ot) is a known distribution. Similarly, the computation
of P(Oo
I::a|Ot) proceeds as follows:
P(Oo
I::a|Ot) =∆ P(Oo
J|Ot) =
M∑
ρo
J
P(Oo
J,ρo
J|Ot) (sum rule)
=
M∑
ρo
J
P(Oo
J|ρo
J,Ot)P(ρo
J|Ot) (product rule)
=
M∑
ρo
J
P(Oo
J|ρo
J)P(ρo
J|Ot) (d-separation)
≈
∑
ρo
J
P(Oo
J|ρo
J)
#ρo
J∏
i=1
P(ρo
J,i|Ot) (mean-ﬁeld approximation)
where #ρo
J is the number of parents of Oo
J, and ρo
J,i is the i-th parent of Oo
J. Importantly, P(Oo
J|ρo
J) is known from the
deﬁnition of the generative model. Moreover, since ρo
J,i ∈SJ, then P(ρo
J,i|Ot) = P(Ss
J|Ot) for some s∈{1,..., #S}.
Thus, P(ρo
J,i|Ot) has already been computed during the ﬁrst stage of the P-step and is a known distribution, c.f.,
derivation of P(Ss
I::a|Ot) =∆ P(Ss
J|Ot).
2.4 Expected Free Energy
In this section, we discuss the deﬁnition of the expected free energy, which quantiﬁes the cost of pursuing a particular
sequence of actions and will be useful for planning, cf. Section 2.5. The expected free energy (see below) is composed
of the risk and ambiguity terms. The risk terms quantify how much the posterior beliefs over future observations
(computed by the P-step) diverge from the prior preferences of the agent. On the other hand, the ambiguity terms
correspond to the expected uncertainty of the likelihood mapping, where the expectation is with respect to the
posterior beliefs over states computed by the P-step.
First, we partition the set of observations OI = {Oo
I | o = 1 ,..., #O}into disjoint subsets XI
i, i.e., OI =
XI
1 ∪... ∪XI
N and XI
i ∩XI
j = ∅if i̸= j. Then, we deﬁne the prior preferences over the i-th subset of observations as:
V(XI
i) = Cat(Ci). This formulation allows us to deﬁne prior preferences over subsets of random variables, and will
be useful in Section 3.1, where the agent needs to possess preferences that depend upon both the shape and ( X,Y )
position of the object. Finally, the expected free energy, which needs to be minimised, is given by:
GI =∆
N∑
i=1
(
DKL[P(XI
i|Ot)||V(XI
i)]  
risk of i-th set of observations
)
+
#O∑
o=1
(
EP(ρo
I|Ot)[H[P(Oo
I|ρo
I)]]
  
ambiguity of o-th observation
)
, (1)
where P(XI
i|Ot) and P(ρo
I|Ot) are the posteriors over thei-th subset of observations and the parent ofOo
I, respectively,
and P(Oo
I|ρo
I) is known from the generative model. Assuming a mean-ﬁeld approximation, those posteriors are given
by:
P(ρo
I|Ot) ≈
#ρo
I∏
i=1
P(ρo
I,i|Ot)
P(XI
i|Ot) ≈
x∏
Oo
I ∈Xi
P(Oo
I|Ot)
where P(Oo
I|Ot) and P(ρo
I,i|Ot) are the posteriors overOo
I and the i-th parent of Oo
I, respectively. Note, both P(Oo
I|Ot)
and P(ρo
I,i|Ot) were computed during the P-step. The deﬁnition of the expected free energy given by (1) may not be
7
Champion et al.
very intuitive. Fortunatly, the special case where each subset contains a single observation, i.e., XI
o = Oo
I, leads to
the following equation:
GI =∆
#O∑
o=1
(
DKL[P(Oo
I|Ot)||V(Oo
I)]  
risk of o-th observation
+ EP(ρo
I|Ot)[H[P(Oo
I|ρo
I)]]
  
ambiguity of o-th observation
)
,
which is the summation over all observations Oo
I of the expected free energy of Oo
I, i.e., the risk of Oo
I plus the
ambiguity of Oo
I. Finally, our framework allows to specify prior preferences over only a subset of variables in OI. For
example, if a task contains four variables, i.e., Ox
I, Oy
I, Oshape
I and Oscale
I , but it only makes sense to have preferences
over three of them, i.e., Ox
I, Oy
I and Oshape
I , then the prior preference over the fourth variable is set to the posterior
over this random variable, i.e., V(Oshape
I ) =∆ P(Oshape
I |Ot). In other words, not having prior preferences over a random
variable is viewed by our framework as liking whatever we predict will happen. Eﬀectively, this renders the risk term
associated with such variable equal to zero, i.e.,
DKL[P(Oshape
I |Ot)||V(Oshape
I )] = DKL[P(Oshape
I |Ot)||P(Oshape
I |Ot))] = 0.
2.5 Planning: the MCTS algorithm
In this section, we describe the planning algorithm used by BTAI3MF. At the beginning of a trial when t= 0, the
agent is provided with the initial observations O0. The I-step is performed and returns the posterior over all latent
states, i.e., P(Ss
0|O0) for all s ∈{1,..., #S}, according to the prior over the initial hidden states provided by the
modeller, i.e., P(Ss
0) for all s∈{1,..., #S}, and the available observations O0.
Then, we use the UCT criterion to determine which node in the tree should be expanded. Let the tree’s root TSt
be called the current node. If the current node has no children, then it is selected for expansion. Alternatively, the
child with the highest UCT criterion becomes the new current node and the process is iterated until we reach a leaf
node (i.e. a node from which no action has previously been selected). The UCT criterion (Browne et al., 2012) for
the j-th child of the current node is given by:
UCTj = −¯Gj + Cexplore
√
ln n
nj
, (2)
where ¯Gj is the average expected free energy calculated with respected to the actions selected from the j-th child,
Cexplore is the exploration constant that modulates the amount of exploration at the tree level, n is the number of
times the current node has been visited, and nj is the number of times the j-th child has been visited.
Let SI be the (leaf) node selected by the above selection procedure. We then expand all the children of SI, i.e.,
all the states of the form SI::a, where a ∈{1,..., #A}is an arbitrary action, # A is the number of available actions,
and I::a is the multi-index obtained by appending the action a at the end of the sequence deﬁned by I. Next, we
perform the P-step for each action a, and obtain P(Ss
I::a|Ot) for each latent state s ∈{1,..., #S}and P(Oo
I::a|Ot)
for each future observation o∈{1,..., #O}.
Then, we need to estimate the cost of (virtually) taking each possible action. The cost in this paper is taken to
be the expected free energy given by (1). Next, we assume that the agent will always perform the action with the
lowest cost, and back-propagate the cost of the best (virtual) action toward the root of the tree. Formally, we write
the update as follows:
∀K ∈AI ∪{I}, GK ←GK + min
a∈{1,...,#A}
GI::a, (3)
where I is the multi-index of the node that was selected for (virtual) expansion, and AI is the set of all multi-indices
corresponding to ancestors of TSI. During the back propagation, we also update the number of visits as follows:
∀K ∈AI ∪{I}, n K ←nK + 1. (4)
8
Multi-Modal and Multi-Factor BTAI.
If we let Gaggr
K be the aggregated cost of an arbitrary node SK obtained by applying Equation 3 after each expansion,
then we are now able to express ¯GK formally as:
¯GK = Gaggr
K
nK
.
The planning procedure described above ends when the maximum number of planning iterations is reached.
2.6 Action selection
After performing planning, the agent needs to choose the action to perform in the environment. As discussed in
Section 3.1 of (Browne et al., 2012), many possible mechanisms can be used to select the action to perform in the
environment. BTAI3MF performs the action corresponding to the root child with the highest number of visits.
Formally, this is expressed as:
a∗= arg max
a∈{1,...,#A}
n(a), (5)
where a∗is the action performed in the environment, and n(a) is the number of visits of the root child corresponding
to action a.
2.7 Closing the action-perception cycle
After performing an action a∗ in the environment, the agent receives a new observation Ot+1, and needs to use this
observation to compute the posterior over the latent states at time t+ 1, i.e., P(Ss
t+1|Ot+1) for all s∈{1,..., #S}.
This can be achieved by performing the I-step, but requires the agent to have prior beliefs over the latent states at time
t+ 1, i.e., P(Ss
t+1) for all s∈{1,..., #S}, in addition to the new observation Ot+1 obtained from the environment.
In this paper, we deﬁne those prior beliefs as:
P(Ss
t+1) = P(Ss
I|Ot), for all s∈{1,..., #S},
where I = ( a∗) is a sequence of actions containing the action a∗ performed in the environment, P(Ss
I|Ot) is the
predictive posterior computed by the P-step when assuming that actiona∗is performed. In other words, the predictive
posterior P(Ss
I|Ot) computed by the P-step at timet, is used as an empirical priorP(Ss
t+1) at time t+1. This empirical
prior P(Ss
t+1) along with the new observation Ot+1 can then be used to compute the posterior P(Ss
t+1|Ot+1) for all
9
Champion et al.
s ∈{1,..., #S}. This posterior will be used to perform planning in the next action-perception cycle. Algorithm 1
concludes this section by summarizing our approach.
Algorithm 1: BTAI3MF: action-perception cycles (with relevant equations indicated in round brackets).
Input: env the environment,
O0 = {Oo
0 |o= 1,... #O}the initial observations,
A = {Ao |o= 1,... #O}the likelihood mapping of each observation,
B = {Bs |s= 1,..., #S}the transition mapping for each hidden state,
C = {Ci |i= 1,...N }the prior preferences of each subset of observations,
D0 = {Ds
0 |s= 1,... #S}the prior over each initial state,
N the number of planning iterations,
M the number of action-perception cycles.
space
P(Ss
0|O0) ←I-step(O0, A, D0) // I-step from Section 2.3.1
root←CreateTreeNode(
beliefs = P(Ss
0|O0), action = -1, cost = 0, visits = 1
) // Create the root node for the MCTS, where -1 is a dummy value
repeat M times
repeat N times
node←SelectNode(root) // Using (2) recursively
eNodes ←ExpandChildren(node, B) // P-step from Section 2.3.2 for each action
Evaluate(eNodes, A, C) // Compute (1) for each expanded node
Backpropagate(eNodes) // Using (3) and (4)
end
a∗←SelectAction(root) // Using (5)
Ot+1 ←env.Execute(a∗)
child←root.children[a∗] // Get root child corresponding to a∗
P(Ss
t+1) ←child.beliefs // Get the empirical prior P(Ss
t+1) = Cat(Ds
t+1)
P(Ss
t+1|Ot+1) ←I-step(Ot+1, A, Dt+1) // I-step from Section 2.3.1
root←CreateTreeNode(
beliefs = P(Ss
t+1|Ot+1), action = a∗, cost = 0, visits = 1
) // Create the root node of the next action-perception cycle
end
3. Results
In this section, we compare our new approach to BTAI with variational message passing ( BTAIVMP ) and BTAI
with Bayesian ﬁltering ( BTAIBF). Section 3.1 presents the simpliﬁed version of the dSprites environment on which
the agents are compared. Section 3.2 describes how the task is modelled by the BTAIVMP agent and reports its
performance, ﬁnally, Sections 3.3 and 3.4 do the same for the BTAIBF and BTAI3MF agents. For the reader
interested in implementing a custom BTAI3MF agent, Appendix A provides a tutorial of how to create such an
agent using our framework, and Appendix B desbribes a graphical user interface (GUI) that can be used to inspect
the model. This GUI displays the structure of the generative model and prior preferences, the posterior beliefs of
each latent variable, the messages sent throughout the factor graph to perform inference, the information related to
the MCTS algorithm, and the expected free energy (EFE) of each node in the future. It also shows how the EFE
decomposes into the risk and ambiguity terms.
3.1 dSprites Environment
The dSprites environment is based on the dSprites dataset (Matthey et al., 2017) initially designed for analysing
the latent representation learned by variational auto-encoders (Doersch, 2016). The dSprites dataset is composed of
images of squares, ellipses and hearts. Each image contains one shape (square, ellipse or heart) with its own scale,
10
Multi-Modal and Multi-Factor BTAI.
orientation, and ( X,Y ) position. In the dSprites environment, the agent is able to move those shapes around by
performing four actions (i.e., UP, DOWN, LEFT, RIGHT). To make planning tractable, the action selected by the
agent is executed eight times in the environment before the beginning of the next action-perception cycle, i.e., the X
or Y position is increased or decreased by eight between time step t and t+ 1. The goal of the agent is to move all
squares towards the bottom-left corner of the image and all ellipses and hearts towards the bottom-right corner of
the image, c.f. Figure 4.
Figure 4: This ﬁgure illustrates the dSprites environment, in which the agent must move all squares towards the
bottom-left corner of the image and all ellipses and hearts towards the bottom-right corner of the image. The red
arrows show the behaviour expected from the agent.
Since BTAI is a tabular model whose likelihood and transition mappings are represented using matrices, the agent
does not directly take images as inputs. Instead, the metadata of the dSprites dataset is used to specify the state
space. In particular, the agent observes the type of shape (i.e., square, ellipse, or heart), the scale and orientation
of the shape, as well as a coarse-grained version of the shape’s true position. Importantly, the original images are
composed of 32 possible values for both the X and Y positions of the shapes. A coarse-grained representation with a
granularity of two means that the agent is only able to perceive 16 ×16 images, and thus, the positions at coordinate
(0,0), (0 ,1), (1 ,0) and (1 ,1) are indistinguishable. Figure 5 illustrates the coarse grained representation with a
granularity of eight and the corresponding indices observed by the BTAIVMP and BTAIBF agents. Note that this
modiﬁcation of the observation space can be seen as a form of state aggregation (Ren and Krogh, 2002). Finally, as
shown in Figure 5, the prior preferences of the agent are speciﬁed over an absorbing row below the dSprites image.
This absorbing row ensures that the agent selects the action “down” when standing in the “appropriate corner”, i.e.,
bottom-left corner for squares and bottom-right coner for ellipses and hearts.
□ ♥
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
Figure 5: This ﬁgure illustrates the observations made by the agent when using a coarse-grained representation with
a granularity of eight on the input image. On the left, one can see an image from the dSprites dataset and a grid
containing red squares of 8 ×8 pixels. Any positions in those 8 ×8 squares are indistinguishable from the perspective
of the agent. Also, the bottom most row is an absorbing row used to specify the prior preferences of the agent, i.e.
the green square is the goal state and the orange squares correspond to undesirable states. Finally, the three tables on
the right contain the indices observed by the BTAIVMP and BTAIBF agents for each type of shape at each possible
position.
The evaluation of the agent’s performance is based on the reward obtained by the agent. Brieﬂy, the agent receives
a reward of −1, if it never enters the absorbing row or if it does so at the antipode of the appropriate corner. As
the agent enters the absorbing row closer and closer to the appropriate corner, its reward increases until reaching a
11
Champion et al.
maximum of 1. The percentage of the task solved (i.e., the evaluation metric) is calculated as follows:
P(solved) = total rewards + number of runs
2.0 ×number of runs .
Intuitively, the numerator shifts the rewards so that they are bounded between zero and two, and the denominator
renormalises the reward to give a score between zero and one. A score of zero therefore corresponds to an agent
always failing to enter the absorbing row or doing so at the antipode of the appropriate corner. In contrast, a score
of one corresponds to an agent always entering the absorbing row through the appropriate corner.
3.2 BTAIVMP modeling approach and results
In this section, we evaluate BTAIVMP (Champion et al., 2022b,a) on the dSprites environment. As shown in Figure
5, BTAIVMP observes one index for each possible conﬁguration of shape, and ( X,Y ) positions. Importantly, this
version of BTAI suﬀers from the exponential growth described in the introduction, and thus does not model the
scale and orientation modalities. Also, to make the inference and planning process tractable, the granularity of the
coarse-grained representation was set to four or eight. Table 1 provides the value of each hyper-parameter used by
BTAIVMP in this section. Note, the hyper-parameter values are the same for all BTAI models presented in this paper.
Only the number of action perception cycles, and the number of planning iterations may vary from one experiment
to the next.
Name Value
NB_SIMULATIONS 100
NB_ACTION_PERCEPTION_CYCLES 30
NB_PLANNING_STEPS 10, 25 or 50
EXPLORATION_CONSTANT 2.4
PRECISION_PRIOR_PREFERENCES 2
PRECISION_ACTION_SELECTION 100
EVALUATION_TYPE EFE
Table 1: The value of each hyper-parameter used by BTAIVMP in this section. NB_SIMULATIONS is the number of
simulations run during the experiment. NB_ACTION_PERCEPTION_CYCLES is the maximum number of actions executed
in each simulation, after which the simulation is terminated. NB_PLANNING_STEPS is the number of planning iterations
performed by the agent. EXPLORATION_CONSTANT is the exploration constant of the UCT criterion. PRECISION_
PRIOR_PREFERENCES is the precision of the prior preferences. PRECISION_ACTION_SELECTION is the precision of the
distribution used for action selection. EVALUATION_TYPE is the type of cost used to evaluate the node during the
tree search. Those hyper-parameters can be used to re-run the experiments using the code of the following GitHub
repository: https://github.com/ChampiB/Experiments_AI_TS.
Brieﬂy, the agent is able to solve 88.5% of the task when using a granularity of eight, c.f. Table 2. To understand
why BTAIVMP was not able to solve the task with 100% accuracy, let us consider the example of an ellipse at position
(24,31). With a granularity of eight, the agent perceives that the ellipse is in the bottom-right corner of the image,
i.e., in the red square just above the goal state in Figure 5. From the agent’s perspective, it is thus optimal to pick the
action “down” to reach the goal state. However, in reality, the agent will not receive the maximum reward because
its true X position is 24 instead of the optimal X position of 31.
12
Multi-Modal and Multi-Factor BTAI.
Planning iterations P(solved) Time (sec)
10 0.813 0.859 ±0.868
25 0.846 0.862 ±0.958
50 0.885 1.286 ±1.261
Table 2: The percentage of the dSprites environment solved by the BTAIVMP agent when using a granularity of
eight, c.f. Figure 5. The last column reports the average execution time required for one simulation and the associated
standard deviation.
As shown in Table 3, we can improve the agent’s perfomance, by using a granularity of four. This allows the
agent to diﬀerentiate between a larger number of ( X,Y ) positions, i.e., it reduces the size of the red square in Figure
5. With this setting, the agent is able to solve 96.9% of the task. However, when decreasing the granularity, the
number of states goes up, and so does the width and height of the A and B matrices. As a result, more memory
and computational time is required for the inference and planning process. This highlights a trade-oﬀ between the
agent’s performance and the amount of memory and time required. Indeed, a smaller granularity leads to better
performance, but requires more time and memory.
Planning iterations P(solved) Time (sec)
10 0.859 3.957 ±4.027
25 0.933 3.711 ±4.625
50 0.969 5.107 ±5.337
Table 3: The percentage of the dSprites environment solved by the BTAIVMP agent when using a granularity of
four. In this setting, there are 9 ×8 ×3 = 216 states. The last column reports the average execution time required
for one simulation and the associated standard deviation.
3.3 BTAIBF modeling approach and results
In this section, we evaluate BTAIBF (Champion et al., 2021a) on the dSprites environment. As shown in Figure
5, BTAIBF observes one index for each possible conﬁguration of shape, and ( X,Y ) positions. Also, to make the
inference and planning process tractable, the granularity of the coarse-grained representation was set to two, four or
eight. Table 4 provides the value of each hyper-parameter used byBTAIBF in this section. Note, the hyper-parameter
values are the same for all BTAI models presented in this paper. Only the number of action perception cycles, and
the number of planning iterations may vary from one experiment to the next.
13
Champion et al.
Name Value
NB_SIMULATIONS 100
NB_ACTION_PERCEPTION_CYCLES 20
NB_PLANNING_STEPS 50
EXPLORATION_CONSTANT 2.4
PRECISION_PRIOR_PREFERENCES 1
PRECISION_ACTION_SELECTION 100
EVALUATION_TYPE EFE
Table 4: The value of each hyper-parameter used by BTAIBF in this section. NB_SIMULATIONS is the number of
simulations run during the experiment. NB_ACTION_PERCEPTION_CYCLES is the maximum number of actions executed
in each simulation, after which the simulation is terminated. NB_PLANNING_STEPS is the number of planning iterations
performed by the agent. EXPLORATION_CONSTANT is the exploration constant of the UCT criterion. PRECISION_
PRIOR_PREFERENCES is the precision of the prior preferences. PRECISION_ACTION_SELECTION is the precision of the
distribution used for action selection. EVALUATION_TYPE is the type of cost used to evaluate the node during the
tree search. Those hyper-parameters can be used to re-run the experiments using the code of the following GitHub
repository: https://github.com/ChampiB/Branching_Time_Active_Inference.
As shown in Table 5, the agent is able to solve: 86.1% of the task when using a granularity of eight, 97.7% of
the task when using a granularity of four, and 98.6% of the task when using a granularity of two. However, as the
performance improves from 86.1% to 98.6%, the computational time required to run each simulation skyrockets from
around 50 milliseconds to around 17.5 seconds. In other words, a simulation with a granularity of two is 350 times
slower than a simulation with a granularity of eight.
Planning iterations Granularity P(solved) Time (ms)
50 8 0.861 49.93 ±36.4124
50 4 0.977 241.63 ±118.379
50 2 0.986 17503.8 ±12882.8
Table 5: The percentage of the dSprites environment solved by the BTAIBF agent when using a granularity of eight,
four and two. Note, when a granularity of two is used, there are 17 ×16 ×3 = 816 possible states. The last column
reports the average execution time required for one simulation and the associated standard deviation. Note, the
change in time granularity to milliseconds.
3.4 BTAI3MF modeling approach and results
In this section, we evaluate our new approach (BTAI3MF) on the dSprites environment. In contrast to what is shown
in Figure 5, BTAI3MF does not observe one index for each possible conﬁguration of shape, and ( X,Y ) positions.
Instead, BTAI3MF has ﬁve observed variables representing the shape, the orientation, the scale, as well as the X and
Y position, respectively. Each of those observed variable has its hidden state counterparts. Each observation depends
on its hidden state counterparts through an identity matrix. This parametrisation is common in the literature on
active inference, see (Sajid et al., 2021) for an example. The transition mappings of the hidden variables representing
the shape, orientation, and scale, are deﬁned as an indentity matrix. This forwards the state value at time t to the
next time step t+ 1. For the hidden variables representing the X and Y position of the shape, the transition is
set to reﬂect the dynamics of the dSprites environment when the actions taken are repeated eight times, i.e., if the
action “DOWN” is selected, then the agent’s position in Y will be decreased by eight before the start of the next
action-perception cycle (Fountas et al., 2020).
The hyper-parameters used in those simualtions are presented in Table 6. Note, the hyper-parameter values are
the same for all BTAI models presented in this paper. Only the number of action perception cycles, and the number
of planning iterations may vary from one experiment to the next.
14
Multi-Modal and Multi-Factor BTAI.
Table 7 shows the results obtained by BTAI3MF on the dSprites environment when running 100 trials. Due to
the change in the format of representations, the agent exhibits little increase in execution time as the granularity
decreases, however, in general, the capacity to solve the task increases with this reduction in granularity. When a
granularity of one is used, the agent is able to solve the task perfectly with 150 planning iterations.
Note, the agent using a granularity of 1 and 150 planning iterations is as fast as the agent using a granularity of
1 and 50 planning iterations. This is because as the number of planning iterations increases the agent requires more
computation time per action-perception cycle, but as the agent performance increases on the task, the agent reaches
the goal state faster, and therefore requires less action-perception cycles per simulation. To conclude, the agent with
150 planning iterations requires less action-perception cycles per simulation, but more time per action-perception
cycle than the agent with 50 planning iterations. The code relevant to this section is available at the following URL:
https://github.com/ChampiB/BTAI_3MF.
Name Value
NB_SIMULATIONS 100
NB_ACTION_PERCEPTION_CYCLES 50
NB_PLANNING_STEPS 50 or 100 or 150
EXPLORATION_CONSTANT 2.4
PRECISION_PRIOR_PREFERENCES 1
EVALUATION_TYPE EFE
Table 6: The value of each hyper-parameter used by BTAI3MF in this section. NB_SIMULATIONS is the number of
simulations run during the experiment. NB_ACTION_PERCEPTION_CYCLES is the maximum number of actions executed
in each simulation, after which the simulation is terminated. NB_PLANNING_STEPS is the number of planning iterations
performed by the agent. EXPLORATION_CONSTANT is the exploration constant of the UCT criterion.PRECISION_PRIOR_
PREFERENCES is the precision of the prior preferences. EVALUATION_TYPE is the type of cost used to evaluate the node
during the tree search. Those hyper-parameters can be used to re-run the experiments using the code of the following
GitHub repository: https://github.com/ChampiB/BTAI_3MF.
Planning iterations Granularity P(solved) Time (sec)
50 8 0.895 1.279 ±12.8
50 4 0.977 1.279 ±12.8
50 2 0.996 1.279 ±12.8
50 1 0.72 2.559 ±18.01
100 1 0.77 5.119 ±25.209
150 1 1 2.559 ±18.01
Table 7: This table presents the percentage of the dSprites environment solved by the BTAI3MF agent when using a
granularity of eight, four, two and one. Note, when a granularity of one is used, there are 33×32×3×40×6 = 760,320
possible state conﬁgurations. The last column reports the average execution time required of one simulation and the
associated standard deviation.
4. Conclusion
In this paper, we presented a new version of Branching Time Active Inference that allows for modelling of several
observed and latent variables. Taken together, those variables constitute a temporal slice. Within a slice, the model
is equipped with prior beliefs over the initial latent variables, and each observation depends on a subset of the latent
variables through the likelihood mapping. Additionally, the latent states evolve over time according to the transition
mapping that describes how each latent variable at time t+ 1 is generated from a subset of the hidden states at time
t and the action taken.
15
Champion et al.
At the beginning of each trial, the agent makes an observation for each observed variable, and computes the
posterior over the latent variables using belief propagation. Then, a Monte-Carlo tree search is performed to explore
the space of possible policies. During the tree search, each planning iteration starts by selecting a node to expand
using the UCT criterion. Then, the children of the selected node are expanded, i.e., one child per action. Next, the
posterior over the latent variables of the expanded nodes is computed by performing forward predictions using the
known transition mapping, and the posterior beliefs over the latent states of the node selected for expansion. Once
the posterior is computed, the expected free energy can be computed and back-propagated through the tree. The
planning process stops after reaching a maximum number of iterations.
In the results section, we compared our new approach, called BTAI3MF, to two earlier versions of branching
time active inference, named BTAIVMP (Champion et al., 2022b,a) and BTAIBF (Champion et al., 2021a). Brieﬂy,
at the current time step t: BTAIVMP performs variational message passing (VMP) with a variational distribution
composed of only one factor, BTAIBF performs exact inference using Bayes theorem, and BTAI3MF implements
belief propagation to compute the marginal posterior over each latent variable. For the hidden variables in the future,
BTAIVMP does the same mean-ﬁeld approximation as at time step tand performs VMP, BTAIBF performs Bayesian
prediction to compute the posterior over the only latent variable being modelled, and likewise, BTAI3MF performs
prediction to compute the posterior over all future latent variables.
Since, none of the aforementioned approaches are equipped with deep neural networks, we compared them on a
version of the dSprites environment in which the metadata of the dSprites dataset are used as inputs to the model
instead of the dSprites images. The best performance obtained by BTAIVMP was to solve 96.9% of the task in
5.1 seconds. Importantly, BTAIVMP was previously compared to active inference as implemented in SPM both
theoretically and experimentally (Champion et al., 2022b,a). BTAIBF was able to solve 98.6% of the task but at the
cost of 17.5 seconds of computation. Note, BTAIBF was using a granularity of two (i.e., 816 states) while BTAIVMP
was using a granularity of four (i.e., 216 states), which is whyBTAIBF seems to be three times slower thanBTAIVMP .
In reality, if BTAIBF had been using a granularity of four, it would have been much faster than BTAIVMP while
maintaining a similar performance, i.e., around 96.9% of the task solved. Finally, BTAI3MF outperformed both of its
predecessors by solving the task completely (100%, granularity of 1) in only 2.559 seconds. Importantly, BTAI3MF
was able to model all the modalities of the dSprites environment for a total of 760 ,320 possible states.
In addition to the major boost in performance and computational time, BTAI3MF provides an improved mod-
elling capacity. Indeed, the framework can now handle the modelling of several observed and latent variables, and
takes advantage of the factorisation of the generative model to perform inference eﬃciently. As described in detail
in Appendix A, we also provide a high level notation for the creation of BTAI3MF that aims to make our approach
as staightforward as possible to apply to new domains. The high-level notational language allows the user to create
models by simply declaring the variables it contains, and the dependencies between those variables. Then, the frame-
work performs the inference process automatically. Moreover, driven by the need for interpretability, we developed a
graphical user interface to analyse the behaviour and reasoning of our agent, which is described in Appendix B.
There are two major directions of future research that may be explored to keep scaling up this framework. First,
BTAI3MF is not yet equipped with deep neural networks (DNNs), and is therefore unable to handle certain types
of inputs, such as images. In addition to the integration of DNNs into the framework, further research should be
performed in order to learn useful sequences of actions. Typically, in the current version of BTAI3MF, we built in
the fact that each action should be repeated eight times in a row. This inductive bias works well in the context of
the dSprites environment, but may be a limitation in other contexts.
It is also worth reﬂecting on how the BTAI3MF model sits with theories of brain function. In this respect, it is
interesting to consider neural correlates of the “standard” approach that BTAI3MF is being placed in opposition to.
As previously discussed, this standard active inference approach could be considered as monolithically tabular; that
is, the key matrices, such as the likelihood mapping (the A matrix) and the transition mapping (the B matrix), grow
in size exponentially with the number of states and observations. This is simply due to a combinatorial explosion,
e.g. the set of all combinations of states grows intractably with the number of states.
How would the combinations of states in the monolithic tabular approach be represented in the brain? The
obvious neural correlate would be conjunctive (binding) neurons (O’Reilly and Rudy, 2001), which become active
when multiple feature values are present; for example, one might have a neural unit for every X, Y combination in
the dSprites environment. If this is to be realised with a fully localist code, i.e. one unit for every combination, in
16
Multi-Modal and Multi-Factor BTAI.
the absence of any hierarchical structure, the required number of conjunctive units would explode in the same way
as the A and B matrices do. This is why some models have proposed a binding resource that supports distributed
(rather than localist) representations (Bowman and Wyble, 2007), which scale more tractably.
BTAI3MF avoids this combinatorial explosion by not combining features, enabling them to be represented sepa-
rately. In a very basic sense, this separated representation is consistent with the observation that the brain contains
distinct, physically separated, feature maps, e.g. Itti et al. (1998). Thus, at least to some extent, diﬀerent feature
dimensions are processed separately in the brain, as they are in BTAI3MF.
The time-slice idea in BTAI3MF assumes a kind of discrete synchronising global clock. Thus, even though features
have been separated from one another and may be considered to execute in diﬀerent parts of the system, they update
in lock-step. That is, implicitly, time is a binder, it determines which values of diﬀerent feature dimensions/states are
associated, e.g. an X-dimension value is associated with a particular Y-dimension value because they are so assigned
in the same temporal slice. In this sense, in BTAI3MF, time synchronisation resolves the binding problem.
This aspect of BTAI3MF resonates with theories of binding based upon oscillatory synchrony (Uhlhaas et al.,
2009). These theories suggest that diﬀerent feature dimensions are bound by the corresponding neurons ﬁring in
synchrony relative to an ongoing oscillation, with that ongoing oscillation potentially playing the role of a global clock.
Such oscillatory synchrony can be seen as a way to resolve the binding problem that does not require conjunctive
units.
Conjunction error experiments, e.g. Botella et al. (2001), are also relevant here. In these experiments, participants
make errors in associating multiple feature dimensions, perceiving illusory percepts, e.g. if a red K is presented before a
blue A in a rapid serial visual presentation stream, in some cases, a red A and a blue K is perceived. These experiments
ﬁrstly, re-emphasize that diﬀerent feature dimensions are processed separately, as perBTAI3MF: if feature dimensions
were not separated, then conjunction errors could not happen. Additionally though, these experiments suggest that
there is not a “perfect” synchronising global clock, since if there were, there would not be any conjunction errors even
despite separation of feature dimensions. Generating such conjunction error patterns is an interesting topic for future
BTAI3MF modelling work.
Acknowledgments
TO BE FILLED
References
J Botella, M Suero, and MI Barriopedro. A model of the formation of illusory conjunctions in the time domain.
Journal of experimental psychology. Human perception and performance , 27(6):1452—1467, December 2001. ISSN
0096-1523. doi: 10.1037//0096-1523.27.6.1452. URL https://doi.org/10.1037//0096-1523.27.6.1452.
Matthew Botvinick and Marc Toussaint. Planning as inference. Trends in Cognitive Sciences, 16(10):485 – 488, 2012.
ISSN 1364-6613. doi: https://doi.org/10.1016/j.tics.2012.08.006.
Howard Bowman and Brad Wyble. The simultaneous type, serial token model of temporal attention and working
memory. Psychological review, 114(1):38, 2007.
C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. Perez, S. Samoth-
rakis, and S. Colton. A survey of monte carlo tree search methods.IEEE Transactions on Computational Intelligence
and AI in Games , 4(1):1–43, 2012.
Th´ eophile Champion, Marek Grze´ s, and Howard Bowman. Branching Time Active Inference with Bayesian Filtering,
2021a.
Th´ eophile Champion, Marek Grze´ s, and Howard Bowman. Realizing Active Inference in Variational Message Passing:
The Outcome-Blind Certainty Seeker. Neural Computation, 33(10):2762–2826, 09 2021b. ISSN 0899-7667. doi:
10.1162/neco a 01422. URL https://doi.org/10.1162/neco_a_01422.
17
Champion et al.
Th´ eophile Champion, Howard Bowman, and Marek Grze´ s. Branching time active inference: Empirical study and
complexity class analysis. Neural Networks, 2022a. ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2022.
05.010. URL https://www.sciencedirect.com/science/article/pii/S0893608022001824.
Th´ eophile Champion, Lancelot Da Costa, Howard Bowman, and Marek Grze´ s. Branching time active inference: The
theory and its generality. Neural Networks, 151:295–316, 2022b. ISSN 0893-6080. doi: https://doi.org/10.1016/j.
neunet.2022.03.036. URL https://www.sciencedirect.com/science/article/pii/S0893608022001149.
Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and Karl Friston. Active inference
on discrete state-spaces: a synthesis, 2020.
Maell Cullen, Ben Davey, Karl J. Friston, and Rosalyn J. Moran. Active inference in openai gym: A paradigm
for computational investigations into psychiatric illness. Biological Psychiatry: Cognitive Neuroscience and Neu-
roimaging, 3(9):809 – 818, 2018. ISSN 2451-9022. doi: https://doi.org/10.1016/j.bpsc.2018.06.010. URL http:
//www.sciencedirect.com/science/article/pii/S2451902218301617. Computational Methods and Modeling
in Psychiatry.
Carl Doersch. Tutorial on variational autoencoders, 2016.
Thomas H. B. FitzGerald, Raymond J. Dolan, and Karl Friston. Dopamine, reward learning, and active inference.
Frontiers in Computational Neuroscience , 9:136, 2015. ISSN 1662-5188. doi: 10.3389/fncom.2015.00136. URL
https://www.frontiersin.org/article/10.3389/fncom.2015.00136.
Zafeirios Fountas, Noor Sajid, Pedro A. M. Mediano, and Karl Friston. Deep active inference agents using Monte-Carlo
methods, 2020.
V. Fox, J. Hightower, Lin Liao, D. Schulz, and G. Borriello. Bayesian ﬁltering for location estimation.IEEE Pervasive
Computing, 2(3):24–33, 2003. doi: 10.1109/MPRV.2003.1228524.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, John O Doherty, and Giovanni Pezzulo.
Active inference and learning. Neuroscience & Biobehavioral Reviews , 68:862 – 879, 2016. ISSN 0149-7634. doi:
https://doi.org/10.1016/j.neubiorev.2016.06.022.
Karl J. Friston, Thomas Parr, and Bert de Vries. The graphical brain: Belief propagation and active inference.
Network Neuroscience, 1(4):381–414, 2017. doi: 10.1162/NETN \a\00018. URL https://doi.org/10.1162/
NETN_a_00018.
L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis.IEEE Transactions
on Pattern Analysis and Machine Intelligence , 20(11):1254–1259, 1998. doi: 10.1109/34.730558.
Laurent Itti and Pierre Baldi. Bayesian surprise attracts human attention. Vision Research, 49(10):1295 – 1306,
2009. ISSN 0042-6989. doi: https://doi.org/10.1016/j.visres.2008.09.007. URL http://www.sciencedirect.com/
science/article/pii/S0042698908004380. Visual Attention: Psychophysics, electrophysiology and neuroimag-
ing.
Frank R Kschischang, Brendan J Frey, and H-A Loeliger. Factor graphs and the sum-product algorithm. IEEE
Transactions on information theory, 47(2):498–519, 2001.
Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement testing sprites
dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
Beren Millidge. Combining active inference and hierarchical predictive coding: A tutorial introduction and case
study., 2019. URL https://doi.org/10.31234/osf.io/kf6wc.
Randall C O’Reilly and Jerry W Rudy. Conjunctive representations in learning and memory: principles of cortical
and hippocampal function. Psychological review, 108(2):311, 2001.
18
Multi-Modal and Multi-Factor BTAI.
Corrado Pezzato, Carlos Hernandez, and Martijn Wisse. Active inference and behavior trees for reactive action
planning and execution in robotics, 2020.
Zhiyuan Ren and B.H. Krogh. State aggregation in Markov decision processes. In Proceedings of the 41st IEEE
Conference on Decision and Control, 2002. , volume 4, pages 3819–3824 vol.4, 2002. doi: 10.1109/CDC.2002.
1184960.
Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. Active Inference: Demystiﬁed and Compared. Neural
Computation, 33(3):674–712, 03 2021. ISSN 0899-7667. doi: 10.1162/neco a 01357. URL https://doi.org/10.
1162/neco_a_01357.
Cansu Sancaktar, Marcel van Gerven, and Pablo Lanillos. End-to-end pixel-based deep active inference for body
perception and action, 2020.
Philipp Schwartenbeck, Johannes Passecker, Tobias U Hauser, Thomas H B FitzGerald, Martin Kronbichler, and Karl
Friston. Computational mechanisms of curiosity and goal-directed exploration. bioRxiv, 2018. doi: 10.1101/411272.
URL https://www.biorxiv.org/content/early/2018/09/07/411272.
Peter Uhlhaas, Gordon Pipa, Bruss Lima, Lucia Melloni, Sergio Neuenschwander, Danko Nikoli´ c, and Wolf Singer.
Neural synchrony in cortical networks: history, concept and current status. Frontiers in integrative neuroscience,
3:17, 2009.
John Winn and Christopher Bishop. Variational message passing. Journal of Machine Learning Research, 6:661–694,
2005.
Jonathan S. Yedidia. Message-passing algorithms for inference and optimization. Journal of Statistical Physics ,
145(4):860–890, Nov 2011. ISSN 1572-9613. doi: 10.1007/s10955-011-0384-7. URL https://doi.org/10.1007/
s10955-011-0384-7 .
Ozan C ¸ atal, Tim Verbelen, Johannes Nauta, Cedric De Boom, and Bart Dhoedt. Learning perception and planning
with deep active inference, 2020.
Appendix A: How to create aBTAI 3MF agent?
In this appendix, we describe how to build a BTAI3MF agent using our framework. The relevant code can be found
in the ﬁle main_BTAI_3MF.py at the following URL: https://github.com/ChampiB/BTAI_3MF. Any script running
a BTAI3MF agent must start by instantiating an environment in which the agent will be run. Our code provides an
implementation of the dSprites environment, which can be created as follow:
# Create the environment.
env = dSpritesEnv(granularity=1, repeat=8)
env = dSpritesPreProcessingWrapper(env)
The ﬁrst line creates the dSprites environment, the second makes sure that the observations generated by the envi-
ronment are in the format expected by the agent. Once the environment has been created, we need to deﬁne the
parameters of the model. Assume that we want to have a latent variable Sshape
t representing the shape in the current
image. This variable can takes three values, i.e., zero for squares, one for ellipses and two for hearts. In this case, the
parameters of the prior over Sshape
t may be created as:
# Create the parameters of the prior over the latent variable shape.
d = {}
d["S_shape"] = torch.tensor([0.2, 0.3, 0.5])
19
Champion et al.
The ﬁrst line above creates a python dictionary, the second line adds a vector of parameters in the dictionary. This
vector can be accessed using the key “S shape”, which corresponds to the name of the latent variable. The values
in d[“S shape”] mean that a priori the agent believes it will observe a square with probability 0.2, an ellipse with
probability 0.3, and a heart with probability 0.5. Also, by convention, the name of a latent variable must start with
“S ”. Similarly, if we assume that the shape is provided to the agent through an observed variable Oshape
t , we can
create the parameters of the likelihood mapping for this variable as:
# Create the parameters of the likelihood mapping for the shape variable.
a = {}
a["O_shape"] = torch.eye(3)
The ﬁrst line above creates a python dictionary, and the second line adds a 3 ×3 identity matrix1 in the dictionary.
This reﬂects the fact that there is a one-to-one relationship between the value taken by Sshape
t and Oshape
t . Also, by
convention, the observations name must start with “O ”. Since, deﬁning all the parameters manually can be tedious,
our framework provides built-in functions that return the model parameters for the dSprites environment. Using
those functions, the parameters can be retrieved as follows:
# Define the parameters of the generative model.
a = env.a()
b = env.b()
c = env.c()
d = env.d(uniform=True)
Once all the parameters have been created, it is time to deﬁne the structure of the generative model. This can be
done using a temporal slice builder, which is an object used to facilitate the creation of a temporal slice. First, we
need to create the builder as follows:
# Create the temporal slice builder.
ts_builder = TemporalSliceBuilder("A_1", env.n_actions)
The builder takes two parameters, i.e., the name of the action random variable (i.e., “A 1”) that must start by “A ”,
and the number of possible actions (i.e., env.n actions = 4). Then, we need to tell the builder what state variables
should be created, and what are the parameters of the prior beliefs over those variables. For the dSprites environment,
this can be done as follows:
# Add the latent states of the model to the temporal slice.
ts_builder.add_state("S_pos_x", d["S_pos_x"]) \
.add_state("S_pos_y", d["S_pos_y"]) \
.add_state("S_shape", d["S_shape"]) \
.add_state("S_scale", d["S_scale"]) \
.add_state("S_orientation", d["S_orientation"])
The function “add state” adds a state variable to the temporal slice. The ﬁrst parameter of this function is the name
of the state to be added, and the second argument is the parameters of the prior beliefs over this new state. Next, we
need to add the variables corresponding to the observations made by the agent. For the dSprites environment, this
can be done as follows:
# Define the likelihood mapping of the temporal slice.
ts_builder.add_observation("O_pos_x", a["O_pos_x"], ["S_pos_x"]) \
.add_observation("O_pos_y", a["O_pos_y"], ["S_pos_y"]) \
.add_observation("O_shape", a["O_shape"], ["S_shape"]) \
.add_observation("O_scale", a["O_scale"], ["S_scale"]) \
.add_observation("O_orientation", a["O_orientation"], ["S_orientation"])
1. Note, in practice the identity matrix is noisy to avoid taking the logarithm of zero.
20
Multi-Modal and Multi-Factor BTAI.
The function “add observation” adds an observation variable to the temporal slice. The ﬁrst parameter of this
function is the name of the observation to be added, the second argument is the parameters of the likelihood mapping
for this new observation, and the third parameter is the list of parents on which the observation depends. The next
step is the deﬁnition of the transition mapping for each hidden state, which can be performed as follows:
# Define the transition mapping of the temporal slice.
ts_builder.add_transition("S_pos_x", b["S_pos_x"], ["S_pos_x", "A_1"]) \
.add_transition("S_pos_y", b["S_pos_y"], ["S_pos_y", "A_1"]) \
.add_transition("S_shape", b["S_shape"], ["S_shape"]) \
.add_transition("S_scale", b["S_scale"], ["S_scale"]) \
.add_transition("S_orientation", b["S_orientation"], ["S_orientation"])
The function “add transition” adds a transition mapping to the temporal slice. The ﬁrst parameter of this function
is the name of the state for which the transition is deﬁned, the second argument is the parameters of the transition
mapping for this state, and the third parameter is the list of parents on which the state depends. Importantly, in
the above snippet of code, only the states representing the position in x and y of the shape depends on the action
variable “A 1”. The ﬁnal step is about the deﬁntion of the prior preferences of the agent, and can be done as follows:
# Define the prior preferences of the temporal slice.
ts_builder.add_preference(["O_pos_x", "O_pos_y", "O_shape"], c["O_shape_pos_x_y"])
The function “add preference” adds some prior preferences to the temporal slice. The ﬁrst parameter of this function
is the list of observations for which the prior preferences are deﬁned, and the second argument are the parameters of
the prior preferences for those observations. At this stage, the initial temporal slice can be built:
# Create the initial temporal slice.
ts = ts_builder.build()
Once the initial temporal slice has been created, it is possible to instantiate the agent and implement the action-
perception cycle as follows:
# Create the agent.
agent = BTAI_3MF(ts, max_planning_steps=150, exp_const=2.4)
# Implement the action-perception cycles.
n_trials = 100
for i in range(n_trials):
obs = env.reset()
env.render()
agent.reset(obs)
while not env.done():
action = agent.step()
obs = env.execute(action)
env.render()
agent.update(action, obs)
Most of the above code is self explanatory. Put simply, this code runs “n trials” simulations of the dSprites en-
vironment. The line “action = agent.step()” performs inference, planning and action selection. The line “obs =
env.execute(action)” executes the selected action in the environment, and the line “agent.update(action, obs)” up-
dates the agent so that it has taken into account the action taken in the environment and the observations received.
Appendix B: How to inspect aBTAI 3MF agent?
In this appendix, we describe how to analyse a BTAI3MF agent using our graphical user interface (GUI). The relevant
code can be found in the ﬁleanalysis_BTAI_3MF.py at the following URL:https://github.com/ChampiB/BTAI_3MF.
21
Champion et al.
The ﬁrst step is to create the environment and agent as described in Appendix A. Then, we create a GUI object and
run the main loop as follows:
# Create the GUI for analysis.
gui = GUI(env, agent)
gui.loop()
The above two lines should open a graphical user interface as shown in Figure 6. When clicking on the node of the
current temporal slice TS(t), one can obtain additional information about this temporal slice, c.f., Figure 8. When
clicking on the button named “Next planning iteration” in Figure 6, a planning iteration is performed and the tree
displayed on the right-hand-side of this frame is updated as shown in Figure 7. When clicking on the root’s children,
e.g., “TS(1)”, it is possible to navigate through the tree created by the MCTS algorithm as shown in Figure 9. When
“TS(1)” is displayed as the new root as in Figure 9, clicking on “TS(1)” again will display the information of this
node as depicted by Figure 10. Finally, Figure 11 shows how the ambiguity term of the expected free energy can be
decomposed into its component parts.
Figure 6: This ﬁgure illustrates the visualisation frame of the GUI used to analyse a BTAI3MF agent. The image
corresponding to the current state of the environment is displayed in the upper-left corner. Under the image are four
buttons allowing the user to: reset the environment and agent, perform the next planning iteration, perform all the
remaining planning iterations, and perform the current best action in the environment. Finally, on the right hand
side of the image is a depiction of the MCTS planning, where TS(t) represents the current temporal slice. At the
moment, the current temporal slice has no children, and therefore its children are displayed in orange with the text
“None”. Additionally, the current slice has no parent because it is the tree’s root. Therefore, the arrow above the
TS(t) node is also orange.
22
Multi-Modal and Multi-Factor BTAI.
Figure 7: This ﬁgure illustrates the visualisation frame of the GUI used to analyse a BTAI3MF agent after performing
one planning iteration. The children of the root node are now available. One of them is displayed in green, it
corresponds to the best action found so far by the MCTS algorithm. The root node has a red square surronding it,
which means that it was selected for expansion by the MCTS algorithm.
Figure 8: This ﬁgure illustrates the frame displaying the information of the current temporal slice of the BTAI3MF
agent. Six widgets are displayed. The ﬁrst displays the structure of the likelihood model using the factor graph
formalism. On this graph, we see that the model is composed of ﬁve obervations and ﬁve hidden states. Each
observation depends on only one hidden state. The second widget displays the structure of the transition mapping.
We see that only two hidden states depend on the action taken by the agent, i.e., the hidden states corresponding
to the X and Y position of the shape. The third widget shows the structure of the prior preferences. Here, there is
only one factor over three random variables, i.e., the shape and its (X, Y) position. Note, when moving your mouse
over a variable in the likelihood, transition or prior preference widget the complete name of the variable is displayed,
e.g., when moving over “S1” the label “S shape” is displayed. The fourth widget illustrates the posterior over the
latent variable corresponding to the x position of the shape. The random variable whose posterior is displayed can be
changed either by using the combo box in the bottom-right corner of the widget or by clicking on a latent variable in
the likelihood model widget. The ﬁfth widget displays information related to the Monte-Carlo tree search. Finally,
the last widget illustrates the message sent from the observation variable corresponding to the X position of the shape
to its likelihood factor.
23
Champion et al.
Figure 9: This ﬁgure illustrates what happens when clicking on the child “TS(1)” in Figure 7. Put simply, “TS(1)”
becomes the new root and we see that its children have not been expanded yet. Additionally, the arrow above the
“TS(1)” node is gray meaning that this node has a parent, i.e., “TS(t)”. Clicking on this arrow leads us back to
Figure 7.
Figure 10: This ﬁgure illustrates what happens when clicking on “TS(1)” in Figure 9. Most of the widgets have
already been explained with the exception of the one in the bottom right-corner, which displays how the expected
free energy decomposes into risk (blue box) and ambiguity (red box). When clicking on the blue or red box, the
decomposition of the risk or ambiguity term is displayed as shown in Figure 11.
24
Multi-Modal and Multi-Factor BTAI.
Figure 11: This ﬁgure illustrates how the ambiguity term decomposes into the ambiguity of the likelihood of each
observed variable, i.e., the ambiguity of “O shape” in blue, “O scale” in red, “O orientation” in orange, “O pos x”
in green, and “O pos y” in gray.
Appendix C: sum-rule, product-rule and d-separation criterion.
In this appendix, we explain three important properties than are used in the core of the paper, namely: the sum-rule
and product-rule of probability and the d-separation criterion.
Sum-rule of probability
Given a set of random variables X = {X1,...,X n}, and a joint distribution P(X1,...,X n) over X. The sum-rule allows
to sum out a subset of the random variables. Here are a few examples:
P(X1,...,X n−1) =
∑
Xn
P(X1,...,X n),
P(X1,...,X n−2) =
∑
Xn−1
∑
Xn
P(X1,...,X n),
P(X1,...,X n−3) =
∑
Xn−2
∑
Xn−1
∑
Xn
P(X1,...,X n).
Note, the sum-rule can also be used with a conditional distribution P(X1,...,X n|Y1,...,Y m), for examples:
P(X1,...,X n−1|Y1,...,Y m) =
∑
Xn
P(X1,...,X n|Y1,...,Y m),
P(X1,...,X n−2|Y1,...,Y m) =
∑
Xn−1
∑
Xn
P(X1,...,X n|Y1,...,Y m),
P(X1,...,X n−3|Y1,...,Y m) =
∑
Xn−2
∑
Xn−1
∑
Xn
P(X1,...,X n|Y1,...,Y m).
Product-rule of probability
Given a set of random variables X = {X0,...,X n}, and a joint distribution P(X0,...,X n) over X. The product-rule
allows us to factorise the joint into a product of factors without doing any conditional independence assumptions
about P(X1,...,X n). More formally:
P(X0,...,X n) = P(Xn)
n−1∏
i=0
P(Xi|Xi+1:n),
25
Champion et al.
where Xi:j = {Xi,...,X j}is the set of random variables containing all the variables between Xi and Xj (included).
Note, the product-rule can also be used with a conditional distribution P(X0,...,X n|Y1,...,Y m):
P(X0,...,X n|Y1,...,Y m) = P(Xn|Y1,...,Y m)
n−1∏
i=0
P(Xi|Xi+1:n,Y1,...,Y m).
The d-separation criterion
The d-separation criterion is a tool than can be used to check whether two sets of random variables ( X and Y) are
independent given a third set of random variables Z. More formally, the d-separation criterion is a tool to check
whether X ⊥ ⊥Y |Z. Knowing that X ⊥ ⊥Y |Z holds in a distribution P is useful because if X ⊥ ⊥Y |Z, then:
P(X,Y |Z) = P(X|Y,Z)P(Y|Z) (product-rule)
= P(X|Z)P(Y|Z). (X ⊥ ⊥Y |Z)
First, let G= (X,E) be a graph over a set of nodes Xconnected by a set of directed edges E. Given two nodes in the
graph (i.e., Ni,Nj ∈X), we note: (i) Ni →Nj if there is a directed edge from Ni to Nj in the graph, (ii) Ni ←Nj if
the graph contains a directed edge from Nj to Ni, and (iii) Ni ⇄ Nj if (i) or (ii) holds. Second, we say that there is
a trail between two nodes (i.e., N1,Nn) in the graph, if there is a sequence of distinct nodes N = (N1,...,N n), such
that: Ni ⇄ Ni+1 holds for all i∈{1,...,n −1}. Third, we say that a trail between N1 and Nn is active if: (a) each
time there is a v-structure (i.e., Ni−1 →Ni ←Ni+1) in the trail, then either Ni or (at least) one of its descendants
are in Z, and (b) no other node along the trail are in Z. Finally, we say that X and Y are d-separated by Z if for all
Xi ∈X and Yi ∈Y there is no active trail between Xi and Yi (given Z).
Using our terminology, the d-separation criterion states that if X and Y are d-separated by Z in a graph G
representing the factorisation of a distribution P, then X ⊥ ⊥Y |Z holds in the distribution P. Intuitively, the d-
separation criterion help us to determine whether X ⊥ ⊥Y |Z holds in P by looking at the topology of the graph G.
For example, consider the Bayesian network illustrated in Figure 12, and let P be the joint distribution represented
by this Bayesian network. Using the product rule, we get:
P(A,B,C,D,E,F ) = P(F|A,B,C,D,E )P(E|A,B,C,D )P(C|A,B,D )P(D|A,B)P(B|A)P(A).
Note, that all trails between C and A,D are blocked by B, i.e., there is no active trails between C and A,D given B.
Thus, we have C ⊥ ⊥A,D |B and:
P(A,B,C,D,E,F ) = P(F|A,B,C,D,E )P(E|A,B,C,D )P(C|B)P(D|A,B)P(B|A)P(A).
Moreover, there is no active trail between B and A given ∅, therefore B ⊥ ⊥A|∅ and:
P(A,B,C,D,E,F ) = P(F|A,B,C,D,E )P(E|A,B,C,D )P(C|B)P(D|A,B)P(B)P(A).
Using the same reasoning, one can see that F ⊥ ⊥A,B,C,D |E and thus:
P(A,B,C,D,E,F ) = P(F|E)P(E|A,B,C,D )P(C|B)P(D|A,B)P(B)P(A).
Finally, using the d-separation one more time leads to the following factorisation for P:
P(A,B,C,D,E,F ) = P(F|E)P(E|B,D )P(C|B)P(D|A,B)P(B)P(A).
A B C
D E F
Figure 12: This ﬁgure illustrates a Bayesian network in which the following independences assumptions hold: A ⊥
⊥B|∅; A,D ⊥ ⊥C|B; and A⊥ ⊥E|D,B,C . In contrast, the following independences assumptions does not hold :
A⊥ ⊥B|D; A⊥ ⊥E|B,C; and A⊥ ⊥B|E .
26