Probabilistic Principles for
Biophysics and Neuroscience
Entropy production,
Bayesian mechanics
&
the free-energy principle.
A thesis presented for the degree of
Doctor of Philosophy of Imperial College London
by,
Lancelot Da Costa
Department of Mathematics
Imperial College London
180 Queen’s Gate, London SW7 2AZ
October 2023
arXiv:2410.11735v1  [math-ph]  15 Oct 2024
Abstract
This thesis focuses on three fundamental aspects of biological systems; namely, entropy production, Bayesian
mechanics, and the free-energy principle. The contributions are threefold: 1) We compute the entropy
production for a greater class of systems than before, including almost any stationary diffusion process, such
as degenerate diffusions where the driving noise does not act on all coordinates of the system. Importantly,
this class of systems encompasses Markovian approximations of stochastic differential equations driven by
colored noise, which is significant since biological systems at the macro- and meso-scale are generally subject
to colored fluctuations. 2) We develop a Bayesian mechanics for biological and physical entities that interact
withtheirenvironmentinwhichwegivesufficientandnecessaryconditionsfortheinternalstatesofsomething
to infer its external states, consistently with variational Bayesian inference in statistics and theoretical
neuroscience. 3) We refine the constraints on Bayesian mechanics to obtain a description that is more
specific to biological systems, called the free-energy principle. This says that active and internal states of
biological systems unfold as minimising a quantity known as free energy. The mathematical foundation to
the free-energy principle, presented here, unlocks a first principles approach to modeling and simulating
behavior in neurobiology and artificial intelligence, by minimising free energy given a generative model of
external and sensory states.
1
Acknowledgements
I dedicate this thesis to my loving partner Pauline Chatellard who continuously and restlessly supports me
through highs and lows, and so for the past many years. I could not have done this without you.
My heartfelt gratitude goes to my awesome team of supervisors, Grigorios A. Pavliotis and Karl Friston,
who have been incredibly supportive, helpful, and insightful throughout my research journey. Through your
mentorship you have both given me a rich and complementary perspective, and a unique technical expertise.
It is in large part to you that I owe my present and future academic success.
I thank my family for their support; especially my brother for inspiring me to be dedicated, resilient, and
never give up, my father for encouraging me to pursue mathematics and neuroscience, and stimulating my
curiosity since childhood, my mother for supporting me whatever happened, and teaching me to be kind and
generous in all areas of life, and my aunt for being there in the most difficult moments. I am grateful to my
longtime friends, particularly Nicolas Ward, Mathieu Binder, Arnaud Pedrazzani, Baptiste Pesanti, Ritwick
Sundar, Stéphane Nordin, and Wojtek Reise.
Thank you to my amazing colleagues and collaborators. Your perspectives have been mind expanding, and
many of you have become dear friends over the years. You are the reason why the PhD has been such a
fun, interesting and rewarding experience. At the risk of omitting many, I particularly thank Conor Heins,
Maxwell Ramstead, Thomas Parr, Kai Ueltzhöffer, Noor Sajid, Alessandro Barp, Victorita Neacsu, Laura
Convertino, Anjali Bhat, Ryan Smith, Biswa Sengupta, Pablo Lanillos, Tim Verbelen, Dalton Sakthivadivel,
Théophile Champion, Guillermo B. Morales, Guilherme França, Aswin Paul, Magnus T. Koudahl, Beren
Millidge, Peter Zeidman, Anil Seth, Christopher Buckley, Zafeirios Fountas, Alexey Zakharov, Umais Zahid,
Sergio Rubin, Cyrus Mostajeran, Come Annicchiarico, Jeremie Mattout, Lars Sandved-Smith, Antoine Lutz,
Danijar Hafner, Mahault Albarracin, Riddhi Pitliya Jain and Jonas Mago. Not least, I am deeply grateful
to Michael I. Jordan, Joshua B. Tenenbaum, Philipp Hennig, and Samuel Gershman for invaluable career
advice. Lastly, I thank my friends and classmates from the Centre for Doctoral Training in Mathematics
of Random Systems, and particularly Rémy Messadene, Felix Prenzel, Alain Rossier, Victoria Klein, Julian
Sieber, Benedikt Petko, Jonathan Tam, and Alessandro Micheli.
I thank the staff of the Centre for Doctoral Training in Mathematics of Random Systems, particularly our
centre manager Lydia Noa, for always being available, incredibly helpful, and supportive; and the directors,
who took a chance on me in spite of my atypical academic background.
Last but not least, I will always be grateful too the Fonds National de la Recherche of Luxembourg (Project
code: 13568875) for funding my PhD research. Your generous funding has allowed me to pursue my PhD
free of many administrative burdens, with unparalleled intellectual freedom, and with the ability to attend
and contribute to many scientific events.
2
Integrity
I certify that this thesis, and the research to which it refers, are the product of my own work, and that
any ideas or quotations from the work of other people, published or otherwise, are fully acknowledged in
accordance with the standard referencing practices of the discipline. In addition, I certify that I have obtained
the necessary copyright allowances for reproducing some of my own academic papers in this thesis.
Copyright
The copyright of this thesis rests with the author. Unless otherwise indicated, its contents are licensed under
a Creative Commons Attribution-Non Commercial 4.0 International Licence (CC BY-NC).
Under this licence, you may copy and redistribute the material in any medium or format. You may also
create and distribute modified versions of the work. This is on the condition that: you credit the author and
do not use it, or any derivative works, for a commercial purpose.
When reusing or sharing this work, ensure you make the licence terms clear to others by naming the licence
and linking to the licence text. Where a work has been adapted, you should indicate that the work has been
changed and describe those changes.
Please seek permission from the copyright holder for uses of this work that are not included in this licence
or permitted under UK Copyright Law.
3
Contents
Abstract 1
Acknowledgements 2
Integrity 3
Copyright 3
1 Introduction 9
1.1 Background and motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.2 Overview of thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.3 Limitations of thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.4 A note on notations and terminologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.5 Publications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
1.6 Awards for PhD work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2 Entropy production 17
2.1 Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
2.2 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
2.3 The ep of stationary Markov processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.4 Time reversal of stationary diffusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.5 The ep of stationary diffusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
2.6 Examples and ep of numerical simulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
2.7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
4
2.8 Addendum: Proofs for Chapter 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
3 Bayesian mechanics 62
3.1 Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.2 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.3 Markov blankets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
3.4 Bayesian mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
3.5 Active inference and stochastic control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
3.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
3.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
3.8 Addendum: Proofs for Chapter 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
4 The free-energy principle 83
4.1 Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
4.2 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
4.3 Systems, states and fluctuations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
4.4 Solutions, steady-states and nonequilibria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
4.5 Particles, partitions and things . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
4.6 From self-organisation to self-evidencing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
4.7 Lagrangians, generalised states and Bayesian filtering . . . . . . . . . . . . . . . . . . . . . . . 97
4.8 From statistical to classical particles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
4.9 Path integrals, planning and curious particles . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
4.10 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
5 Conclusion 115
5.1 Summary of thesis achievements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
5.2 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
5.3 Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
Data availability 117
5
Author contributions 117
Bibliography 117
6
List of Figures
2.1 Helmholtz decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.2 Entropy production as a function of time-irreversible drift . . . . . . . . . . . . . . . . . . . . 33
2.3 Exact simulation of linear diffusion process withbirr(x) ∈ Range σ . . . . . . . . . . . . . . . 38
2.4 Exact simulation of linear diffusion process withbirr(x) ̸∈ Range σ . . . . . . . . . . . . . . . 39
2.5 Exact simulation of underdamped Langevin dynamics . . . . . . . . . . . . . . . . . . . . . . 41
2.6 Euler-Maruyama simulation of underdamped Langevin dynamics . . . . . . . . . . . . . . . . 42
2.7 BBK simulation of underdamped Langevin dynamics . . . . . . . . . . . . . . . . . . . . . . . 43
3.1 Markov blanket . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
3.2 Synchronisation map: example and non-example . . . . . . . . . . . . . . . . . . . . . . . . . 69
3.3 Markov blanket evolving in time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
3.4 Processes at a Gaussian steady-state . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
3.5 Variational inference and predictive processing, averaging internal variables for any blanket
state . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
3.6 Variational inference and predictive processing . . . . . . . . . . . . . . . . . . . . . . . . . . 74
3.7 Markov blanket evolving in time comprising sensory and active states . . . . . . . . . . . . . 76
3.8 Active inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
3.9 Stochastic control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
3.10 Continuous-time Hidden Markov model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
4.1 Markov blankets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
4.2 Autonomous flows and Bayesian filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
4.3 Markov blankets and self-evidencing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
7
4.4 Generic and precise particles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
4.5 Bayesian mechanics and active inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
4.6 Sentient behaviour and action observation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
4.7 Expected free energy and active inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
4.8 Bayesian mechanics and active inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
4.9 Epistemic foraging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
8
Chapter 1
Introduction
1.1 Background and motivation
The ambition for this thesis started many years ago as I was finishing Part III of the Mathematical Tripos
at Cambridge. I already knew that I wanted to work in neuroscience – deriving a fundamental theory of
brain function that could also be used for principled artificial intelligence. A friend of the time, Sebastiano
Cultrera, told me about the free-energy principle. This is a principle postulated by the British neuroscientist
Karl Friston promising a powerful and unifying account of brain function in terms of minimising one single
objective – variational free-energy [1]. I did not understand the theory at the time but I thought it – given my
idealistic pure mathematician’s background – extremely exciting and a promising starting point towards the
endgoalofacompletemathematicaltheoryofthebrain. Followingmymastersinmathematics, Iundertooka
masters in brain science at UCL where I was very fortunate to work with Karl Friston himself on applications
of the free-energy principle for modelling neural dynamics in the brain. During that year, I realised that the
free-energy principle was a physicist’s intuition that did not yet admit rigorous mathematical and physical
underpinnings – although attempts led by Karl Friston over the last decade had made considerable progress
towards this. I realised that there was a significant opportunity for a mathematician like me to develop
this fundamental theory. To this end, I organised an inter-university co-supervision with Professor Pavliotis,
a world-renowned expert in stochastic differential equations – the type of mathematics that seemed to be
needed to formalise the principle – and Karl Friston.
This collaboration and co-supervision has exceeded my expectations for learning and contributing to math-
ematical neuroscience and various related fields. For instance, I learned that the free-energy principle had
been postulated to hold in biological systems and some physical systems, and a burgeoning line of work –
which started at about the same time as I started my PhD – applies the free-energy principle to generate
intelligent agents for artificial intelligence.
My PhD work has been devoted to developing the mathematical foundation of the free-energy principle
for neuroscience and biophysics more broadly, as a description of human and biological cognition, and
establishing principled and rigorous methodologies for its application in artificial intelligence. I also explored
its potential for generating intelligent agents that are robust, whose decisions are explainable, and that are
9
safe to interact with, which are current main challenges in artificial intelligence [2]. The overarching goal
was to obtain an unbroken narrative from mathematical physics, to cognitive neuroscience, and finally to
artificial intelligence – and a coherent framework to think about these fields.
This thesis focuses on three highlights from my PhD’s work which pertain to fundamental principles in
biophysics and neuroscience, and practically ignores all of the applications to artificial intelligence. The
reason for this is twofold: to keep the length of this thesis within reasonable bounds, and to focus on some
of the most mathematically significant aspects of my PhD project.
1.2 Overview of thesis
This thesis starts with the fundamental assumption any fundamental theory for biophysics and neuroscience
has to be consistent with the rest of physics. Its starting point must therefore be a stochastic differential
equation (a.k.a. a Langevin equation)
˙xt = f(xt) +wt. (1.1)
This equation decomposes the motion of a system into a deterministic partf(xt) governed by a vector field
f , which summarises what we know about the system (this part is usually called thedrift or theflow), and
a stochastic partwt which summarises what we don’t know about the system (this part is usually called the
noise or therandom fluctuations). So why start here? Because this description is consistent with the rest of
physics. Forinstance, stochasticdifferentialequationsformthebasisofstatisticalmechanics[3]. Inparticular
classical mechanics is the limit of statistical mechanics as particles become large and random fluctuations
on the motion become infinitesimally small. Note that we did not commit to any specific interpretation of
the stochastic differential equation at this stage – e.g. Stratonovich or Ito – and we did not commit to any
specific assumptions about the functional form of the random fluctuations. This will come in later.
This thesis is the collection of three papers from my PhD’s work which study three fundamental aspects of
biological systems; namely, entropy production, Bayesian mechanics, and the free-energy principle.
1.2.1 Chapter 2: Entropy production1
One of the fundamental traits of living systems is time-reversibility and entropy production. Entropy pro-
duction measures the amount of time-irreversibility present in a system, that is to what extent the process
as time goes forward differs from the process as time goes backwards. The entropy production also measures
the minimum amount of energy consumed by the system for the period of time over which it is measured,
or equivalently the minimal amount of heat dissipated by the system during that period of time.
In this chapter we develop a comprehensive and general theory for the entropy production of systems de-
scribed by stochastic differential equations, and contextualise it within the broader literatures of probability
and statistical physics.
1Adapted from: L Da Costa, GA Pavliotis. The entropy production of stationary diffusions.Journal
of Physics A: Mathematical and Theoretical. 2023.
10
From a mathematical perspective we are able to give a computable expression for the entropy production in
a wider range of equations than was previously available, allowing for so-called degenerate diffusions which
are SDEs where the noise process need not act on all coordinates. Although we focus on diffusion processes,
that is SDEs driven by white noise, the fact that we allow degenerate diffusions means that the results
apply to Markovian approximations of systems driven by coloured noise. This is fundamentally important
because any biological system that is not modelled at a molecular level will plausibly be subject to coloured
fluctuations. Although we focus on Euclidean state spaces, our results can easily be generalised to manifold
state spaces.
Technically, we show that any stationary diffusion can be decomposed into a time reversible part and a
time irreversible part, which generalises the classical Helmholtz-Hodge decomposition from vector calculus.
We show that this decomposition is equivalent to writing the Fokker-Planck equation of the system in pre-
GENERIC form, where GENERIC stands for General Equations for Non-Equilibrium Reversible-Irreversible
Coupling – a prominent framework for analysing Dynamics in non-equilibrium statistical physics pioneered
by Ottinger [4]. This is also equivalent to a decomposition of the backward Kolmogorov equation of the
system considered in hypocoercivity theory, a theory developed by C. Villani for quantifying the rate of
convergence of dynamics to their non-equilibrium steady state [5].
All in all, this paper reviews essential concepts related to entropy production and time irreversibility and
provides a wide range of tools for statistical physicists and biological physicists to study non-equilibrium
phenomena.
1.2.2 Chapter 3: Bayesian mechanics2
The main idea that underwrites this chapter, is that if a ’thing’ (e.g. a cell) and its surrounding environment,
together defining a ’system’xt, have an attracting stationary state, then the thing can be interpreted as
being adaptive in the sense that its interactions on the environment look as if the thing is bringing itself –
and the environment – towards a preferred set of states (i.e. homeostatic) following any kind of perturbation.
The implications of this simple observation are quite rich. For instance, the existence of a thing and what is
not the thing entails the existence of boundary statesbt between the internal states of the thingµt and the
states of every thing elseηt. We formalise the idea of a boundary between a thing and its environment as a
Markov blanket of the stationary densityp (i.e. conditional independence given the blanket states)
µt ⊥ ηt | bt, where xt = (µt, bt, ηt) ∼ p. (1.2)
that exists over the period of time that the thing exists.
The existence of a Markov blanket means that internal states synchronise with external states via vicarious
interactions through the blanket. We can go beyond this and show that internal states encode probabilistic
beliefs about external states and update this continuously in the face of new blanket states (e.g. new sensory
information) consistently with variational Bayesian inference in theoretical neuroscience and statistics.
2Adapted from: L Da Costa, K Friston, C Heins, GA Pavliotis. Bayesian mechanics for stationary
processes. Proceedings of the Royal Society A. 2021.
11
If we further decompose blanket states into sensory and active states, then we can show that active states
controlthesystembacktoitsstationarystatedefiningitsrangeofpreferredstatesfollowinganyperturbation,
consistently with active inference in theoretical neuroscience and generalized forms of PID (Proportional-
Integral-Derivative) control in engineering.
Since the publication of this article, it has become clear that the proper way to define a boundary between
a thing and everything else is through a Markov blanket over trajectories, that is the fulfilment of (1.2)
where states are replaced by paths over some period of time, as explored in the next Chapter of this thesis.
Indeed, although articulated in a different language, this coincides with the notion of a multipartite process
in statistical physics [6,7]. Fortunately for this paper, a Markov blanket over paths often entails a Markov
blanket over states. This was first realized empirically in simulations of a primordial soup [8], and then in
further simulations involving sparsely coupled stochastic Lorenz systems [9]. The sparse coupling conjecture
claims that for sufficiently high dimensional and nonlinear random dynamical systems, a Markov blanket over
paths implies the existence of a Markov blanket over states (partial demonstrations are given in [10,11]). In
particular, this conjecture always holds in SDEs driven by coloured noise that can be expanded in generalized
coordinates [12,13].
1.2.3 Chapter 4: the free-energy principle3
Chapter 3 goes one step further in defining the type of constraints that a biological organism must comply
with, and derives stronger conclusions. In this chapter, the boundary between an organism and its environ-
ment is defined as a Markov blanket over paths. As before, the boundary is composed of sensory and active
states. Further, sensory states are permitted to influence internal states directly but internal states may
not directly influence sensory states. Similarly for active states: they may influence external states directly,
however external states may not directly influence active states. These influences are summarised by sparse
coupling in the flow/drift vector field in the stochastic differential equation defining the whole system.
With this special set up, the conclusions of chapter 2 still hold and then some. We can show that the
internal and active parts of the organism have paths of least action (i.e. most likely path) that minimise
a quantity known as variational free-energy – the objective that used in variational Bayesian inference and
theoretical neuroscience to describe the purpose of brain function. When we consider stochastic differential
equations driven by coloured noise, we can show that internal dynamics correspond to generalized (Bayesian)
filtering [14] – perhaps the most generic filtering algorithm in the literature.
When we consider organisms at a sufficiently high-level of coarse-graining such that the dynamics of the
organism itself (and not necessarily its environment) are governed by classical mechanics, we can see that the
most likely path of internal and active states satisfy a principle of least action by minimizing quantity known
as expected free-energy. The expected free-energy is the analog of the variational free-energy for trajectories.
What is interesting is that by examining the expected free-energy carefully we recover as special cases several
objective functions that are used to describe optimal behavior in various fields of science and engineering,
e.g. economics, machine learning, and psychology. This suggests that several of these theories that have
been arrived at independently may be unified and contextualized within a generic free-energy framework.
3Adapted from:K Friston, L Da Costa, N Sajid, C Heins, K Ueltzhoeffer, GA Pavliotis, T Parr. The
free energy principle made simpler but not too simple.Physics Reports. 2023
12
Pragmatically, the free-energy principle on offer gives us equations of motion, in terms of minimizing varia-
tional and expected free-energy, for describing and simulating intelligent biological behavior. This is shown
by rehearsing some simulations from the literature such as a simulation of handwriting and a simulation of
saccadic eye movements.
Much remains to be done in terms of analyzing the precise relationship between the free-energy principle and
other theories of purposeful biological behavior that exist in the literature. Much also remains in refining
the principle to obtain a description that is exclusive to the human brain, with more specific implications
for neuroscience and artificial intelligence. However, the vast body of work utilizing the free-energy principle
for modeling and generating intelligent biological behavior both in neuroscience and artificial intelligence,
and its mathematical foundation – in terms of statistical physics – presented in this thesis suggests that the
free-energy principle provides a solid foundation for a physics of sentient and cognitive beings.
1.3 Limitations of thesis
Perhaps the main limitation of this thesis is the reliance on white noise assumptions for the fluctuations
in the SDE (1.1) in chapter 1 (Entropy production) and chapter 3 (the free-energy principle). Clearly
white noise may be a good model for the random fluctuations driving systems at the molecular level (e.g.
molecular biology) but is not fit for purpose for describing complex biological systems at a higher level of
coarse graining, because in that case random fluctuations are usually the output of other dynamical systems,
and thus have a nontrivial autocorrelation structure. In these two chapters the only place where coloured
noise is treated explicitly is in chapter 3. (Note that there exists a refinement of chapter 3 that is specific to
the case of coloured noise [13], however it is less mathematically rigorous than the current state of chapter
3).
Practically all of my fourth year of PhD has been devoted developing rigorous analysis, numerical simulation
and filtering techniques for SDEs driven by coloured noise. My original plan was to include this in the thesis
as an additional chapter placed between current chapters 1 and 2. However, the administration of Imperial
College has been very inflexible in refusing to grant me an unpaid two month extension to the submission
deadline, the time I need to complete this work. As a result I am not including this in my thesis submission,
but would be happy to add it during revision stage as, crucially, it 1) nicely complements chapter 1, addressing
its main limitations, 2) underpins chapters 2 and 3, and 3) offers a systematic treatment of a fundamentally
important topic in the modelling and simulation of biological and neural systems.
1.4 A note on notations and terminologies
While I have done my best to keep notation and terminology consistent across papers, i.e. chapters, there
remain discrepancies which reflect the fact that each of the papers was originally written with a different
audience in mind. For instance, probabilists use the letterb for the drift of a stochastic differential equation,
while physicists use the letterf and call it the flow. Here I mostly chose to maintain the notation and
terminology of the community that each paper was originally addressed to. This highlights the parallels
13
between different and converging fields of research, which is perhaps the main implicit thread linking the
various chapters.
1.5 Publications
My privileged position as one of the only mathematicians working on the free-energy principle, a topic that
is becoming widely adopted in neuroscience and other application domains, means that I have been able and
fortunate to collaborate on a wide range of projects, with collaborators of many different disciplines, which
has been an incredibly mind expanding and enthralling experience.
1.5.1 List of papers (32)
∗ Corresponding author,† Equal contribution,⋆ Highlighted work,⋄ Cover article.
Manuscripts Under Review (6)
N Da Costa, M Pförtner,L Da Costa, P Hennig. Sample Path Regularity of Gaussian Processes from the Covariance
Kernel. Under review in JMLR. arXiv:2312.14886. 2023.
KFriston, LDaCosta , etal. Supervisedstructurelearning. Under review in Neural Computation. arXiv:2311.10300
2023.
C Heins, B Millidge,L Da Costa, RP Mann, K Friston, ID Couzin. Collective behavior from surprise minimization.
Under review in PNAS.2023.
A Paul, N Sajid, L Da Costa, A Razi. On efficient computation in active inference. Under review in IEEE
Transactions on Neural Networks and Learning Systems. arXiv:2307.00504. 2023.
S Rubin, C Heins, T Mitsui, L Da Costa, K Friston. Climate homeostasis by and for active inference in the
biosphere. Under review in Nature Communications.2023.
N Sajid, E Holmes,L Da Costa, C Price, K Friston. A mixed generative model of auditory word repetition.Under
review in Frontiers in Neuroscience. BioRxiv. 2022.
Book Chapters (2)
⋆ A Barp†, L Da Costa†, G Franca†, K Friston, M Girolami, MI Jordan, GA Pavliotis. Geometric Methods for
Sampling, Optimisation, Inference and Adaptive Agents.Geometry and Statistics, Handbook of Statistics Series vol
46 (Eds. F Nielsen, AS Rao, CR Rao). 2022.
N Sajid,L Da Costa, T Parr, K Friston. Active inference, Bayesian optimal design, and expected utility.The Drive
for Knowledge (Eds. IC Dezza, E Schulz, CM Wu). 2022.
14
Workshop Papers (2)
N Sajid, P Tigas, Z Fountas, Q Guo, A Zakharov,L Da Costa. Modelling non-reinforced preferences using selective
attention. First Conference on Lifelong Learning Agents: Workshop Track. arXiv:2207.13699. 2022.
L Da Costa∗, N Sajid, D Zhao, S Tenka. Active inference as a model of agency. RLDM 2022 Workshop on
Reinforcement Learning as a Model of Agency. 2022.
Journal Articles (22)
L Da Costa, L Sandved-Smith. Towards a Bayesian mechanics of metacognitive particles: A commentary on "Path
integrals, particular kinds, and strange things" by Friston, Da Costa, Sakthivadivel, Heins, Pavliotis, Ramstead, and
Parr. Physics of Life Reviews.2023.
K Friston, L Da Costa∗, DAR Sakthivadivel, C Heins, GA Pavliotis, MJD Ramstead, T Parr. Path integrals,
particular kinds, and strange things.Physics of Life Reviews. arXiv:2210.12761. 2023.
⋆ K Friston,L Da Costa∗, N Sajid, C Heins, K Ueltzhoeffer, GA Pavliotis, T Parr. The free-energy principle made
simpler but not too simple.Physics Reports. 2023.
⋆ L Da Costa∗, GA Pavliotis. The entropy production of stationary diffusions.Journal of Physics A: Mathematical
and Theoretical. 2023.
⋆ L Da Costa∗, N Sajid, T Parr, K Friston, R Smith. Reward Maximisation through Discrete Active inference.Neural
Computation. 2023.
MJD Ramstead, DAR Sakthivadivel, C Heins, M Koudahl, B Millidge,L Da Costa, B Klein, K Friston. On Bayesian
Mechanics: A Physics of and by Beliefs.Journal of the Royal Society Interface. 2023.
C Heins,L Da Costa. Sparse coupling and Markov blankets: A comment on "How particular is the physics of the
free-energy Principle?" by Aguilera, Millidge, Tschantz and Buckley.Physics of Life Reviews. 2022.
T Champion,L Da Costa, H Bowman, M Grzes. Branching Time Active Inference: the theory and its generality.
Neural Networks. 2022.
N Sajid, F Faccio,L Da Costa, T Parr, J Schmidhuber, K Friston. Bayesian brains and the Rényi divergence.
Neural Computation. 2022.
⋄ L Da Costa†∗, P Lanillos†, N Sajid, K Friston, S Khan. How active inference could help revolutionise robotics.
Entropy. 2022.
⋆ L Da Costa∗, K Friston, C Heins, GA Pavliotis. Bayesian mechanics for stationary processes.Proceedings of the
Royal Society A. 2021.
K Friston, C Heins, K Ueltzhoeffer,L Da Costa, T Parr. Stochastic Chaos and Markov Blankets.Entropy. 2021.
K Ueltzhoeffer, L Da Costa, D Cialfi, K Friston. A Drive towards Thermodynamic Efficiency for Dissipative
Structures in Chemical Reaction Networks.Entropy. 2021.
T Parr,L Da Costa, C Heins, MJD Ramstead, K Friston. Memory and Markov Blankets.Entropy. 2021.
K Friston,L Da Costa, T Parr. Some interesting observations on the free-energy principle.Entropy. 2021.
15
K Ueltzhoeffer,L Da Costa, K Friston. Variational free-energy, individual fitness, and population dynamics under
acute stress. Comment on "Dynamic and thermodynamic models of adaptation" by Alexander N. Gorban et al.
Physics of Life Reviews. 2021.
T Parr, N Sajid,L Da Costa, MB Mirza, K Friston. Generative models for active vision.Frontiers in Neurobotics.
2021.
⋆ L Da Costa∗, T Parr, B Sengupta, K Friston. Neural dynamics under active inference: plausibility and efficiency of
information processing. Entropy. 2021.
K Friston,L Da Costa, D Hafner, C Hesp, T Parr. Sophisticated inference.Neural Computation. 2021.
⋆ L Da Costa∗, T Parr, N Sajid, S Veselic, V Neacsu, K Friston. Active inference on discrete state-spaces: a synthesis.
Journal of Mathematical Psychology. 2020.
S Rubin, T Parr,L Da Costa, K Friston. Future climates: Markov blankets and active inference in the biosphere.
Journal of the Royal Society Interface. 2020.
T Parr, L Da Costa, K Friston. Markov blankets, information geometry and stochastic thermodynamics.Philo-
sophical Transactions of the Royal Society A. 2020.
1.6 Awards for PhD work
My PhD’s work has been honoured by several awards including a Doris Chen Award by Imperial College London
(2023), a Best Paper Award by the journal Entropy (2023), an Excellence Grant by G-Research (2022), and a
Distinguished Student Award by the American Physical Society at the APS March meeting held in Chicago, IL, USA
(2022).
16
Chapter 2
Entropy production
The entropy production of stationary diffusions
By Lancelot Da Costa and Grigorios A. Pavliotis
Adapted from: L Da Costa, GA Pavliotis. The entropy production of stationary diffusions.Journal
of Physics A: Mathematical and Theoretical. 2023
17
2.1 Abstract
The entropy production rate is a central quantity in non-equilibrium statistical physics, scoring how far a stochastic
process is from being time-reversible. In this chapter, we compute the entropy production of diffusion processes at
non-equilibrium steady-state under the condition that the time-reversal of the diffusion remains a diffusion. We start
by characterising the entropy production of both discrete and continuous-time Markov processes. We investigate the
time-reversal of time-homogeneous stationary diffusions and recall the most general conditions for the reversibility
of the diffusion property, which includes hypoelliptic and degenerate diffusions, and locally Lipschitz vector fields.
We decompose the drift into its time-reversible and irreversible parts, or equivalently, the generator into symmetric
and antisymmetric operators. We show the equivalence with a decomposition of the backward Kolmogorov equation
consideredinhypocoercivitytheory, andadecompositionoftheFokker-PlanckequationinGENERICform. Themain
resultshowsthatwhenthetime-irreversiblepartofthedriftisintherangeofthevolatilitymatrix(almosteverywhere)
the forward and time-reversed path space measures of the process are mutually equivalent, and evaluates the entropy
production. When this does not hold, the measures are mutually singular and the entropy production is infinite. We
verify these results using exact numerical simulations of linear diffusions. We illustrate the discrepancy between the
entropy production of non-linear diffusions and their numerical simulations in several examples and illustrate how
the entropy production can be used for accurate numerical simulation. Finally, we discuss the relationship between
time-irreversibility and sampling efficiency, and how we can modify the definition of entropy production to score how
far a process is from being generalised reversible.
Keywords: measuring irreversibility; time-reversal; hypoelliptic; degenerate diffusion; Helmholtz decomposition;
numerical simulation; Langevin equation; stochastic differential equation; entropy production rate.
2.2 Introduction
The entropy production rateep is a central concept in statistical physics. In a nutshell, it is a measure of the time-
irreversibility of a stochastic process, that is how much random motion differs statistically speaking as one plays it
forward, or backward, in time.
At non-equilibrium steady-state, theep is quantified by the relative entropyH (a.k.a Kullback-Leibler divergence)
ep = 1
T H[P[0,T] | ¯P[0,T]]
between the path-wise distributions of the forward and time-reversed processes in some time interval[0, T], denoted
by P[0,T], ¯P[0,T], respectively.
Physically, theep measures the minimal amount of energy needed, per unit of time, to maintain a system at non-
equilibrium steady-state. Equivalently, it quantifies the heat dissipated by a physical system at non-equilibrium
steady-state per unit of time [15, p. 86]. The second law of thermodynamics for open systems is the non-negativity
of the entropy production.
The ep plays a crucial role in stochastic thermodynamics. It is the central quantity in the so-called Gallavotti-Cohen
fluctuation theorem, which quantifies the probability of entropy decrease along stochastic trajectories [16–18]. More
recently, entropy production is at the heart of the so-called thermodynamic uncertainty relations, which provide
estimates of ep from observations of the system [19]. Theep is also an important tool in biophysics, as a measure
of the metabolic cost of molecular processes, such as molecular motors [16], and it was shown empirically that brain
states associated with effortful activity correlated with a higher entropy production from neural activity [20].
18
In this chapter, we will be primarily concerned with the entropy production of diffusion processes, that is, solutions
of stochastic differential equations. Consider an Itô stochastic differential equation (SDE)
dxt = b (xt) dt + σ (xt) dwt
with driftb : Rd → Rd and volatilityσ : Rd → Rd×m, and wt a standard Brownian motion onRm, whose solution
is stationary at a probability measureµ with densityρ. A result known as the Helmholtz decomposition, which is
central in non-equilibrium statistical physics [3,21–23] but also in statistical sampling [24–27], tells us that we can
decompose the drift into time-reversible and time-irreversible parts:b = brev +birr. In particular,birrρ is the stationary
probability current, as considered by Nelson [28]. Jiang and colleagues derived the entropy production rate for such
systems under the constraint that the coefficients of the SDEb, σare globally Lipschitz, and the solution uniformly
elliptic (i.e., the diffusion matrix fieldD = 1
2 σσ⊤ is uniformly positive definite). This takes the form of [15, Chapter
4]:
ep =
Z
Rd
b⊤
irrD−1birrρ(x)dx.
In this chapter, we extend their work by computing the entropy production for a greater range of diffusion processes,
which includes non-elliptic, hypoelliptic and degenerate diffusions, and SDEs driven by locally Lipschitz coefficients.
Non-ellipticdiffusionsaresolutionstoSDEswhosediffusionmatrixfield D = 1
2 σσ⊤ isnotpositivedefiniteeverywhere;
this means that there are regions of space in which the random fluctuations cannot drive the process in every possible
direction. Depending on how the volatility interacts with the drift (i.e., Hörmander’s theorem [29, Theorem 1.3]),
solutions initialised at a point may still have a density—the hypoelliptic case—or not—the degenerate case; prominent
examples are underdamped Langevin dynamics and deterministic dynamics, respectively. In our treatment, we only
assume that the time-reversal of a diffusion is a diffusion (the necessary and sufficient conditions for which were first
established by Millet, Nualart and Sanz [30]) and sufficient regularity to apply Girsanov’s theorem or the Stroock-
Varadhan support theorem.
This extension has become important since many processes that are commonplace in non-equilibrium statistical
physics or statistical machine learning are hypoelliptic. For instance, the underdamped and generalised Langevin
equations in phase space [3], which model the motion of a particle interacting with a heat bath, and which form
the basis of efficient sampling schemes such as Hamiltonian Monte-Carlo [26], or, stochastic gradient descent in deep
neural networks [27], or, the linear diffusion process with the fastest convergence to stationary state [31], which
informs us of the properties of efficient samplers. Much research in statistical sampling has drawn the connection
between time-irreversibility and sampling efficiency [26,32–34], so it is informative to understand the amount of time-
irreversibility associated with the most efficient samplers. Lastly, it is known that numerically integrating a diffusion
processes can modify the amount of irreversibility present in the original dynamic [35]. The entropy production rate is
thus an important indicator of the fidelity of a numerical simulation, and can serve as a guide to developing sampling
schemes that preserve the statistical properties of efficient (hypoelliptic) samplers.
The outline of the chapter and our contribution are detailed below.
2.2.1 Chapter outline and contribution
Section 2.3: We give various characterisations and formulas for theep of stationary Markov processes in discrete
and continuous-time, and recall a crude but general recipe for numerical estimation.
Section 2.4: We investigate the time-reversal of time-homogeneous diffusions. We give the general conditions
under which the time-reversal of a time-homogeneous diffusion remains a diffusion, based on the results of Millet,
19
Nualart and Sanz [30]. We then recall how the drift vector field of an SDE can be decomposed into time-reversible and
irreversible parts. We show that this decomposition is equivalent to a decomposition of the generator of the process
into symmetric and antisymmetric operators on a suitable function space. Then we show that this decomposition is
equivalent to two other fundamental decompositions in the study of far from equilibrium systems: the decomposition
of the backward Kolmogorov equation considered in hypocoercivity theory [5,36] and the decomposition of the
Fokker-Planck equation considered in the GENERIC formalism [4,36].
Section 2.5: We compute theep of stationary diffusion processes under the condition that the time-reversal of
the diffusion remains a diffusion. We show that:
• Section 2.5.1:If birr(x) ∈ Range σ(x), forµ-almost everyx ∈ Rd1 (in particular for elliptic or time-reversible
diffusions). Then the forward and backward path-space measures are mutually equivalent, in other words, the
sets of possible trajectories by forward and backward processes are equal—and the entropy production equals
ep =
R
b⊤
irrD−birrdµ, where·− denotes the Moore-Penrose matrix pseudo-inverse. See Theorems 2.5.1, 2.5.2 for
details.
• Section 2.5.2: When the above does not hold, the forward and backward path-space measures are mutually
singular, in other words, there are trajectories that are taken by the forward process that cannot be taken by
the backward process—and vice-versa2. In particular, the entropy production rate is infiniteep = +∞. See
Theorem 2.5.4 for details.
Section 2.6: We compute theep of various models such as the multivariate Ornstein-Uhlenbeck and the under-
damped Langevin process. We numerically simulate and verify the value ofep when the coefficients are linear. We
then discuss how numerical discretisation can influence the value ofep. As examples, we compute and compare the
ep of Euler-Maruyama and BBK [37,38] discretisations of the underdamped Langevin process. We summarise the
usefulness of ep as a measure of the accuracy of numerical schemes in preserving the time-irreversibility properties
of the underlying process, and give guidelines, in terms ofep, for developing accurate simulations of underdamped
Langevin dynamics.
Section 2.7: We give a geometric interpretation of our main results and discuss future perspectives: what this
suggests about the relationship between time-irreversibility and mixing in the context of sampling and optimisation,
and how we could modify the definition—and computation—ofep to quantify how a process is far from being time-
reversible up to a one-to-one transformation of its phase-space.
2.3 The ep of stationary Markov processes
In this section,(xt)t∈[0,T], T >0 is a time-homogeneous Markov process on a Polish spaceX with almost surely (a.s.)
continuous trajectories.
Definition 2.3.1(Time reversed process). The time reversed process(¯xt)t∈[0,T] is defined as¯xt = xT−t.
Note that since the final state of the forward process is the initial state of the backward process this definition makes
sense only on finite time intervals.
1µ-almost everywhere: This means that the statement holds with probability1 when x is distributed according to the
probability measureµ.
2Precisely, two measures are mutual singular if and only if they are not mutually equivalent.
20
We define(C([0, T], X), d∞) as the space ofX-valued continuous paths endowed with the supremum distanced∞,
defined asd∞(f, g) = supt∈[0,T] d(f(t), g(t)), whered is a choice of distance on the Polish spaceX. Naturally, when
we later specialise toX = Rd, the supremum distance will be given by theL∞-norm ∥ · ∥∞.
Definition 2.3.2 (Path space measure). Each Markov process(xt)0≤t≤T with a.s. continuous trajectories defines
probability measureP[0,T] on the canonical path space(C([0, T], X), B), where B is the Borel sigma-algebra asso-
ciated with the supremum distance. This probability measure determines the probability of the process to take any
(Borel set of) paths.
Remark 2.3.3 (Cadlag Markov processes). All definitions and results in this section hold more generally for Markov
processes with cadlag paths (i.e., right continuous with left limits), simply replacing the canonical path-space
(C([0, T], X), d∞) with the Skorokhod space. We restrict ourselves to processes with continuous paths for simplicity.
Definition 2.3.4(Restriction to a sub-interval of time). Given a path space measureP[0,T] and timesa < b∈ [0, T],
we define P[a,b] to be the path space measure describing (xt)t∈[a,b]. This is the restriction of P[0,T] to the sub
sigma-algebra B[a,b] :=
n
A ∈ B : A|[a,b] ∈ Borel sigma-algebra on(C([a, b], X), d∞)
o
.
Let P be the path space measure of the Markov processxt and ¯P be that of its time-reversal¯xt, respectively. We
can measure the statistical difference between the forward and time-reversed processes at timeτ ∈ [0, T] with the
entropy production rate
lim
ε↓0
1
ε H

P[τ,τ+ε], ¯P[τ,τ+ε]

, (2.1)
where H is the relative entropy (a.k.a. Kullback-Leibler divergence). This measures the rate at which the forward
and backward path space measures differ in the relative entropy sense at timeτ.
The following result [15, Theorem 2.2.4] (see also [39, Theorem 10.4]) shows that the limit exists in stationary and
time-homogeneous Markov processes. Obviously, the limit is independent ofτ in this case.
Theorem 2.3.5.Suppose that(xt)t∈[0,T] is a stationary time-homogeneous Markov process on a Polish spaceX with
continuous sample paths. Stationarity implies that we can set the time-horizonT > 0 of the process to arbitrarily
large values. Then the quantity
1
t H

P[τ,τ+t] | ¯P[τ,τ+t]

for allτ ∈ [0, +∞), t∈ (0, +∞)
is a constant∈ [0, +∞].
This yields the following general definition of entropy production rate for stationary Markov processes:
Definition 2.3.6 (Entropy production rate of a stationary Markov process). Let (xt)t∈[0,T] be a stationary time-
homogeneous Markov process. Stationarity implies that we can set the time-horizonT > 0 to be arbitrarily large.
For such processes, the entropy production rate is a constantep ∈ [0, +∞] defined as
ep := 1
t H

P[0,t] | ¯P[0,t]

(2.2)
for anyt ∈ (0, +∞). ep scores the amount to which the forward and time-reversed processes differ per unit of time.
In particular,T ep is the total entropy production in a time interval of lengthT. Note that, in the literature, theep
is often defined asep = limt→+∞ 1
t H

P[0,t] | ¯P[0,t]

, e.g., [15, Definition 4.1.1]; this is just (2.2) in the limit of large
t. However, Theorem 2.3.5 showed us that (2.2) is constant w.r.t.t ∈ (0, +∞) so we do not need to restrict ourselves
to defining theep as (2.2) in the limit of larget. This added generality will be very helpful to computeep later, by
exploiting the fact that (2.2) is often more easily analysed in the regime of finite or smallt.
21
Remark 2.3.7 (Physical relevance of Definition 2.3.6). In some stationary processes (e.g., Hamiltonian systems),
physicists define entropy production as Definition 2.3.6 with an additional operator applied to the path space measure
of the time-reversed process; that is,
egen,θ
p := lim
ε↓0
1
ε H

P[0,ε], θ# ¯P[0,ε]

, (2.3)
where θ# is the pushforward operator associated to an involution of phase-spaceθ (e.g., the momentum flip [40–42])
that leaves the stationary distribution invariant3. In this article, we will refer to (2.2) asentropy production and
to (2.3) asgeneralised entropy production, and proceed to derive general results for (2.2). The results we derive are
informative of the process and applicable independently of whether (2.2) is the physically meaningful definition of
entropy production; yet, physicists looking to interpret these results should bear in mind that they are physically
informative about entropy production insofar as Definition 2.3.6 is physically meaningful for the system at hand. We
will briefly revisit generalised entropy production in the discussion (Section 2.7.2).
Proposition 2.3.8. Let (xt)t∈[0,T] be a time-homogeneous Markov process on a Polish spaceX, stationary at the
probability measureµ. Then the entropy production rate equals
ep = 1
t Ex∼µ

H

Px
[0,t] | ¯Px
[0,t]

for anyt ∈ (0, +∞).
A proof is provided in Section 2.8.1.
Notation 2.3.9. By Px we mean the path space measure of the process initialised (possibly out of stationarity) at a
deterministic initial conditionx0 = x ∈ X.
Proposition 2.3.10 (ep in terms of transition kernels). Let (xt)t∈[0,T] be a time-homogeneous Markov process on
a Polish spaceX, stationary at the probability measureµ. Denote by pt(dy, x) the transition kernels of the Markov
semigroup, and by¯pt(dy, x) those of the time-reversed process. Then the entropy production rate equals
ep = lim
ε↓0
1
ε Ex∼µ [H [pε(·, x) | ¯pε(·, x)]] .
The fact that the time-reversed process possesses transition kernels holds as it is also a stationary Markov process [15,
p. 113]. A proof of Proposition 2.3.10 is provided in Section 2.8.1.
2.3.1 The ep of numerical simulations
Proposition 2.3.10 entails a formula for theep of Markov processes in discrete time:
Definition 2.3.11. The entropy production rate of a discrete-time Markov process with time-stepε equals
eNS
p (ε) = 1
ε Ex∼˜µ
Z
pε(y, x) log pε(y, x)
pε(x, y)dy, (2.4)
where ˜µ is the invariant measure of the process.
This definition is useful, for example, to quantify the entropy production of numerical simulations of a stochastic
process [35]. In particular, it suggests a simple numerical estimator of the entropy production rate for numerical sim-
ulations (at stationarity). Consider a smallδ (e.g., δ is the time-step of the numerical discretisation). Given samples
3This generalised definition of entropy production is taken as a limit ofε ↓ 0 analogously to (2.1) to capture the fact that
we are modelling arate. We cannot, a priori state that the expression is constant for anyε ∈ (0, +∞), as in Definition 2.3.6,
since we do not know whether a result analogous to Theorem 2.3.5 holds in this case.
22
from the process atδ time intervals, discretise the state-space into a finite partitionU1, . . . , Un, and approximate
P[0,δ] and ¯P[0,δ] by the empirical transition probabilitiespi→j between Ui, Uj from time0 to δ.
eNS
p = lim
ε→0
1
ε H

P[0,ε] | ¯P[0,ε]

≈ 1
δ H

P[0,δ] | ¯P[0,δ]

≈ 1
δ
X
i,j
pi→j log pi→j
pj→i
.
Note that this method measures the entropy production rate of the numerical discretisation as opposed to that of
the continuous process. This typically produces results close toep, but does not necessarily converge toep in the
continuum limitδ → 0 of the numerical discretisation. Indeed [35] showed that numerical discretisations can break
detailed balance, so that the continuum limit of the numerical discretisation can differ from the initial process. Thus
one should choose numerical schemes carefully when preserving the entropy production rate of a process is important.
We will return to this in Section 2.6.
2.4 Time reversal of stationary diffusions
We now specialise to diffusion processes inRd. These are Markov processes with an infinitessimal generator that
is a second order linear operator without a constant part [43, Definition 1.11.1], which entails almost surely (a.s.)
continuoussamplepaths. Conveniently, diffusionprocessesareusuallyexpressibleassolutionstostochasticdifferential
equations. From now on, we consider an Itô stochastic differential equation
dxt = b (xt) dt + σ (xt) dwt (2.5)
with driftb : Rd → Rd and volatilityσ : Rd → Rd×m, andwt a standard Brownian motion onRm.
Notation 2.4.1. Let D = σσ⊤/2 ∈ Rd×d be the diffusion tensor. Denote by ∥ · ∥the Euclidean distance, and, for
matrices
∥σ∥2 :=
dX
i=1
nX
j=1
|σij|2 .
Throughout, ∇ and ∇· are the gradient and the divergence in the distributional sense. We operationally define the
divergence of a matrix fieldQ : Rd → Rd×d by (∇ ·Q)i := Pd
j=1 ∂jQij for 0 ≤ i ≤ d. We will denote byµ the
stationary probability measure of the processxt and by ρ its density with respect to the Lebesgue measure, i.e.,
µ(dx) = ρ(x)dx (assuming they exist).
2.4.1 On the time-reversibility of the diffusion property
There is a substantial literature studying the time-reversal of diffusion processes. In general, the time-reversal of a
diffusion need not be a diffusion [30], but Haussman and Pardoux showed that the diffusion property is preserved
under some mild regularity conditions on the diffusion process [44]. A few years later Millet, Nualart, Sanz derived
necessary and sufficient conditions for the time-reversal of a diffusion to be a diffusion [30, Theorem 2.2 & p. 220]. We
provide these conditions here, with a proof of a different nature that exploits the existence of a stationary distribution.
Lemma 2.4.2(Conditions for the reversibility of the diffusion property). Let an Itô SDE(2.5) with locally bounded,
Lebesgue measurable coefficientsb : Rd → Rd, σ: Rd → Rd×m. Consider a strong solution(xt)t∈[0,T], i.e., a process
23
satisfying
xt = x0 +
Z t
0
b (xs) ds +
Z t
0
σ (xs) dws,
and assume that it is stationary with respect to a probability measureµ with density ρ. Consider the time-reversed
stationary process(¯xt)t∈[0,T]. Then, the following are equivalent:
• (¯xt)t∈[0,T] is a Markov diffusion process.
• The distributional derivative∇ ·(Dρ) is a function, which is then necessarily inL1
loc(Rd, Rd).
A proof is provided in Section 2.8.2.
2.4.2 Setup for the time-reversal of diffusions
From now on, we will work under the assumption that the time-reversal of the diffusion is a diffusion. We assume
that:
Assumption 2.4.3. 1. The coefficients of the SDE (2.5) b, σare locally Lipschitz continuous. In other words,
∀x ∈ Rd, ∃r >0, k >0 s.t. ∀y ∈ Rd :
∥x − y∥ < r⇒ ∥b(x) − b(y)∥ + ∥σ(x) − σ(y)∥ ≤k∥x − y∥,
2. The solution xt to (2.5) is defined globally up to timeT > 0. Sufficient conditions in terms of the drift and
volatility for Itô SDEs are given in Theorem [45, Theorem 3.1.1].
Assumption 2.4.3.1 ensures the existence and uniqueness of strong solutions locally in time [46, Chapter IV Theorem
3.1], while Assumption 2.4.3.2 ensures that this solution exists globally in time (i.e., non-explosiveness). Altogether,
Assumption 2.4.3 ensures that the SDE (2.5) unambiguously defines a diffusion process.
Furthermore, we assume some regularity on the stationary distribution of the process.
Assumption 2.4.4. 1. (xt)t∈[0,T] is stationary at a probability distributionµ, with densityρ with respect to the
Lebesgue measure, i.e.,µ(dx) = ρ(x)dx.
Then, ρ ∈ L1(Rd) and, under local boundedness of the diffusion tensor (e.g., Assumption 2.4.3),Dρ ∈ L1
loc(Rd, Rd×d).
Thus, we can define the distributional derivative∇ ·(Dρ). We assume that:
2. ∇ ·(Dρ) ∈ L1
loc(Rd, Rd), i.e., the distributional derivative∇ ·(Dρ) is a function.
Assumption 2.4.4 ensures that the time-reversal of the diffusion process remains a diffusion process, as demonstrated
in Lemma 2.4.2.
2.4.3 The time reversed diffusion
Now that we know sufficient and necessary conditions for the time-reversibility of the diffusion property, we proceed to
identify the drift and volatility of the time-reversed diffusion. This was originally done by Hausmann and Pardoux [44,
Theorem 2.1], and then by Millet, Nualart, Sanz under slightly different conditions [30, Theorems 2.3 or 3.3]. Inspired
by these, we provide a different proof, which applies to stationary diffusions with locally Lipschitz coefficients.
24
Theorem 2.4.5 (Characterisation of time-reversal of diffusion). Let an Itô SDE(2.5) with coefficients satisfying
Assumption 2.4.3. Assume that the solution(xt)t∈[0,T] is stationary with respect to a densityρ satisfying Assumption
2.4.4. Then, the time-reversed process(¯xt)t∈[0,T] is a Markov diffusion process, stationary at the densityρ, with drift
¯b(x) =



−b(x) + 2ρ−1∇ ·(Dρ) (x) when ρ(x) > 0,
−b(x) when ρ(x) = 0.
(2.6)
and diffusion ¯D = D. Furthermore, any such stationary diffusion process induces the path space measure of the
time-reversed process¯P[0,T].
A proof is provided in Section 2.8.2. Similar time-reversal theorems exist in various settings: for more singular
coefficients on the torus [47], under (entropic) regularity conditions on the forward path space measure [48,49], for
infinite-dimensional diffusions [50–52], for diffusions on open time-intervals [53], or with boundary conditions [54].
Furthermore, we did not specify the Brownian motion driving the time-reversed diffusion but this one was identified
in [55, Remark 2.5].
We illustrate the time-reversal of diffusions with a well-known example:
Example 2.4.6(Time reversal of underdamped Langevin dynamics). Underdamped Langevin dynamics is an im-
portant model in statistical physics and sampling [3,38,56]. Consider a HamiltonianH(q, p) function of positions
q ∈ Rn and momentap ∈ Rn. We assume that the Hamiltonian has the form
H(q, p) = V (q) + 1
2p⊤M−1p,
for some smooth potential functionV : Rd → R and diagonal mass matrixM ∈ Rd×d. The underdamped Langevin
process is given by the solution to the SDE [38, eq 2.41]



dqt = M−1ptdt
dpt = −∇V (qt) dt − γM −1ptdt +
p
2γβ−1dwt
(2.7)
for some friction, and inverse temperature coefficientsγ, β >0. The stationary density, assuming it exists, is the
canonical density [38, Section 2.2.3.1]
ρ(q, p) ∝ e−βH(q,p) = e−βV (q)−β
2 p⊤M−1p. (2.8)
Thus, the time-reversal of the stationary process (Theorem 2.4.5) is a weak solution to the SDE



d¯qt = −M−1 ¯ptdt
d¯pt = ∇V (¯qt) dt − γM −1 ¯ptdt +
p
2γβ−1dwt.
Letting ˆpt = −¯pt, the tuple(¯qt, ˆpt) solves the same SDE as(qt, pt) but with a different Brownian motionˆwt



d¯qt = M−1 ˆptdt
dˆpt = −∇V (¯qt) dt − γM −1 ˆptdt +
p
2γβ−1d ˆwt.
Since path space measures are agnostic to changes in the Brownian motion, this leads to the statement that time-
reversal equals momentum reversal in underdamped Langevin dynamics (with equality in law, i.e., in the sense of
path space measures)
(¯qt, ¯pt)t∈[0,T] = (¯qt, −ˆpt)t∈[0,T]
ℓ
= (qt, −pt)t∈[0,T].
25
In other words, we haveP[0,T] = θ# ¯P[0,T] where θ(q, p) = ( q, −p) is the momentum flip transformation in phase
space.
2.4.4 The Helmholtz decomposition
Armed with the time-reversal of diffusions we proceed to decompose the SDE into its time-reversible and time-
irreversible components. This decomposition is called the Helmholtz decomposition because it can be obtained
geometrically by decomposing the drift vector fieldb into horizontalbirr (time-irreversible, conservative) and vertical
brev (time-reversible, non-conservative) components with respect to the stationary density [24]. These vector fields
are called horizontal and vertical, respectively, because the first flows along the contours of the stationary density,
while the second ascends the landscape of the stationary density (see the schematic in the upper-left panel of Figure
2.1). For our purposes, we provide a self-contained, non-geometric proof of the Helmholtz decomposition in Section
2.8.2.
Proposition2.4.7 (Helmholtzdecomposition) . Consider the solution(xt)t∈[0,T] of the Itô SDE(2.5) with coefficients
satisfying Assumption 2.4.3. Let a probability densityρ satisfying ∇ ·(Dρ) ∈ L1
loc(Rd, Rd). Then, the following are
equivalent:
1. The densityρ is stationary for(xt)t∈[0,T].
2. We can write the drift as
b = brev + birr
brev =



D∇log ρ + ∇ ·D if ρ(x) > 0
0 if ρ(x) = 0
∇ ·(birrρ) = 0.
(2.9)
Furthermore,brev is time-reversible, whilebirr is time-irreversible, i.e.,
b = brev + birr, ¯b = brev − birr.
The fundamental importance of the Helmholtz decomposition was originally recognised in the context of non-
equilibrium thermodynamics by Graham in 1977 [21], but its inception in this field dates from much earlier: for
instance, the divergence free vector fieldbirrρ is precisely the stationary probability current or flux introduced by
Nelson in 1967 [28]. More recently, the decomposition has recurrently been used in non-equilibrium statistical physics
[3,9,22,23,57–59], and in statistical machine learning as the basis of Monte-Carlo sampling schemes [3,24,25,60].
Remark 2.4.8 (Probabilistic reversibility). Here, time-reversible means reversibility in a probabilistic sense; that is,
invariance under time reversal, also known as detailed balance [15, Proposition 3.3.4]. Probabilistic reversibility often
leads to the non-conversation of quantities like the potential−log ρ(x). For example, the identity∇ ·(birrρ) = 0
implies that the time-irreversible driftbirr flows along the contours of the probability density; in other words, the
probability density and the potential are conserved along the time-irreversible vector field. In contrast, none of them
are conserved when flowing along the time-reversible vector fieldbrev. See Figure 2.1 for an illustration.
Remark 2.4.9 (Stratonovich formulation of Helmholtz decomposition). There exists an equivalent decomposition of
the drift of Stratonovich SDEs into time-reversible and irreversible parts. Assuming thatσ is differentiable, we can
rewrite the Itô SDE (2.5) into its equivalent Stratonovich SDE
dxt = bs(xt)dt + σ(xt) ◦ dwt.
26
Figure 2.1: Helmholtz decomposition. The upper left panel illustrates the Helmholtz decomposition of the drift into time-
reversible and time-irreversible parts: the time-reversible part of the drift flows towards the peak of the stationary density,
while the time-irreversible part flows along its contours. The upper right panel shows a sample trajectory of a two-dimensional
diffusion process stationary at a Gaussian distribution. The lower panels plot sample paths of the time-reversible (lower left)
and time-irreversible (lower right) parts of the dynamic. Purely conservative dynamics (lower right) are reminiscent of the
trajectories of massive bodies (e.g., planets) whose random fluctuations are negligible, as in Newtonian mechanics. Together,
the lower panels illustrate time-irreversibility: If we were to reverse time, the trajectories of the time-reversible process would
be statistically identical, while the trajectories of the time-irreversible process be distinguishable by flow, say, clockwise instead
of counterclockwise. The full process (upper right) is a combination of both time-reversible and time-irreversible dynamics.
The time-irreversible part defines a non-equilibrium steady-state and induces its characteristic wandering, cyclic behaviour.
where bs = b − ι and ι is the Itô to Stratonovich correction [3, eq. 3.31]. Note that the correction is time-reversible.
It follows that
bs
irr = birr, (2.10)
and forx s.t. ρ(x) > 0,
bs
rev(x) = (brev − ι)(x) = D∇log ρ(x) + 1
2σ∇ ·σ⊤(x).
In particular, forx s.t. ρ(x) > 0,
bs(x) ∈ Range σ(x) ⇐⇒ bs
irr(x) ∈ Range σ(x) (2.11)
as bs
rev(x) ∈ Range σ(x). For diffusions driven by additive noise, Itô and Stratonovich formulations coincidebs = b.
27
Thus, we conclude
σ is constant⇒ b(x) =



D∇log ρ(x) + birr(x) if ρ(x) > 0
birr(x) if ρ(x) = 0
⇒ (b(x) ∈ Range σ(x) ⇐⇒ birr(x) ∈ Range σ(x)) (2.12)
These identities will be useful to compute the entropy production rate later on.
The time-irreversible part of the drift often takes a simple form:
Proposition 2.4.10 (Characterisation of time-irreversible drift). Consider a smooth, strictly positive probability
density ρ and an arbitrary smooth vector fieldbirr. Then
∇ ·(birrρ) = 0 ⇐⇒ birr = Q∇log ρ + ∇ ·Q
where Q = −Q⊤ is a smooth antisymmetric matrix field.
A proof is provided in Section 2.8.2. We conclude this section by unpacking the Helmholtz decomposition of under-
damped Langevin dynamics.
Example 2.4.11(Helmholtz decomposition of underdamped Langevin). Following Example 2.4.6, it is straightfor-
ward to decompose underdamped Langevin dynamics into its time-irreversible and time-reversible parts. Indeed we
just need to identify the parts of the drift whose sign changes, and remains invariant, under time reversal:
brev(q, p) =
"
0
−γM −1p
#
, b irr(q, p) =
"
M−1p
−∇V (q)
#
.
We can rewrite these in canonical form recalling the gradient of the stationary density (2.8)
brev(q, p) = D∇log ρ(q, p), b irr(q, p) = Q∇log ρ(q, p)
∇log ρ(q, p) = −β
"
∇V (q)
M−1p
#
, D =
"
0 0
0 γβ−1 Idn
#
, Q = β−1
"
0 −Idn
Idn 0
#
.
Clearly, the time-irreversible part of the processd[qt, pt] = birr(qt, pt)dt is a Hamiltonian dynamic that preserves the
energy (i.e., the Hamiltonian), while the time-reversible part is a reversible Ornstein-Uhlenbeck process. Example
trajectories of the time-irreversible trajectory are exemplified in Figure 2.1 (bottom right).
2.4.5 Multiple perspectives on the Helmholtz decomposition
The Helmholtz decomposition is a cornerstone of the theory of diffusion processes. In addition to being a geometric
decomposition of the drift [24], it is, equivalently, a time-reversible and irreversible decomposition of the SDE (2.13),
of the generator and the (backward and forward) Kolmogorov PDEs describing the process. Briefly, the Helmholtz
decomposition is equivalent to a functional analytic decomposition of the generator into symmetric and antisymmetric
operators in a suitable function space. This corresponds to a decomposition of the backward Kolmogorov equation—
which determines the evolution of (macroscopic) observables under the process—into a conservative and a dissipative
flow. This decomposition can be used as a starting point to quantify the speed of convergence of the process to its
stationary state from arbitrary initial conditions using hypocoercivity theory [5]. The same goes for the Fokker-Planck
28
equation, which can be decomposed into a dissipative gradient flow, and a flow that is conservative in virtue of being
orthogonal to the gradient flow in a suitable function space. This casts the Fokker-Planck equation in GENERIC
form (General Equations for Non-Equilibrium Reversible-Irreversible Coupling), a general framework for analysing
dynamical systems arising in non-equilibrium statistical physics [4,36].
Below we outline these different equivalent perspectives. This section is provided for independent interest but will
not be used to derive our main results on entropy production;you may conveniently skip it on a first reading.
Helmholtz decomposition of the SDE
Proposition 2.4.7 is equivalent to a Helmholtz decomposition of the SDE into its time-reversible and time-irreversible
parts, noting that the volatility is invariant under time-reversal (Theorem 2.4.5)
dxt = birr (xt) dt| {z }
Time-irreversible
+ brev (xt) dt + σ (xt) dwt
| {z }
Time-reversible
. (2.13)
Figure 2.1 illustrates this decomposition with simulations.
Helmholtz decomposition of the infinitesimal generator
Following the differential geometric viewpoint, a deterministic flow—namely, a vector fieldb—is given by a first order
differential operatorb · ∇. Similarly, a stochastic flow given by a diffusion—namely, a vector fieldb and a diffusion
tensor D—is characterised by a second order differential operator
L = b · ∇+ D∇ · ∇, (2.14)
known as the generator. Note that the first order part is the deterministic flow given by the drift while the second
order part is the stochastic flow determined by the diffusion. More precisely, the generator of a diffusion process
solving the SDE (2.5) under Assumptions 2.4.3 and 2.4.4 is a linear, unbounded operator defined as
L : C∞
c (Rd) ⊂ Dom L ⊂ Lp
µ(Rd) → Lp
µ(Rd), 1 ≤ p ≤ ∞, L f(y) := lim
t↓0
1
t E[f(xt) − f(y) | x0 = y], (2.15)
f ∈ Dom L =

f ∈ Lp
µ(Rd) | ∃g ∈ Lp
µ(Rd) s.t. 1
t E[f(xt) − f(y) | x0 = y]
t↓0
− − →g(y) in Lp
µ(Rd)

Diffusions are among the simplest and most canonical Markov processes because they are characterised by generators
that are second order differential operators (with no constant part). Indeed, starting from (2.15), a quick computation
using Itô’s formula yields (2.14).
Recall that we have a duality pairing⟨·, ·⟩µ : Lp′
µ (Rd)⊗Lp
µ(Rd) → R defined by⟨f, g⟩µ =
R
Rd fg dµ, where 1
p′ + 1
p = 1.
A well-known fact is that the generator¯L of the time-reversed diffusion is the adjoint of the generator under the
above duality pairing [61,62], [15, Thm 4.3.2]. The adjoint¯L is implicitly defined by the relation
⟨f, L g⟩µ = ⟨¯Lf, g⟩µ, ∀f ∈ Dom ¯L, g∈ Dom L,
Dom ¯L = {f ∈ L1
µ(Rd) | ∃h ∈ L1
µ(Rd), ∀g ∈ Dom L : ⟨f, L g⟩µ = ⟨h, g⟩µ}.
(The concept of adjoint generalises the transpose of a matrix in linear algebra to operators on function spaces). The
proof of Lemma 2.4.2 explicitly computes the adjoint and shows that it is a linear operator¯L : L1
µ(Rd) → L1
µ(Rd)
29
which equals
¯Lf = −b · ∇f + 2ρ−1∇ ·(Dρ) · ∇f + D∇ · ∇f.
Notice how the first order part of the adjoint generator is the drift of the time-reversed diffusion, while the second
order part is its diffusion, as expected (cf. Theorem 2.4.5).
Much like we derived the Helmholtz decomposition of the drift by identifying the time-reversible and irreversible
parts (see the proof of Proposition 2.4.7), we proceed analogously at the level of the generator. Indeed, just as any
matrix can be decomposed into a sum of antisymmetric and symmetric matrices, we may decompose the generator
into a sum of antisymmetric and symmetric operators
L = A + S, A :=
 
L −¯L

/2, S :=
 
L +¯L

/2, Dom A = Dom S = Dom L∩Dom ¯L. (2.16)
By its analogous construction, this decomposition coincides with the Helmholtz decomposition; indeed, the sym-
metric operator recovers the time-reversible part of the dynamic while the antisymmetric operator recovers the
time-irreversible part. In a nutshell, the Helmholtz decomposition of the generator is as follows
L = A + S, A = birr · ∇| {z }
Time-irreversible
S = brev · ∇+ D∇ · ∇| {z }
Time-reversible
,
where the summands are symmetric and antisymmetric operators because they behave accordingly under the duality
pairing:
⟨A f, g⟩µ = −⟨f, A g⟩µ
| {z }
Antisymmetric
, ∀f, g∈ Dom A, ⟨S f, g⟩µ = ⟨f, S g⟩µ
| {z }
Symmetric
, ∀f, g∈ Dom S.
Noting that−S is a positive semi-definite operator, we can go slightly further and decompose it into its square roots.
To summarise:
Proposition 2.4.12. We can rewrite the generator of the diffusion process asL = A −Σ∗ Σ where A is the an-
tisymmetric part of the generator, and −Σ∗ Σ is the symmetric part, as defined in (2.16). Here ·∗ denotes the
adjoint with respect to the duality pairing⟨·, ·⟩µ. The operators have the following functional forms:A f = birr · ∇f,√
2 Σf = σ⊤∇f,
√
2 Σ∗ g = −∇log ρ · σg − ∇ ·(σg).
A proof is provided in Section 2.8.2.
Helmholtz decomposition of the backward Kolmogorov equation
We say that a real-valued function over the state-space of the processf : Rd → R is anobservable. Intuitively, this
is a macroscopic quantity that can be measured or observed in a physical process (e.g., energy or pressure) when the
(microscopic) process is not easily accessible. The evolution of an observablef given that the process is prepared at
a deterministic initial condition is given byft(x) = E[f(xt)|x0 = x].
The backward Kolmogorov equationis a fundamental equation describing a Markov process, as it encodes the motion
of observables
∂tft = L ft, f 0 = f ∈ Dom L.
In other words,ft = E[f(xt)|x0 = x] solves the equation. This highlights the central importance of the generator as
providing a concise summary of the process.
30
The Helmholtz decomposition entails a decomposition of the backward Kolmogorov equation
∂tut = A ft + Sft = (A −Σ∗ Σ)ft, f 0 = f ∈ Dom L. (2.17)
This decomposition is appealing, as it allows us to further characterise the contributions of the time-reversible and
irreversible parts of the dynamic. Along the time-irreversible part of the backward Kolmogorov equation∂tft = A ft,
the L2
µ(Rd)-norm ∥·∥ µ is conserved. Indeed, sinceA is antisymmetric,⟨A f, f⟩µ = 0 for everyf ∈ Dom A, and hence
∂t ∥ft∥2
µ = 2 ⟨A ft, ft⟩µ = 0.
On the other hand, along the time-reversible part of the backward Kolmogorov equation generated by−Σ∗ Σ, the
L2
µ(Rd)-norm is dissipated:
∂t ∥ft∥2
µ = −2 ⟨Σ∗ Σ ft, ft⟩µ = −2 ∥Σ ft∥2
µ ≤ 0.
This offers another perspective on the fact that the time-irreversible part of the dynamic is conservative, while the
time-reversible part is dissipative—of theL2
µ(Rd)-norm.
Beyond this, hypocoercivity theory allows us to analyse the backward Kolmogorov equation, once one has written
its Helmholtz decomposition. Hypocoercivity is a functional analytic theory developed to analyse abstract evolution
equations of the form (2.17), originally devised to systematically study the speed of convergence to stationary state of
kinetic diffusion processes like the underdamped Langevin dynamics and the Boltzmann equation. As an important
result, the theory provides sufficient conditions on the operatorsA, Σ to ensure an exponentially fast convergence of
the backward Kolmogorov equation to a fixed point [5, Theorems 18 & 24]. Dually, these convergence rates quantify
the speed of convergence of the process to its stationary density from a given initial condition.
GENERIC decomposition of the Fokker-Planck equation
This perspective can also be examined directly from the Fokker-Planck equation. The Fokker-Planck equation is
another fundamental equation describing a diffusion process: it encodes the evolution of the density of the process
over time (when it exists). The Fokker-Planck equation is theL2(Rd)-dual to the backward Kolmogorov equation. It
reads
∂tρt = L′ ρt = ∇ ·(−bρt + ∇ ·(Dρt)),
where L′ is the adjoint of the generator with respect to the standard duality pairing⟨·, ·⟩; in other words⟨L′ f, g⟩ =
⟨f, L g⟩ where ⟨f, g⟩ =
R
Rd fg(x) dx.
The Helmholtz decomposition implies a decomposition of the Fokker-Planck equation into two terms: assuming for
now thatρt, ρ >0 (e.g., if the diffusion is elliptic)
∂tρt = ∇ ·(−birrρt) + ∇ ·(−brevρt + ∇ ·(Dρt))
= ∇ ·(−birrρt) + ∇ ·(−ρtρ−1∇ ·(Dρ) + ∇ ·(Dρt))
= ∇ ·(−birrρt) + ∇ ·

ρtD∇log ρt
ρ

.
(2.18)
We will see that this decomposition casts the Fokker-Planck equation in pre-GENERIC form.
GENERIC (General Equations for Non-Equilibrium Reversible-Irreversible Coupling) is an important theory for
analysing dynamical systems arising in non-equilibrium statistical physics like the Fokker-Planck equation. The
framework rose to prominence through the seminal work of Ottinger [4] and was later developed by the applied
31
physics and engineering communities. Only recently, the framework developed into a rigorous mathematical theory.
We refer to [36] for mathematical details. The following Proposition shows how we can rewrite the Fokker-Planck
equation in pre-GENERIC form:
∂tρt = W( ρt)| {z }
time-irreversible
−Mρt (d H[ρt | ρ])| {z }
time-reversible
, (2.19)
Proposition 2.4.13(GENERIC decomposition of the Fokker-Planck equation). The Fokker-Planck equation(2.18)
is in pre-GENERIC form(2.19), with
W(ρt) = ∇ ·(−birrρt), −Mρt (d H[ρt | ρ]) = ∇ ·

ρtD∇log ρt
ρ

,
Mρt(ξ) = Σ′(ρtΣξ) = −∇ ·(ρtD∇ξ), d H[ρt | ρ] = log ρt
ρ + 1,
where H[· | ·] is the relative entropy,·′ denotes the adjoint under the standard duality pairing⟨·, ·⟩, d is the Fréchet
derivative inL2(Rd), andW, Mρt satisfy the following relations:
• Orthogonality: ⟨W(ρt), d H[ρt | ρ]⟩ = 0,
• Semi-positive definiteness: ⟨Mρt(h), g⟩ = ⟨h, Mρt(g)⟩, and⟨Mρt(g), g⟩ ≥0.
A proof is provided in Section 2.8.2.
Writing the Fokker-Planck equation in pre-GENERIC form (2.19) explicits the contributions of the time-reversible
and time-irreversible parts at the level of density dynamics. Indeed, the relative entropy functionalH[ρt | ρ] is
conserved along the time-irreversible part of the Fokker-Planck equation∂tρt = W(ρt)
d H[ρt | ρ]
dt = ⟨∂tρt, d H[ρt | ρ]⟩ = ⟨W(ρt), d H[ρt | ρ]⟩ = 0.
Contrariwise, the relative entropy is dissipated along the time-reversible part of the equation
d H[ρt | ρ]
dt = ⟨∂tρt, d H[ρt | ρ]⟩ = −⟨Mρt (d H[ρt | ρ]) , d H[ρt | ρ]⟩ ≤0.
Aggregating these results, we obtain the well-known fact that the relative entropy with respect to the stationary
density is a Lyapunov function of the Fokker-Planck equation; a result sometimes known as de Bruijn’s identity or
Boltzmann’s H-theorem [63, Proposition 1.1].
2.5 The ep of stationary diffusions
We are now ready to investigate the entropy production of stationary diffusions. First, we give sufficient conditions
guaranteeing the mutual absolute continuity of the forward and time-reversed path space measures and compute the
entropy production rate. Second, we demonstrate that when these conditions fail the entropy production is infinite.
2.5.1 Regular case
Theorem 2.5.1. Let an Itô SDE (2.5) with coefficients satisfying Assumption 2.4.3. Assume that the solution
(xt)t∈[0,T] is stationary with respect to a densityρ satisfying Assumption 2.4.4. Denote bybirr the time-irreversible
32
Figure 2.2: Entropy production as a function of time-irreversible drift.This figure illustrates the behaviour of sample
paths and the entropy production rate as one scales the irreversible driftbirr by a parameterθ. The underlying process is a
two-dimensional Ornstein-Uhlenbeck process, for which exact sample paths and entropy production rate are available (Section
2.6.1). The heat map represents the density of the associated Gaussian steady-state. One sees that a non-zero irreversible drift
induces circular, wandering behaviour around the contours of the steady-state, characteristic of a non-equilibrium steady-state
(top right and bottom left). This is accentuated by increasing the strength of the irreversible drift. The entropy production rate
measures the amount of irreversibility of the stationary process. It grows quadratically as a function of the irreversible scaling
factor θ (bottom right). When there is no irreversibility (top left), we witness an equilibrium steady-state. This is characterised
by a vanishing entropy production (bottom right).
part of the drift (Proposition 2.4.7), and by·− the Moore-Penrose matrix pseudo-inverse. Suppose that:
1. For ρ-almost everyx ∈ Rd, birr(x) ∈ Range σ(x), and
2. The productσ−birr : Rd → Rm is Borel measurable (e.g., ifσ−birr is continuous), and
3.
R
Rd b⊤
irrD−birrρ(x)dx <+∞.
Denote byP[0,T], ¯P[0,T] the path space measures of the forward and time-reversed diffusions, respectively, onC([0, T], Rd)
(Definition 2.3.2). Then,
1. The path-space measures are equivalentP[0,T] ∼ ¯P[0,T], and
2. ep =
R
Rd b⊤
irrD−birrρ(x)dx.
Under the assumptions of Theorem 2.5.1, theep is a quadratic form of the time-irreversible drift, see Figure 2.2.
A proof of Theorem 2.5.1 is provided in Section 2.8.3. The idea of the proof is simple: in the elliptic case, the
approach follows [15, Chapter 4] with some generalisations. In the non-elliptic case, the conditionbirr(x) ∈ Range σ(x)
33
intuitively ensures that the solution to the SDE, when initialised at any point, evolves on a sub-manifold ofRd and
is elliptic on this manifold (e.g., Figure 2.3). The pseudo-inverse of the diffusion tensor is essentially the inverse on
this sub-manifold. Thus, a proof analogous to the elliptic case, but on the sub-manifold (essentially replacing all
matrix inverses by pseudo-inverses and making sure everything still holds), shows that the path space measures of
the forward and backward processes initialised at a given point are equivalent—and Girsanov’s theorem gives us their
Radon-Nykodym derivative. Finally, Proposition 2.3.8 gives us the usual formula for the entropy production rate but
with the matrix inverse replaced by the pseudo-inverse. Please see Section 2.7.3 for a geometric discussion of this
proof.
Suppose either of assumptions 2, 3 of Theorem 2.5.1 do not hold. Then we have the following more general (and
technical) result:
Theorem 2.5.2. Let (Ω, F, P) be a probability space and(wt)t⩾0 a standard Wiener process onRm, with respect to
the filtration(Ft)t≥0 [45, Definition 2.1.12]. Consider the Itô SDE(2.5) with coefficients satisfying Assumption 2.4.3.
Consider its unique strong solution(xt)t∈[0,T] with respect to the given Brownian motion on the filtered probability
space (Ω, F, (Ft)t≥0, P). Assume that the solution is stationary with respect to a densityρ satisfying Assumption
2.4.4. Denote bybirr the time-irreversible part of the drift (Proposition 2.4.7), and by·− the Moore-Penrose matrix
pseudo-inverse. Suppose that:
1. For ρ-almost everyx ∈ Rd, birr(x) ∈ Range σ(x), and
2. σ−birr(xt) is anFt-adapted process (e.g.,σ−birr : Rd → Rm is Borel measurable), and
3. The following holds
EP [ZT ] = 1, ZT := exp

−2
Z T
0
⟨σ−birr(xt), dwt⟩ + |σ−birr(xt)|2dt

. (2.20)
Denote byP[0,T], ¯P[0,T] the path space measures onC([0, T], Rd) of the forward and time-reversed diffusions, respec-
tively (Definition 2.3.2). Then,
1. The path-space measures are equivalentP[0,T] ∼ ¯P[0,T], and
2. ep =
R
Rd b⊤
irrD−birrρ(x)dx.
A proof is provided in Section 2.8.3. The proof is similar to that of Theorem 2.5.1, but much shorter, since (2.20)
allows one to apply Girsanov’s theorem directly; in contrast, a large part of the proof of Theorem 2.5.1 is dedicated
to showing that indeed, a version of Girsanov’s theorem can be applied.
In relation to assumption 2 of Theorem 2.5.2, note that if a matrix fieldσ : Rd → Rd×m is Borel measurable, then
its pseudo-inverseσ− : Rd → Rm×d is also Borel measurable. We now give sufficient conditions for the exponential
condition (2.20).
Proposition 2.5.3. Consider a stochastic process(xt)t∈[0,T] on the probability space(Ω, F, (Ft)t≥0, P), which is
stationary at the densityρ. Assume that σ−birr(xt) is Ft-adapted. Then, either of the following conditions implies
(2.20):
1. Zt = exp

−2
Rt
0 ⟨σ−birr(xs), dws⟩ + |σ−birr(xs)|2ds

, t∈ [0, T] is a martingale on the probability space(Ω, F, {Ft}t≥0, P).
2. EP

e2
R T
0 |σ−birr(xt)|2dt

< +∞ (Novikov’s condition).
3. There existsδ >0 such thatEρ

eδ|σ−birr(x)|2 
< +∞.
4. supt∈[0,T] EP
h
exp

−
Rt
0 ⟨σ−birr(xs), dws⟩
i
< +∞ (Kazamaki’s criterion).
34
5. There existsK <1 s.t. for allt ∈ [0, T]
EP

2
Z T
t
σ−birr(xt)
2
ds
Ft

≤ K.
6. The tail of|σ−birr(x)|2, x∼ ρ decays exponentially fast, i.e., there exists positive constantsc, C, R >0 such that
for allr > R
P(|σ−birr(x)|2 > r) ≤ Ce−cr. (2.21)
Furthermore, (2 or 4)⇒ 1; 5⇒ 2; and 6⇒ 3.
A proof is provided in Section 2.8.3.
2.5.2 Singular case
When the time-irreversible part of the drift is not always in the range of the volatility matrix field, we have a different
result.
Theorem 2.5.4. Suppose that the Itô SDE(2.5) satisfies Assumption 2.4.3 and that the volatility is twice contin-
uously differentiableσ ∈ C2(Rd, Rd×m). Furthermore suppose that the solution(xt)t∈[0,T] is stationary with respect
to a densityρ satisfying Assumption 2.4.4. Denote byP[0,T], ¯P[0,T] the path space measures onC([0, T], Rd) of the
forward and time-reversed processes, respectively (Definition 2.3.2). Ifbirr(x) ∈ Range σ(x) does not hold forρ-a.e.
x ∈ Rd, then
P[0,T] ⊥ ¯P[0,T] and ep = +∞.
A proof is provided in Section 2.8.3. The proof uses a version of the Stroock-Varadhan support theorem to show
that there are paths that can be taken by the forward diffusion process that cannot be taken by the backward
diffusion process—and vice-versa. Specifically, when considering the two processes initialised at a pointx ∈ Rd where
birr(x) /∈ Range σ(x), we can see that the derivatives of their respective possible paths at time0 span different tangent
sub-spaces atx. Thus the path space measuresPx
[0,T], ¯Px
[0,T] are mutually singular. Since suchx occur with positive
probability under the stationary density, it follows that the path space measures of the forward and time-reversed
stationary processes Px
[0,T], ¯Px
[0,T] are also mutually singular. Finally, the relative entropy between two mutually
singular measures is infinity, hence theep must be infinite.
By Theorem 2.5.4 and (2.12) we can readily see that the underdamped (2.7) and generalised Langevin [3, eq. 8.33]
processes in phase-space have infinite entropy production. Expert statistical physicists will note that this contrasts
with previous results in the literature stating that these diffusions have finite entropy production. There is no
contradiction as physicists usually add an additional operator to the definition of the entropy production in these
systems (Remark 2.3.7). While obviously informative of the underlying process, statistical physicists should take the
results of Theorems 2.5.1, 2.5.2 and 2.5.4 to be physically relevant to entropy production insofar as the definition of
entropy production we adopted (Definition 2.3.6) is physically meaningful for the system at hand. What if it is not?
We will return to this in the discussion (Section 2.7.2).
In contrast to the underdamped and generalised Langevin equations, there exist hypoelliptic, non-elliptic diffusions
35
with finite entropy production. For example, consider the following volatility matrix field
σ(x, y, z) =


x 1
1 1
0 1

.
By Hörmander’s theorem [29, Theorem 1.3], for any smooth, confining (e.g., quadratic) potentialV : R3 → R, the
process solving the SDE
dxt = −D∇V (xt)dt + ∇ ·D(xt)dt + σ(xt)dwt
is hypoelliptic and non-elliptic. Furthermore, it is stationary and time-reversible at the Gibbs density ρ(x) ∝
exp(−V (x)).
2.6 Examples and ep of numerical simulations
We illustrate these results for linear diffusion processes, underdamped Langevin dynamics and their numerical simu-
lations.
2.6.1 Linear diffusion processes
Given matricesB ∈ Rd×d, σ∈ Rd×m, and a standard Brownian motion(wt)t∈[0,+∞) on Rm, consider a linear diffusion
process (i.e., a multivariate Ornstein-Uhlenbeck process)
dxt = b(xt)dt + σ(xt)dwt, b (x) = −Bx, σ (x) ≡ σ. (2.22)
This process arises, for example, in statistical physics as a model of the velocity of a massive Brownian particle
subject to friction [64]; it covers the case of interacting particle systems when the interactions are linear in the states
(e.g., the one dimensional ferromagnetic Gaussian spin model [65]); or when one linearises the equations of generic
diffusion processes near the stationary density.
By solving the linear diffusion process (e.g., [65, Section 2.2]) one sees that the solution can be expressed as a linear
operation on Brownian motion—a Gaussian process—thus the process must itself be Gaussian, and its stationary
density as well (when it exists). Consider a Gaussian densityρ
ρ(x) = N(x; 0, Π−1), −log ρ(x) = 1
2x⊤Πx,
where Π ∈ Rd×d is the symmetric positive definiteprecision matrix. By the Helmholtz decomposition (Propositions
2.4.7 & 2.4.10),ρ is a stationary density if and only if we can decompose the drift as follows:
b = brev + birr, b rev(x) = −DΠx, b irr(x) = −QΠx,
where Q = −Q⊤ ∈ Rd×d is an arbitrary antisymmetric matrix, and, recallD = σσ⊤/2 ∈ Rd×d is the diffusion tensor.
In particular, the drift of the forward and the time-reversed dynamic, are, respectively,
b(x) = −Bx, B = (D + Q)Π, ¯b(x) = −Cx, C := (D − Q)Π.
Suppose that birr(x) ∈ Range σ for anyx ∈ Rd. By definiteness ofΠ this is equivalent toRange Q ⊆ Range σ. By
36
Theorem 2.5.1, and applying the trace trick to compute the Gaussian expectations of a bilinear form, we obtain4
ep =
Z
Rd
b⊤
irrD−birrρ(x)dx = −
Z
Rd
x⊤ΠQD−QΠx ρ(x)dx
= −Tr(ΠQD−QΠΠ−1) = −Tr(D−QΠQ)
= −Tr(D−BQ) + Tr(D−DΠQ)| {z }
=0
= −Tr(D−BQ).
(2.23)
This expression for the entropy production is nice as it generalises the usual formula to linear diffusion processes to
degenerate noise, simply by replacing inverses with pseudo-inverses, cf. [65, eqs. 2.28-2.29] and [66,67].
Contrariwise, suppose thatRange Q ̸⊆ Range σ. Then by Theorem 2.5.4,
ep = +∞. (2.24)
Exact numerical simulation and entropy production rate
Linear diffusion processes can be simulated exactly as their transition kernels are known. Indeed, by solving the
process, one obtains the transition kernels of the Markov semigroup as a function of the drift and volatility [68,
Theorem 9.1.1]. The forward and time-reserved transition kernels are the following:
pε(·, x) = N(e−εBx, Sε), S ε :=
Z ε
0
e−tBσσ⊤e−tB⊤
dt,
¯pε(·, x) = N(e−εCx, ¯Sε), ¯Sε :=
Z ε
0
e−tCσσ⊤e−tC⊤
dt.
(2.25)
Sampling from the transition kernels allows one to simulate the process exactly, and offers an alternative way to
express the entropy production rate. Recall from Proposition 2.3.10 that the ep is the infinitesimal limit of the
entropy production rate of an exact numerical simulationep(ε) with time-stepε
ep = lim
ε↓0
ep(ε), e p(ε) = 1
ε Ex∼ρ[H[pε(·, x) | ¯pε(·, x)]].
We can leverage the Gaussianity of the transition kernels to compute the relative entropy and obtain an alternative
formula for the entropy production rate:
Lemma 2.6.1. The entropy production rate of the stationary linear diffusion process can also be expressed as
ep = lim
ε↓0
ep(ε),
ep(ε) = 1
2ε

Tr(¯S−
ε Sε) − rank σ + log det∗( ¯Sε)
det∗(Sε)
+ Tr

Π−1(e−εC − e−εB)⊤ ¯S−
ε (e−εC − e−εB)
i
,
(2.26)
where ·− is the Moore-Penrose pseudo-inverse anddet∗ is the pseudo-determinant.
A proof is provided in Section 2.8.4. Computing the limit (2.26) analytically, gives us back (2.23), (2.24), however,
we will omit those details here. For our purposes, this gives us a way to numerically verify the value ofep that was
obtained from theory. See Figures 2.3 and 2.4 for illustrations.
4To obtain the last equality we usedTr(D−DΠQ) = Tr(Π QD−D) = −Tr(DD−QΠ). By standard properties of the
pseudo-inverse DD− is the orthogonal projector ontoRange D = Range σ. Thus, Range Q ⊆ Range σ implies DD−Q = Q.
Finally, the trace of a symmetric matrixΠ times an antisymmetric matrixQ vanishes.
37
Figure 2.3: Exact simulation of linear diffusion process withbirr(x) ∈ Range σ. This figure considers an OU process
in 3d space driven by degenerate noise, i.e.,rank σ < 3. The coefficients are such thatσ = Q are of rank2. In particular,
birr(x) ∈ Range σ holds for everyx. The process is not elliptic nor hypoelliptic, but it is elliptic over the subspace in which it
evolves. The upper-left panel shows a sample trajectory starting fromx0 = (1, 1, 1). The upper-right panel shows samples from
different trajectories after a time-stepε. There are only two principal components to this point cloud as the process evolves on
a two dimensional subspace. In the bottom panel, we verify the theoretically predicted value ofep by evaluating the entropy
production of an exact simulationep(ε) with time-stepε. As predicted, we recover the trueep in the infinitesimal limit as the
time-step of the exact simulation tends to zeroε → 0. Furthermore, since the process is elliptic in its subspace, the entropy
production is finite.
2.6.2 Underdamped Langevin dynamics
In this sub-section, we consider the entropy production rate of underdamped Langevin dynamics and its numerical
simulations. Recall that the ep we compute here is definedwithout an additional momentum flip operator on the
path space measure of the time-reversed process (i.e., (2.2) and not (2.3)), and may be a distinct quantity from the
entropy production that physicists usually consider in such systems (see the discussion in Section 2.7.2).
Consider a HamiltonianH(q, p) function of positionsq ∈ Rn and momentap ∈ Rn of the form
H(q, p) = V (q) + 1
2p⊤M−1p (2.27)
for some smooth potential functionV : Rn → R and diagonal mass matrixM ∈ Rn×n.
38
Figure 2.4:Exact simulation of linear diffusion process withbirr(x) ̸∈ Range σ. This figure considers an OU process in 3d
space driven by degenerate noise. The coefficients are such thatRange birr is two-dimensional whileRange σ is one-dimensional,
and such that the process does not satisfy Hörmander’s hypoellipticity condition. As such the process is hypoelliptic on a two-
dimensional subspace; see a sample trajectory in the upper-left panel. By hypoellipticity its transition kernels are equivalent
in the sense of measures, although far removed: On the upper right panel we show samples from different trajectories after a
time-step ε. There are only two principal components to this data-cloud as the process evolves on a two dimensional subspace.
In the bottom panel, we verify the theoretically predictedep by evaluating the entropy production of an exact simulationep(ε)
with time-stepε. As predicted, we recoverep = +∞ in the infinitessimal limit as the time-step of the exact simulation tends
to zeroε ↓ 0. This turns out to be as the transition kernels of the forward and time-reversed processes become more and more
mutually singular as the time-step decreases.
The underdamped Langevin process is the solution to the SDE [38, eq 2.41]



dqt = M−1ptdt
dpt = −∇V (qt) dt − γM −1ptdt +
p
2γβ−1dwt
(2.28)
for some friction coefficientγ > 0. This process arises in statistical physics, as a model of a particle coupled to a
heat bath [69], [3, Chapter 8]; in Markov chain Monte-Carlo as an accelerated sampling scheme [26,56]; and also as
a model of interacting kinetic particles.
The invariant density, assuming it exists, is [38, Section 2.2.3.1]
ρ(q, p) = 1
Z e−βH(q,p) = 1
Z e−βV (q)e−β
2 p⊤M−1p.
Since the noise is additive the Itô interpretation of the SDE coincides with the Stratonovich interpretation, thus the
39
irreversible drift is in the range of the volatility if and only if the drift is in the range of the volatility (2.12). Observe
that when the momentum is non-zerop ̸= 0 the drift is not in the image of the volatility: in theq components the
drift is non-zero while the volatility vanishes. Sincep ̸= 0 has full measure under the stationary densityρ(q, p) we
obtain, from Theorem 2.5.4,
ep = +∞. (2.29)
Note that the entropy production of an exact numerical simulation with time-stepε >0 is usually finite by hypoel-
lipticity5
ep(ε) < +∞. (2.30)
Figure 2.5 illustrates this with an exact simulation of underdamped Langevin dynamics in a quadratic potential.
When the potential is non-quadratic, the underdamped process is a non-linear diffusion and one is usually unable
to simulate it exactly. Instead, one resolves to numerical approximations to the solution of the process. We now
turn to two common numerical discretisations of underdamped: the Euler-Maruyama and BBK discretisations. We
will examine whether these discretisations are good approximations to the true process by computing their entropy
production rate.
Euler-Maruyama discretisation
In this section, we show that an Euler-Maruyama (E-M) discretisation of underdamped Langevin dynamics at any
time-step ε >0 has infinite entropy production
eE-M
p (ε) = +∞. (2.31)
To see this, we take a step back and consider an arbitrary Itô SDE inRd
dxt = b(xt)dt + σ(xt)dwt
The Euler-Maruyama discretisation for some time-stepε >0 is
xi+1 = xi + b(xi)ε + σ(xi)ωi, ω i ∼ N(0, εIdd).
This is a Markov chain with the following transition kernels
pE-M
ε (xi+1, xi) = N(xi+1; xi + εb(xi), 2εD(xi)),
¯pE-M
ε (xi+1, xi) := pE-M
ε (xi, xi+1) ,
(2.32)
where ¯pE-M
ε denotes the transition kernel of the backward chain6.
It turns out that when the SDE is not elliptic the transition kernelspE-M
ε (·, x) , ¯pE-M
ε (·, x) tend to have different
supports:
5(2.30) always holds in a quadratic potential, whence the process is a linear diffusion and the results from Section 2.6.1
apply. We conjecture this to hold in the non-linear case as well, as hypoellipticity guarantees that the transition kernels are
mutually equivalent in the sense of measures.
6Caution: this is different from the E-M discretisation of the time-reversed process.
40
Figure 2.5: Exact simulation of underdamped Langevin dynamics.This figure plots underdamped Langevin dynamics
in a quadratic potential. Here, the process is two dimensional, i.e., positions and momenta evolve on the real line. We exploit the
fact that underdamped Langevin in a quadratic potential is an Ornstein-Uhlenbeck process to simulate sample paths exactly.
The choice of parameters was:V (q) = q2/2, M= γ = 1. The upper left panel plots a sample trajectory. One observes that the
process is hypoelliptic: it is not confined to a prespecified region of space, cf. Figures 2.3, 2.4, even though random fluctuations
affect the momenta only. The upper right panel plots samples of the forward and time-reversed processes after a time-step ofε.
In the bottom panel, we verify the theoretically predictedep by evaluating the entropy production of an exact simulationep(ε)
with time-stepε. As predicted, we recoverep = +∞ in the infinitessimal limit as the time-step of the exact simulation tends
to zeroε ↓ 0. This turns out to be because the transition kernels of the forward and time-reversed processes become more and
more mutually singular as the time-step decreases.
Lemma 2.6.2. For anyx ∈ Rd
supp pE-M
ε (·, x) = {y : y ∈ x + εb(x) + RangeD(x)}
supp ¯pE-M
ε (·, x) = {y : x ∈ y + εb(y) + RangeD(y)}
Lemma 2.6.2 is immediate from (2.32) by noting that the support of¯pE-M
ε (·, x) is the closure of those elements whose
successor by the forward process can bex.
Unpacking the result of Lemma 2.6.2 in the case of underdamped Langevin dynamics yields
supp pE-M
ε (·, x) = {y : yq = xq + εxp}, supp ¯pE-M
ε (·, x) = {y : yq + εyp = xq},
where x := (xq, xp) respectively denote position and momenta. One can see thatpE-M
ε (·, x) ⊥ ¯pE-M
ε (·, x) , ∀x ∈ Rd.
From Definition 2.3.11, we deduce that the entropy production rate of E-M applied to the underdamped process is
41
infinite for any time-stepε >0.
Figure 2.6: Euler-Maruyama simulation of underdamped Langevin dynamics. This figure compares the Euler-
Maruyama simulation of underdamped Langevin dynamics with the exact simulation available in Figure 2.5. The choice
of parameters was the same:V (q) = q2/2, M= γ = 1. The upper left panel plots a sample trajectory of the numerical scheme.
One observes that the numerical scheme is not confined to a prespecified region of space like the true process. The upper right
panel plots samples of the numerical scheme after a time-step ofε (in orange) given an initial condition atx0 (in red). This is
superimposed onto a heat map of the true transition kernel (in black). We see that samples from the numerical scheme are in
the right region of space, but are confined to a subspace which is not aligned with the heat map of the true transition kernel.
The support of the transition kernel of the time-reversed scheme is shown in blue. One sees that the supports of forward and
reverse transition kernels are mutually singular, thus the entropy production of the numerical discretisation is infinite for any
time-step, which differs from the true process which has finite entropy production for any positive time-step.
BBK discretisation
Contrariwise to Euler, the BBK integrator [37,38] is a splitting scheme that when applied to underdamped Langevin
yields absolutely continuous transition kernels. The numerical scheme consists of three intermediate steps
pi+ 1
2
= pi − ∇V (qi) ε
2 − γM −1pi
ε
2 +
p
2γβ−1ωi
qi+1 = qi + M−1pi+ 1
2
ε
pi+1 = pi+ 1
2
− ∇V (qi+1) ε
2 − γM −1pi+ 1
2
ε
2 +
p
2γβ−1ωi+ 1
2
with ωi, ωi+ 1
2
∼ N
 
0, ε
2 Id

. Its stability and convergence properties were studied in [37,38] and its ergodic properties
in [70–72].
It was shown in [35, Theorem 4.3] that the BBK discretisation of the underdamped Langevin process is quasi time-
reversible, so that
eBBK
p ≤ O(ε). (2.33)
One sees (cf. Figures 2.6 and 2.7 vs Figure 2.5) that the BBK integrator better approximates the transition kernels
than E-M does, however BBK still is largely inaccurate from the point of view of the entropy production rate as the
simulation becomes reversible when the time-step tends to zero.
42
Figure 2.7: BBK simulation of underdamped Langevin dynamics.This figure compares the BBK simulation of un-
derdamped Langevin dynamics with the exact simulation available in Figure 2.5. The choice of parameters was the same:
V (q) = q2/2, M= γ = 1. The upper left panel plots a sample trajectory of the numerical scheme. One observes that the
numerical scheme is not confined to a prespecified region of space like the true process. The upper right panel plots samples of
the numerical scheme after a time-step ofε (in orange) given an initial condition atx0 (in red). This is superimposed onto a
heat map of the true transition kernel (in black). We see that samples from the numerical scheme fit the true transition kernel
relatively well, but have a higher variance. The bottom panel estimates the entropy production rate of the numerical scheme
for several choices of time-stepε. This is done by discretising the state-space into a number of bins and numerically evaluating
the entropy production of the resulting Markov chain using samples, see Section 2.3.1 for details. The numerical values are
consistent with the theoretical result (2.33).
Summary
In summary, the underdamped Langevin process has infinite entropy production rate in phase space7, but finite
entropy production rate for any exact simulation with a positive time-step. When the potential is non-quadratic, the
process is a non-linear diffusion that one usually cannot simulate exactly. To simulate it as accurately as possible,
one should seek an approximating numerical scheme that has finite entropy production for any time-step, and whose
entropy production tends to infinity for infinitesimally small time-steps.
Two well-known choices of numerical discretisation are the Euler-Maruyama and BBK schemes. By comparing their
transition kernels with an exact simulation, we saw that the BBK scheme is a much better approximation to the true
process than Euler-Maruyama. Analysis of the entropy production rate shows how these discretisations still far short
in capturing important statistical properties of the process: the E-M discretisation has infinite entropy production for
7Recall that theep we computed here is definedwithout an additional momentum flip operator on the path space measure
of the time-reversed process, i.e., (2.2) and not (2.3). See also the discussion in Section 2.7.2,
43
any time-step; while the BBK discretisation has finite entropy production for any time-step, and vanishing entropy
production for infinitesimally small time-steps. Whenever possible, a good way to choose a time-step size for the BBK
integrator might be matching its entropy production rate with that of an exact simulation. These results indicate
that employing a BBK scheme with very small time-steps might be inadequate. Luckily, large step-sizes are usually
preferred in practice.
In conclusion, the entropy production rate is a useful statistic of stochastic processes that can be used as a tool to
devise accurate numerical schemes, particularly in a non-equilibrium statistical physics or sampling context where
preserving the amount of time-irreversibility is important. Future development of numerical schemes should take
entropy production into account; for example, in developing numerical schemes for underdamped Langevin, one
should seek a finite entropy production rate for any positive time-step, which tends to infinity when time-steps
become infinitesimally small. Other numerical schemes should be analysed in future work, such as those based on
the lexicon for the approximation of the underdamped process developed by Leimkühler and Matthews [73, p. 269
& 271].
2.7 Discussion
Briefly, we unpack a couple of observations and possible extensions of this work.
2.7.1 ep and sampling efficiency
A well-known criterion for efficient sampling is time-irreversibility [26,33,34,74]. Intuitively, non-reversible processes
backtrack less often and thus furnish more diverse samples [75]. Furthermore, the time-irreversible part of the drift
flows along the contours of the stationary probability density which yields mixing and accelerates convergence to
the target measure. It is well known that removing non-reversibility worsens the spectral gap and the asymptotic
variance of the MCMC estimator [33,34,74], which are two main indicators of the speed of convergence to stationary
state [26]. Thus efficient samplers at non-equilibrium steady-state have positive entropy production.
In elliptic linear diffusions, one can construct the optimal time-irreversible drift to optimise the spectral gap [60,76]
or the asymptotic variance [74]. This indicates that one cannot optimise elliptic samplers by simply increasing their
entropy production at steady-state without any other constraints, as, we recall,ep is a quadratic form of the strength
of the time-irreversible drift (Figure 2.2).
Beyond this, the entropy production rate of general diffusions (Theorems 2.5.1, 2.5.2) bears a formal resemblance
to the Donsker-Varadhan functional [33, Theorem 2.2], from which the asymptotic variance of MCMC estimators is
derived [33]. It is entirely possible that one might be able to relate the non-stationary entropy production rate ((2.1)
or [15, eq. 3.19]) to the Donsker-Varadhan functional, and thus give a more complete characterisation of sampling
efficiency in terms of entropy production.
Many diffusion models of efficient sampling (the underdamped (2.7) and generalised [3, eq. 8.33] Langevin dynamics,
the fastest converging linear diffusion [31]), and stochastic optimisation (stochastic gradient descent in deep neural
networks [27]) are not elliptic; that is, they are driven by less Brownian drivers than there are dimensions to their
phase space. In particular, these processes have their forward and backward path space measures which are mutually
singular, and infinite entropy production8. In light of this, we conjecture that mutual singularity of the forward
8 [27, Section 5] shows that stochastic gradient descent is out of equilibrium. Furthermore, it shows empirically that the
rank of the diffusion matrix is about 1% of its dimension in deep neural networks. The sparsity of the noise with respect to the
highly out-of-equilibrium behaviour they observe conjecturesbirr(x) ̸∈ Range σ(x) and thus, mutual singularity of forward and
44
and backward path space measures is an important facet of sampling efficiency (provided the process is ergodic).
Mutual singularity apparently exacerbates the mixing effect that time-irreversibility introduces in the elliptic case.
Heuristically, if some paths can be taken by the forward process and not by the backward process, these trajectories
cannot be reversed, thus the process is constantly forced to visit new regions of phase space, which contributes to the
(non-reversible) convergence to steady-state.
If the above intuition holds, a useful statistic of sampling efficiency might be the probability that the forward
process takes paths that cannot be taken by the backward process. By the Lebesgue decomposition theorem we can
decompose the forward path space measureP into Preg + Psing such thatPreg ≪ ¯P and Psing ⊥ ¯P. This statistic is
the non-negative real number
P({γ ∈ C([0, T], Rd) : dP/d¯P(γ) = +∞}) = Psing

C([0, T], Rd)

,
where dP/d¯P is the Lebesgue derivative between forward and backward path space measures. Note that the linear
diffusion that converges fastest to steady-state maximises the latter (under the constraint that the process remains
ergodic) since it has only one Brownian driver [31]. However, this statistic does not tell us all since the direction of the
Brownian driver with respect to the drift and the stationary density is important to determine sampling efficiency.
Yet, these observations indicate that employing diffusions with less Brownian drivers might be an advantage for
sampling and optimisation (provided ergodicity is maintained). A careful investigation of these relationships is left
to future work.
2.7.2 Generalised non-reversibility and entropy production rate
Many diffusions studied in statistical physics are not time-reversible but they are generalised reversible; that is,
they are time-reversible up to a one-to-one transformationθ of phase-space which leaves the stationary measure
invariant [36, Section 5.1], [38, eq. 2.32]. For example, the underdamped langevin equation is generalised reversible—
it is reversible up to momentum reversal (Example 2.4.6); the generalised Langevin equation is also generalised
reversible.
The entropy production, as defined in Definition 2.3.6, measures time-irreversibility as opposed to generalised non-
reversibility. However, as pointed out in Remark 2.3.7, the physically meaningful definition of entropy production
rate sometimes comprises additional operators applied to the path-space measure of the time-reversed process. This
modified notion ofep, which we refer to as generalised entropy production, usually takes the form of
egen,θ
p := lim
ε↓0
1
ε H

P[0,ε], θ# ¯P[0,ε]

, (2.34)
where θ# is the pushforward operator associated to an involution of phase-spaceθ that leaves the stationary distribu-
tion invariant. The generalised entropy production rate measures the generalised non-reversibility of the process; that
is, the extent to which the process is time-irreversible up to the one-to-one transformationθ. Of course, generalised
entropy production reduces to entropy production, as defined in Definition 2.3.6, whenθ ≡ Id.
Since generalised entropy production can sometimes be more physically meaningful, we spend the rest of this section
computing it in a couple of examples.
Itseemstobeageneralconsensusinstatisticalphysicsthatthephysicallyrelevantnotionofentropyproductionforthe
underdamped Langevin process is the generalised entropy production whenθ is the momentum reversal [40–42,77].
It is then a by-product of Example (2.4.6) that underdamped Langevin dynamics has zero (generalised) entropy
backward path space measures.
45
production egen,θ
p = 0, which contrasts with the infinite entropy production one obtains in the non-generalised case
(Section 2.5.2) when one setsθ ≡ Id.
Beyond this, generalised entropy production could be a useful construct to quantify how far certain diffusion pro-
cesses are from being generalised reversible. For example we can quantify to what extent certain time-irreversible
perturbations of underdamped Langevin dynamics are far from being generalised reversible up to momentum reversal.
Example 2.7.1 (egen,θ
p of perturbed underdamped Langevin dynamics). Consider the following perturbations of
underdamped Langevin dynamics [78, eq. 8]



dqt = M−1pt dt − Q1∇V (qt) dt
dpt = −∇V (qt) dt − Q2M−1pt dt − γM −1pt dt +
p
2γβ−1dwt,
(2.35)
where Q1, Q2 ∈ Rd×d are constant antisymmetric matrices. By inspection this equation has a Helmholtz decomposi-
tion that is similar to underdamped Langevin dynamics (cf. Example 2.4.11)
brev(q, p) = D∇log ρ(q, p), b irr(q, p) = Q∇log ρ(q, p)
∇log ρ(q, p) = −β
"
∇V (q)
M−1p
#
, D =
"
0 0
0 γβ−1 Idn
#
, Q = β−1
"
Q1 −Idn
Idn Q2
#
.
The time-reversed process solves the following SDE (Section 2.4.4)



d¯qt = −M−1 ¯ptdt + Q1∇V (¯qt) dt
d¯pt = ∇V (¯qt) dt + Q2M−1 ¯pt dt − γM −1 ¯ptdt +
p
2γβ−1dwt.
Define θ(q, p) = ( q, −p) to be the momentum reversal transformation of phase space (that leaves underdamped
Langevin dynamics invariant as shown in Example 2.4.6). Lettingˆpt = −¯pt, the time-reversed momentum-flipped
equation looks like



d¯qt = M−1 ˆptdt + Q1∇V (¯qt) dt
dˆpt = −∇V (¯qt) dt + Q2M−1 ˆpt dt − γM −1 ˆptdt +
p
2γβ−1d ˆwt.
(2.36)
Denote bybgen,θ
irr the vector field whose sign changes after successively applying these two transformations:
bgen,θ
irr (q, p) =
"
−Q1∇V (q)
−Q2M−1p
#
.
It follows that the time-reversed, momentum-flipped equation (2.36) does not induce the same path space measure
as the initial equation (2.35) unlessQ1 = Q2 = 0. To see this, we follow the proofs of Theorems 2.5.4 and 2.5.1 to
compute the generalised entropy production rate
Q1 ̸= 0 ⇒ P ⊥ θ# ¯P ⇒ egen,θ
p = +∞,
Q1 = 0 ⇒ P ∼ θ# ¯P ⇒ egen,θ
p =
ZZ
Rn
bgen,θ
irr · D−bgen,θ
irr ρ(q, p) dp dq
= γ−1β
Z
Rn
(Q2M−1p)2ρ(p)dp
= −γ−1β Tr
 
Q2M−1Q2

< +∞.
The last line equality follows from a standard result about expectations of bilinear forms under Gaussian distributions,
since ρ(p) is Gaussian with covariance matrixM. As usual, the generalised entropy production rate is a quadratic
46
form of the (generalised) irreversible drift.
2.7.3 Geometric interpretation of results
Our main results concerning the value of entropy production have a straightforward geometric interpretation. The
Stratonovich interpretation of the SDE
dxt = bs(xt) + σ(xt) ◦ dwt
is the natural one to consider in a geometric context, when looking at the directions of the driftbs and volatility
vector fieldsσ·i, i= 1, . . . , m(i.e., the columns of the volatility matrix field).
Recall from Remark 2.4.9 that the Stratonovich SDE also admits a Helmholtz decompositionbs = bs
rev + bs
irr with
bs
irr = birr, so that
bs(x) ∈ Range σ(x) ⇐⇒ bs
irr(x) ∈ Range σ(x) ⇐⇒ ¯bs(x) ∈ Range σ(x) for anyx ∈ supp µ, (2.37)
where¯bs is the drift of the time-reversed Stratonovich SDE. In particular, time-reversal is a transformation that sends
bs to ¯bs, bs
irr to −bs
irr, or, equivalently, adds−2bs
irr to the drift.
Our main results can be summarised in a nutshell:
µ
n
x ∈ Rd : bs(x) ∈ Range σ(x)
o
= 1 ⇒ ep =
Z
Rd
bs
irr · D−bs
irrdµ (see Theorem 2.5.1 or 2.5.2 for details),
µ
n
x ∈ Rd : bs(x) ∈ Range σ(x)
o
< 1 ⇒ ep = +∞ (see Theorem 2.5.4 for details).
(2.38)
We derived our main results using the Itô interpretation of an SDE because this allowed us to make more general
statements, notably in the context of the general existence and uniqueness theorem of strong solutions to Itô SDEs;
it turns out, however, that these results are more naturally interpreted in the Stratonovich context.
Consider the case where there is noise in the direction of the vector fieldbs, (almost every-) where the process is; in
other words, assume thatµ
 
x ∈ Rd : bs(x) ∈ Range σ(x)
	
= 1. Consider the process at any pointx ∈ supp µ. In
virtue of (2.37), the drifts of the forward and time reversed processes both live inRange σ(x), the subset of the tangent
space that is spanned by the volatility vector fields. Since the driving fluctuations are Gaussian onRange σ(x), the
time-reversal transformation will be reversed by the random fluctuations with positive probability. Thus, the forward
and time-reversed Markov transition kernels (for an infinitesimally small time-step) have the same support—they are
mutually equivalent. Under sufficient regularity, made explicit in Theorems 2.5.1 or 2.5.2, their relative entropy is
finite. The ep is the relative entropy between such Markov kernels on an infinitesimally small time-step (Proposition
2.3.10), so it too will be finite.
On the other hand, if there existsx ∈ supp µ such that there is no noise in the direction of the vector fieldbs, that
is bs(x) ̸∈ Range σ(x), then the direction of the forward and time-reversed dynamics in an infinitesimal time-step lie
on different tangent spaces,bs(x) + Rangeσ(x) and ¯bs(x) + Rangeσ(x), respectively. This means that the forward
and time-reversed transition kernels (for an infinitesimally small time-step) are mutually singular and their relative
entropy is infinite; thus, theep is also infinite.
In particular, it should be straightforward to extend these observations and calculations to diffusions on manifolds.
47
2.8 Addendum: Proofs for Chapter 1
Here we provide proofs supporting Chapter 1.
2.8.1 The ep of stationary Markov processes
ep in terms of path space measures with deterministic initial condition
We prove Proposition 2.3.8:
Proof. The proof is straightforward
ep = 1
t H

P[0,t] | ¯P[0,t]

= 1
t Ex•∼P

log dP[0,t]
d¯P[0,t]
(x•)

= 1
t Ex∼µ

Ex•∼Px
[0,t]

log dP[0,t]
d¯P[0,t]
(x•)

= 1
t Ex∼µ
"
Ex•∼Px
[0,t]
"
log
dPx
[0,t]
d¯Px
[0,t]
(x•)
##
= 1
t Ex∼µ

H

Px
[0,t] | ¯Px
[0,t]

.
ep in terms of transition kernels
We prove Proposition 2.3.10:
Proof. By Proposition 2.3.8,
ep = lim
ε↓0
1
ε Ex∼µ
"
Ex•∼Px
[0,ε]
"
log
dPx
[0,ε]
d¯Px
[0,ε]
(x•)
##
= lim
ε↓0
1
ε Ex∼µ

Ey∼pε(·,x)

log dpε(·, x)
d¯pε(·, x)(y)

= lim
ε↓0
1
ε Ex∼µ [H [pε(·, x) | ¯pε(·, x)]] .
2.8.2 Time-reversal of stationary diffusions
Conditions for the reversibility of the diffusion property
We prove Lemma 2.4.2:
Proof. Recall the following facts:
• (xt)t∈[0,T] is a Markov diffusion process. Its generator is an unbounded, linear operator given by
L : C∞
c (Rd) ⊂ Dom L ⊂ Lp
µ(Rd) → Lp
µ(Rd), 1 ≤ p ≤ ∞, L f = b · ∇f + D∇ · ∇f. (2.39)
48
• The time-reversal of a Markov process is also a Markov process. Let¯L be the generator of the time-reversed
process (¯xt)t∈[0,T]. It is known that¯L is the adjoint ofL. In other words, we have the identity
Z
Rd
f L g dµ =
Z
Rd
g¯Lf dµ, ∀f ∈ Dom ¯L, g∈ Dom L, (2.40)
where Dom ¯L =

f ∈ L1
µ(Rd) | ∃h ∈ L1
µ(Rd), ∀g ∈ Dom L :
R
Rd f L gdµ =
R
Rd hgdµ
	
. This follows from the fact
that the Markov semigroup of the time-reversed process is the adjoint semigroup [15, p. 113], and thus the
infinitesimal generator is the adjoint generator [61,62], [15, Thm 4.3.2].
• L1
loc-functions define distributions, and hence admit distributional derivatives (which need not be functions).
We identify the generator of the time-reversed process by computing the adjoint of the generator. In the following, all
integrals are with respect to thed-dimensional Lebesgue measure. Letf, g∈ C∞
c (Rd). Noting thatfρb · ∇g, fρD∇ ·
∇g ∈ L1(Rd), we have
Z
Rd
f L gρ =
Z
Rd
fρb · ∇g +
Z
Rd
fρD∇ · ∇g.
On the one hand, noting thatfρb, ρb∈ L1
loc(Rd, Rd), we have
Z
Rd
fρb · ∇g = −
Z
Rd
g∇ ·(fρb) = −
Z
Rd
g (ρb · ∇f + f∇ ·(ρb))
= −
Z
Rd
g (ρb · ∇f + f∇ · ∇ ·(ρD)) ,
where the last equality follows from the stationary Fokker-Planck equation. (Recall that local boundedness of coeffi-
cients b, σ, and Itô’s formula imply that the stationary densityρ satisfies ∇ ·(−bρ + ∇ ·(Dρ)) = 0 where the equality
is in a distributional sense).
On the other hand, noting thatfρD, ρD∈ L1
loc(Rd, Rd×d), we have
Z
Rd
fρD∇ · ∇g =
Z
Rd
fρD · ∇ · ∇g =
Z
Rd
g∇ · ∇ ·(fρD)
=
Z
Rd
g∇ ·(ρD∇f + f∇ ·(ρD))
=
Z
Rd
g (2∇ ·(ρD) · ∇f + ρD∇ · ∇f + f∇ · ∇ ·(ρD)) .
Finally, summing the previous two equations yields:
Z
Rd
f L gρ =
Z
Rd
g (−ρb · ∇f + 2∇ ·(Dρ) · ∇f + ρD∇ · ∇f) =
Z
Rd
g¯Lfρ.
And thus, the generator of the time-reversed process satisfiesρ¯Lf = −ρb · ∇f + 2∇ ·(Dρ) · ∇f + ρD∇ · ∇f for all
f ∈ C∞
c (Rd). The time-reversed process is a diffusion if its generator is a second order differential operator with
no constant part. This is the case here, except for the fact that the generator outputs distributions as opposed to
functions. For the generator to be a diffusion operator we need to assume that the distributional derivative∇ ·(Dρ)
is indeed a function (which is then necessarily inL1
loc(Rd, Rd)). Thus, the following are equivalent:
• ∇ ·(Dρ) ∈ L1
loc(Rd, Rd),
• ¯Lf ∈ L1
µ(Rd) for anyf ∈ C∞
c (Rd), where¯Lf = −b · ∇f + 2ρ−1∇ ·(Dρ) · ∇f + D∇ · ∇f,
• (¯xt)t∈[0,T] is a Markov diffusion process.
49
The time-reversed diffusion
We prove Theorem 2.4.5.
Proof. Since (¯xt)t∈[0,T] is a Markov diffusion process with generator¯L, we have shown that its drift and diffusion are
indeed ¯b, D, in the proof of Lemma 2.4.2.
To show that any such diffusion process induces the path space measure of the time-reversed process, it suffices to
show that the martingale problem associated to(¯L, ρ) is well-posed. First note that, by Assumption 2.4.3, the Itô
SDE (2.5) has a unique strong solution. Therefore it also has a unique weak solution. Therefore,(xt)t∈[0,T] is the
unique solution to the martingale problem associated to the generatorL = b · ∇+ D∇ · ∇[79, Theorem 1.1]. In
other words, the martingale problem associated to(L, ρ) is well-posed. It remains to show that there is a one-to-one
correspondence between stationary solutions to the martingale problem associated toL and ¯L.
Consider Markov processes(yt)t∈[0,T], (¯yt)t∈[0,T], ¯yt = yT−t stationary at the densityρ. We show that (¯yt)t∈[0,T]
solves the martingale problem wrt¯L if and only if(yt)t∈[0,T] solves the martingale problem wrtL.
• (¯yt)t∈[0,T] solves the martingale problem wrt¯L if and only if for arbitrary0 ≤ s ≤ t ≤ T, f, g∈ C∞
c (Rd)
E

f(¯yt) −
Z t
0
¯Lf(¯yr)dr | ¯yθ, 0 ≤ θ ≤ s

= f(¯ys) −
Z s
0
¯Lf(xr)dr
Markov
⇐⇒ E

f(¯yt) −
Z t
0
¯Lf(¯yr)dr | ¯ys

= f(¯ys) −
Z s
0
¯Lf(xr)dr
⇐⇒E

f(¯yt) − f(¯ys) −
Z t
s
¯Lf(¯yr)dr | ¯ys

= 0
⇐⇒E

f(¯yt) − f(¯ys) −
Z t
s
¯Lf(¯yr)dr

g(¯ys)

= 0.
(2.41)
If we make the change of variablet ← T − s, s← T − t, so that0 ≤ s ≤ t ≤ T, this is equivalent to:
⇐⇒E

f(ys) − f(yt) −
Z T−s
T−t
¯Lf(yT−r)dr

g(yt)

= 0
⇐⇒E

f(ys) − f(yt) −
Z t
s
¯Lf(yr)dr

g(yt)

= 0
• Repeating the equivalences in (2.41),(yt)t∈[0,T] solves the martingale problem wrtL if and only if for arbitrary
0 ≤ s ≤ t ≤ T, f, g∈ C∞
c (Rd)
E

g(yt) − g(ys) −
Z t
s
L g(yr)dr

f(ys)

= 0.
• Thus, it suffices to show that the two last expressions are equal, i.e.,
E

f(ys) − f(yt) −
Z t
s
¯Lf(yr)dr

g(yt)

= E

g(yt) − g(ys) −
Z t
s
L g(yr)dr

f(ys)

By stationarity, we have
E[(f(ys) − f(yt)) g(yt)] = E[f(ys)g(yt) − f(yt)g(yt)]
= E[f(ys)g(yt) − f(ys)g(ys)] = E[(g(yt) − g(ys)) f(ys)] .
50
Thus, it remains to show that
E
Z t
s
g(yt)¯Lf(yr)dr

= E
Z t
s
f(ys) Lg(yr)dr

We proceed to do this. On the one hand:
E
Z t
s
f(ys) Lg(yr)dr

=
Z t
s
E[f(ys) Lg(yr)] dr
=
Z t
s
E[E[f(ys) Lg(yr) | ys]] dr
=
Z t
s
E[f(ys)E[L g(yr) | ys]] dr
=
Z t
s
E[f(ys) Pr−s L g(ys)] dr
=
Z t
s
Z
Rd
f(y) Pr−s L g(y)ρ(y)dydr (stationarity)
=
Z
Rd
f(y)
Z t
s
Pr−s L g(y)drρ(y)dy
=
Z
Rd
f(y) (Pt−s −P0) g(y)ρ(y)dy (∂t Pt = Pt L)
=
Z
Rd
g(y)
 ¯Pt−s − ¯P0

f(y)ρ(y)dy
On the other hand:
E
Z t
s
g(yt)¯Lf(yr)dr

=
Z t
s
E

g(yt)¯Lg(yr)

dr
=
Z t
s
E

E

g(yt)¯Lf(yr) | yt

dr
=
Z t
s
E

g(yt)E
¯Lf(yr) | yt

dr
=
Z t
s
E

g(yt)¯Pt−r ¯Lf(yt)

dr
=
Z t
s
Z
Rd
g(y)¯Pt−r ¯Lf(y)ρ(y)dydr (stationarity)
=
Z
Rd
g(y)
Z t
s
¯Pt−r ¯Lf(y)drρ(y)dy
=
Z
Rd
g(y)
 ¯Pt−s − ¯P0

f(y)ρ(y)dy (∂t ¯Pt = ¯Pt¯L)
This shows the one-to-one correspondence and completes the proof.
The Helmholtz decomposition
We prove Proposition 2.4.7.
Proof. "⇒" We define the time-reversible and time-irreversible parts of the drift
brev := b + ¯b
2 , b irr := b − ¯b
2 .
51
We now show that the have the predicted functional form. Forx such thatρ(x) = 0, brev =
 
b + ¯b

/2 = 0. For
x such thatρ(x) > 0
brev = b + ¯b
2 = ρ−1∇ ·(Dρ) = ρ−1D∇ρ + ρ−1ρ∇ ·D = D∇log ρ + ∇ ·D. (2.42)
For the time-irreversible drift, first note that the stationary densityρ solves the stationary Fokker-Planck
equation [3,80]
∇ ·(−bρ + ∇ ·(Dρ)) = 0.
Decomposing the drift into time-reversible and time-irreversible parts, from (2.42)
−bρ + ∇ ·(Dρ) = −brevρ − birrρ + ∇ ·(Dρ) = −birrρ,
we obtain that the time-irreversible part produces divergence-free (i.e., conservative) flow w.r.t. the steady-state
density
∇ ·(birrρ) = 0.
"⇐" From (2.42) the time-reversible part of the drift satisfies the following identity
brevρ = ∇ ·(Dρ). (2.43)
It follows that the densityρ solves the stationary Fokker-Planck equation
∇ ·(−bρ + ∇ ·(Dρ)) = ∇ ·(−brevρ − birrρ + ∇ ·(Dρ)) = ∇ ·(−birrρ) = 0.
We prove Proposition 2.4.10:
Proof. "⇒" Recall that any smooth divergence-free vector field is the divergence of a smooth antisymmetric matrix
field A = −A⊤ [21,22,81,82]
birrρ = ∇ ·A.
This result holds most generally a consequence of Poincaré duality in de Rham cohomology [82, Appendix D].
We define a new antisymmetric matrix fieldQ := ρ−1A. From the product rule for divergences we can rewrite
the time-irreversible drift as required
birr = Q∇log ρ + ∇ ·Q.
"⇐" Conversely, we define the auxiliary antisymmetric matrix fieldA := ρQ. Using the product rule for divergences
it follows that
birr = ρ−1∇ ·A.
Finally,
∇ ·(birrρ) = ∇ ·(∇ ·A) = 0
as the matrix fieldA is smooth and antisymmetric.
Multiple perspectives on the Helmholtz decomposition
We prove Proposition 2.4.12:
52
Proof of Proposition 2.4.12.The proof is analogous to [5, Proposition 3]. In view of Section 2.4.5, we only need to
check that: 1) if
√
2 Σf = σ⊤∇f, then
√
2 Σ∗ g = −∇ ·(σg) − ∇log ρ · σg; 2) the symmetric part of the generator
factorises asS = −Σ∗Σ.
1) For anyf, g∈ C∞
c (Rd) :
⟨f,
√
2Σ∗g⟩L2µ(Rd) = ⟨
√
2Σf, g⟩L2µ(Rd) =
Z
gσ⊤∇fρ(x) dx
=
Z
σgρ · ∇f(x) dx = −
Z
f∇ ·(σgρ)(x) dx
=
Z
−f∇log ρ · σgρ(x) − f∇ ·(σg)ρ(x) dx
= ⟨f, −∇log ρ · σg − ∇ ·(σg)⟩L2µ(Rd)
This implies
√
2Σ∗g = −∇log ρ · σg − ∇ ·(σg).
2) For anyf ∈ C∞
c (Rd):
−Σ∗Σf = ∇log ρ · D∇f + ∇ ·(D∇f)
= ∇log ρ · D∇f + (∇ ·D) · ∇f + D∇ · ∇f
= brev · ∇f + D∇ · ∇f = S f
where the penultimate equality follows sincebrev = D∇log ρ + ∇ ·D, µ-a.e.
We now prove Proposition 2.4.13:
Proof of Proposition 2.4.13. • We compute the Fréchet derivative ofH[· |ρ]. First of all, we compute its Gâteaux
derivative in the direction ofη.
d
dε H[ρt + εη | ρ] = d
dε
Z
Rd
(ρt + εη) log ρt + εη
ρ dx =
Z
Rd
η log ρt + εη
ρ + ηdx =
Z
Rd
η

log ρt + εη
ρ + 1

dx.
By definition of the Fréchet derivative, we haved
dε H[ρt + εη | ρ]|ε=0 = ⟨d H[ρt | ρ], η⟩. This implies d H[ρt |
ρ] = log ρt
ρ + 1 by the Riesz representation theorem.
• Recall, from Proposition 2.4.12, that
√
2Σ = σ⊤∇. We identifyΣ′. For anyf, g∈ C∞
c (Rd)
⟨g,
√
2Σf⟩ =
Z
Rd
gσ⊤∇fdx =
Z
Rd
σg · ∇fdx = −
Z
Rd
f∇ ·(σg)dx.
This yields
√
2Σ′g = −∇ ·(σg). And in particular,Σ′(ρtΣξ) = −∇ ·(ρtD∇ξ).
• We define Mρt(ξ) := Σ ′(ρtΣξ) = −∇ ·(ρtD∇ξ) and verify that this is a symmetric semi-positive definite
operator. For anyg, h∈ C∞
c (Rd):
⟨Mρt h, g⟩ = ⟨Σh, Σg⟩ = ⟨h, Mρt g⟩, ⟨Mρt g, g⟩ = ⟨Σg, Σg⟩ρt ≥ 0.
Also, −Mρt (d H[ρt | ρ]) = ∇ ·(ρtD∇log ρt
ρ ) is immediate.
53
• We defineW(ρt) = ∇ ·(−birrρt) and verify the orthogonality relation:
⟨W(ρt), d H[ρt | ρ]⟩ =
Z
Rd

log ρt
ρ + 1

∇ ·(−birrρt)dx =
Z
Rd
birrρt∇

log ρt
ρ + 1

dx
=
Z
Rd
birrρt
ρ
ρt
∇
ρt
ρ

dx = −
Z
Rd
∇ ·(birrρ)ρt
ρ dx = 0,
where the last equality holds by Proposition 2.4.7.
2.8.3 The ep of stationary diffusions
Regular case
We prove Theorem 2.5.1:
Proof. By Assumption 2.4.3 the Itô SDE (2.5) has a unique non-explosive strong solution(xt)t≥0 with respect to
the given Brownian motion(wt)t≥0 on a filtered probability space(Ω, F, {Ft}t≥0 , P). (Even though Assumption
2.4.3 ensures non-explosiveness of the solution on a finite time-interval, stationarity implies that we may prolong the
solution up to arbitrary large times).
By Theorem 2.4.5 we know that a solution to the following SDE
d¯xt = ¯b(¯xt)dt + σ(¯xt)dwt, ¯x0 = x0, (2.44)
induces the path space measure of the time-reversed process. By Proposition 2.4.7, we can rewrite the (forward and
time-reversed) drifts asb = brev + birr and ¯b = brev − birr.
We define thelocalised coefficients
b(n)(x) := b

1 ∧ n
|x|

x

=



b(x) if |x| ≤n
b

n x
|x|

if |x| > n,
and analogously for¯b(n), σ(n), b(n)
rev, b(n)
irr . Note that the assignment·(n) respects sums and products, in particular
b(n) = (brev + birr)(n) = b(n)
rev + b(n)
irr ,
¯b(n) = (brev − birr)(n) = b(n)
rev − b(n)
irr .
(2.45)
It is easy to see that the localised SDE
dx(n)
t = b(n)(x(n)
t )dt + σ(n)(x(n)
t )dwt, x (n)
0 = x0
alsohasauniquestrongsolution x(n) = (x(n)
t )t≥0 withrespecttothegivenBrownianmotion (wt)t≥0 ontheprobability
space (Ω, F, {Ft}t≥0 , P). This follows from the fact that the localised SDE has locally Lipschitz continuous and
bounded coefficients that satisfy the assumptions of Theorem [45, Theorem 3.1.1].
54
From assumption 1, we obtain that forρ-a.e. x ∈ Rd
birr(x) ∈ Range σ(x) ⇒ birr(x) = σσ−birr(x)
b(n)
irr (x) ∈ Range σ(n)(x) ⇒ b(n)
irr (x) = σ(n)σ(n)−
b(n)
irr (x).
(2.46)
Then, (2.45) and (2.46) imply that we can rewrite the localised SDE as
dx(n)
t = b(n)
rev(x(n)
t )dt + σ(n)(x(n)
t )
h
σ(n)−
b(n)
irr

x(n)
t

dt + dwt
i
, x (n)
0 = x0.
By the definition of Itô’s stochastic calculus, x(n)
t is an Ft-adapted process. By assumption 2, σ−birr is Borel
measurable and thus it follows that the localised mapσ(n)−
b(n)
irr is Borel measurable. Thus −2σ(n)−
b(n)
irr

x(n)
t

is
also an Ft-adapted process. In addition, by continuity and localisation,σ(n)−
b(n)
irr is bounded. Therefore, by [83,
Proposition 10.17 (i)] applied to−2σ(n)−
b(n)
irr

x(n)
s

,
Z(n)
t = exp

−2
Z t
0
D
σ(n)−
b(n)
irr

x(n)
s

, dws
E
+
σ(n)−
b(n)
irr

x(n)
s

2
ds

, t≥ 0,
is a martingale on the probability space

Ω, F, {Ft}t≥0 , P

. We define a new probability measure¯Pn on the sample
space Ω through
d ¯Pn
dP

Ft
= Z(n)
t , ∀t ≥ 0. (2.47)
By Girsanov’s theorem [83, Theorem 10.14],x(n)
t solves the SDE
dx(n)
t = b(n)
rev(x(n)
t )dt − σ(n)(x(n)
t )
h
σ(n)−
b(n)
irr

x(n)
t

dt + dwt
i
, x (n)
0 = x0.
on the probability space

Ω, F, {Ft}t≥0 , ¯Pn

. Using (2.46),x(n)
t solves
dx(n)
t = ¯b(n)(x(n)
t )dt + σ(n)(x(n)
t )dwt, x (n)
0 = x0
on said probability space.
We define a sequence of stopping timesτ0 = 0 and τn = inf {t ≥ 0 : |xt| > n} for n >0. Since(xt)t≥0 is non-explosive,
P-a.s. limn→∞ τn = +∞. As xt = x(n)
t when t ≤ τn, we haveP-a.s.
Z(n)
t∧τn = exp

−2
Z t∧τn
0

σ−birr(xs), dws

+
σ−birr(xs)
2
ds

.
As Z(n+1)
t 1{t<τn} = Z(n)
t 1{t<τn}, we define the limit asn → +∞
Zt ≡
+∞X
n=1
Z(n)
t 1{τn−1≤t<τn} = lim
n→+∞
Z(n)
t 1{t<τn} = lim
n→+∞
Z(n)
t∧τn.
By definition,Zt is a continuous local martingale on

Ω, F, {Ft}t≥0 , P

.
We computeZt. Let’s write−log Z(n)
t∧τn = M(n)
t + Y (n)
t , where
M(n)
t = 2
Z t∧τn
0

σ−birr(xs), dws

, and, Y (n)
t = 2
Z t∧τn
0
σ−birr(xs)
2
ds.
We also defineMt = 2
Rt
0

σ−birr(xs), dws

and Yt = 2
Rt
0
σ−birr(xs)
2
ds.
55
From assumption 3, we have
Z
Rd
σ−birr(x)
2
ρ(x)dx = 1
2
Z
Rd
b⊤
irrD−birrρ(x)dx <+∞.
Thus, we obtain that
EP
M(n)
t − Mt

2
= 4EP

Z t
0

σ−birr(xs)1{s>τn}, dws

2
= 4EP
Z t
0
σ−birr(xs)
2
1{s>τn}ds (Itô’s isometry)
n→+∞
− − − − − →0,
EP
Y (n)
t − Yt
 = 2EP
Z t
0
σ−birr(xs)
2
1{s>τn}ds
n→+∞
− − − − − →0.
Thus, −log Zt = Mt + Yt. By Itô calculus,Mt is a martingale on the probability space

Ω, F, {Ft}t≥0 , P

, and in
particular EP [Mt] = 0.
Let T >0. Let
 
C([0, T], Rd), B

denote the path space, whereB is the Borel sigma-algebra generated by the sup
norm ∥ · ∥∞. Denote trajectories of the process byx• := (xt)t∈[0,T] : Ω → C([0, T], Rd). By definition of Itô calculus,
ZT is measurable with respect to⟨xs : 0 ≤ t ≤ T⟩, so there exists a positive measurable functionZC
T on the path
space, such thatP-a.s. ZC
T (x•(ω)) = ZT (ω) for ω ∈ Ω, i.e., the following diagram commutes
C([0, T], Rd)
Ω R>0.
ZC
Tx•
ZT
Note that the path space
 
C([0, T], Rd), B

admits a canonical filtration(Bt)t∈[0,T], where
Bt =
n
A ⊂ C([0, T], Rd) : A|[0,t] ∈ Borel sigma-algebra on

C([0, t], Rd), ∥ · ∥∞
o
.
For any pathγ ∈ C([0, T], Rd) and n ∈ N, we define the hitting timetn(γ) = inf {t ≥ 0 : |γt| > n}.
Claim: Thesehittingtimesarestoppingtimeswrttothecanonicalfiltration, i.e., {γ ∈ C([0, T], Rd) : tn(γ) ≤ t} ∈Bt
for anyt ∈ [0, T].
Proof of claim.Let A := {γ ∈ C([0, T], Rd) : tn(γ) ≤ t}. Then,
A|[0,t] = {γ ∈ C([0, t], Rd) : tn(γ) ≤ t}
=
n
γ ∈ C([0, t], Rd) : min{s ≥ 0 : |γs| > n} ≤t
o
(continuity ofγ)
=
n
γ ∈ C([0, t], Rd) : ∥γ∥∞ > n
o
,
which is clearly a Borel set in
 
C([0, t], Rd), ∥ · ∥∞

. ■
Thus we can define stopping time sigma-algebras in the usual way
BT∧tn =
n
A ∈ BT : A ∩ {γ ∈ C([0, T], Rd) : T ∧ tn(γ) ≤ t} ∈Bt, ∀t ∈ [0, T]
o
.
We showed above that, under P, ¯Pn, the distributions of x restricted to (C([0, T], Rd), BT∧tn) are P[0,T∧tn] :=
P[0,T]

BT∧tn
and ¯P[0,T∧tn] := ¯P[0,T]

BT∧tn
, respectively.
56
By inspection, we have, for anyt ≥ 0,
{T < τn} ∩ {T ∧ τn ≤ t} =



{T < τn} ∈FT ⊂ Ft if T ≤ t
∅ ⊂Ft if T > t.
Setting t = T ∧ τn, we have{T < τn} ∈FT∧τn, which also yields{T ≥ τn} ∈FT∧τn and {τn−1 ≤ T < τn} ∈ FT∧τn.
Fix i ≥ 0 and A ∈ BT∧ti. Thenx−1
• A ∈ F as x• is measurable. Thusx−1
• A∩{T < τi} ⊂FT∧τi and x−1
• A∩{τn−1 ≤
T < τn} ⊂FT∧τn for anyn > i. Finally,
EP
h
ZC
T 1A
i
= EP
h
ZT 1x−1
• A
i
= EP
h
Z(i)
T 1x−1
• A∩{T<τi}
i
+
+∞X
n=i+1
EP
h
Z(n)
T 1x−1
• A∩{τn−1≤T<τn}
i
= E ¯Pi
h
1x−1
• A∩{T<τi}
i
+
+∞X
n=i+1
E ¯Pn
h
1x−1
• A∩{τn−1≤T<τn}
i
= E¯P

1A∩{T<ti}

+
+∞X
n=i+1
E¯P
h
1A∩{tn−1≤T<tn}
i
= E¯P [1A] .
From the arbitrariness ofi and thatlimn→∞ τn = +∞ P-a.s., it follows that
d¯P[0,T]
dP[0,T]
= ZC
T .
Finally, we compute the relative entropy between the forward and backward path-space measures
H

P[0,T], ¯P[0,T]

= EP

log dP[0,T]
d¯P[0,T]
(γ)

= EP

log dP[0,T]
d¯P[0,T]
(x(ω))

= EP
h
−log ZC
T (x(ω))
i
= EP [−log ZT ] =
=0
z }| {
EP [MT ] +EP [YT ]
= EP

2
Z T
0
σ−birr(xs)
2
ds

= 2T
Z
Rd
σ−birr(x)
2
ρ(x)dx
= T
Z
Rd
b⊤
irrD−birrρ(x)dx,
where we used Tonelli’s theorem and stationarity for the penultimate equality. By Theorem 2.3.5, we obtain the
entropy production rate
ep = 1
T H

P[0,T], ¯P[0,T]

=
Z
Rd
b⊤
irrD−birrρ(x) dx.
We now prove Theorem 2.5.2:
Proof. By assumption 1, forρ-a.e. x ∈ Rd
birr(x) ∈ Range σ(x) ⇒ birr(x) = σσ−birr(x). (2.48)
57
Then, (2.48) implies that we can rewrite the SDE (2.5) as
dxt = brev(xt)dt + σ(xt)

σ−birr (xt) dt + dwt

, x 0 ∼ ρ.
By assumptions 2, 3, we may define a new probability measure¯P on the sample spaceΩ through the relation
d ¯P
dP

FT
= ZT , (2.49)
and it follows by Girsanov’s theorem [83, Theorem 10.14] applied to the process−2σ−birr (xt), that(xt)t∈[0,T] solves
the SDE
dxt = brev(xt)dt − σ(xt)

σ−birr (xt) dt + dwt

, x 0 ∼ ρ.
on the probability space(Ω, F, {Ft}t≥0 , ¯P). Using (2.48),(xt)t∈[0,T] solves
dxt = ¯b(xt)dt + σ(xt)dwt, x 0 ∼ ρ
on said probability space. By Theorem 2.4.5 we know that under¯P, (xt)t∈[0,T] induces the path space measure of
the time-reversed process.
Let
 
C([0, T], Rd), B

denote the path space, whereB is the Borel sigma-algebra generated by the sup norm∥ · ∥∞.
Denote trajectories of the process byx• := (xt)t∈[0,T] : Ω → C([0, T], Rd). In summary, we showed that, underP, ¯P,
the distribution ofx• on (C([0, T], Rd), B) are P[0,T] and ¯P[0,T], respectively.
By definition of Itô calculus,ZT is measurable with respect to⟨xs : 0 ≤ t ≤ T⟩, so there exists a positive measurable
function ZC
T on the path space, such thatP-a.s. ZC
T (x•(ω)) = ZT (ω) for ω ∈ Ω, i.e., the following diagram commutes
C([0, T], Rd)
Ω R>0.
ZC
Tx•
ZT
Fix A ∈ B. Then x−1
• A ∈ F as x• is measurable. Obviously,
EP
h
ZC
T 1A
i
= EP
h
ZT 1x−1
• A
i
= E ¯P
h
1x−1
• A
i
= E¯P [1A] .
It follows that
d¯P[0,T]
dP[0,T]
= ZC
T .
58
Through this, we obtain the relative entropy between the forward and backward path-space measures
H

P[0,T], ¯P[0,T]

= EP

log dP[0,T]
d¯P[0,T]
(γ)

= EP

log dP[0,T]
d¯P[0,T]
(x(ω))

= EP
h
−log ZC
T (x(ω))
i
= EP [−log ZT ]
= EP

2
Z T
0
⟨σ−birr(xt), dwt⟩

+ EP

2
Z T
0
|σ−birr(xt)|2dt

= 2T
Z
Rd
σ−birr(x)
2
ρ(x)dx
= T
Z
Rd
b⊤
irrD−birrρ(x)dx.
The penultimate equality follows from the fact that Itô stochastic integrals are martingales (and hence vanish in
expectation), Tonelli’s theorem and stationarity. Finally, by Theorem 2.3.5, we obtain the entropy production rate
ep = 1
T H

P[0,T], ¯P[0,T]

=
Z
Rd
b⊤
irrD−birrρ(x)dx.
We prove Proposition 2.5.3:
Proof. 1. Follows asZ0 = 1 and by definition of a martingale.
We define theFt-adapted processψt = −2σ−birr(xt).
2 ⇒ 1. This follows from [84, Theorem 1].
3. By[83,Proposition10.17(ii)], asufficientconditionfor(2.20)istheexistenceofa δ >0 suchthat supt∈[0,T] EP

eδ|ψt|2 
<
+∞. By stationarity ofxt at ρ, EP

eδ|ψt|2 
= Eρ

e4δ|σ−birr(x)|2 
, and so the result follows.
4 ⇒ 1. Follows from [84, Theorem 1].
5 ⇒ 2. At := 1
2
Rt
0 |ψs|2 ds = 2
Rt
0
σ−birr(xs)
2
ds isanon-decreasing, Ft-adaptedprocess. Byassumption, EP [AT − At | Ft] ≤
K for allt ∈ [0, T]. By [85, Theorem 105 (b)]EP [exp(AT )] < +∞.
6 ⇒ 3. Let δ ∈ (0, c). The first equality follows a standard fact about non-negative random variables:
Eρ
h
eδ|σ−birr(x)|2 i
=
Z ∞
0
P

eδ|σ−birr(x)|2
> r

dr
≤ 1 +
Z ∞
ecR
P

eδ|σ−birr(x)|2
> r

dr
= 1 +
Z ∞
ecR
P
 
|σ−birr(x)|2 > δ−1 log r

dr
≤ 1 + C
Z ∞
ecR
r−cδ−1
dr (by 6 asδ−1 log r > R)
< +∞.
59
Singularity
We prove Theorem 2.5.4:
Proof. Under the assumption thatbirr(x) ∈ Range σ(x) does not hold forρ-a.e. x ∈ Rd, we will show that there are
paths taken by the forward process that are not taken by the backward process—and vice-versa—resulting in the
mutual singularity of forward and backward path space measures.
Recall from Theorem 2.4.5 that any solution to the following SDE
d¯xt = ¯b(¯xt)dt + σ(¯xt)dwt, ¯x0 ∼ ρ, (2.50)
induces the path space measure of the time-reversed process.
We rewrite the forward and backward SDEs into their equivalent Stratonovich SDEs [3, eq. 3.31]
dxt = bs(xt)dt + σ(xt) ◦ dwt, x 0 ∼ ρ,
d¯xt = ¯bs(¯xt)dt + σ(¯xt) ◦ dwt, ¯x0 ∼ ρ,
By Remark 2.4.9, time-reversal and the transformation from Itô to Stratonovich commute so¯bs is unambiguous, and
birr = bs(x) − ¯bs(x). The volatility and Stratonovich drifts are locally Lipschitz asσ ∈ C2.
Consider an initial conditionx = x0 = ¯x0 to the trajectories, withρ(x) > 0. Consider trajectories in the Cameron-
Martin space
γ ∈ C:= {γ ∈ AC ([0, T]; Rm) | γ(0) = 0 and ˙γ ∈ L2 ([0, T]; Rm)}
Given such a trajectory, the approximating ODEs
dxt = bs(xt)dt + σ(xt)dγt, x 0 = x,
d¯xt = ¯bs(¯xt)dt + σ(¯xt)dγt, ¯x0 = x,
have a unique solution in[0, T], withT >0 uniform inγ.
We can thus apply the Stroock-Varadhan support theorem [86, Theorem 3.10] to state the possible paths under the
forward and backward protocols. These are as follows
supp Px
[0,T] =

xγ
t = x +
Z t
0
bs(xγ
s )ds +
Z t
0
σ(xγ
s )γ′(s)ds, t∈ [0, T] | γ ∈ C

,
supp ¯Px
[0,T] =

¯xγ
t = x +
Z t
0
¯bs(¯xγ
s )ds +
Z t
0
σ(¯xγ
s )γ′(s)ds, t∈ [0, T] | γ ∈ C

.
where the closure is with respect to the supremum norm onC
 
[0, T]; Rd
. The time derivatives of these paths at
t = 0 are
∂t supp Px
[0,T] |t=0 = {∂txγ
t |t=0 ∈ Rd | γ ∈ C}= {bs(x) + σ(x)v | v ∈ Rd},
∂t supp ¯Px
[0,T] |t=0 = {∂t¯xγ
t |t=0 ∈ Rd | γ ∈ C}= {¯bs(x) + σ(x)v | v ∈ Rd}.
where the closure is with respect to the sup norm onRd.
60
Consider an initial conditionx with birr(x) ̸∈ Range σ(x). This implies that the forward and backward path space
measures are mutually singular because the time derivatives of the possible paths differ at the origin
2birr(x) = bs(x) − ¯bs(x) ̸∈ Range σ(x)
⇐⇒bs(x) + Rangeσ(x) ̸= ¯bs(x) + Rangeσ(x)
⇐⇒∂t supp Px
[0,T] |t=0̸⊂ ∂t supp ¯Px
[0,T] |t=0 and vice-versa
⇒supp Px
[0,T] ̸⊂ supp ¯Px
[0,T] and vice-versa
⇒Px
[0,T] ⊥ ¯Px
[0,T]
Finally, from Proposition 2.3.8
ep = Ex∼ρ

H

Px
[0,T] | ¯Px
[0,T]

≥ ρ

{x ∈ Rd | Px
[0,T] ⊥ ¯Px
[0,T]}

· ∞
≥ ρ

{x ∈ Rd : birr(x) ̸∈ Range σ(x)}

| {z }
>0
·∞
= +∞.
2.8.4 Entropy production rate of the linear diffusion process
We require the following Lemma, which can be proved by adjusting the derivation of the relative entropy between
non-degenerate Gaussian distributions, cf. [87, Section 9].
Lemma 2.8.1. On Rd
2 H[N(µ0, Σ0) | N(µ1, Σ1)] = tr
 
Σ−
1 Σ0

− rank Σ0 + log
det∗ Σ1
det∗ Σ0

+ (µ1 − µ0)⊤ Σ−
1 (µ1 − µ0) .
where ·− is the Moore-Penrose pseudo-inverse anddet∗ is the pseudo-determinant.
Proof of Lemma 2.6.1.We insert the definitions of the transition kernels (2.25) into Lemma 2.8.1.
Ex∼ρ[2 H[pε(·, x) | ¯pε(·, x)]]
= Tr(¯S−
ε Sε) − rank σ + log det∗( ¯Sε)
det∗(Sε)
+ Ex∼ρ
h
x⊤(e−εC − e−εB)⊤ ¯S−
ε (e−εC − e−εB)x
i
= Tr(¯S−
ε Sε) − rank σ + log det∗( ¯Sε)
det∗(Sε)
+ Tr(Π−1(e−εC − e−εB)⊤ ¯S−
ε (e−εC − e−εB))
To obtain the last line, we used the trace trick for computing Gaussian integrals of bilinear forms. The proof follows
by Proposition 2.3.10.
61
Chapter 3
Bayesian mechanics
Bayesian mechanics for stationary processes
By Lancelot Da Costa, Karl Friston, Conor Heins, Grigorios A. Pavliotis
Adapted from: L Da Costa, K Friston, C Heins, GA Pavliotis. Bayesian mechanics for stationary
processes. Proceedings of the Royal Society A. 2021.
62
3.1 Abstract
This chapter develops a Bayesian mechanics for adaptive systems.
Firstly, we model the interface between a system and its environment with a Markov blanket. This affords conditions
under which states internal to the blanket encode information about external states.
Second, we introduce dynamics and represent adaptive systems as Markov blankets at steady-state. This allows us
to identify a wide class of systems whose internal states appear to infer external states, consistent with variational
inference in Bayesian statistics and theoretical neuroscience.
Finally, we partition the blanket into sensory and active states. It follows that active states can be seen as performing
active inference and well-known forms of stochastic control (such as PID control), which are prominent formulations
of adaptive behaviour in theoretical biology and engineering.
Keywords: Markovblanket, variationalBayesianinference, activeinference, non-equilibriumsteady-state, predictive
processing, free-energy principle
3.2 Introduction
Any object of study must be, implicitly or explicitly, separated from its environment. This implies a boundary that
separates it from its surroundings, and which persists for at least as long as the system exists.
In this article, we explore the consequences of a boundary mediating interactions between states internal and external
to a system. This provides a useful metaphor to think about biological systems, which comprise spatially bounded,
interacting components, nested at several spatial scales [88,89]: for example, the membrane of a cell acts as a boundary
through which the cell communicates with its environment, and the same can be said of the sensory receptors and
muscles that bound the nervous system.
By examining the dynamics of persistent, bounded systems, we identify a wide class of systems wherein the states
internal to a boundary appear to infer those states outside the boundary—a description which we refer to as Bayesian
mechanics. Moreover, if we assume that the boundary comprises sensory and active states, we can identify the
dynamics of active states with well-known descriptions of adaptive behaviour from theoretical biology and stochastic
control.
In what follows, we link a purely mathematical formulation of interfaces and dynamics with descriptions of belief
updating and behaviour found in the biological sciences and engineering. Altogether, this can be seen as a model of
adaptive agents, as these interface with their environment through sensory and active states and furthermore behave
so as to preserve a target steady-state.
3.2.1 Outline of chapter
This chapter has three parts, each of which introduces a simple, but fundamental, move.
1. The first is to partition the world into internal and external states whose boundary is modelled with a Markov
blanket [90,91]. This allows us to identify conditions under which internal states encode information about
external states.
63
2. Thesecondmoveistoequipthis partitionwithstochasticdynamics. Thekeyconsequenceofthisisthatinternal
states can be seen as continuously inferring external states, consistent with variational inference in Bayesian
statistics and with predictive processing accounts of biological neural networks in theoretical neuroscience.
3. The third move is to partition the boundary into sensory and active states. It follows that active states can
be seen as performing active inference and stochastic control, which are prominent descriptions of adaptive
behaviour in biological agents, machine learning and robotics.
3.2.2 Related work
Theemergenceandsustainingofcomplex(dissipative)structureshavebeensubjectsoflong-standingresearchstarting
from the work of Prigogine [92,93], followed notably by Haken’s synergetics [94], and in recent years, the statistical
physics of adaptation [95]. A central theme of these works is that complex systems can only emerge and sustain
themselves far from equilibrium [96–98].
Information processing has long been recognised as a hallmark of cognition in biological systems. In light of this,
theoretical physicists have identified basic instances of information processing in systems far from equilibrium using
tools from information theory, such as how a drive for metabolic efficiency can lead a system to become predictive
[99–102].
A fundamental aspect of biological systems is a self-organisation of various interacting components at several spatial
scales [88,89]. Much research currently focuses on multipartite processes—modelling interactions between various
sub-components that form biological systems—and how their interactions constrain the thermodynamics of the whole
[6,103–106].
At the confluence of these efforts, researchers have sought to explain cognition in biological systems. Since the advent
of the 20th century, Bayesian inference has been used to describe various cognitive processes in the brain [1,107–110].
In particular, the free energy principle [1], a prominent theory of self-organisation from the neurosciences, postulates
that Bayesian inference can be used to describe the dynamics of multipartite, persistent systems modelled as Markov
blankets at non-equilibrium steady-state [8,9,111–113].
This chapter connects and develops some of the key themes from this literature. Starting from fundamental con-
siderations about adaptive systems, we develop a physics of things that hold beliefs about other things–consistently
with Bayesian inference–and explore how it relates to known descriptions of action and behaviour from the neuro-
sciences and engineering. Our contribution is theoretical: from a biophysicist’s perspective, this chapter describes how
Bayesian descriptions of biological cognition and behaviour can emerge from standard accounts of physics. From an
engineer’s perspective this chapter contextualises some of the most common stochastic control methods and reminds
us how these can be extended to suit more sophisticated control problems.
3.2.3 Notation
Let Π ∈ Rd×d be a square matrix with real coefficients. Letη, b, µdenote a partition of the states[ [1, d] ], so that
Π =


Πη Πηb Πηµ
Πbη Πb Πbµ
Πµη Πµb Πµ

.
We denote principal submatrices with one index only (i.e., we useΠη instead ofΠηη). Similarly, principal submatrices
involving various indices are denoted with a colon
64
Πη:b :=
"
Πη Πηb
Πbη Πb
#
.
When a square matrixΠ is symmetric positive-definite we writeΠ ≻ 0. ker, Im and ·− respectively denote the kernel,
image and Moore-Penrose pseudo-inverse of a linear map or matrix, e.g., a non-necessarily square matrix such as
Πµb. In our notation, indexing takes precedence over (pseudo) inversion, for example,
Π−
µb := (Πµb)− ̸= (Π−)µb.
3.3 Markov blankets
The section formalises the notion of boundary between a system and its environment as a Markov blanket [90,91],
depicted graphically in Figure 3.1. Intuitive examples of a Markov blanket are that of a cell membrane, mediating
all interactions between the inside and the outside of the cell, or that of sensory receptors and muscles that bound
the nervous system.
Figure 3.1: Markov blanketdepicted graphically as an undirected graphical model, also known as a Markov random field
[91,114]. (A Markov random field is a Bayesian network whose directed arrows are replaced by undirected arrows). The circles
represent random variables. The lines represent conditional dependencies between random variables. The Markov blanket
condition means that there is no line betweenµ and η. This means that, µ and η are conditionally independent givenb. In
other words, knowing the internal stateµ, does not afford additional information about the external stateη when the blanket
state b is known. Thus blanket states act as an informational boundary between internal and external states.
To formalise this intuition, we model the world’s state as a random variablex with corresponding probability distri-
bution p over a state-spaceX = Rd. We partition the state-space ofx into external, blanket and internal states:
x = (η, b, µ)
X = E × B × I.
External, blanket and internal state-spaces (E, B, I) are taken to be Euclidean spaces for simplicity.
A Markov blanket is a statement of conditional independence between internal and external states given blanket
states.
Definition 3.3.1(Markov blanket). A Markov blanket is defined as
η ⊥ µ | b (M.B.)
That is, blanket states are a Markov blanket separatingµ, η[90,91].
65
The existence of a Markov blanket can be expressed in several equivalent ways
(M.B.) ⇐⇒ p(η, µ|b) = p(η|b)p(µ|b) ⇐⇒ p(η|b, µ) = p(η|b) ⇐⇒ p(µ|b, η) = p(µ|b). (3.1)
For now, we will consider a (non-degenerate) Gaussian distributionp encoding the distribution of states of the world
p(x) := N(x; 0, Π−1), Π ≻ 0,
with associated precision (i.e., inverse covariance) matrixΠ. Throughout, we will denote the (positive definite)
covariance byΣ := Π−1. Unpacking (3.1) in terms of Gaussian densities, we find that a Markov blanket is equivalent
to a sparsity in the precision matrix
(M.B.) ⇐⇒ Πηµ = Πµη = 0. (3.2)
Example 3.3.2. For example,
Π =


2 1 0
1 2 1
0 1 2

 ⇒ Σ−1
η:b =
"
2 1
1 1 .5
#
, Σ−1
b:µ =
"
1.5 1
1 2
#
Then,
p(η, µ|b) ∝ p(η, µ, b) ∝ exp

−1
2x · Πx

∝ exp
 
−1
2
h
η, b
i
Σ−1
η:b
"
η
b
#
− 1
2
h
b, µ
i
Σ−1
b:µ
"
b
µ
#!
∝ p(η, b)p(b, µ) ∝ p(η|b)p(µ|b).
Thus, the Markov blanket condition (3.1) holds.
3.3.1 Expected internal and external states
Blanket states act as an information boundary between external and internal states. Given a blanket state, we can
express the conditional probability densities over external and internal states (using (3.1) and [115, Prop. 3.13])1
p(η|b) = N(η; ΣηbΣ−1
b b, Π−1
η ),
p(µ|b) = N(µ; ΣµbΣ−1
b b, Π−1
µ ).
(3.3)
This enables us to associate to any blanket state its corresponding expected external and expected internal states:
η(b) = E[η | b] = Ep(η|b)[η] = ΣηbΣ−1
b b ∈ E
µ(b) = E[µ | b] = Ep(µ|b)[µ] = ΣµbΣ−1
b b ∈ I.
1Note thatΠη, Πµ are invertible as principal submatrices of a positive definite matrix.
66
Pursuing the example of the nervous system, each sensory impression on the retina and oculomotor orientation
(blanket state) is associated with an expected scene that caused sensory input (expected external state) and an
expected pattern of neural activity in the visual cortex (expected internal state) [116].
3.3.2 Synchronisation map
A central question is whether and how expected internal states encode information about expected external states.
For this, we need to characterise a synchronisation functionσ, mapping the expected internal state to the expected
external state, given a blanket stateσ(µ(b)) = η(b). This is summarised in the following commutative diagram:
b ∈ B
Image(η) Image(µ)
η µ
σ
The existence ofσ is guaranteed, for instance, if the expected internal state completely determines the blanket state—
that is, when no information is lost in the mappingb 7→ µ(b) in virtue of it being one-to-one. In general, however,
many blanket states may correspond to an unique expected internal state. Intuitively, consider the various neural
pathways that compress the signal arriving from retinal photoreceptors [117], thus many different (hopefully similar)
retinal impressions lead to the same signal arriving in the visual cortex.
Existence
The key for the existence of a functionσ mapping expected internal states to expected external states given blanket
states, is that for any two blanket states associated with the same expected internal state, these be associated with
the same expected external state. This non-degeneracy means that the internal states (e.g., patterns of activity in
the visual cortex) have enough capacity to represent all possible expected external states (e.g., 3D scenes of the
environment). We formalise this in the following Lemma:
Lemma 3.3.3. The following are equivalent:
1. There exists a functionσ : Image(µ) → Image(η) such that for any blanket stateb ∈ B
σ(µ(b)) = η(b).
2. For any two blanket statesb1, b2 ∈ B
µ(b1) = µ(b2) ⇒ η(b1) = η(b2).
3. ker Σµb ⊂ ker Σηb.
4. ker Πµb ⊂ ker Πηb.
See Section 3.8.1 for a proof of Lemma 3.3.3.
Example 3.3.4. • When external, blanket and internal states are one-dimensional, the existence of a synchro-
nisation map is equivalent toΠµb ̸= 0 or Πµb = Πηb = 0.
67
• If Πµb is chosen at random–its entries sampled from a non-degenerate Gaussian or uniform distribution–then
Πµb has full rank with probability1. If furthermore the blanket state-spaceB has lower or equal dimensionality
than the internal state-spaceI, we obtain thatΠµb is one-to-one (i.e.,ker Πµb = 0) with probability1. Thus,
in this case, the conditions of Lemma 3.3.3 are fulfilled with probability1.
Construction
The key idea to map an expected internal stateµ(b) to an expected external stateη(b) is to: 1) find a blanket state
that maps to this expected internal state (i.e., by invertingµ) and 2) from this blanket state, find the corresponding
expected external state (i.e., by applyingη):
b ∈ B
Image(η) Image(µ)
η µ
σ=η◦µ−
µ−
We now proceed to solving this problem. Given an internal stateµ, we study the set of blanket statesb such that
µ(b) = µ
µ(b) = ΣµbΣ−1
b b = µ ⇐⇒ b ∈ µ−1(µ) = ΣbΣ−1
µb µ. (3.4)
Here the inverse on the right hand side of (3.4) is understood as the preimage of a linear map. We know that this
system of linear equations has a vector space of solutions given by [118]
µ−1(µ) =

ΣbΣ−
µbµ +
 
Id −ΣbΣ−
µbΣµbΣ−1
b

b : b ∈ B
	
. (3.5)
Among these, we choose
µ−(µ) = ΣbΣ−
µbµ.
Definition 3.3.5 (Synchronisation map). We define a synchronisation function that maps to an internal state a
corresponding most likely internal state23
σ : Im µ → Im η
µ 7→ η(µ−(µ)) = ΣηbΣ−
µbµ = Π−1
η ΠηbΠ−
µbΠµµ.
The expression in terms of the precision matrix is a byproduct of Section 3.8.1.
Note that we can always define suchσ, however, it is only when the conditions of Lemma 3.3.3 are fulfilled thatσ
maps expected internal states to expected external statesσ(µ(b)) = η(b). When this is not the case, the internal
states do not fully represent external states, which leads to a partly degenerate type of representation, see Figure
3.2 for a numerical illustration obtained by sampling from a Gaussian distribution, in the non-degenerate (left) and
degenerate cases (right), respectively.
2This mapping was derived independently of our work in [119, Section 3.2].
3Replacing µ−(µ) by any other element of (3.5) would lead to the same synchronisation map provided that the conditions
of Lemma 3.3.3 are satisfied.
68
Figure 3.2: Synchronisation map: example and non-example.This figure plots expected external states given blanket
states η(b) (in orange), and the corresponding prediction encoded by internal statesσ(µ(b)) (in blue). In this example, external,
blanket and internal state-spaces are taken to be one dimensional. We show the correspondence under the conditions of Lemma
3.3.3 (left panel) and when these are not satisfied (right panel). To generate these data, 1) we drew106 samples from a Gaussian
distribution with a Markov blanket, 2) we partitioned the blanket state-space into several bins, 3) we obtained the expected
external and internal states given blanket states empirically by averaging samples from each bin, and finally, 4) we applied the
synchronisation map to the (empirical) expected internal states given blanket states.
3.4 Bayesian mechanics
In order to study the time-evolution of systems with a Markov blanket, we introduce dynamics into the external,
blanket and internal states. Henceforth, we assume a synchronisation map under the conditions of Lemma 3.3.3.
3.4.1 Processes at a Gaussian steady-state
We consider stochastic processes at a Gaussian steady-statep with a Markov blanket. The steady-state assumption
means that the system’s overall configuration persists over time (e.g., it does not dissipate). In other words, we have
a Gaussian densityp = N(0, Π−1) with a Markov blanket (3.2) and a stochastic process distributed according top
at every point in time
xt ∼ p = N(0, Π−1) for anyt.
Recalling our partition into external, blanket and internal states, this affords a Markov blanket that persists over
time, see Figure 3.3
xt = (ηt, bt, µt) ∼ p ⇒ ηt ⊥ µt | bt. (3.6)
Note that we do not requirext to be independent samples from the steady-state distributionp. On the contrary,xt
may be generated by extremely complex, non-linear, and possibly stochastic equations of motion. See Example 3.4.1
and Figure 3.4 for details.
Example 3.4.1.The dynamics ofxt are described by a stochastic process at a Gaussian steady-statep = N(0, Π−1).
There is a large class of such processes, for example:
• Stationary diffusion processes, with initial conditionx0 ∼ p. Their time-evolution is given by an Itô stochastic
69
Figure 3.3: Markov blanket evolving in time.We use a bacillus to depict an intuitive example of a Markov blanket that
persists over time. Here, the blanket states represent the membrane and actin filaments of the cytoskeleton, which mediate all
interactions between internal states and the external medium (external states).
Figure 3.4:Processes at a Gaussian steady-state. This figure illustrates the synchronisation map and transition probabil-
ities of processes at a Gaussian steady-state.Left: We plot the synchronisation map as in Figure 3.2, only, here, the samples
are drawn from trajectories of a diffusion process (3.7) with a Markov blanket. Although this is not the case here, one might
obtain a slightly noisier correspondence between predictionsσ(µ(bt)) and expected external statesη(bt)—compared to Figure
3.2—in numerical discretisations of a diffusion process. This is because the steady-state of a numerical discretisation usually
differs slightly from the steady-state of the continuous-time process [70].Right: This panel plots the transition probabilities of
the same diffusion process (3.7), for the blanket state at two different times. The joint distribution (depicted as a heat map) is
not Gaussian but its marginals—the steady-state density—are Gaussian. This shows that in general, processes at a Gaussian
steady-state are not Gaussian processes. In fact, the Ornstein-Uhlenbeck process is the only stationary diffusion process (3.7)
that is a Gaussian process, so the transition probabilities of non-linear diffusion processes (3.7) are never multivariate Gaussians.
70
differential equation (see Section 2.4.4):
dxt = (Γ + Q)(xt)∇log p(xt)dt + ∇ ·(Γ + Q)(xt)dt + ς(xt)dWt,
= −(Γ + Q)(xt)Πxtdt + ∇ ·(Γ + Q)(xt)dt + ς(xt)dWt
Γ := ςς ⊤/2, Q = −Q⊤.
(3.7)
Here, Wt is a standard Brownian motion (a.k.a., Wiener process) [3,120] andς, Γ, Qare sufficiently well-behaved
matrix fields (see Section 2.4.4). Namely,Γ is the diffusion tensor (half the covariance of random fluctuations),
which drives dissipative flow; Q is an arbitrary antisymmetric matrix field which drives conservative (i.e.,
solenoidal) flow. We emphasise that there are no non-degeneracy conditions on the matrix fieldς—in particular,
the process is allowed to be non-ergodic or even completely deterministic (i.e.,ς ≡ 0). Also, ∇· denotes the
divergence of a matrix field defined as(∇ ·(Γ + Q))i := P
j
∂
∂xj
(Γ + Q)ij.
• More generally,xt could be generated by any Markov process at steady-statep, such as the zig-zag process or
the bouncy particle sampler [121–123], by any mean-zero Gaussian process at steady-statep [124] or by any
random dynamical system at steady-statep [125].
Remark 3.4.2. When the dynamics are given by an Itô stochastic differential equation (3.7), a Markov blanket of the
steady-state density (3.2) does not preclude reciprocal influences between internal and external states [126,127]. For
example,
Π =


2 1 0
1 2 1
0 1 2

, Q ≡


0 0 1
0 0 0
−1 0 0

, ς ≡ Id3
⇒ d


ηt
bt
µt

 = −


1 1 .5 2
0.5 1 0 .5
−2 −0.5 1




ηt
bt
µt

dt + ςdWt.
Conversely, the absence of reciprocal coupling between two states in the drift in some instances, though not always,
leads to conditional independence [9,119,126].
3.4.2 Maximum a posteriori estimation
The Markov blanket (3.6) allows us to harness the construction of Section 3.3 to determine expected external and
internal states given blanket states
ηt := η(bt) µt := µ(bt).
Note thatη, µ are linear functions of blanket states; sincebt generally exhibits rough sample paths,ηt, µt will also
exhibit very rough sample paths.
We can view the steady-state densityp as specifying the relationship between external states (η, causes) and particular
states (b, µ, consequences). In statistics, this corresponds to a generative model, a probabilistic specification of how
(external) causes generate (particular) consequences.
By construction, the expected internal states encode expected external states via the synchronisation map
σ(µt) = ηt,
which manifests a form of generalised synchrony across the Markov blanket [128–130]. Moreover, the expected internal
71
state µt effectively follows the most likely cause of its sensations
σ(µt) = arg maxp(ηt | bt) for anyt.
This has an interesting statistical interpretation as expected internal states perform maximum a posteriori (MAP)
inference over external states.
3.4.3 Predictive processing
We can go further and associate to each internal stateµ a probability distribution over external states, such that
each internal state encodes beliefs about external states
qµ(η) := N(η; σ(µ), Π−1
η ). (3.8)
We will callqµ the approximate posterior belief associated with the internal stateµ due to the forecoming connection
to inference. Under this specification, the mean of the approximate posterior depends upon the internal state, while
its covariance equals that of the true posterior w.r.t. external states (3.3). It follows that the approximate posterior
equals the true posterior when the internal stateµ equals the expected internal stateµ(b) (given blanket states):
qµ(η) = p(η|b) ⇐⇒ µ = µ(b). (3.9)
Note a potential connection with epistemic accounts of quantum mechanics; namely, a world governed by classical
mechanics (σ ≡ 0 in (3.7)) in which each agent encodes Gaussian beliefs about external states could appear to the
agents as reproducing many features of quantum mechanics [131].
Under this specification (3.9), expected internal states are the unique minimiser of a Kullback-Leibler divergence [132]
µt = arg min
µ
DKL[qµ(η)∥p(η|b)]
that measures the discrepancy between beliefs about the external worldqµ(η) and the posterior distribution over
external variables. Computing the KL divergence (see Section 3.8.2), we obtain
µt = arg min
µ
(σ(µ) − ηt)Πη(σ(µ) − ηt) (3.10)
In the neurosciences, the right hand side of (3.10) is commonly known as a (squared) precision-weighted prediction
error: the discrepancy between the prediction and the (expected) state of the environment is weighted with a precision
matrix [109,133,134] that derives from the steady-state density. This equation is formally similar to that found in
predictivecodingformulationsofbiologicalfunction[109,135–137], whichstipulatethatorganismsminimiseprediction
errors, and in doing so optimise their beliefs to match the distribution of external states.
3.4.4 Variational Bayesian inference
We can go further and associate expected internal states to the solution to the classical variational inference problem
from statistical machine learning [138] and theoretical neurobiology [133,139]. Expected internal states are the unique
72
Figure 3.5: Variational inference and predictive processing, averaging internal variables for any blanket state.
This figure illustrates a system’s behaviour after experiencing a surprising blanket state, averaging internal variables for any
blanket state. This is a multidimensional Ornstein-Uhlenbeck process, with two external, blanket and internal variables,
initialised at the steady-state density conditioned upon an improbable blanket statep(x0|b0). Upper left: we plot a sample
trajectory of the blanket states as these relax to steady-state over a contour plot of the free energy (up to a constant).Upper
right: this plots the free energy (up to a constant) over time, averaged over multiple trajectories. In this example, the rare
fluctuations that climb the free energy landscape vanish on average, so that the average free energy decreases monotonically.
This need not always be the case: conservative systems (i.e.,ς ≡ 0 in (3.7)) are deterministic flows along the contours of the
steady-state density (see Section 2.4.4). Since these contours do not generally coincide with those ofF(b, µ) it follows that the
free energy oscillates between its maximum and minimum value over the system’s periodic trajectory. Luckily, conservative
systems are not representative of dissipative, living systems. Yet, it follows that the average free energy of expected internal
variables may increase, albeit only momentarily, in dissipative systems (3.7) whose solenoidal flow dominates dissipative flow.
Lower left: we illustrate the accuracy of predictions over external states of the sample path from the upper left panel. At
steady-state (from timestep∼ 100), the predictions become accurate. The prediction of the second component is offset by four
units for greater visibility, as can be seen from the longtime behaviour converging to four instead of zero.Lower right: We
show the evolution of precision-weighted prediction errorsξt := Πη(ηt −σ(µt)) over time. These are normally distributed with
zero mean at steady-state.
73
Figure 3.6:Variational inference and predictive processing. This figure illustrates a system’s behaviour after experiencing
a surprising blanket state. This is a multidimensional Ornstein-Uhlenbeck process, with one external, blanket and internal
variable, initialised at the steady-state density conditioned upon an improbable blanket statep(x0|b0). Upper left: this plots a
sample trajectory of particular states as these relax to steady-state over a contour plot of the free energy. The white line shows
the expected internal state given blanket states, at which point inference is exact. After starting close to this line, the process
is driven by solenoidal flow to regions where inference is inaccurate. Yet, solenoidal flow makes the system converge faster to
steady-state [32,33] at which point inference becomes accurate again.Upper right:this plots the free energy (up to a constant)
over time, averaged over multiple trajectories.Lower left: we illustrate the accuracy of predictions over external states of the
sample path from the upper left panel. These predictions are accurate at steady-state (from timestep∼ 100). Lower right:
we illustrate the (precision weighted) prediction errors over time. In orange we plot the prediction error corresponding to the
sample path in the upper left panel; the other sample paths are summarised as a heat map in blue.
74
minimiser of a free energy functional (i.e., an evidence bound [138,140])
F(bt, µt) ≥ F(bt, µt)
F(b, µ) = DKL[qµ(η)∥p(η|b)] − log p(b, µ)
= Eqµ(η)[−log p(x)]
| {z }
Energy
− H[qµ]| {z }
Entropy
.
(3.11)
The last line expresses the free energy as a difference between energy and entropy: energy or accuracy measures to
what extent predicted external states are close to the true external states, while entropy penalises beliefs that are
overly precise.
At first sight, variational inference and predictive processing are solely useful to characterise the average internal state
given blanket states at steady-state. It is then surprising to see that the free energy says a great deal about a system’s
expected trajectories as it relaxes to steady-state. Figure 3.5 and 3.6 illustrate the time-evolution of the free energy
and prediction errors after exposure to a surprising stimulus. In particular, Figure 3.5 averages internal variables
for any blanket state: In the neurosciences, perhaps the closest analogy is the event-triggered averaging protocol,
where neurophysiological responses are averaged following a fixed perturbation, such a predictable neural input or an
experimentally-controlled sensory stimulus (e.g., spike-triggered averaging, event-related potentials) [141–143].
The most striking observation is the nearly monotonic decrease of the free energy as the system relaxes to steady-state.
This simply follows from the fact that regions of high density under the steady-state distribution have a low free
energy. Thisoverall decreasein free energy is the essence of the free-energy principle, which describes self-organisation
at non-equilibrium steady-state [1,112,113]. Note that the free energy, even after averaging internal variables, may
decrease non-monotonically. See the explanation in Figure 3.5.
3.5 Active inference and stochastic control
In order to model agents that interact with their environment, we now partition blanket states into sensory and active
states
bt = (st, at)
xt = (ηt, st, at, µt).
Intuitively, sensory states are the sensory receptors of the system (e.g., olfactory or visual receptors) while active
states correspond to actuators through which the system influences the environment (e.g., muscles). See Figure 3.7.
The goal of this section is to explain how autonomous states (i.e., active and internal states) respond adaptively to
sensory perturbations in order to maintain the steady-state, which we interpret as the agent’s preferences or goal.
This allows us to relate the dynamics of autonomous states to active inference and stochastic control, which are
well-known formulations of adaptive behaviour in theoretical biology and engineering.
75
Figure 3.7:Markov blanket evolving in time comprising sensory and active states.We continue the intuitive example
from Figure 3.3 of the bacillus as representing a Markov blanket that persists over time. The only difference is that we partition
blanket states into sensory and active states. In this example, the sensory states can be seen as the bacillus’ membrane, while
the active states correspond to the actin filaments of the cytoskeleton.
3.5.1 Active inference
We now proceed to characterise autonomous states, given sensory states, using the free energy. Unpacking blanket
states, the free energy (3.11) reads
F(s, a, µ) = DKL[qµ(η)∥p(η|s, a)] − log p(µ|s, a) − log p(a|s) − log p(s).
Crucially, it follows that the expected autonomous states minimise free energy
F(st, at, µt) ≥ F(st, at, µt),
at := a(st) := Ep(at|st)[at] = ΣasΣ−1
s st,
where at denotes the expected active states given sensory states, which is the mean ofp(at|st). This result forms
the basis of active inference, a well-known framework to describe and generate adaptive behaviour in neuroscience,
machine learning and robotics [110,139,144–151]. See Figure 3.8.
3.5.2 Multivariate control
Active inference is used in various domains to simulate control [144,148,150–155], thus, it is natural that we can
relate the dynamics of active states to well-known forms of stochastic control.
By computing the free energy explicitly (see Section 3.8.2), we obtain that
(at, µt) minimises (a, µ) 7→
h
st, a, µ
i
K


st
a
µ

 (3.12)
K := Σ−1
b:µ
where we denoted byK the concentration (i.e., precision) matrix ofp(s, a, µ). We may interpret(a, µ) as controlling
how far particular states[s, a, µ] are from their target set-point of[0, 0, 0], where the error is weighted by the precision
matrix K. See Figure 3.9. (Note that we could choose any other set-point by translating the frame of reference or
76
Figure 3.8: Active inference. This figure illustrates a system’s behaviour after experiencing a surprising sensory state,
averaging internal variables for any blanket state. We simulated an Ornstein-Uhlenbeck process with two external, one sensory,
one active and two internal variables, initialised at the steady-state density conditioned upon an improbable sensory state
p(x0|s0). Left: The white line shows the expected active state given sensory states: this is the action that performs active
inference and optimal stochastic control. As the process experiences a surprising sensory state, it initially relaxes to steady-
state in a winding manner due to the presence of solenoidal flow. Even though solenoidal flow drives the actions away from the
optimal action initially, it allows the process to converge faster to steady-state [32,33,60] where the actions are again close to
the optimal action from optimal control.Right: We plot the free energy of the expected internal state, averaged over multiple
trajectories. In this example, the average free energy does not decrease monotonically—see Figure 3.5 for an explanation.
equivalently choosing a Gaussian steady-state centred away from zero). In other words, there is a cost associated
to how far aways, a, µare from the origin and this cost is weighed by the precision matrix, which derives from
the stationary covariance of the steady-state. In summary, the expected internal and active states can be seen as
performing multivariate stochastic control, where the matrixK encodes control gains. From a biologist’s perspective,
this corresponds to a simple instance of homeostatic regulation: maintaining physiological variables within their
preferred range.
3.5.3 Stochastic control in an extended state-space
More sophisticated control methods, such as PID (Proportional-Integral-Derivative) control [155,158], involve control-
ling a process and its higher orders of motion (e.g., integral or derivative terms). So how can we relate the dynamics
of autonomous states to these more sophisticated control methods? The basic idea involves extending the sensory
state-space to replace the sensory processst by its various orders of motion˜st =

s(0)
t , . . . , s(n)
t

(integral, position,
velocity, jerk etc, up to ordern). To find these orders of motion, one must solve the stochastic realisation problem.
The stochastic realisation problem
Recall that the sensory processst is a stationary stochastic process (with a Gaussian steady-state). The following is
a central problem in stochastic systems theory: Given a stationary stochastic processst, find a Markov process˜st,
called the state process, and a functionf such that
st = f(˜st) for all t. (3.13)
Moreover, find an Itô stochastic differential equation whose unique solution is the state process˜st. The problem of
characterising the family of all such representations is known as the stochastic realisation problem [159].
77
Figure 3.9:Stochastic control. This figure plots a sample path of the system’s particular states after it experiences a surprising
sensory state. This is the same sample path as shown in Figure 3.8 (left panel), however, here the link with stochastic control
is easier to see. Indeed, it looks as if active states (in red) are actively compensating for sensory states (in green): rises in the
active state-space lead to plunges in the sensory state-space and vice-versa. Notice the initial rise in active states to compensate
for the perturbation in the sensory states. Active states follow a similar trajectory as sensory states, with a slight delay, which
can be interpreted as a reaction time [156]. In fact, the correspondence between sensory and active states is a consequence of
the solenoidal flow–see Figure 3.8 (left panel). The damped oscillations as the particular states approach their target value of
0 (in grey) is analogous to that found in basic implementations of stochastic control, e.g., [157, Figure 4.9].
What kind of processesst can be expressed as a function of a Markov process (3.13)?
There is a rather comprehensive theory of stochastic realisation for the case wherest is a Gaussian process (which
occurs, for example, when xt is a Gaussian process). This theory expresses st as a linear map of an Ornstein-
Uhlenbeck process [3,160,161]. The idea is as follows: as a mean-zero Gaussian process,st is completely determined
by its autocovariance function C(t − r) = E[st ⊗ sr], which by stationarity only depends on |t − r|. It is well
known that any mean-zero stationary Gaussian process with exponentially decaying autocovariance function is an
Ornstein-Uhlenbeck process (a result sometimes known as Doob’s theorem) [3,69,162,163]. Thus ifC equals a finite
sum of exponentially decaying functions, we can expressst as a linear function of several nested Ornstein-Uhlenbeck
processes, i.e., as an integrator chain from control theory [164,165]
st = f(s(0)
t )
ds(0)
t = f0(s(0)
t , s(1)
t )dt + ς0dW(0)
t
ds(1)
t = f1(s(1)
t , s(2)
t )dt + ς1dW(1)
t
...
...
...
ds(n−1)
t = fn−1(s(n−1)
t , s(n)
t )dt + ςn−1dW(n−1)
t
ds(n)
t = fn(s(n)
t )dt + ςndW(n)
t .
(3.14)
In this example,f, fi are suitably chosen linear functions,ςi are matrices andW(i) are standard Brownian motions.
Thus, we can seest as the output of a continuous-time hidden Markov model, whose (hidden) statess(i)
t encode its
various orders of motion: position, velocity, jerk etc. These are known as generalised coordinates of motion in the
Bayesian filtering literature [14,166,167]. See Figure 3.10.
More generally, the state process ˜st and the function f need not be linear, which enables to realise non-linear,
non-Gaussian processes st [166,168,169]. Technically, this follows as Ornstein-Uhlenbeck processes are the only
78
Figure 3.10: Continuous-time Hidden Markov model. This figure depicts (3.14) in a graphical format, as a Bayesian
network [90,114]. The encircled variables are random variables—the processes indexed at an arbitrary sequence of subsequent
times t1 < t2 < . . . < t9. The arrows represent relationships of causality. In this hidden Markov model, the (hidden) state
process ˜st is given by an integrator chain—i.e., nested stochastic differential equationss(0)
t , s(1)
t , . . . , s(n)
t . These processes
s(i)
t , i≥ 0, can respectively be seen as encoding the position, velocity, jerk etc, of the processst.
stationary Gaussian Markov processes. Note that stochastic realisation theory is not as well developed in this general
case [159,166,169–171].
Stochastic control of integrator chains
Henceforth, we assume that we can expressst as a function of a Markov process˜st (3.13). Inserting (3.13) into (3.12),
we now see that the expected autonomous states minimise how far themselves andf(˜st) are from their target value
of zero
(at, µt) minimises (a, µ) 7→
h
f(˜st), a, µ
i
K


f(˜st)
a
µ

. (3.15)
Furthermore, if the state process˜st can be expressed as an integrator chain, as in (3.14), then we can interpret
expected active and internal states as controlling each order of motions(i)
t . For example, iff is linear, these processes
control each order of motions(i)
t towards its target value of zero.
PID-like control
Proportional-integral-derivative (PID) control is a well-known control method in engineering [155,158]. More than
90% of controllers in engineered systems implement either PID or PI (no derivative) control. The goal of PID control
is to control a signals(1)
t , its integrals(0)
t , and its derivatives(2)
t close to a pre-specified target value [155].
This turns out to be exactly what happens here when we consider the stochastic control of an integrator chain (3.15)
with three orders of motion(n = 2). When f is linear, expected autonomous states control integral, proportional
79
and derivative processess(0)
t , s(1)
t , s(2)
t towards their target value of zero. Furthermore, fromf and K one can derive
integral, proportional and derivative gains, which penalise deviations ofs(0)
t , s(1)
t , s(2)
t , respectively, from their target
value of zero. Crucially, these control gains are simple by-products of the steady-state density and the stochastic
realisation problem.
Why restrict ourselves to PID control when stochastic control of integrator chains is available? It turns out that
when sensory statesst are expressed as a function of an integrator chain (3.14), one may get away by controlling an
approximation of the true (sensory) process, obtained by truncating high orders of motion as these have less effect
on the dynamics, though knowing when this is warranted is a problem in approximation theory. This may explain
why integral feedback control (n = 0), PI control (n = 1) and PID control (n = 2) are the most ubiquitous control
methods in engineering applications. However, when simulating biological control—usually with highly non-linear
dynamics—it is not uncommon to consider generalised motion to fourth (n = 4) or sixth (n = 6) order [168,172].
It is worth mentioning that PID control has been shown to be implemented in simple molecular systems and is
becoming a popular mechanistic explanation of behaviours such as bacterial chemotaxis and robust homeostatic
algorithms in biochemical networks [155,173,174]. We suggest that this kind of behaviour emerges in Markov
blankets at non-equilibrium steady-state. Indeed, stationarity means that autonomous states will look as if they
respond adaptively to external perturbations to preserve the steady-state, and we can identify these dynamics as
implementations of various forms of stochastic control (including PID-like control).
3.6 Discussion
In this chapter, we considered the consequences of a boundary mediating interactions between states internal and
external to a system. On unpacking this notion, we found that the states internal to a Markov blanket look as if they
perform variational Bayesian inference, optimising beliefs about their external counterparts. When subdividing the
blanket into sensory and active states, we found that autonomous states perform active inference and various forms
of stochastic control (i.e., generalisations of PID control).
Interacting Markov blankets: The sort of inference we have described could be nuanced by partitioning the
external state-space into several systems that are themselves Markov blankets (such as Markov blankets nested at
several different scales [88]). From the perspective of internal states, this leads to a more interesting inference problem,
with a more complex generative model. It may be that the distinction between the sorts of systems we generally
think of as engaging in cognitive, inferential, dynamics [175] and simpler systems rest upon the level of structure of
the generative models (i.e., steady-state densities) that describe their inferential dynamics.
Temporally deep inference:This distinction may speak to a straightforward extension of the treatment on offer,
from simply inferring an external state to inferring the trajectories of external states. This may be achieved by
representing the external process in terms of its higher orders of motion by solving the stochastic realisation problem.
By repeating the analysis above, internal states may be seen as inferring the position, velocity, jerk, etc of the external
process, consistently with temporally deep inference in the sense of a Bayesian filter [14] (a special case of which is
an extended Kalman–Bucy filter [176]).
Bayesian mechanics in non-Gaussian steady-states:The treatment from this chapter extends easily to non-
Gaussian steady-states, in which internal states appear to perform approximate Bayesian inference over external
states. Indeed, any arbitrary (smooth) steady-state density may be approximated by a Gaussian density at one of
its modes using a so-called Laplace approximation. This Gaussian density affords one with a synchronisation map in
closed form4 that maps the expected internal state to an approximation of the expected external state. It follows that
4Another option is to empirically fit a synchronisation map to data [8].
80
the system can be seen as performing approximate Bayesian inference over external states—precisely, an inferential
scheme known as variational Laplace [177]. We refer the interested reader to a worked-out example involving two
sparsely coupled Lorenz systems [9]. Note that variational Laplace has been proposed as an implementation of
various cognitive processes in biological systems [110,133,139] accounting for several features of the brain’s functional
anatomy and neural message passing [134,149,175,178,179].
Modelling real systems:The simulations presented here are as simple as possible and are intended to illustrate
general principles that apply to all stationary processes with a Markov blanket (3.6). These principles have been
used to account for synthetic data arising in more refined (and more specific) simulations of an interacting particle
system [8] and synchronisation between two sparsely coupled stochastic Lorenz systems [9]. Clearly, an outstanding
challenge is to account for empirical data arising from more interesting and complex structures. To do this, one
would have to collect time-series from an organism’s internal states (e.g., neural activity), its surrounding external
states, and its interface, including sensory receptors and actuators. Then, one could test for conditional independence
between internal, external and blanket states (3.6) [180]. One might then test for the existence of a synchronisation
map (using Lemma 3.3.3). This speaks to modelling systemic dynamics using stochastic processes with a Markov
blanket. For example, one could learn the volatility, solenoidal flow and steady-state density in a stochastic differential
equation (3.7) from data, using supervised learning [181].
3.7 Conclusion
This chapter outlines some of the key relationships between stationary processes, inference and control. These
relationships rest upon partitioning the world into those things that are internal or external to a (statistical) boundary,
known as a Markov blanket. When equipped with dynamics, the expected internal states appear to engage in
variational inference, while the expected active states appear to be performing active inference and various forms of
stochastic control.
The rationale behind these findings is rather simple: if a Markov blanket derives from a steady-state density, the
states of the system will look as if they are responding adaptively to external perturbations in order to recover the
steady-state. Conversely, well-known methods used to build adaptive systems implement the same kind of dynamics,
implicitly so that the system maintains a steady-state with its environment.
3.8 Addendum: Proofs for Chapter 3
3.8.1 Existence of synchronisation map: proof
We prove Lemma 3.3.3.
Proof. (i) ⇐⇒ (ii) follows by definition of a function.
(ii) ⇐⇒ (iii) is as follows
∀b1, b2 ∈ B: µ(b1) = µ(b2) ⇒ η(b1) = η(b2)
⇐⇒
 
∀b1, b2 ∈ B: ΣµbΣ−1
b b1 = ΣµbΣ−1
b b2 ⇒ ΣηbΣ−1
b b1 = ΣηbΣ−1
b b2

⇐⇒
 
∀b ∈ B: ΣµbΣ−1
b b = 0 ⇒ ΣηbΣ−1
b b = 0

⇐⇒ ker Σµb ⊂ ker Σηb
81
(iii) ⇐⇒ (iv) From [182, Section 0.7.3], using the Markov blanket condition (3.2), we can verify that
ΠµΣµb = −ΠµbΣb
ΠηΣηb = −ΠηbΣb.
Since Πµ, Πη, Σb are invertible, we deduce
ker Σµb ⊂ ker Σηb
⇐⇒ ker ΠµΣµb ⊂ ker ΠηΣηb
⇐⇒ ker−ΠµbΣb ⊂ ker−ΠηbΣb
⇐⇒ ker Πµb ⊂ ker Πηb.
3.8.2 Free energy computations
The free energy reads (3.11)
F(b, µ) = DKL[qµ(η)∥p(η|b)] − log p(b, µ).
Recalling from (3.3), (3.8) thatqµ(η) and p(η|b) are Gaussian, the KL divergence between multivariate Gaussians is
well-known
qµ(η) = N(η; σ(µ), Π−1
η ), p (η|b) = N(η; η(b), Π−1
η ),
⇒ DKL[qµ(η)∥p(η|b)] = 1
2(σ(µ) − η(b))Πη(σ(µ) − η(b)).
Furthermore, we can compute the log partition
−log p(b, µ) = 1
2
h
b, µ
i
Σ−1
b:µ
"
b
µ
#
(up to a constant).
Note thatΣ−1
b:µ is the inverse of a principal submatrix ofΣ, which in general differs fromΠb:µ, a principal submatrix
of Π. Finally,
F(b, µ) = 1
2(σ(µ) − η(b))Πη(σ(µ) − η(b)) + 1
2
h
b, µ
i
Σ−1
b:µ
"
b
µ
#
(up to a constant).
82
Chapter 4
The free-energy principle
The free-energy principle made simpler but not too
simple
By Karl Friston, Lancelot Da Costa, Noor Sajid, Conor Heins, Kai
Ueltzhoeffer, Grigorios A. Pavliotis, Thomas Parr
Adapted from:K Friston, L Da Costa, N Sajid, C Heins, K Ueltzhoeffer, GA Pavliotis, T Parr. The
free energy principle made simpler but not too simple.Physics Reports. 2023
83
4.1 Abstract
This chapter provides a concise description of the free energy principle, starting from a formulation of random
dynamical systems in terms of a Langevin equation and ending with a Bayesian mechanics that can be read as a
physics of sentience.1 It rehearses the key steps using standard results from statistical physics. These steps entail (i)
establishing a particular partition of states based upon conditional independencies that inherit from sparsely coupled
dynamics, (ii) unpacking the implications of this partition in terms of Bayesian inference and (iii) describing the
paths of particular states with a variational principle of least action. Teleologically, the free energy principle offers
a normative account of self-organisation in terms of optimal Bayesian design and decision-making, in the sense of
maximising marginal likelihood or Bayesian model evidence. In summary, starting from a description of the world in
terms of random dynamical systems, we end up with a description of self-organisation as sentient behaviour that can
be interpreted as self-evidencing; namely, self-assembly, autopoiesis or active inference.
Keywords: self-organisation, nonequilibrium, variational inference, Bayesian, Markov blanket.
4.2 Introduction
It is said that the free energy principle is difficult to understand. This is ironic on three counts. First, the free energy
principle (FEP) is so simple that it is (almost) tautological. Indeed, philosophical accounts compare its explanandum
to a desert landscape, in the sense of Quine [183]. Second, a tenet of the FEP is that everything must provide an
accurate account of things that is as simple as possible—including itself. Finally, the FEP rests on straightforward
results from statistical physics. This review tries to present the free energy principle as simply as possible but without
sacrificing too much technical detail. It steps through the formal arguments that lead from a description of the world
as a random dynamical system [125,184] to a description of self-organisation in terms of active inference and self-
evidencing [185]. The evidence in question is Bayesian model evidence, which speaks to the Bayesian mechanics on
offer [113]. These mechanics have the same starting point as = statistical and classical mechanics. The only difference
is that careful attention is paid to the way that the internal states of something couple to its external states.
To make the following account accessible, we use a conversational style, explaining the meaning of key mathematical
expressions intuitively. Accordingly, simplifying notation and assumptions are used to foreground the basic ideas.
Before starting, it might help to clarify what the free energy principle is—and why it is useful. Many theories in
the biological sciences are answers to the question: “what must things do, in order to exist?” The FEP turns this
question on its head and asks: “if things exist, what must they do?” More formally, if we can define what it means to
be something, can we identify the physics or dynamics that a thing must possess? To answer this question, the FEP
calls on some mathematical truisms that follow from each other. Much like Hamilton’s principle of least action2, it
is not a falsifiable theory about the way ‘things’ behave—it is a general description of ‘things’ that are defined in a
particular way. As such, the FEP is not falsifiable as a mathematical statement, but it may as well be falsifiable to
the extent that its postulates refer to a specific class of empirical phenomena that the principle aims to describe.
Is such a description useful? In itself, the answer is probably no—in the sense that the principle of least action does
not tell you how to throw a ball. However, the principle of least action furnishes everything we need to know to
simulate the trajectory of a ball in a particular instance. In the same sense, the FEP allows one to simulate and
predict the sentient behaviour of a particle, person, artefact or agent (i.e., some ‘thing’). This allows one to build
sentient artefacts or use simulations as observation models of particles (or people). These simulations rest upon
1Sentience here is meant for a thing as having sensory states through which it is coupled with its external states, and being
described as behaving adaptively in accordance to the sensory states.
2Perhaps a better analogy would be Noether’s theorem (Beren Millidge – personal communication) [186].
84
specifying agenerative modelthat is apt to describe the behaviour of the particle (or person) at hand. At this point,
committing to a specific generative model can be taken as a commitment to a specific—and falsifiable—theory. Later,
we will see some examples of these simulations.
The remaining sections describe the FEP. Each section focuses on an equation—or set of equations—used in subse-
quent sections. The ensuing narrative is meant to be concise, taking us from the beginning to the end as succinctly as
possible. To avoid disrupting the narrative, we use footnotes to address questions that are commonly asked at each
step. We also use figure legends to supplement the narrative with examples from neurobiology. Most of the following
can be found in the literature [9,58,113]; however, there are a few simplifications that replace earlier accounts.
4.3 Systems, states and fluctuations
We start by describing the world with a stochastic differential equation [3]. So why start here? The principal reason
is that we want a description that is consistent with physics. This follows because things like the fluctuation theorems
in statistical mechanics and the Lagrangian formulation of classical mechanics can all be derived from this starting
point [16]. In short, if one wants a physics of sentience, this is a good place to start.
We are interested in systems that have characteristic states. Technically, this means the system has a pullback
attractor; namely, a set of states a system will come to occupy from any initial state [184,187]. Such systems can be
described with stochastic differential equations, such as the Langevin equation describing the rate of change of some
states x(τ), in terms of their flowf(x), and random fluctuationsω(τ). The fluctuations are usually assumed to be a
normally distributed (white noise) process, with a covariance of2Γ:
˙x(τ) = f(x) + ω(τ)
p(ω | x) = N(ω; 0, 2Γ) ⇒ p( ˙x | x) = N( ˙x; f, 2Γ)
p(x) =?
(4.1)
The dot notation denotes a derivative with respect to time3. This means that time and causality are baked into
everything that follows, in the sense that states cause their motion. The Langevin equation is itself an approximation
to a simpler mapping from some variables to changes in those variables with time. This follows from the separation
into states and random fluctuations implicit in (4.1), where states change slowly in relation to fast fluctuations. This
(adiabatic) approximation is ubiquitous in physics [94,189,190]. In brief, it means we can ignore temporal correlations
inthefastfluctuationsandassume—bythecentrallimittheorem—thattheyhaveaGaussiandistribution. Thisequips
the fluctuations with a probability density, which means we know their statistical behaviour but not their trajectory
or path, which itself is a random variable [3,125,184].
The next step, shared by all physics, is to ask whether anything can be said about the probability density over the
states—the ‘?’ in (4.1). A lot can be said about this probability density, which can be expressed in two complementary
ways; namely, asdensity dynamicsusing the Fokker-Planck equation (a.k.a. the forward Kolmogorov equation) or in
termsoftheprobabilityofapaththroughstate-spaceusingthe path-integral formulation. TheFokker-Planckequation
describes the change in the density due to random fluctuations and the flow of states through state-space [3,80]:
˙p(x, τ) = ∇ ·(Γ∇ −f(x))p(x, τ) (4.2)
3Question: why is the flow in (4.1) not a function of time? Many treatments of stochastic thermodynamics allow for
time-dependent flows when coupling one system (e.g., an idealised gas) to another (e.g., a heat reservoir), where it is assumed
that the other system changes very slowly, e.g., [16,188]. However, the ambition of the FEP is to describe this coupling under
a partition of states. In this setting, separation of temporal scales is an emergent property, where (4.1) holds at any given
temporal scale. See [113] for a treatment using the apparatus of the renormalisation group.
85
The Fokker-Planck equation describes our stochastic process in terms of deterministic density dynamics—instead of
specific realisations—where the density in question is overstates x(τ) = xτ . Conversely, the path-integral formulation
considers the probability of a trajectory orpath x[τ] ≜ [x(t) : 0 ≤ t ≤ τ] in terms of itsaction A (omitting additive
constants here and throughout)4:
A(x[τ]) = −ln p (x[τ] | x0)
= τ
2 ln |(4π)nΓ| +
Z τ
0
dtL(x, ˙x)
L(x, ˙x) = 1
2

( ˙x − f) · 1
2Γ( ˙x − f) + ∇ ·f

(4.3)
Both the Fokker-Planck and path-integral formulations inherit their functional form from assumptions about the
statistics of random fluctuations in (4.1). For example, the most likely path—or path of least action—is the path
taken when the fluctuations take their most likely value of zero. This means that variations away from this path
always increase the action. This is expressed mathematically by saying that its variation is zero when the action is
minimised.5
x[τ] = arg min
x[τ]
A(x[τ])
⇔ δxA(x[τ]) = 0
⇔ ˙x(τ) = f(x)
(4.4)
In short, the motion on the path of least action is just the flow without random fluctuations. Paths of least action
will figure prominently below; especially, when considering systems that behave in a precise or predictable way. We
will denote the most likely states and paths with a bold typeface.
Although equivalent, the Fokker-Planck and path-integral formalisms provide complementary perspectives on dy-
namics. The former deals with time-dependent probability densities over states, while the latter considers time-
independent densities overpaths. The density over n states at any particular time is the time-marginal of the density
over trajectories. These probabilities can be conveniently quantified in terms of their negative logarithms (or poten-
tials) leading to surprisal and action, respectively (omitting the divergence of the flow in the last line for simplicity):
ℑ(x, τ) ≜ −ln p(x, τ)
A(x[τ]) ≜ −ln p (x[τ] | x0)
H[p(x, τ)] = E[ℑ(x, τ)]
H [p (x[τ] | x0)] = E[A(x[τ])]
= τ
2 ln [(4π)n|Γ|] +
Z τ
0
dt1
2Ep(ω)

ω(t) · 1
2Γω(t)

= τ
2 ln [(4πe)n|Γ|]
(4.5)
The second set of equalities shows that the uncertainty (or entropy) about states and their paths is the expected
surprisal and action, respectively. Perhaps counterintuitively, the entropy of paths is easier to specify than the entropy
4Question: where does the divergence in the third equality come from? This term arises from the implicit use of
Stratonovich path integrals [16]. Note that we have assumed that the amplitude of random fluctuations is state—and therefore
path—independent in (4.1), which means we can place it outside the integral in the second equality.
5Omitting the contribution of the divergence term in the Lagrangian to obtain the expression for the path of least action
for simplicity, cf. [191]. Taking this simplification at face value means that we are either: 1) considering a description on a short
time-scale as the flow can be approximated by a linear function with impunity (e.g., linear response theory, see [3]); or 2) we
are considering the limit where random fluctuations have vanishingly small amplitude (e.g., precise particles, see Sections 4.8
and 4.9).
86
of states. This follows because the only source of uncertainty about paths—given an initial state—are the random
fluctuations [3,16], whose probability density does not change with time. The last pair of equalities in (4.5) show
that the amplitude of random fluctuations determines the entropy of paths. Intuitively, if the fluctuations are large,
then many distinct paths become equally plausible, and the entropy of paths increases6.
4.4 Solutions, steady-states and nonequilibria
So far, we have equations that describe the relationship between the dynamics of a system and probability densities
over fluctuations, states and their paths. This is sufficient to elaborate much of physics. For example,we could focus on
systems that comprise statistical ensembles of similar states to derive stochastic and statistical mechanics in terms of
fluctuation theorems [16]. Finally, we could consider large systems—in which the fluctuations are averaged away—to
derive classical mechanics such as electromagnetism.All of these mechanics require some boundary conditions: for
example, a heat bath or reservoir in statistical mechanics and a classical potential for Lagrangian mechanics. At this
point, the FEP steps back and asks, where do these boundary conditions come from? Indeed, this was implicit in
Schrodinger’s question:
“How can the events in space and time which take place within the spatial boundary of a living organism be accounted
for by physics and chemistry?” [192].
We read a boundary in a statistical sense as a Markov boundary [90]7. Why? Because the only thing we have
at hand is a probabilistic description of the system. And the only way to separate the states of something from
its boundary states is in terms of probabilistic independencies—in this instance, conditional independencies8. This
means we need to identify a partition of states that assigns a subset to a ‘thing’ or particle and another subset to
the boundary that separates the thing from some ‘thing’ else. In short, one has to define ‘thingness’ in terms of
conditional independencies.
However, if things are defined in terms of conditional independencies and conditional independencies are attributes
of a probability density, where does the density come from? The Fokker-Planck equation shows that the density over
states depends upon time, even if the flow does not. This means that if we predicate ‘thingness’ on a probability
density, it may only exist for a vanishingly small amount of time. This simple observation compels us to consider
probability densities that do not change with time, namely: (i) steady-state solutions to the Fokker-Planck equation
or (ii) the density over paths. We will start with the (slightly more delicate) treatment of steady-state solutions
and then show that the (slightly more straightforward) treatment of densities over paths leads to the same notion of
‘thingness’.
The existence of things over a particular timescale implies the density in (4.2) does not change over that timescale.
This is what is meant by a steady-state solution to the Fokker-Planck equation. The ensuing density is known as a
steady-state densityand, in random dynamical systems, implies the existence of a pullback attractor [125,184]. The
notion of an attractor is helpful here, in the sense that it comprises a set of characteristic states, to which the system
is attracted over time9. In short, to talk about ‘things’, we are implicitly talking about a partition of states in a
random dynamical system that has an attracting set—i.e., a steady-state solution to the Fokker-Planck equation.
6From a thermodynamic perspective, uncertainty about paths increases with temperature. For example, the Einstein-
Smoluchowski relation relates the amplitude of random fluctuations to a mobility coefficient times the temperatureΓ = µmkBT.
7A Markov boundary is a subset of states of the system that renders the states of a ‘thing’ or particle conditionally
independent from all other states [193].
8Notingthatiftwosubsetsofstateswereindependent, asopposedtobeingconditionallyindependent, wewouldbedescribing
two separate systems.
9More precisely, the time-dependent solutions to the Fokker-Planck equation will tend towards the stationary solution, or
steady-state. In other words, the steady-state density becomes a point attractor in the space of probability densities.
87
In short, we consider systems that self-organise towards a steady-state density10. This solution is also known as a
nonequilibrium steady-state (NESS) density, where the ‘nonequilibrium’ aspect rests upon solenoidal flow, as we will
see next.
The existence of a solution to the Fokker-Planck equation—i.e., the existence of something—means that we can
express the flow of states in terms of the steady-state density (or corresponding surprisal) using a generalisation of
the Helmholtz decomposition. This decomposes the flow into conservative (rotational, divergence-free) and dissipative
(irrotational, curl-free) components—with respect to the steady-state density—referred to assolenoidal and gradient
flows, respectively [3,21,22,24,25,194,195]:
˙p(x) = 0 ⇔ f(x) = Ω(x)∇ℑ(x) − Λ(x) = Q(x)∇ℑ(x)| {z }
Solenoidal flow
− Γ∇ℑ(x)| {z }
Gradient flow
−Λ(x)
ℑ(x) = −ln p(x), Q = −QT , Λi ≜
X
j
∂Ωij
∂xj
=
X
j
∂Qij
∂xj
.
(4.6)
This can be understood intuitively as a decomposition of the flow into two parts. The first (conservative) part of
the flow is asolenoidal circulation on the isocontours of the steady-state density (or surprisal). This component
breaks detailed balance and renders the steady-state density anonequilibrium steady-state density [23,196]. The
second (dissipative) part performs a (natural) gradient descent on the steady-state surprisal and depends upon the
amplitude of random fluctuations [197,198]. The final term,Λ, can be regarded as a correction term, which is neither
curl-free nor divergence-free, and which ensures that the probability density remains constant over time [195].
Summary
We now have a probabilistic description of a system in terms of a (NESS) density that admits conditional inde-
pendencies among states. These conditional independencies are necessary to separate the states of things from
their boundaries. In the next step, we will see how conditional independencies inherit from sparse coupling among
states—and how they are used to establish a particular partition of states.
4.5 Particles, partitions and things
In associating some (stochastic differential) equations of motion with a unique (NESS) density, we have a somewhat
special setup, in which the influences entailed by the equations of motion place constraints on the conditional inde-
pendencies of the NESS density. These conditional independencies can be used to identify a particular partition of
states intoexternal, sensory, active and internal states as summarised below. This is an important move because it
separates the states of aparticle (i.e., internal states and their sensory and active states) from the remaining (i.e.,
external) states. However, to do this we have to establish how the causal dynamics in (4.1) underwrite conditional
independencies. This can be done simply by using the curvature (Hessian) of surprisal as follows:
(xu ⊥ xv) | b ⇔ p(x) = p (xu | b) p (xv | b) p(b)
⇔ ℑ(x) = ℑ(xu | b) + ℑ(xv | b) + ℑ(b) ⇔ ∂2ℑ
∂xu∂xv
= Huv = Hvu = 0.
(4.7)
10At this point, the formalism applies equally to steady-states with a high or low entropy, as we have not committed to a
particular form of the steady-state density. Later, we will specialise to steady-states with a low entropy to characterise the sort
of self-organisation that describes biological systems, e.g., swarming or flocking [92,94]
88
This says that if theu-th state is conditionally independent of thev-th state, given the remaining statesb, then the
corresponding element of the curvature—or Hessian matrix—of surprisal must be zero. Conversely, a zero entry in
the Hessian implies conditional independence. In sum, any two states are conditionally independent if, and only if,
the change of surprisal with one state does not depend on the other. We can now use the Helmholtz decomposition
(4.6) to express the Jacobian—i.e., the (linear) coupling—of the flow in terms of the Hessian—that entails conditional
independencies (with a slight abuse of the dot product notation):
f(x) = Ω∇ℑ −Λ
⇒ J = ΩH + ∇Ω · ∇ℑ − ∇Λ
⇒ Juv = ∂fu
∂xv
=
X
i
ΩuiHiv +
X
i
∂Ωui
∂xv
∂ℑ
∂xi
−
X
i
∂2Ωui
∂xi∂xv
.
(4.8)
We can now definesparse couplingas a solution to this equation, in which all the terms are identically zero11:
QuiHiv
ΓuHuv
∂Ωui/∂xv



= 0 : ∀i ⇒ J(x)uv = 0. (4.9)
Sparse coupling means that the Jacobian coupling statesu and v is zero, i.e., an absence of coupling from one state to
another. This definition precludes solenoidal coupling withu that depends onv. BecauseH(x)vv and Γu are positive
definite, sparse coupling requires associated elements of the solenoidal operator and Hessian to vanish at every point
in state-space, which in turn, implies conditional independence:
QuvHvv = 0 ⇒ Quv = −Qvu = 0
ΓuHuv = 0 ⇒ Huv = Hvu = 0 ⇔ (xu ⊥ xv) | b.
(4.10)
In short, sparse coupling means that any two states are conditionally independentif one state does not influence
the other. This is an important observation; namely, that sparse coupling implies a NESS density with conditional
independencies. In turn, this means any dynamical influence graph with absent or directed edges admits aMarkov
blanket (the statesb above). These independencies can now be used to build a particular partition as follows:
• The Markov boundarya ⊂ x of a set of internal statesµ ⊂ x is the minimal set of states for which there exists
a nonzero Hessian submatrix:Haµ ̸= 0. In other words, the internal states are independent of the remaining
states, when conditioned upon their Markov boundary, calledactive states. The combination of active and
internal states will be referred to asautonomous states: α = (a, µ).
• The Markov boundarys ⊂ x of autonomous states is the minimal set of states for which there exists a nonzero
Hessian submatrix: Hsα ̸= 0. In other words, the autonomous states are independent of the remaining states,
when conditioned upon their Markov boundary, calledsensory states. The combination of active and sensory
(i.e., boundary) states constituteblanket states: b = (s, a). The internal and blanket states will be referred to
as particular states: π = (s, α) = (b, µ).
• The remaining states constituteexternal states: x = (η, π).
The names of active and sensory (i.e., blanket) states inherit from the literature, where they are often associated
with biotic systems that act on—and sense—their external milieu12. In this setting, one can regard external states as
11This implicitly precludes edge cases, in which some non-zero terms cancel.
12Question: why does a particular partition comprises four sets of states? In other words, why does a particular partition
consider two Markov boundaries; namely, sensory and active states? The reason is that this is the minimal partition that
allows for directed coupling with blanket states. For example, sensory states can influence internal states—and active states
can influence external states—without destroying the conditional independencies of the particular partition (these directed
influences are illustrated in the upper panel of Figure 4.1 as dotted arrows).
89
influencing internal states via sensory states (directly or through active states). And internal states influence external
states via active states (directly or through sensory states13). We will see later how this implies a synchronisation
between internal and external states, in the sense that internal states can be seen as actively inferring external
states [9,58]. The ensuing conditional independencies implied by a particular partition can be summarised as follows:
Jµη = 0 ⇒ Hµη = 0 ⇔ (µ ⊥ η) | b
Jaη = 0 ⇒ Haη = 0 ⇔ (a ⊥ η) | s, µ
Jsµ = 0 ⇒ Hsµ = 0 ⇔ (s ⊥ µ) | a, η
(4.11)
A normal form for the flow and Jacobian of a particular partition—with sparse coupling—can be expressed as follows,
where α = (a, µ) and β = (η, s):
f(x) = Ω∇ℑ −Λ


fη(η, b)
fs(η, b)
fa(b, µ)
fµ(b, µ)


=


Qηη − Γη Qηs
−QT
ηs Qss − Γs
Qaa − Γa Qaµ
−QT
aµ Qµµ − Γµ




∇ηℑ(η | b)
∇sℑ(b | η)
∇aℑ(b | µ)
∇µℑ(µ | b)


− Λ
J(x) = ΩH + ∇Ω · ∇ℑ − ∇Λ


Jηηη Jηs Jηa
Jsη Jss Jsa
Jas Jaa Jaµ
Jµs Jµa Jµµ


=


Qηη − Γη Qηs
−QT
ηs Qss − Γs
Qaa − Γa Qaµ
−QT
aµ Qµµ − Γµ




Hηη Hηs
HT
ηs Hss Hsa
HT
sa Haa Haµ
HT
aµ Hµµ


+ ∇Ω · ∇ℑ − ∇Λ
∇ηΩαα = 0, ∇µΩββ = 0, ∇ηΛα = 0, ∇µΛβ = 0
(4.12)
This normal form means that particular partitions can be defined in terms of sparse coupling. Perhaps the simplest
definition—that guarantees a Markov blanket14—is as follows: external states only influence sensory states and
internal states only influence active states. This means that sensory states are not influenced by internal states and
active states are not influenced by external states,


˙η(τ)
˙s(τ)
˙a(τ)
˙µ(τ)


=


fη(η, s, a) + ωη(τ)
fs(η, s, a) + ωs(τ)
fa(s, a, µ) + ωa(τ)
fµ(s, a, µ) + ωµ(τ)


(4.13)
and the noise processesωi(τ), i∈ {η, s, a, µ} are independent. Under this sparse coupling, it is simple to show that
not only are internal and external states conditionally independent, but their paths are conditionally independent,
given initial states, using the path integral formulation.
The uncertainty (i.e., entropy) over paths derives from random fluctuations. This means that if we knew all the
13Question: does this mean that I can act on my world through my sense organs? Yes: much of biotic action is mediated
by (active) motile cytoskeletal filaments, muscles and secretory organs that lie beneath (sensory) epithelia, such as receptors
on the skin or a cell surface.
14In the absence of solenoidal coupling between autonomous and non-autonomous states, and constraints on the partial
derivatives of the solenoidal coupling in (4.12); i.e., solenoidal coupling among autonomous states does not depend upon
external states. Similarly, for non-autonomous and internal states.
90
influences on the flow at every point in time, we can evaluate the entropy of external and internal paths from (4.5):
H [p (η[τ] | b[τ], x0)] = τ
2 ln [(4πe)nη |Γη|]
H [p (µ[τ] | b[τ], x0)] = τ
2 ln [(4πe)nµ |Γµ|]
⇒
H [p (η[τ] | b[τ], x0)] = H [p (η[τ] | µ[τ], b[τ], x0)] ⇒ (µ[τ] ⊥ η[τ]) | b[τ], x0
H [p (µ[τ] | b[τ], x0)] = H [p (µ[τ] | η[τ], b[τ], x0)] ⇒ (µ[τ] ⊥ η[τ]) | b[τ], x0
(4.14)
The final equalities say that the uncertainty about external (resp., internal) paths does not change when we know the
internal (resp., external) path because external (resp., internal) states do not influence internal (resp., external) flow.
This means the external and internal paths do not share any mutual information and are therefore independent when
conditioned on blanket paths (and initial states). From (4.11), the initial external and internal states are themselves
independent, when conditioned on blanket states.
Note that the conditional independence of paths inherits directly from the sparse coupling, without any reference to
the NESS density or Helmholtz decomposition. This can be seen clearly by replacing the partial derivatives in (4.7)
with functional derivatives and noting, from (4.12), that there are no flows that depend on both internal and external
states:
∂2f
∂η∂µ = 0 ⇒ ∂2L
∂η∂µ = ∂2L
∂η∂ ˙µ = ∂2L
∂ ˙η∂µ = ∂2L
∂ ˙η∂ ˙µ = 0
⇒ δ2A(x[τ])
δη[t]δµ[t] = 0 ⇔ (µ[τ] ⊥ η[τ]) | b[τ], x0
δA(x[τ])
δµ[t] =
Z
dt′
 ∂L
∂µ [t′]
δµ [t′]
δµ[t] + ∂L
∂ ˙µ [t′]
d
dt′
δµ [t′]
δµ[t] + . . .

δ2A(x[τ])
δη[t]δµ[t] =
Z
dt′dt′′
δη [t′′]
δη[t]
∂2L
∂η∂µ
δµ [t′]
δµ[t] + δη [t′′]
δη[t]
∂2L
∂η∂ ˙µ
d
dt′
δµ [t′]
δµ[t] + . . .

(4.15)
These expressions mean that the probability of an internal path, given a blanket path (and initial states), does not
depend on the external path andvice versa.
Summary
In summary, the internal dynamics (i.e., paths) of some ‘thing’ are conditionally independent of external paths if, and
only if, the flow of internal states does not depend on external states andvice versa(given initial states). We take this
as a necessary and sufficient condition for something to exist, in the sense that it can be distinguished from everything
else. When the initial states are sampled from the NESS density, the internal states are conditionally independent of
external states (given blanket states), under certain constraints on solenoidal flow. Figure 4.1 illustrates the ensuing
particular partition. Note that the edges in this graph represent the influence of one state on another, as opposed
to conditional dependencies. This is important because directed influences admit conditional independence. These
conditional independencies are manifest as zero entries in the Hessian matrices, which inherit from the sparse, directed
coupling of the dynamics.
4.6 From self-organisation to self-evidencing
Equipped with a particular partition, we can now talk about things in terms of their internal states and Markov
boundary; namely autonomous states. And we can talk about autonomous states and their Markov boundary;
91
Figure 4.1: Markov blankets. This influence diagram illustrates a particular partition of states into internal states (blue)
and external states (cyan) that are separated by a Markov blanket comprising sensory (green) and active states (red). The
edges in this graph represent the influence of one state on another, as opposed to conditional dependencies. The diagram shows
this partition as it would be applied to a single-cell organism, where internal states are associated with intracellular states, the
sensory states become the surface states or cell membrane overlying active states (e.g., the actin filaments of the cytoskeleton).
The dotted lines indicate allowable directed influences from sensory (resp., active) to internal (resp., external) states. Particular
states constitute a particle; namely, autonomous and sensory states—or blanket and internal states.
namely, particular states—the states of a particle. The next step is to characterise the flow of the autonomous states
(of a particle, plant or person) in relation to external states. In other words, we consider the nature of the coupling
between the outside and inside of a particle, across its Markov blanket. It is at this point that we move towards a
(Bayesian) mechanics that is the special provenance of systems with particular partitions.
The existence of a particular partition means that—given sensory states—one can stipulatively define the conditional
density over external states as being parameterised by the most likely internal state [58]15. We will call this a
variational density parameterised by theinternal mode µ(τ)16:
qµ(η) ≜ p(η | s)
α(τ) = (a(τ), µ(τ))
α(τ) = arg min
α
ℑ(α(τ) | s(τ)) ⇒
α[τ] = arg min
α
A(α[τ] | s[τ]) ⇒
˙α(τ) = fα(s, α)
(4.16)
As with the paths of least action, we will use bold typeface to denote a mode or most likely state, given all the states
necessary to specify its likelihood. For autonomous states, we only need the sensory states, because the autonomous
states are conditionally independent of external states.
Inducing the variational density is an important move. It means that for every sensory state there is a corresponding
active mode and an internal mode (or an autonomous mode in the joint space of active and internal states). The
active a(τ), internal µ(τ) and autonomous α(τ) modes evolve on active, internal and autonomous manifolds17,
15In other words, the internal mode supplies the sufficient statistics of the conditional density over external states.
16Question: what if the conditional densities are not well-behaved, e.g., what if there are no unique modes? The answer
is that well-behaved densities are generally guaranteed when increasing the dimensionality of state-spaces using generalised
coordinates of motion [13,14,199]. In other words, instead of just dealing with states, we consider states and their generalised
motion to arbitrarily high order. We will see examples of this later.
17A manifold is a topological (state-) space where each state has a neighbourhood that is homeomorphic to a portion of an
Euclidean space of the same dimension [200]. Intuitively, it is a curved space, such as a smooth surface, in a possibly large but
92
respectively, whose dimensionality is the same as the sensory states18. We will see later that these manifolds play the
role ofcentre manifolds; namely, manifolds on which dynamics do not diverge (or converge) exponentially fast [189].
Crucially, theinternalmanifoldisalsoa statistical manifoldbecauseitsstatesaresufficientstatisticsforthevariational
density. In turn, this means that it is equipped with a metric and implicit information geometry [112,201,202]. Indeed,
the Fisher information metric tensor, which measures changes in the Kullback-Leibler (KL) divergence resulting from
infinitesimal changes in the internal mode, is a Riemannian metric that yields an information distance [203, Appendix
B]. This means we can interpret dynamics on the internal manifold as updating Bayesian beliefsabout external states.
This interpretation can be unpacked in terms of Bayesian inference as follows.
Equation (4.16) means that for every sensory state there is a conditional density over external states and a correspond-
ing internal mode with the smallest surprisal. This mode specifies the variational density, where—by definition—the
KL divergence between the variational density and the conditional density over external states is zero19. This means
we can express the autonomous flow as a gradient flow on a free energy functional of the variational density20. From
(4.12) 

fη(x)
fs(x)
fa(π)
fµ(π)


= Ω


∇ηℑ(x)
∇sℑ(x)
∇aF(π)
∇µF(π)


− Λ, (4.17)
where the free energy in question is (an upper bound on) the surprisal of particular states:
F(π(τ)) = Eq[ln q(η(τ)) − ln p(η(τ)) − ln p(π(τ) | η(τ))] = ℑ(π(τ))
= Eq[ℑ(η(τ), π(τ))]| {z }
Expected energy
−H[q(η(τ))]| {z }
Entropy
= Eq[ℑ(π(τ) | η(τ))]| {z }
-ve Accuracy
+ D[q(η(τ))∥p(η(τ))]| {z }
Complexity
= D[q(η(τ))∥p(η(τ) | π(τ))]| {z }
=0
+ℑ(π(τ))
q = qµ(η) = p(η | s) = p(η | π)
E[F(π)] = E[ℑ(π)] = H[p(π)]
(4.18)
This variational free energy21 can be rearranged in several ways. First, it can be expressed as expectedenergy minus
finite number of dimensions. In this instance, the states are conditional modes.
18The dimensionality of the active, internal and autonomous manifolds corresponds to the number of sensory states. This
means that both the number of active and internal states must be greater than the number of sensory states. In turn, this limits
the straightforward application of the free energy principle to particular partitions where the number of active states—and the
number of internal states—exceeds the number of sensory states. In other words, the FEP applies to large particles with a
nontrivial internal dynamics.
19Since the variational and conditional densities over external states are equal, any divergence between them will vanish,
see [204, Section 3.2].
20A functional is a function of a function, here, the free energy is a function of a conditional density parameterised by the
internal mode.
21Question: why is this functional calledvariational free energy? More generally (for instance in engineering applications
where the free energy in question is also called an evidence lower bound [91]) the free energy is a functional of an approximate
posterior densityq that is an approximation to the Bayesian posterior, as follows:
qµ(η) ≈ p(η | π) ⇒ F[q] = D[q(η(τ))∥p(η(τ) | π(τ))]| {z }
≥0
+ℑ(π(τ))
(4.19)
The variational density considered in this article is the minimiser of (4.19), and the free energy evaluated at the variational
density is the variational free energy. The term ’variational’ inherits from the use of the calculus of variations in variational
Bayes (a.k.a., approximate Bayesian inference), applied in the context of a mean field approximation or factorised form of the
variational density. The term ’free energy’ inherits from Richard Feynman’s path integral formulation, in the setting of quantum
electrodynamics.
93
the entropy of the variational density, which licences the namefree energy22. In this decomposition, minimising
variational free energy corresponds to the maximum entropy principle, under the constraint that the expected energy
is minimised [206,207]. The expected energy is a functional of the NESS density that plays the role of agenerative
model; namely, a joint distribution over causes (external states) and their consequences (particular states)23.
Second, variational free energy can be decomposed into the (negative) log likelihood of particular states (i.e., negative
accuracy) and the KL divergence between posterior and prior densities (i.e.,complexity). Finally, it can be written as
the self-information associated with particular states (i.e.,surprisal) plus the KL divergence between the variational
and conditional (i.e., posterior) density, which—by construction—is zero. In variational Bayesian inference [140],
negative surprisal is read as a log marginal likelihood or model evidence, having marginalised over external states.
In this setting, negative free energy is anevidence lower boundor ELBO [91,208].
So, in what sense can we interpret (4.17) in terms of inference? Let us start by considering the response of autonomous
statestosomesensoryperturbation: thatis, thepathofautonomousstatesconditioneduponsensorystates. Ifsensory
states change slowly, then the autonomous states will flow towards their most likely value (i.e., their conditional mode)
and stay there24. However, if sensory states are changing, the autonomous states will look as if they are trying to hit
a moving target. One can formulate this along the lines of the centre manifold theorem [189,209], where we have a
(fast) flowoff the centre manifold and a (slow) flow of the autonomous modeon the manifold.
α(τ) = ε(τ)|{z}
Off manifold
+ α(τ)|{z}
On manifold
ε(τ) ≜ α(τ) − α(τ)
(4.20)
In effect, this is a decomposition in a frame of reference that moves with the autonomous mode, whose path lies on
the centre manifold. We further describe the off manifold flow using a Taylor expansion around the (time-varying)
autonomous mode25
˙ε(τ) = ˙α(τ) − ˙α(τ) = fε(0) + ∂fε
∂ε · ε + . . .= ∂fα
∂α · ε + . . .
⇒
˙α(τ) − ˙α(τ)| {z }
Off manifold flow
= Jα · (α − α) + . . .
= −(Γα∇ααF) · (α − α)| {z }
Flow to centre manifold
+ ( Qαα∇ααF) · (α − α)| {z }
Flow parallel to the manifold
+ . . .
(4.21)
This means that the flow at the expansion point is zero, leaving the second term of the expansion as the first non-
vanishing term. This is the Jacobian of the autonomous flow times the displacement of the current autonomous state
22Question: is variational free energy the same kind of free energy found in statistical mechanics? The answer is no: the
entropy term in the variational free energy is the entropy of a variational density—over external states—parameterised by
internal states. This entropy is distinct from the entropy ofinternal states. Minimising variational free energyincreases the
entropy of the variational density and, usually,reducesthe entropy of internal states (see [205] for an example). Mathematically,
we can express the different kind of entropies asH[q(η(τ))] ̸= H[p(µ(τ))].
23Question: in practical applications, variational free energy is usually a function of data or observed (sensory) states.
So, why is variational free energy a function of particular states? Later, we will see that practical applications correspond to
Bayesian filtering, under the assumption that particular dynamics are very precise. This means that there is no uncertainty
about autonomous paths given sensory paths, and the action of a particular path is the action of a sensory path. In generalised
coordinates of motion—used in Bayesian filtering—the action of a path becomes the surprisal of a state. In this setting, the
variational free energy of particular states is the same as the variational free energy of sensory states.
24Or, at least in the vicinity, if there are random fluctuations on its motion.
25Note that we are performing a Taylor expansion of a (generally rough) stochastic processε, see [210, Chapter 5]. Alterna-
tively, it may be possible to instead consider motion in generalised coordinates to introduce smooth random fluctuations (see
next Section), so thatε becomes smooth and the usual Taylor expansion applies.
94
Figure 4.2: Autonomous flows and Bayesian filters. This figure shows two components of the autonomous flow; namely,
a (fast) flow ˙α(τ) − ˙α(τ) off the (centre) manifold, and a (slow) flow˙α(τ) on the manifold. The manifold here is the set of
autonomous modesα(τ) given sensory statess(τ) for all timeτ, see (4.16). The decomposition into fast and slow flows means
that the manifold can be thought of as a centre manifold. The left panel shows two components of the fast flow off the manifold;
namely, a flow towards the centre manifold and a flow parallel to the manifold, see (4.21). This decomposition rests upon a
first-order Taylor expansion of the off manifold flow. The right panel plots the external mode as a function of the autonomous
mode—what is known as asynchronisation manifold—as a black curvilinear line. The Gaussian (blue and red) distributions
show possible variations in (external and autonomous) conditional modes due to variations in the sensory states. The arrows
represent the centre manifold flow˙α(τ) in the context of this synchronisation manifold, where the tangent vectors represent
possible directions of the flow.
from its corresponding mode. The second-order derivatives of the free energy arise from the Jacobian of the flow,
i.e., substituting (4.17) into (4.8). Therefore, the off manifold flow has a component that flowstowards the centre
manifold,26 afforded by the gradient flow, and a component that isparallel to the manifold, afforded by the solenoidal
flow, cf. (4.6). Taken together, this means that the autonomous states flow in ever-decreasing circles towards the
centre manifold, as illustrated in Figure 4.2.
But what about the flowon the centre manifold? We know from (4.17) that the flow of the autonomous mode can
be expressed in terms of free energy gradients:
˙α(τ) = (Qαα − Γα) ∇αF(s, α) + . . .
= (Qαα − Γα) ∇αEq[ℑ(s, α | η)]| {z }
-ve Accuracy
+ (Qαα − Γα) ∇αD [qµ(η)∥p(η)]| {z }
Complexity
+ . . . (4.22)
This expression unpacks the centre manifold flow in terms of the accuracy and complexity parts of free energy,
where the accuracy part depends upon the sensory states, while the complexity part is a function of, and only of,
autonomous states. In short, the flow on the centre manifold will look as if it is trying to maximise the accuracy of
its predictions, while complying with prior (Bayesian) beliefs.27 Here, predictions are read as the expected sensory
states, under posterior (Bayesian) beliefs about their causes afforded by the variational density over external states.
26We know that the flow must be towards the centre manifold because the covariance of random fluctuations is positive
definite, and the curvature of the free energy is positive definite at its minima: i.e., around the expansion point.
27The covariance of random fluctuationsΓα is positive definite and the solenoidal matrix fieldQαα is skew-symmetric,
therefore the flow in (4.22) will seek to minimise complexity minus accuracy.
95
Summary
In summary, a particular partition of a nonequilibrium steady-state density implies that autonomous dynamics can
be interpreted as performing a particular kind of inference. This entails a fast flow towards an autonomous centre
manifold and a slow flow on the centre manifold. The centre manifold flow can be interpreted as Bayesian belief
updating, where posterior (Bayesian) beliefs are encoded by points on an internal (statistical) manifold. In other
words, for every point on the statistical manifold, there is a corresponding variational density or Bayesian belief over
external states. We are now in a position to express this belief updating as a variational principle of least action:
α[τ] = arg min
α[τ]
A(α[τ] | s[τ])
⇔ δαA(α[τ] | s[τ]) = 0
⇔
˙a(τ) = fa(s, α) = (Qaa − Γa) ∇aF(s, α) + . . .
˙µ(τ) = fµ(s, α) = (Qµµ − Γµ) ∇µF(s, α) + . . .
(4.23)
This is a basis of the free energy principle. Put simply, it means that the internal states of a particular partition
can be cast as encoding conditional or posterior Bayesian beliefsabout external states. Equivalently, the autonomous
path of least action can be expressed as a gradient flow on a variational free energy that can be read as log evidence.
This licences a somewhat poetic description of self-organisation as self-evidencing [185], in the sense that the surprisal
or self-information is known as log model evidence or marginal likelihood in Bayesian statistics28.
Interestingly, because of the symmetric setup of the Markov blanket, it would be possible to repeat everything above
but switch the labels of internal and external states—and active and sensory states—and tell the same story about
external states tracking internal states. This evinces a form of generalised synchrony [58,128,212,213], where internal
and external states track each other. Technically, if we consider the (internal and external) manifolds in the joint space
of internal and external states, we have something called a synchronisation manifold that offers another perspective
on the coupling between the inside and outside [58,112,214].
These teleological interpretations cast particular paths of least action as an optimisation process, where different
readings of free energy link nicely to various normative (i.e., optimisation) theories of sentient behaviour. Some
cardinal examples are summarised in Figure 4.3; see [1,113,139,215] for some formal accounts of these relationships.
Because internal states do not influence sensory (or external) states, they will look as if they are concerned purely
with inference, in the sense that they parameterise the variational density over external states. However, active states
influence sensory (and external) states and will look as if they play an active role in configuring (and causing) the
sensory states that underwrite inference. In the neurosciences, this is known asactive inference[110,144,152].
The link between optimisation and inference is simply that inference is belief optimisation. However, it is worth
unpacking the gradients that ‘drive’ this optimisation. In statistics, variational free energy is used to score the
divergence between a variational density and the conditional density over external (i.e., hidden) states, given blanket
states [208]. Unlike the definition in (4.18), these densities are not assumed to be equivalent. Variational inference
proceeds by optimising the variational density such that it minimises free energy—often using the gradient flows in
(4.17). However, there is a subtle difference between the dynamics of (4.17) and variational inference. In the former,
28Question: this Bayesian mechanics seems apt for inference but what about learning over time? We have been dealing
with states in a generic sense. However, one can have states that change over different timescales. One can read slowly changing
states as special states that play the role of parameters; either parameters of the flow or, implicitly, the generative model. In
mathematical and numerical analyses, states and parameters are usually treated identically; i.e., as minimising variational free
energy. Indeed, in practical applications of Bayesian filtering schemes that learn, the parameters are treated as slowly changing
states. See [14,211] for worked examples.
96
there is no contribution from the KL-divergence as it is stipulated to be zero. In the latter, it is only the divergence
term that contributes to free energy gradients. So, is it tenable to interpret gradient flows on variational free energy
as variational inference, or is this just teleological window-dressing? The next section addresses this question through
the lens of Bayesian filtering. In brief, we will see that the autonomous paths of least action—implied by a particular
partition—are the paths of least action of a Bayesian filter. This takes us beyond ‘as if’ arguments by establishing a
formal connection between particular dynamics and variational inference.
4.7 Lagrangians, generalised states and Bayesian filtering
Now, say we wanted to emulate or simulate active inference. Given some equations of motion and statistics of random
fluctuations, we could find the stationary solution to the Fokker Planck equation and accompanying Helmholtz
decomposition. We could then solve (4.23) for the autonomous paths of least action that characterise the expected
behaviour of this kind of particle, and obtain realisations of synchronisation and inference. See [9] for a worked
example using a system of coupled Lorentz attractors.
In this section, we take a somewhat pragmatic excursion to suggest a simpler way to recover the paths of least action;
namely, as the solution to a generic (Bayesian) filtering scheme that is widely used in the engineering literature.
4.7.1 Dynamics in generalised coordinates of motion
Let us go back to the Langevin equation governing our system
˙x(τ) = f(x) + ω(τ). (4.24)
In this section, we assume that the random fluctuations driving the motion have smooth (analytic) sample paths;
thus, the Langevin equation considered in the rest of the article can be seen as the limit of (4.24) as the fluctuations
become rough [234]. This setup speaks nicely to the fact that, in biology, fluctuations are often smooth up to a
certain order—contrariwise to thermal (white noise) fluctuations—as they are the output of other random dynamical
systems. As before, we assume that the fluctuations are state-independent, and a stationary Gaussian process, e.g.,
the smoothing of white noise fluctuations with a Gaussian kernel. Just like in the case of white noise, Gaussianity
can be motivated by the central limit theorem—fluctuations should be normally distributed at each point in time.
We denote the autocovariance of fluctuations byΓh = 1
2 E[ω(τ) ⊗ ω(τ + h)]. The underlying dynamical systems
giving rise to this generic type of smooth noise can be recovered through a procedure known as stochastic realisation
[58,159,161]. The solution to the Langevin equation (4.24) can be approximated, on a suitably small interval of time,
by a linear Langevin equation in generalised coordinates of motion⃗ x= (x, x′, x′′, . . .) [235, Section 4]:2930
29The expansion (4.25) is a linear approximation of (4.24) [236], obtained by recursively differentiating (4.24) and ignoring
the contribution of the derivatives of the flow of order higher than one. In other words, the expansion is exact when the flow is
linear, and it is accurate on a short time-scale when the flow is non-linear.
30The curvature (i.e., second derivative) of the autocovarianceΓ′′
0 is a ubiquitous measure of roughness of a stochastic
process [237]. Note that in the limit where the fluctuationsω are uncorrelated (e.g., white noise fluctuations),Γ′′
0 (and higher
derivatives) become infinitely large.
97
Figure 4.3: Markov blankets and self-evidencing. This schematic illustrates various points of contact between minimising
variational free energy and other normative theories of optimal behaviour. The existence of a Markov blanket entails a certain
lack of influences among internal, blanket and external states. These have an important consequence—internal and active states
are not influenced by external states, which means their dynamics (i.e., perception and action) are a function of, and only of,
particular states, given by a variational (free energy) bound on surprisal. This has a number of interesting interpretations.
Given surprisal is the negative log probability of finding a particle or creature in a particular state, minimising surprise
corresponds to maximising the value of that state. This interpretation is licensed by the fact that the states with a high
probability are, by definition, characteristic of the particle in question. On this view, one could relate this to dynamics in
reinforcement learning [216], optimal control theory [217] and, in economics, expected utility theory [218,219]. Gradient flows
thatminimise surprisal(i.e., self-information)leadto aseries ofinfluentialaccountsofneuronaldynamics; including theprinciple
of maximum mutual information [220,221], the principles of minimum redundancy and maximum efficiency [222] and the free
energy principle [223]. Crucially, the average or expected surprise (over time of particular states) corresponds to entropy.
This means that action and perception look as if they are bounding the entropy of particular states. This links nicely with
theories of self-organisation, such as synergetics in physics [92,94,224] or homoeostasis in physiology [225–227]. Finally, the
probability of a particular state, is, on a statistical view, model evidence or marginal likelihood [228,229], marginalising over
the causes of particular states (i.e., external states). This means that all the above formulations are internally consistent with
things like the Bayesian brain hypothesis, evidence accumulation and predictive coding [1,133,139]. Most of these formulations
inherit from Helmholtz’s motion of unconscious inference [230], later unpacked in terms of perception as hypothesis testing
in psychology [231] and machine learning [232]. Although not depicted here, the minimisation of complexity—inherent in the
minimisation of free energy—enables thermodynamic and metabolic efficiency via Landauer’s principle [233].
98
˙x = x′ = ∇f · x + ω
˙x′ = x′′ = ∇f · x′ + ω′
˙x′′ = x′′′ = ∇f · x′′ + ω′′
...



⇔
˙⃗ x= f(⃗ x) + ⃗ ω
D⃗ x= J⃗ x+ ⃗ ω
p(⃗ ω(τ)) = N(⃗ ω(τ); 0, 2Γ)
D =


0 1
0 1
0 ...
...


, J =


∇f
∇f
∇f
...


, Γ =


Γ0 Γ′′
0
−Γ′′
0 −Γ′′′′
0
Γ′′
0 Γ′′′′
0
−Γ′′′′
0
...


(4.25)
Here, the different variables⃗ x=

x, x′, x′′, . . . , x(n), . . .

can be seen as the position, velocity, acceleration, jerk, and
higher orders of motion of the process, which are treated as separate (generalised) states that are coupled through
the JacobianJ. These are driven by smooth fluctuations⃗ ω(i.e., the serial derivatives ofω) whose covariance2Γ can
be expressed in terms of the serial derivatives of the autocovariance [238, Appendix A.5.3].
The generalised states are the coefficients of a Taylor series expansion of the solution to the Langevin equation (4.24):
x(τ) = x(0) + x′(0)τ + x′′(0)
2 τ2 + . . .+ x(n)(0)
n! τn + . . . , (4.26)
where (4.26) holds, typically, only on a small time-interval to which we restrict ourselves henceforth. In other
words, the generalised states at any time-point determine the system’s trajectory, and vice versa; that is, there is an
isomorphism between generalised states and paths.
This line of reasoning has two advantages. First, it means one can let go of white noise assumptions on the random
fluctuations and deal with smooth or analytic fluctuations. Second, the linear expansion in generalised coordinates
of motion (4.25) means that the distribution of generalised states has a simple Gaussian form
L(⃗ x(τ)) ≜ −ln p(⃗ x(τ)) = 1
2⃗ x(τ) · M⃗ x(τ)
M = (D − J) · 1
2Γ(D − J).
(4.27)
Here, M can be read as a mass matrix. This suggests that precise particles, with low amplitude random fluctuations,
behave like massive bodies. Furthermore, (4.27) is seen as the Lagrangian in generalised coordinates of motion, due
to its formal similarity with (4.3). Under the isomorphism between points and paths in generalised coordinates, the
Lagrangian is equivalent to the action; it scores the likelihood of paths of (4.24), as a path corresponds to a point in
generalised coordinates of motion31. We will reason about the trajectories of the system by analysing the Lagrangian
of generalised states henceforth.
The path of least action corresponds to the minimiser of the Lagrangian, which can be expressed as follows:
− →x (τ) = arg min
⃗ x(τ)
L(⃗ x(τ))
⇔ ∇L(− →x (τ)) = 0 ∀τ.
(4.28)
31Question: how can a point be a path? The generalised states (i.e., temporal derivatives) approximate the path of the
solution to (4.24) on a suitably small time interval because they are the coefficients of a Taylor expansion of the path as a
function of time (4.26).
99
We can recover the path of least action by solving the following equation of motion
˙⃗ x(τ) = D⃗ x− ∇L(⃗ x)
∇ ·D⃗ x= 0.
(4.29)
Indeed, this motion can be interpreted as a gradient descent on the Lagrangian, in a frame of reference that moves
with the mode of the distribution of generalised states [14]. Thus, the convexity of the Lagrangian means that any
solution to (4.29) converges to the path of least action. In this setting, the divergence-free flow (i.e., the first term) is
known as aprediction of the generalised state based upon generalised motion, while the curl-free, gradient flow (i.e.,
the second term) is called anupdate.
4.7.2 Particular partitions in generalised coordinates of motion
We now reintroduce the distinction between internal, external, sensory and active statesx = (η, s, a, µ). Briefly, as
before, we assume that the Langevin equation (4.24) is sparsely coupled as in (4.13). This implies that the trajectories
internal and external to the particle are conditionally independent given the trajectories of the blanket (4.15). The
same sparse coupling structure carries through the expansion in generalised coordinates (4.25) so that the motion of
generalised states entails trajectories with the same conditional independencies. Since paths correspond to generalised
states, this yields conditional independence between generalised states, as follows:
(⃗ µ⊥ ⃗ η) |⃗b ⇐⇒ L(⃗ x) = L(⃗ η|⃗b) + L(⃗ µ|⃗b) + L(⃗b). (4.30)
We can now recover paths of least action of the particle by equating the Lagrangian with the variational free energy
of generalised states. This allows us to express the internal path of least action as a gradient flow on variational free
energy, which can itself be expressed in terms of generalised prediction errors. From (4.29), we have
˙⃗ µ(τ) = D⃗ µ− ∇⃗ µL(⃗ x) = D⃗ µ− ∇⃗ µL(⃗ µ|⃗b)
= D⃗ µ− ∇⃗ µL(⃗ π) = D⃗ µ− ∇⃗ µF(⃗ π),
(4.31)
where the free energy of generalised states is analogous to (4.18)
F(⃗ s,⃗ a, ⃗ µ) = Eq[L(⃗ η, ⃗ π)]| {z }
Expected energy
−H[q(⃗ η)]| {z }
Entropy
= Eq[L(⃗ π| ⃗ η)]| {z }
Accuracy
+ D[q(⃗ η)∥p(⃗ η)]| {z }
Complexity
= D[q(⃗ η)∥p(⃗ η| ⃗ π)] + L(⃗ π)
q⃗ µ(⃗ η) = N(⃗ η; − →µ, Σ(− →µ)) = p(⃗ η| ⃗ π) = p(⃗ η|⃗b)
L(⃗ η, ⃗ π) = ε⃗ η· 1
4Γη
ε⃗ η+ ε⃗ s· 1
4Γs
ε⃗ s+ . . .
ε⃗ η≜ D⃗ η− f⃗ η(⃗ η,⃗ s)
ε⃗ s≜ D⃗ s− f⃗ s(⃗ η,⃗ s).
(4.32)
The variational free energy of generalised states is easy to evaluate, given a generative model in the form of a
state-space model [14]; that is, the generalised flow of external and sensory statesf⃗ η, f⃗ s, and the covariance of their
generalised fluctuationsΓη, Γs. Note that the parameterisation of the variational density is very simple: the internal
states parameterise the expected external states. Furthermore, the quadratic form of the Lagrangian means that the
100
variational density over the generalised motion of external states is Gaussian32. This licenses a ubiquitous assumption
in variational Bayes called the Laplace assumption. Please see [177] for a discussion of the simplifications afforded
by the Laplace assumption.
Crucially, intheabsenceofactivestates, thedynamicin(4.31)coincideswithageneralisedBayesianfilter. Generalised
filtering is a generic Bayesian filtering scheme for nonlinear state-space models formulated in generalised coordinates
of motion [14]; special cases include variational filtering [166], dynamic expectation maximisation [167], extended
Kalman filtering [239], and generalised predictive coding.
Furthermore, if the autonomous paths are conditionally independent from external paths, given sensory paths33, the
autonomous paths of least action can be recovered from a generalised gradient descent on variational free energy:
˙⃗ α(τ) = D⃗ α− ∇⃗ αL(⃗ x) = D⃗ α− ∇⃗ αL(⃗ α| ⃗ s)
= D⃗ α− ∇⃗ αL(⃗ π) = D⃗ α− ∇⃗ αF(⃗ π).
(4.33)
In this case, the most likely paths of both internal and active states can be recovered by a gradient descent on
variational free energy, and one can simulate active inference using generalisations of linear quadratic control or
model predictive control [240,241]:


˙⃗ η(τ)
˙⃗ s(τ)
˙⃗ a(τ)
˙⃗ µ(τ)


=


f⃗ η(⃗ η,⃗ s,⃗ a) + ⃗ ωη(τ)
f⃗ s(⃗ η,⃗ s,⃗ a) + ⃗ ωs(τ)
D⃗ a− ∇⃗ aF(⃗ s,⃗ a, ⃗ µ)
D⃗ µ− ∇⃗ µF(⃗ s,⃗ a, ⃗ µ)


(4.34)
This is effectively a (generalised) version of the particular dynamics in (4.23).
Summary
This section has taken a somewhat pragmatic excursion from the FEP narrative to consider generalised coordinates
of motion. This excursion is important because it suggests that the gradient flows in systems with attracting sets are
the paths of least action in Bayesian filters used to assimilate data in statistics [239] and, indeed, control theory [242].
Working in generalised coordinates of motion is effectively working with paths and the path integral formulation.
Practically, this is useful because one can use the density over paths directly to evaluate the requisite free energy
gradients, as opposed to solving the Fokker-Planck equation to find the NESS density. Effectively, the generative
model becomes a state-space model, specified with flows and the statistics of random fluctuations: see (4.32). These
are the sufficient statistics of the joint density over external and sensory paths.
Hitherto, we have largely ignored random fluctuations in the motion of particular states to focus on the underlying
flows. Are these flows ever realised or does the principle of least action in (4.23) only apply to the most likely
autonomous paths? In what follows, we will consider a special class of systems, where we suppress particular
fluctuations to recover the behaviour of particles that show a precise or predictable response to external states. For
this kind of particle, the particular paths are always the paths of least action.
32Question: why is the covariance of the variational density only a function of the internal mode? This follows from the
quadratic Lagrangian that furnishes an analytic solution to the free energy minimum. Please see [177] for details.
33This is the case for precise particles, which are defined by particular fluctuations of infinitesimally small amplitude—see
next Section and [13].
101
4.8 From statistical to classical particles
So far, we have a Bayesian mechanics that would be apt to describe a particle or person with a pullback attractor. But
what is the difference between a particle and a person? This question speaks to distinct classes of things to which the
free energy principle could apply; e.g., molecular versus biological. Here, we associate biotic self-organisation with the
precise and predictable dynamics of large particles. Thanks to the Helmholtz decomposition (4.6), it is known that
when random fluctuations are large, dissipative flow dominates conservative flow, and we have ensembles described
by statistical mechanics (i.e., small particles). Conversely, when random fluctuations have a low amplitude, solenoidal
flow34 dominates and we have classical mechanics and deterministic chaos (i.e., of heavenly andn-body problems).
Here, we consider the distinction between statistical and classical mechanics in the setting of a particular partition.
It is often said that the free energy principle explains why biological systems resist the second law and a natural
tendency to dissipation and disorder [8]. However, this is disingenuous on two counts. First, the second law only
applies to closed systems, while the free energy principle describes open systems in which internal states are exposed
to—and exchange with—external states through blanket states. Second, there is nothing, so far, to suggest that the
entropy of particular states or paths is small. Everything we have done would apply equally to particles with high
and low entropy densities. So, what distinguishes between high and low entropy systems (e.g., between candle flames
and concierges), respectively?
One answer can be found in the path-integral formulation: from (4.5), we can associate the entropy of a path (i.e.,
history or trajectory of particular states) with the amplitude of random fluctuations. This licences the notion of
precise particlesthat are characterised by low or vanishing random fluctuations35. In essence, precise particles are
simply ‘things’ that are subject to the classical laws of nature; i.e., Lagrangian mechanics. In the case of vanishing
fluctuations on particular states, every autonomous trajectory is a path of least action. From (4.5) and (4.23) this
can be expressed as follows:
Γπ ≡ 0
⇒
˙α(τ) = fα(π(τ)) ⇔ δαA(α[τ] | s[τ]) = 0 ⇔ α[τ] = α[τ]
⇒
˙a(τ) = ˙a(τ) = (Qaa − Γa) ∇aF(π) + . . .
˙µ(τ) = ˙µ(τ) = (Qµµ − Γµ) ∇µF(π) + . . .
(4.35)
This suggests that precise particles—such as you and me—will respond to environmental flows and fluctuations in a
precise and predictable fashion. Figure 4.4 illustrates the difference between generic and precise particles using an
information diagram. Note that for precise particles, there is no uncertainty about autonomous states, given sensory
states. This follows because the flow of autonomous states depends only on sensory states and themselves. Is the
behaviour of precise particles a sufficient description of sentient behaviour?
On one reading, perhaps: one can reproduce biological behaviour by numerically integrating (4.23) or (4.34) under a
suitable generative (state-space) model specifying the motion of external and sensory states36. Figure 4.5 illustrates
34And its accompanying correction termΛ, see (4.6).
35Question: but surely my neurons are noisy? There is a substantial literature that refers to neuronal and synaptic noise:
e.g., [243]. However, the population dynamics of neuronal ensembles or assemblies are virtually noiseless by the central limit
theorem (because they comprise thousands of neurons), when averaged over suitable spatial and temporal scales. For example,
in electrophysiology, averaging several fluctuating single trial responses yields surprisingly stable and reproducible event-related
potentials. From the perspective of the FEP, studying single neurons (or trials) is like studying single molecules to characterise
fluid dynamics.
36A generative model can be specified through the flow of external or sensory states, and the random fluctuations of their
102
Figure 4.4: Generic and precise particles. These information diagrams depict the entropy of external, sensory and au-
tonomous paths, where intersections correspond to shared or mutual information. A conditional entropy corresponds to an
area that is outside the variable upon which the entropy is conditioned. The diagram on the left shows the generic case, in
which uncertainty about paths inherits from random fluctuations that determine the conditional entropies of paths. When the
amplitude of random fluctuations on the motion of particular states is very small, we have precise particles in which there is no
uncertainty about autonomous paths, given sensory paths (the right information diagram). Similarly, there is no uncertainty
about sensory paths given external and autonomous paths. Note that because we are dealing with continuous states, we are
implicitly interpreting the entropies as the limiting density of discrete points (LDDP), which have a lower bound of zero [206].
(LDDP is an adjustment to differential entropy which ensures that entropy is lower bounded by zero. LDDP equals the negative
KL-divergence between the density in question and a uniform density). Two relative entropies (information gain and risk) are
highlighted as areas of intersection. These will play an important role later, when decomposing the action (i.e., expected free
energy) of autonomous paths.
the implicit computational architecture used to simulate sentient behaviour by integrating (4.23). This scheme
allows one to simulate the internal and active states through sensory states caused by external dynamics. Figure 4.6
showcases an example from the active inference literature, that integrates (4.34) under a suitably specified generative
model, to simulate sentient behaviour that looks like handwriting. The details of the simulation and the details of
the generative model are not relevant here but are summarised in the figure legend; what is important is to get a
sense of the kind of behaviour that can be reproduced by integrating (4.34).
The example in Figure 4.6 illustrates an application of the free energy principle. Here, instead of describing a system
by deriving its NESS density, we have specified some equations of motion (and covariance of random fluctuations)
to realise particular dynamics using (4.34) and (4.32). In effect, we have simulated self-evidencing, starting from a
definition (i.e., state-space generative model) of paths that characterise this kind of particle37.
These simulations speak to a key aspect in the applications of the FEP. Hitherto, we have simply defined the
variational density as the conditional density over external states given a sensory state. However, when simulating
precise particles through a gradient flow on variational free energy, as in (4.23) or (4.34), the requisite gradients
have to be evaluated. In turn, this requires the functional form of the variational density or posterior distribution,
motion; that is, the first two lines of (4.34). Observing that the free energy (4.31) is only a function of these flows and the
covariance of fluctuations, it is sufficient to specify those covariances, rather than the whole structure of the fluctuations.
37The example in Figure 4.6 used (4.34) with generalised coordinates of motion up to fourth order. Numerical analyses
suggest that simulating generalised motion up to order six (i.e., ignoring all subsequent orders of motions) is sufficient in most
circumstances [167].
103
Figure 4.5: Bayesian mechanics and active inference. This graphic summarises the belief updating implicit in gradient
flows on variational free energy. These are the paths taken by a precise particle or the paths of least action of a generic particle.
It illustrates a simple form of (active) inference that has been used in a variety of applications and simulations; ranging from
handwriting and action observation [244], through to birdsong and generalised synchrony in communication [214]. In brief,
sensory states furnish free energy gradients (often expressed as prediction errors), under some generative model. Neuronal
dynamics are simulated as a flow on the resulting gradients to produce internal states that parameterise posterior beliefs about
external states. Similarly, active states are simulated as a flow on free energy gradients that generally play the role of prediction
errors. In other words, active states mediate motor or autonomic reflexes [245,246]. An example of this kind of active inference
is provided in the next figure.
which may be difficult to compute exactly38. In this case, we take a variational density that approximates the true
posterior, whence the variational free energy becomes an upper bound on surprisal: see (4.19). From the perspective
of Bayesian inference, this takes us from (computationally costly)exact Bayesian inference to (computationally cheap)
approximate Bayesian inference [140,208,250]. On one reading of its inception, this is why variational free energy was
introduced [251]; namely, to convert a computationally expensive marginalisation problem into a computationally
manageableoptimisationproblem. Notethatwhenusinggeneralisedcoordinatestorealiseactiveinference; i.e., (4.34),
we are generally employing approximate Bayesian inference: the functional form of the variational density inherits
directly from Gaussian assumptions about random fluctuations, however the expansion in generalised coordinates on
which it is based upon (4.29) is generally an approximation to the underlying dynamic (cf. 29).
38In Bayesian inference, it is well-known that computing the posterior distribution given data and a generative modelp(η |
π) = p(η, π)/p(π) is computationally costly as it involves computing a (typically) high-dimensional integralp(π) =
R
p(η, π)dη
(i.e., a partition function).
104
Figure 4.6: Sentient behaviour and action observation. This figure illustrates a simulation of active inference (here,
writing) evinced by a precise particle, in terms of inferences about external states of the world, consequent predictions about
sensory input, and ensuing action. The autonomous dynamics that underwrite this behaviour rest upon a generative model
of sensory states in the form of Lotka-Volterra dynamics; see sample sensory trajectories as (arbitrarily) coloured lines in the
upper left inset. The generative model defines the joint density under which internal trajectories can be seen as parameterising
external states. This model is not a description of the true external states (which here are simply the positions of the joints
in the simulated arm—with dynamics given by simple Newtonian rules). In this generative model, external trajectories are
assumed to follow predator-prey like dynamics such that a succession of peaks are generated for a subset of external states
(or coordinates) in turn. Each coordinate is associated with a location in Euclidean space that attracts the agent’s finger (the
active states); i.e., with a trajectory towards that attracting point. The resulting attracting point is thus a weighted sum of each
possible attracting point weighted by the coordinates following the Lotka-Volterra trajectory. In turn, the internal states supply
predictions of what sensory states should register if the agent’s beliefs were true. Active states (i.e., the forces driving changes
in the angular velocities of the limb joints) try to suppress the ensuing prediction error by adjusting expected changes in sensed
angular velocity, through exerting forces on the agent’s joints (not shown). The subsequent movement of the arm is traced
out in the lower-left panel. This trajectory has been plotted in a moving frame of reference so that it looks like handwriting
(e.g., a succession of ‘j’ and ‘a’ letters). The lower right panels show the activity of one internal state during distinct phases of
‘action’, and ‘action-observation’. During the action phase, sensory states register the visual and proprioceptive consequences
of movement, while under action observation, only visual sensations are available—as if the agent was watching another agent.
The red dots correspond to the times during which this internal state exceeded an arbitrary threshold. The key thing to note
here is that this internal state responds preferentially when, and only when, the motor trajectory produces a down-stroke, but
not an up-stroke—evincing a cardinal feature of neuronal responses, namely, their functional selectivity. Furthermore, with a
slight delay, this internal state responds during action and action observation. From a biological perspective, this is interesting
because it speaks to an empirical phenomenon known as mirror neuron activity [247–249]. Please see [244] for further details.
105
Summary
Precise particles, immersed in an imprecise world, respond (almost) deterministically to external fluctuations39. This
means, given a generative model (i.e., NESS density), one can solve the equations of motion in (4.34) to predict
how autonomous states evolve as they pursue their path of least action. So, why might this limiting behaviour be
characteristically biological?
Precise particles may be the kind of particles that show lifelike or biotic behaviour, in the sense they respond
predictably, given their initial states and the history of external influences. The distinction between imprecise (e.g.,
statistical) and precise (e.g., classical) particles rests on the relative contribution of dissipative and conservative flow
to their path through state-space, where solenoidal flow predominates in the precise setting. This means precise
particles exhibit solenoidal behaviour such as oscillatory and (quasi) periodic orbits—and an accompanying loss of
detailed balance, i.e., turbulent and time-irreversible dynamics [195,252,253]. On this view, one might associate
precise particles with living systems with characteristic biorhythms [254–257]; ranging from gamma oscillations in
neuronal populations, through slower respiratory and diurnal cycles to, perhaps, lifecycles per se. Turning this on
its head, one can argue that living systems are a certain kind of particle that, in virtue of being precise, evince
conservative dynamics, biorhythms and time irreversibility.
One might ask if solenoidal flow confounds the gradient flows that underwrite self-evidencing. In fact, solenoidal flow
generally augments gradient flows—or at least this is what it looks like. In brief, the mixing afforded by solenoidal
flow can render gradient descent more efficient [34,60,258–260]. An intuitive example is stirring sugar into coffee. The
mixing afforded by the solenoidal stirring facilitates the dispersion of the sugar molecules down their concentration
gradients. On this view, the solenoidal flow can be regarded as circumnavigating the contours of the steady-state
density to find a path of steepest descent.
The emerging picture here is that biotic systems feature solenoidal flow, in virtue of being sufficiently large to average
away random fluctuations, when coarse-graining their dynamics [113]. From the perspective of the information
geometry induced by the FEP, this means biological behaviour may be characterised by internal solenoidal flows
that do not change variational free energy—or surprisal—and yet move on the internal (statistical) manifold to
continually update Bayesian beliefs about external states. Biologically, this may be a description of central pattern
generators [256,261] that underwrite rhythmical activity (e.g., walking and talking) that is characteristic of biological
systems [262]. The example in Figure 4.6 was chosen to showcase the role of solenoidal flows in Bayesian mechanics
that—in this example—arise from the use of Lotka-Volterra dynamics in the generative model. In psychology, this
kind of conservative active inference may be the homologue of being in a ‘flow state’ [263].
In short, precise particles may be the kind of particles we associate with living systems. And precise particles have
low entropy paths. If so, the question now becomes: what long-term behaviour does this class of particle show? In
other words, instead of asking which behaviourslead to low entropy dynamics, we can now ask which behaviours
follow fromlow entropy dynamics? We will see next that precise particles appear to plan their actions and, perhaps
more interestingly, show information and goal-seeking behaviour.
4.9 Path integrals, planning and curious particles
While the handwriting example in Figure 4.6 offers a compelling simulation of self-evidencing—in the sense of an
artefact creating its own sensorium—there is something missing as a complete account of sentient behaviour. This
39Question: does the absence of random fluctuations preclude dissipative gradient flows? No, because the gradients can
increase with the precision of random fluctuations. In the limit of no random fluctuations, the steady-state density tends
towards a delta function (i.e., a fixed-point attractor) and the dissipative gradients tend towards infinity.
106
is because we have only considered the response of autonomous states to sensory states over limited periods of time.
To disclose a deeper Bayesian mechanics, we need to consider the paths of autonomous states over extended periods.
This takes us to the final step and back to the path-integral formulation.
In the previous section, we focused on linking dynamics to densities over (generalised) states. In brief, we saw that
internal states can be construed as parameterising (Bayesian) beliefs about external states at any point in time. In
what follows, we move from densities overstates to densities overpaths—to characterise the behaviour of particles
in terms of their trajectories.
In what follows, we will be dealing with predictive posterior densities over external and particular paths, given (initial)
particular states, which can be expressed in terms of the variational density parameterised by the current (initial)
internal state:40
q (η[τ], π[τ] | π0) ≜ Eqµ [p (η[τ], π[τ] | η0, π0)] = p (η[τ], π[τ] | π0)
qµ (η0) = p (η0 | π0) .
(4.36)
All this equation says is that, given the initial particular states, we can evaluate the joint density over external and
particular paths, because we know the density over the initial external states, which is parameterised by the initial
internal state.
We are interested in characterising autonomous responses to initial particular states. This is given by the action of
autonomous paths as a function of particular states. In other words, we seek an expression for the probability of an
autonomous path that(i) furnishes a teleological description of self-organisation and(ii) allows us to simulate the
sentient trajectories of particles, given their sensory streams. Getting from the action of particular paths to the action
of autonomous paths requires a marginalisation over sensory paths. This is where the precise particle assumption
comes in: it allows us to eschew this (computationally costly) marginalisation by expressing the action of particular
paths as anexpected free energy.
Recall that when random fluctuations on the motion of particular states vanish, there is no uncertainty about
autonomous paths, given external and sensory paths. And there is no uncertainty about sensory paths given external
and autonomous paths. If we interpret entropies as the limiting density of discrete points (see Figure 4.4), then the
uncertainty about particular, autonomous and sensory paths, given external paths, become interchangeable:
Γπ ≡ 0
⇒
H [p (π[τ] | η[τ], π0)] = H [p (α[τ] | η[τ], s[τ], π0)]| {z }
=0
+ H [p (s[τ] | η[τ], π0)]
= H [p (s[τ] | η[τ], α[τ], π0)]| {z }
=0
+ H [p (α[τ] | η[τ], π0)]
⇒
Eq [ln p (π[τ] | η[τ], π0)] = Eq [ln p (s[τ] | η[τ], π0)] = Eq [ln p (α[τ] | η[τ], π0)]
(4.37)
We can leverage this exchangeability to express the action of autonomous paths in terms of an expected free energy.
40Question: Why is the variational density parameterised by the initial internal state rather than the initial internal mode?
The answer is that in precise particles, the absence of fluctuations on particular dynamics means that the internal states always
coincides with the internal mode.
107
From (4.36) and (4.37), we have (dropping the conditioning on initial states for clarity):
0 = Eq

ln p(η[τ], α[τ])
q(η[τ], α[τ])

= Eq

ln p(α[τ] | η[τ])p(η[τ])
q(η[τ] | α[τ])q(α[τ])

= Eq

ln p(s[τ] | η[τ])p(η[τ])
q(η[τ] | α[τ]) − ln q(α[τ])

= Eq(α[τ])[A(α[τ]) − G(α[τ])]
= D
h
q(α[τ])∥e−G
i
⇒ G(α[τ]) = A(α[τ])
G(α[τ]) = Eq(η[τ],s[τ]α[τ])[
Risk
z }| {
ln q(η[τ] | α[τ]) − ln p(η[τ])| {z }
Expected complexity
Ambiguity
z }| {
− ln p(s[τ] | η[τ])| {z }
Expected accuracy
]
(4.38)
All we have done here is to exchange the density over autonomous paths, conditioned on external paths, with the
corresponding density over sensory paths (in the second line) thanks to the precise particle assumption. By gathering
terms into a functional of autonomous paths, we recover autonomous action as an expected free energy.
By analogy with the expression for variational free energy (4.18), the expressions for the expected free energy in
(4.38) suggest thataccuracy becomes ambiguity, whilecomplexity becomes risk. So why have we called these terms
ambiguity and risk? Ambiguity is just the expected precision or conditional uncertainty about sensory states given
external states. A heuristic example of an imprecise likelihood mapping—between external and sensory paths—would
be a dark room, where there is no precise information at hand. Indeed, according to (4.38), sensory paths into dark
rooms should be highly unlikely. However, this is not the complete story, in the sense that the risk puts certain
constraints on any manifest tendency to minimise ambiguity.
Here, risk is simply the divergence between external paths given an autonomous path (i.e., policy or plan), relative to
external states of affairs. The marginal density over external paths is often referred to in terms ofprior preferences,
because they constitute the priors of the generative model characterising the particle’s behaviour [289]. In short,
the expression for expected free energy, suggests that particles will look as if they are (i) minimising the risk of
incurring external trajectories that diverge from prior preferences, while (ii) resolving ambiguity in response to
external events. In this formulation, autonomous paths play the dual role of registering the influences of external
events (via ambiguity), while also causing those events (via risk).
The autonomous path with the least expected free energy is the most likely path taken by the autonomous states.
G(α[τ]) = A(α[τ])
⇒ α[τ] = arg min
α[τ]
G(α[τ])
⇒ δαG(α[τ]) = 0
E[G(α[τ])] = E[A(α[τ])] = H[p(α[τ])]
(4.39)
In short, expected free energy scores the autonomous action of particles that do not admit noisy dynamics. Expected
free energy has a specific form that inherits from the assumption that the amplitude of particular fluctuations is
small, which is the case for precise articles by definition. Although variational and expected free energy are formally
similar, they are fundamentally different kinds of functionals: variational free energy is a functional of a density over
states, while expected free energy is a functional of a density over paths. Variational free energy can also be read as a
function of particular states, while expected free energy is a function of an autonomous path. Finally, variational free
energy is a bound on surprisal, while expected free energy is not a bound—it is the action of autonomous trajectories.
Expected free energy plays a definitive role in active inference, where it can be regarded as a fairly universal objective
function for selecting autonomous paths of least action. Figure 4.7 shows that the expected free energy contains
terms that arise in various formulations of optimal behaviour; ranging from optimal Bayesian design [277] through to
108
Figure 4.7:Expected free energy and active inference. This figure illustrates active inference, and highlights various points
of contact with other accounts of sentient, purposeful or intelligent behaviour. The upper panel casts action and perception as
the minimisation of expected and variational free energy, respectively. Crucially, the path integral formulation of active inference
introduces posterior beliefs over autonomous paths (i.e., policies) that entail a description of planning as inference [264–266].
When simulating active inference, posterior beliefs about external paths, under plausible policies, are optimised by a gradient
flowonthevariational(freeenergy)boundonlogevidence—asinFigure4.3. Thesebeliefsarethenusedtoevaluatetheexpected
freeenergyofallowablepolicies, fromwhichactionscanbeselected[267–269]. Crucially, expectedfreeenergycontainstermsthat
ariseinvariousformulationsofoptimalbehaviourthatpredominateincognitivescience, engineeringandeconomics. Theseterms
are disclosed when one removes certain sources of uncertainty. For example, if we remove ambiguity, decision-making minimises
risk, which corresponds to aligning predictions with preferences about the external course of events. This underwrites prospect
theoryofhumanchoicebehaviourineconomics[270]andmodernapproachestocontrolasinference[271–273], variouslyknownas
Kalman duality [176,241], KL control [274] and maximum entropy reinforcement learning [275]. If we further remove preferences,
decision-making maximises the entropy of external trajectories. This maximum entropy principle [206,207] can be interpreted
as least committing to a presupposed external trajectory and therefore keeping options open [276]. If we reintroduce ambiguity,
but ignore preferences, decision-making maximises intrinsic value or expected information gain [229]. This underwrites Bayesian
experimental design [277] and active learning in statistics [278], intrinsic motivation and artificial curiosity in machine learning
and robotics [279–283]. This is mathematically equivalent to optimising expected Bayesian surprise and mutual information,
which underwrites visual search [284,285] and the organisation of our visual apparatus [220–222]. Lastly, if we remove intrinsic
value, we are left with maximising extrinsic value or expected utility, which underwrites expected utility theory [219], game
theory, optimal control [286,287] and reinforcement learning [216]. Bayesian formulations of maximising expected utility under
uncertainty are also known as Bayesian decision theory [288]. The expressions for variational and expected free energy are
arranged to illustrate the relationship betweencomplexity and accuracy, which becomerisk and ambiguity in the path integral
formulation. This suggests that risk-averse policies minimise expected complexity or computational cost [280].
109
control as inference [271,275]. We refer the reader to [290–294] for formal investigations of the relationship between
these formulations.
Equipped with a specification of the most likely autonomous path—in terms of expected free energy—we can simulate
fairly lifelike behaviour, given a suitable generative model. An example is provided in Figure 4.9—relying upon the
computational architecture in Figure 4.8—which illustrates the ambiguity resolving part of the expected free energy
in a simulation of visual epistemic foraging.
This epistemic aspect of expected free energy can be seen more clearly if we replace the conditional uncertainty about
sensory paths with conditional uncertainty about particular paths, noting that they are the same by (4.37). After
rearrangement, wecan expressexpectedfree energy interms ofexpected valueandexpected information gain[269,292]:
G(α[τ]) = Eq(η[τ],s[τ]|α[τ])[ln q(η[τ] | α[τ]) − ln p(η[τ]) − ln p(π[τ] | η[τ])]
=
Expected value
z }| {
Eq(s[τ]|α[τ][A(π[τ])]| {z }
Bayes optimal decisions
−
Expected information gain
z }| {
Eq(s[τ]|α[τ]) [D[p(η[τ] | s[τ], α[τ])∥p(η[τ] | α[τ])]| {z }
Bayes optimal design
(4.40)
This provides a complementary interpretation of expected free energy. The first term can be construed as expected
cost in the sense it is the expected action of particular paths. This marginal likelihood scores the plausibility of a
particle pursuing this kind of path and is usually interpreted in terms of expected loss (i.e., negative expected reward
or utility) [216,219], and pragmatic affordance [267,296]. The second term corresponds to the expected divergence
between posterior beliefs about external paths, given autonomous paths, with and without sensory paths. In other
words, it scores the resolution of uncertainty or expected information gain afforded by sensory trajectories arising
from a commitment to an autonomous path. In this sense, it is sometimes referred to as epistemic affordance [172].
When simulating the kind of planning and active inference afforded by the path integral formulation, one usually
works with discrete state-spaces and belief updating over discrete epochs of time [267,268]. One can see this as a
coarse-graining of continuous space-time into discrete space and time bins, where trajectories of continuous states
become sequences of discrete statesx[τ] = ( x1, . . . , xτ ). In discrete state-spaces, the generative model is usually
formulated as a partially observed Markov decision process [238,268,290,301], in which the paths of autonomous
states constitute policies, which determine transitions among external states. Plausible policies can then be scored
with their expected free energy and the next action is selected from the most likely policyα = (α0, . . . , ατ )41
a = arg min
a
G(a, µ)
G = EQ [ln Qµ (η1, . . . , ητ | η0, a) − ln P (η1, . . . , ητ | η0) − ln P (s1, . . . , sτ | η1, . . . , ητ )]
≈
X
t>0
EQ [ln Qµ (ηt | a) − ln P (ηt | η0)]| {z }
Risk
−EQ [ln P (st | ηt)]| {z }
Ambiguity
µ = arg min
µ
F(s, a, µ)
F =
X
t<τ
EQ [ln Qµ (ηt | a) − ln P (ηt+1 | ηt, a) − ln P (ηt | ηt−1, a)]| {z }
Complexity
−
X
t≤0
EQ [ln P (st | ηt)]| {z }
Accuracy
.
(4.41)
The conditional independencies among states implicit in partially observed Markov decision processes entail the above
functional forms for variational and expected free energies [267,268]. Crucially, the posterior over external states uses
a mean-field approximation, in which the joint distribution over current and future states factorises into marginal
distributions at each point in time [this approximation can be finessed by conditioning on previous states, leading
to a different (Bethe) variational free energy [302,303]]. Note that the discrete version of variational free energy is a
41See [238,268] for a derivation of these functional forms in partially observable Markov decision processes.
110
Figure 4.8: Bayesian mechanics and active inference. This graphic summarises the belief updating implicit in the
minimisation of variational and expected free energy. It describes active inference based upon autonomous paths or policies and
has been used in a variety of applications and simulations; ranging from games in behavioural economics [295] and reinforcement
learning [296,297] through to language understanding [298] and scene construction [299]. In this setup, actions solicit a
sensory outcome that informs approximate posterior beliefs about hidden or external states of the world—via minimisation of
variational free energy under a set of plausible policies (i.e., perceptual inference). The approximate posterior beliefs are then
used to evaluate expected free energy and subsequent action (i.e., active inference). A key insight from simulations is that
the form of the generative model can be quite different from the process by which external states generate sensory states. In
effect, this enables agents (i.e., particles) to author their own sensorium in a fashion that has close connections with econiche
construction [300]. Please see [172,268] for technical details and for a heuristic discussion of how the belief updating could be
implemented in the brain.
functional of a distribution over a sequence of states and can be regarded as the discrete homologue of the variational
free energy of generalised states in (4.32).
The ensuing minimisation of free energy can be formulated as gradient flows following (4.17)—between the discrete
arrival of new sensory input—in a way that relates comfortably to neuronal dynamics [203,267,268]. In some
simulations, one can mix discrete and continuous state-space models by placing the former on top of the latter,
to produce deep generative models that, through active inference, can be used to simulate many known aspects of
computational anatomy and physiology in the brain [172].
Summary
In summary, we now have at hand a way of identifying the most likely autonomous trajectory from any initial
particular state that can be used to simulate the sentient behaviour of precise particles that we have associated with
biotic systems. The expected free energy absorbs two aspects of Bayes optimal behaviour into the same (objective)
111
Figure 4.9: Epistemic foraging. This figure shows the results of a numerical simulation where a face was presented to an
agent, whose responses were obtained by selecting active states that minimised expected free energy following an eye movement.
The agent had three internal images or hypotheses (i.e., internal states) about the external state she might sample (an upright
face (blue), an inverted face (magenta) and a rotated face (green)—shown at the bottom). The agent was presented with
sensory samples of an upright face and her variational posterior over the external state was obtained by descending variational
free energy over a 12ms time bin until the next saccade (i.e., action) was emitted. This perception-action cycle was repeated
eight times. The agent’s eye movements are shown as red dots at the end of each saccade in the upper row. The corresponding
sequence of eye movements is shown in the upper-left inset, where the red circles correspond roughly to the proportion of the
visual image sampled. These saccades are driven by the salience maps in the second row, which correspond to the expected
free energy as a function of the policies; namely, the next saccade or where to look next. As expected free energies are defined
in terms of trajectories, it is best to see the locations of on these salience maps as expressing the expected free energy of a
trajectory that ends in that location. Note that these maps change with successive saccades as variational posterior beliefs
become progressively more confident about the external state. Note also that salience is depleted in locations that were foveated
in the previous saccade because these locations no longer have epistemic affordance or expected information gain (i.e., the ability
to reduce uncertainty in the expected free energy). In neuroscience, this empirical phenomenon is known as inhibition of return.
Oculomotor responses are shown in the third row in terms of the two oculomotor states corresponding to vertical and horizontal
eye movements. The associated portions of the image sampled (at the end of each saccade) are shown in the fourth row. The
fifth row shows the evolution of variational posterior beliefs about external (a.k.a. hidden) in terms of the log probability they
assign to each possible external state (colour coded) and 90% confidence intervals. The key thing to note is that the credence
about the true external state supervenes over alternative expectations and, as a result, confidence about the category increases
(and confidence intervals shrink to the mode). This illustrates the nature of evidence accumulation when selecting a hypothesis
or percept that best explains sensory states. Please see [304] for further details.
112
functional [292]. On a Bayesian reading, the expected information gain is exactly the same quantity that underwrites
the principles of optimal Bayesian design [229,277,305]. In other words, the principles that prescribe the best
way to solicit evidence that reduces uncertainty about various hypotheses. The second imperative comes from
Bayesian decision theory, where the objective is to minimise some expected cost function expected under a choice or
decision [288,306,307].
Teleologically, it is worth reflecting upon the differences between the generative models that underwrite state-wise
and path-wise descriptions of Bayesian mechanics, respectively. For the state-wise formulation (4.23), the generative
model is just a joint density over external and particular states, supplied by—or supplying—the NESS density. For
the path-wise formulation (4.34), (4.41), the generative model is a joint distribution over the paths of external and
sensory states. In other words, there is an implicit state-space model of dynamics that can be summarised heuristically
as modelling the consequences of an action on external and sensory dynamics. Because consequences follow causes,
the generative model acquires a temporal depth [298,308]. This depth required to describe any given particle may,
of course, be another characteristic that distinguishes different kinds of particles. In short, the path-wise formulation
describes particles that plan, under a proximal or distal horizon.
4.10 Conclusion
There are many points of contact between the variational formulation above and other normative theories of self-
organisation and purposeful behaviour. However, to focus the narrative we have deliberately suppressed demonstrat-
ing precedents, variants and special cases. Figure 4.3 highlights a few relationships between the free energy principle
and various formulations of self-organisation and sentient behaviour. In brief, this casts things like reinforcement
learning and optimal control theory as optimising the marginal likelihood of particular states, conditioned upon a
generative model supplied by a nonequilibrium steady-state density. It could be argued that the link between the
free energy principle and established formulations is most direct for synergetics [94,309] and related treatments of
dissipative structures [310]. There is also a formal and direct link to information theoretic formulations and Bayesian
statistics. Furthermore, the free energy principle can be regarded as dual to the constrained maximum entropy prin-
ciple [311], where the constraints are supplied by the generative model. Please see [291,293] for a treatment of things
like empowerment [312], information bottleneck [313] and predictive information [101,314].
In a similar vein, there are several accounts of optimal behaviour—in both its epistemic and pragmatic aspects—that
are closely related to the path integral formulation of active inference. Some key relationships are highlighted in Figure
4.7, such as intrinsic motivation, artificial curiosity [279–281] and optimal control [240,242,274]. The interesting thing
about these other theories is that they are predicated on optimising some objective function that can be recovered
from expected free energy by taking various sources of uncertainty off the table. This discloses things like the
objective optimised in reinforcement learning and expected utility theory in behavioural psychology and economics,
respectively [218,315].
This chapter has focused on a single particle and has largely ignored the (external) context that leads to generalised
synchrony among internal and external states. This synchronisation goes hand-in-hand with existenceper seand the
Bayesianmechanicssuppliedbythefreeenergyprinciple. Theveryfactthatthismechanicsrestsuponsynchronisation
may speak to the emergence of synchronisation among formally similar particles; namely, populations or ensembles.
In other words, an individual member of an ensemble or ecosystem owes its existence to the ensemble of which it
is a member—at the level of multicellular organisation or indeed its conspecifics in evolutionary biology [316]. In
a similar vein, the context established by supra- and subordinate scales plays an existential role. In brief, particles
at one scale can only exist if there is a nonequilibrium steady-state density at a higher scale that entails Markov
blankets of Markov blankets [317]. Due to a separation of temporal scales, much of the self-evidencing at one scale is
113
absorbed into the fast, random fluctuations at the scale above. For example, the fast electrophysiological fluctuations
of a neuron become, random fluctuations from the point of view of neuronal population dynamics and sensory motor
coordination in the brain [318–320]. This follows in a straightforward way from applying the apparatus of the
renormalisation group. Please see [113] for further discussion.
For brevity and focus, we have not considered applications of the free energy principle and active inference in detail. A
brief review of the literature in this area will show that that the majority of applications are in the neurosciences [268]
with some exceptions: e.g., [321,322]. Recently, there has been an increasing focus on active inference in the setting
of machine learning and artificial intelligence [26,144,290,293,323–325]. Much of this literature deals with simulation
and modelling and, specifically, scaling active inference to real-world problems. These developments speak to the
shift in focus from the foundational issues addressed in this article to their applications. It is quite possible that
the foundational aspects of the free energy principle may also shift as simpler interpretations and perspectives reveal
themselves.
114
Chapter 5
Conclusion
5.1 Summary of thesis achievements
This thesis has focused on three fundamental aspects of biological entities; namely, entropy production, Bayesian
mechanics, and the free-energy principle. Its main contributions are threefold: 1) Providing a comprehensive mathe-
matical theory of entropy production for stochastic differential equations that describes a greater number of systems
than before, in particular Markovian approximations of stochastic differential equations driven by colored noise. 2)
Developing a theory of Bayesian mechanics, with sufficient and necessary conditions for the internal states of a parti-
cle or biological entity to synchronize and infer the external states consistently with variational Bayesian inference in
statistics and theoretical neuroscience. 3) Refining these conditions to obtain a description that is more exclusive to
biological entities — as opposed to merely physical ones —called the free-energy principle. Perhaps the main practical
outcome here are equations of motions that biological entities must satisfy in virtue of processing a boundary that
separates them from their environment. These equations of motion, in terms of minimizing free energy and expected
free energy can be used for producing simulations of biotic behavior — and in artificial intelligence.
5.2 Applications
The main applications of this thesis is providing tools and formulas for computing entropy production in stochastic
models of biological systems, and affording equations — via the free-energy principle — to simulate practically any
kind of biological behavior. More broadly, the mathematical foundation to the free-energy principle presented here
unlocks a coherent narrative from stochastic process and statistical physics, to cognitive neuroscience and biophysics,
and to artificial intelligence. Indeed the equations provided by the free-energy principle form the basis of a generic
framework for generating sentient behavior calledactive inference, which has been used in a plethora of applications
that include computational psychiatry, psychology and neurology, all the way to artificial intelligence, robotics and
machine learning, passing by simulations of biological populations and ecosystems [268,326]. The mathematical
foundation of the free-energy principle enables a first principles approach to simulating behavior in all of these fields,
which has been taken up by a multitude of research groups worldwide.
115
5.3 Future work
Perhaps the main strength of the free-energy principle – its genericity – is also its main weakness. By deriving a
formal description of biological systems with as few assumptions as possible, we obtain a description of cognition and
behavior that may apply to viruses and the human brain alike. As we move forward with this research program, one
important goal is to refine the class of biological systems that we are willing to study, by singling out constraints
and properties that are fundamental to the human brain and other organisms possessing higher forms of cognition,
to derive refined forms of the principle that are more informative about these sentient beings. Another important
challenge is to map the local computations done by neurons and neural populations in the brain with the overarching
description provided by the free-energy principle, thereby bridging the detailed and coarse-grained descriptions of
brain dynamics [203,327]. A final challenge is to scale the methodologies of active inference to tackle high dimensional
complex problems in robotics and artificial intelligence [324,328]. This challenge will necessitate significant expertise
in scalable Bayesian statistics and computer science.
116
Data availability
Chapter 2: All data and numerical simulations can be reproduced with code freely available athttps:
//github.com/lancelotdacosta/entropy_production_stationary_diffusions.
Chapter 3: All data and numerical simulations can be reproduced with code freely available athttps:
//github.com/conorheins/bayesian-mechanics-sdes.
Chapter 4:All data is contained within the chapter.
Author contributions
Chapter 2: I conceptualized the paper (together with G.A. Pavliotis); I wrote the initial draft; I did the
mathematical analysis and the proofs in their entirety; I wrote the accompanying software, performed the
simulations and produced the graphics; I reviewed the draft (together with G.A. Pavliotis) and edited for
journal submission; I handled journal correspondence and revised the paper for publication.
Chapter 3: I conceptualized the paper (together with co-authors); I wrote the initial draft; I did the
mathematical analysis and the proofs in their entirety; I wrote the accompanying software, performed the
simulations and produced the graphics (all with C. Heins); I reviewed the draft and edited for journal
submission (all with co-authors); I handled journal correspondence and revised the paper for publication.
Chapter 4:I wrote parts of the draft; I did parts of the mathematical analysis and proofs; I reviewed the
draft and edited for journal submission (all with co-authors); I handled journal correspondence and revised
the paper for publication.
117
Bibliography
[1] Karl Friston. The free-energy principle: A unified brain theory?Nature Reviews Neuroscience, 11(2):127–138,
February 2010. (p. 9, 64, 75, 96, and 98.)
[2] Lancelot Da Costa, Pablo Lanillos, Noor Sajid, Karl Friston, and Shujhat Khan. How Active Inference Could
Help Revolutionise Robotics.Entropy, 24(3):361, March 2022. (p. 10.)
[3] Grigorios A. Pavliotis. Stochastic Processes and Applications: Diffusion Processes, the Fokker-Planck and
Langevin Equations. Number volume 60 in Texts in Applied Mathematics. Springer, New York, 2014. (p. 10,
19, 25, 26, 27, 35, 39, 44, 52, 60, 71, 78, 85, 86, 87, and 88.)
[4] Hans Christian Öttinger.Beyond Equilibrium Thermodynamics. Wiley-Interscience, Hoboken, N.J, 1st edition
edition, February 2005. (p. 11, 20, 29, and 31.)
[5] Cédric Villani. Hypocoercivity, volume 202 of Memoirs of the American Mathematical Society. American
Mathematical Society, November 2009. (p. 11, 20, 28, 31, and 53.)
[6] Gülce Kardeş and David H. Wolpert. Thermodynamic Uncertainty Relations for Multipartite Processes.
arXiv:2101.01610 [cond-mat], March 2021. (p. 12 and 64.)
[7] David H. Wolpert. Minimal entropy production due to constraints on rate matrix dependencies in multipartite
processes, May 2020. (p. 12.)
[8] Karl Friston. Life as we know it.Journal of The Royal Society Interface, 10(86):20130475, September 2013.
(p. 12, 64, 80, 81, and 102.)
[9] Karl Friston, Conor Heins, Kai Ueltzhöffer, Lancelot Da Costa, and Thomas Parr. Stochastic Chaos and
Markov Blankets.Entropy, 23(9):1220, September 2021. (p. 12, 26, 64, 71, 81, 85, 90, and 97.)
[10] Conor Heins and Lancelot Da Costa. Sparse coupling and Markov blankets: A comment on "How particular
is the physics of the Free Energy Principle?" by Aguilera, Millidge, Tschantz and Buckley.arXiv:2205.10190
[cond-mat, physics:nlin], May 2022. (p. 12.)
[11] Dalton A. R. Sakthivadivel. Weak Markov Blankets in High-Dimensional, Sparsely-Coupled Random Dynamical
Systems, August 2022. (p. 12.)
[12] Karl Friston, Lancelot Da Costa, Noor Sajid, Conor Heins, Kai Ueltzhöffer, Grigorios A. Pavliotis, and Thomas
Parr. The free energy principle made simpler but not too simple.Physics Reports, 1024:1–29, June 2023. (p. 12.)
[13] Karl Friston, Lancelot Da Costa, Dalton A. R. Sakthivadivel, Conor Heins, Grigorios A. Pavliotis, Maxwell
Ramstead, and Thomas Parr. Path integrals, particular kinds, and strange things.Physics of Life Reviews,
August 2023. (p. 12, 13, 92, and 101.)
[14] Karl Friston, Klaas Stephan, Baojuan Li, and Jean Daunizeau. Generalised Filtering.Mathematical Problems
in Engineering, 2010:1–34, 2010. (p. 12, 78, 80, 92, 96, 100, and 101.)
[15] Da-Quan Jiang, Min Qian, and Ming-Ping Qian.Mathematical Theory of Nonequilibrium Steady States: On
the Frontier of Probability and Dynamical Systems. Lecture Notes in Mathematics. Springer-Verlag, Berlin
Heidelberg, 2004. (p. 18, 19, 21, 22, 26, 29, 33, 44, and 49.)
118
[16] Udo Seifert. Stochastic thermodynamics, fluctuation theorems and molecular machines.Reports on Progress
in Physics, 75(12):126001, November 2012. (p. 18, 85, 86, and 87.)
[17] G. Gallavotti and E. G. D. Cohen. Dynamical Ensembles in Nonequilibrium Statistical Mechanics.Physical
Review Letters, 74(14):2694–2697, April 1995. (p. 18.)
[18] Joel L. Lebowitz and Herbert Spohn. A Gallavotti-Cohen Type Symmetry in the Large Deviation Functional
for Stochastic Dynamics.Journal of Statistical Physics, 95(1/2):333–365, 1999. (p. 18.)
[19] Andreas Dechant. Multidimensional thermodynamic uncertainty relations.Journal of Physics A: Mathematical
and Theoretical, 52(3):035001, December 2018. (p. 18.)
[20] Christopher W. Lynn, Eli J. Cornblath, Lia Papadopoulos, Maxwell A. Bertolero, and Danielle S. Bassett. Bro-
ken detailed balance and entropy production in the human brain.arXiv:2005.02526 [cond-mat, physics:physics,
q-bio], March 2021. (p. 18.)
[21] Robert Graham. Covariant formulation of non-equilibrium statistical thermodynamics.Zeitschrift für Physik
B Condensed Matter, 26(4):397–405, December 1977. (p. 19, 26, 52, and 88.)
[22] Gregory L. Eyink, Joel L. Lebowitz, and Herbert Spohn. Hydrodynamics and fluctuations outside of local
equilibrium: Driven diffusive systems.Journal of Statistical Physics, 83(3):385–472, May 1996. (p. 19, 26, 52,
and 88.)
[23] P. Ao. Potential in stochastic differential equations: Novel construction.Journal of Physics A: Mathematical
and General, 37(3):L25–L30, January 2004. (p. 19, 26, and 88.)
[24] Alessandro Barp, So Takao, Michael Betancourt, Alexis Arnaudon, and Mark Girolami. A Unifying and
Canonical Description of Measure-Preserving Diffusions.arXiv:2105.02845 [math, stat], May 2021. (p. 19, 26,
28, and 88.)
[25] Yi-An Ma, Tianqi Chen, and Emily B. Fox. A Complete Recipe for Stochastic Gradient MCMC.
arXiv:1506.04696 [math, stat], October 2015. (p. 19, 26, and 88.)
[26] Alessandro Barp, Lancelot Da Costa, Guilherme França, Karl Friston, Mark Girolami, Michael I. Jordan, and
Grigorios A. Pavliotis. Geometric Methods for Sampling, Optimisation, Inference and Adaptive Agents. In
Geometry and Statistics, number 46 in Handbook of Statistics, pages 21–78. Academic Press, 2022. (p. 19, 39,
44, and 114.)
[27] Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference, converges to
limit cycles for deep networks. InInternational Conference on Learning Representations, February 2018. (p. 19
and 44.)
[28] Edward Nelson. Dynamical Theories of Brownian Motion. Princeton University Press, 1967. (p. 19 and 26.)
[29] Martin Hairer. On Malliavin’s proof of H\"ormander’s theorem.arXiv:1103.1998 [math], March 2011. (p. 19
and 36.)
[30] A. Millet, D. Nualart, and M. Sanz. Integration by Parts and Time Reversal for Diffusion Processes.The
Annals of Probability, 17(1):208–238, January 1989. (p. 19, 20, 23, and 24.)
[31] Arnaud Guillin and Pierre Monmarché. Optimal linear drift for the speed of convergence of an hypoelliptic
diffusion. arXiv:1604.07295 [math], October 2021. (p. 19, 44, and 45.)
[32] Michela Ottobre. Markov Chain Monte Carlo and Irreversibility.Reports on Mathematical Physics, 77:267–292,
June 2016. (p. 19, 74, and 77.)
[33] Luc Rey-Bellet and Kostantinos Spiliopoulos. Irreversible Langevin samplers and variance reduction: A large
deviation approach.Nonlinearity, 28(7):2081–2103, July 2015. (p. 19, 44, 74, and 77.)
[34] Chii-Ruey Hwang, Shu-Yin Hwang-Ma, and Shuenn-Jyi Sheu. Accelerating diffusions.The Annals of Applied
Probability, 15(2):1433–1444, May 2005. (p. 19, 44, and 106.)
119
[35] Markos Katsoulakis, Yannis Pantazis, and Luc Rey-Bellet. Measuring the Irreversibility of Numerical Schemes
for Reversible Stochastic Differential Equations. ESAIM: Mathematical Modelling and Numerical Analysis -
Modélisation Mathématique et Analyse Numérique, 48(5):1351–1379, 2014. (p. 19, 22, 23, and 42.)
[36] Manh Hong Duong and Michela Ottobre. Non-reversible processes: GENERIC, Hypocoercivity and fluctua-
tions, October 2021. (p. 20, 29, 32, and 45.)
[37] Axel Brünger, Charles L. Brooks, and Martin Karplus. Stochastic boundary conditions for molecular dynamics
simulations of ST2 water.Chemical Physics Letters, 105(5):495–500, March 1984. (p. 20 and 42.)
[38] Mathias Rousset, Gabriel Stoltz, and Tony Lelievre.Free Energy Computations: A Mathematical Perspective.
World Scientific, June 2010. (p. 20, 25, 39, 42, and 45.)
[39] S. R. S. Varadhan. Large deviations and applications. In Persi Diaconis, David Elworthy, Hans Föllmer, Edward
Nelson, George Papanicolaou, Srinivasa R. S. Varadhan, and Paul-Louis Hennequin, editors,École d’Été de
Probabilités de Saint-Flour XV–XVII, 1985–87, Lecture Notes in Mathematics, pages 1–49, Berlin, Heidelberg,
1988. Springer. (p. 21.)
[40] Tan Van Vu and Yoshihiko Hasegawa. Uncertainty relations for underdamped Langevin dynamics.Physical
Review E, 100(3):032130, September 2019. (p. 22 and 45.)
[41] Jean-Pierre Eckmann, Claude-Alain Pillet, and Luc Rey-Bellet. Entropy Production in Nonlinear, Thermally
Driven Hamiltonian Systems.Journal of Statistical Physics, 95(1):305–331, April 1999. (p. 22 and 45.)
[42] Richard E. Spinney and Ian J. Ford. Nonequilibrium Thermodynamics of Stochastic Systems with Odd and
Even Variables.Physical Review Letters, 108(17):170603, April 2012. (p. 22 and 45.)
[43] Dominique Bakry, Ivan Gentil, and Michel Ledoux.Analysis and Geometry of Markov Diffusion Operators.
Grundlehren Der Mathematischen Wissenschaften. Springer International Publishing, 2014. (p. 23.)
[44] U. G. Haussmann and E. Pardoux. Time Reversal of Diffusions.Annals of Probability, 14(4):1188–1205, October
1986. (p. 23 and 24.)
[45] Claudia Prévôt and Michael Rockner.A Concise Course on Stochastic Partial Differential Equations. Springer
Berlin Heidelberg, Berlin, 2007th edition edition, October 2008. (p. 24, 34, and 54.)
[46] S. Watanabe and N. Ikeda.Stochastic Differential Equations and Diffusion Processes. North Holland, Amster-
dam ; New York : Tokyo : New York, NY, 0 edition edition, February 1981. (p. 24.)
[47] Jeremy Quastel. Time Reversal of Degenerate Diffusions. In Vladas Sidoravicius, editor,In and Out of Equi-
librium: Probability with a Physics Flavor, Progress in Probability, pages 249–257. Birkhäuser, Boston, MA,
2002. (p. 25.)
[48] H. Föllmer. Time reversal on Wiener space. In Sergio A. Albeverio, Philippe Blanchard, and Ludwig Streit,
editors, Stochastic Processes — Mathematics and Physics, LectureNotesinMathematics, pages119–129, Berlin,
Heidelberg, 1986. Springer. (p. 25.)
[49] Patrick Cattiaux, Giovanni Conforti, Ivan Gentil, and Christian Léonard. Time reversal of diffusion processes
under a finite entropy condition, April 2021. (p. 25.)
[50] Ya. Belopolskaya. Time Reversal of Diffusion Processes in Hilbert Spaces and Manifolds. In N. Balakrishnan,
I.A.Ibragimov, andV.B.Nevzorov, editors, Asymptotic Methods in Probability and Statistics with Applications,
Statistics for Industry and Technology, pages 65–79. Birkhäuser, Boston, MA, 2001. (p. 25.)
[51] Annie Millet, David Nualart, and Marta Sanz. Time reversal for infinite-dimensional diffusions.Probability
Theory and Related Fields, 82(3):315–347, August 1989. (p. 25.)
[52] H. Föllmer and A. Wakolbinger. Time reversal of infinite-dimensional diffusions.Stochastic Processes and their
Applications, 22(1):59–77, May 1986. (p. 25.)
120
[53] Masao Nagasawa and Thomas Domenig. Diffusion processes on an open time interval and their time reversal.
In Nobuyuki Ikeda, Shinzo Watanabe, Masatoshi Fukushima, and Hiroshi Kunita, editors,Itô’s Stochastic
Calculus and Probability Theory, pages 261–280. Springer Japan, Tokyo, 1996. (p. 25.)
[54] Patrick Cattiaux. Time reversal of diffusion processes with a boundary condition.Stochastic Processes and
their Applications, 28(2):275–292, June 1988. (p. 25.)
[55] E. Pardoux. Time-reversal of diffusion processes and non-linear smoothing. In Arunabha Bagchi and Huber-
tus Theodorus Jongen, editors,Systems and Optimization, Lecture Notes in Control and Information Sciences,
pages 171–181, Berlin, Heidelberg, 1985. Springer. (p. 25.)
[56] Yi-An Ma, Niladri S. Chatterji, Xiang Cheng, Nicolas Flammarion, Peter L. Bartlett, and Michael I. Jordan.
Is there an analog of Nesterov acceleration for gradient-based MCMC?Bernoulli, 27(3):1942–1992, May 2021.
(p. 25 and 39.)
[57] Hong Qian. A decomposition of irreversible diffusion processes without detailed balance.Journal of Mathe-
matical Physics, 54(5):053302, May 2013. (p. 26.)
[58] Lancelot Da Costa, Karl Friston, Conor Heins, and Grigorios A. Pavliotis. Bayesian mechanics for sta-
tionary processes. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences,
477(2256):20210518, December 2021. (p. 26, 85, 90, 92, 96, and 97.)
[59] Ying-Jen Yang and Yu-Chen Cheng. Potentials of continuous Markov processes and random perturbations.
Journal of Physics A: Mathematical and Theoretical, 54(19):195001, April 2021. (p. 26.)
[60] Tony Lelièvre, Francis Nier, and Grigorios A. Pavliotis. Optimal non-reversible linear drift for the convergence
to equilibrium of a diffusion.Journal of Statistical Physics, 152(2):237–274, July 2013. (p. 26, 44, 77, and 106.)
[61] Kösaku Yosida. Functional Analysis. Classics in Mathematics. Springer-Verlag, Berlin Heidelberg, 6 edition,
1995. (p. 29 and 49.)
[62] Pazy. Semigroups of Linear Operators and Applications to Partial Differential Equations. Springer-Verlag New
York Inc., softcover reprint of the original 1st ed. 1983 edition, October 2011. (p. 29 and 49.)
[63] Djalil Chafaï. Entropies, convexity, and functional inequalities, On $\Phi $-entropies and $\Phi $-Sobolev
inequalities. Journal of Mathematics of Kyoto University, 44(2):325–363, January 2004. (p. 32.)
[64] G. E. Uhlenbeck and L. S. Ornstein. On the Theory of the Brownian Motion.Physical Review, 36(5):823–841,
September 1930. (p. 36.)
[65] Claude Godrèche and Jean-Marc Luck. Characterising the nonequilibrium stationary states of Ornstein-
Uhlenbeck processes. Journal of Physics A: Mathematical and Theoretical, 52(3):035002, January 2019. (p. 36
and 37.)
[66] Alain Mazzolo and Cécile Monthus. Nonequilibrium diffusion processes via non-Hermitian electromagnetic
quantum mechanics with application to the statistics of entropy production in the Brownian gyrator.Physical
Review E, 107(1):014101, January 2023. (p. 37.)
[67] Johan du Buisson and Hugo Touchette. Dynamical large deviations of linear diffusions, December 2022. (p. 37.)
[68] Luca Lorenzi, Marcello Bertoldi, and Marcello Bertoldi.Analytical Methods for Markov Semigroups. Chapman
and Hall/CRC, July 2006. (p. 37.)
[69] Luc Rey-Bellet. Open Classical Systems. In Stéphane Attal, Alain Joye, and Claude-Alain Pillet, editors,Open
Quantum Systems II: The Markovian Approach, Lecture Notes in Mathematics, pages 41–78. Springer, Berlin,
Heidelberg, 2006. (p. 39 and 78.)
[70] Jonathan C. Mattingly, Andrew M. Stuart, and M. V. Tretyakov. Convergence of Numerical Time-Averaging
and Stationary Measures via Poisson Equations.SIAM Journal on Numerical Analysis, 48(2):552–577, January
2010. (p. 42 and 70.)
121
[71] J. C. Mattingly, A. M. Stuart, and D. J. Higham. Ergodicity for SDEs and approximations: Locally Lipschitz
vector fields and degenerate noise.Stochastic Processes and their Applications, 101(2):185–232, October 2002.
(p. 42.)
[72] Denis Talay. Stochastic Hamiltonian Systems: Exponential Convergence to the Invariant Measure, and Dis-
cretization by the Implicit Euler Scheme.Markov Processes and Related Fields, (8):1–36, 2002. (p. 42.)
[73] Ben Leimkuhler and Charles Matthews.Molecular Dynamics: With Deterministic and Stochastic Numerical
Methods. Springer, Cham, 2015th edition edition, June 2015. (p. 44.)
[74] A. B. Duncan, T. Lelièvre, and G. A. Pavliotis. Variance Reduction Using Nonreversible Langevin Samplers.
Journal of Statistical Physics, 163(3):457–491, May 2016. (p. 44.)
[75] Radford M. Neal. Improving Asymptotic Variance of MCMC Estimators: Non-reversible Chains are Better.
arXiv:math/0407281, July 2004. (p. 44.)
[76] Sheng-Jhih Wu, Chii-Ruey Hwang, and Moody T. Chu. Attaining the Optimal Gaussian Diffusion Acceleration.
Journal of Statistical Physics, 155:571–590, May 2014. (p. 44.)
[77] David Luposchainsky and Haye Hinrichsen. Entropy Production in Continuous Phase Space Systems.Journal
of Statistical Physics, 153(5):828–841, December 2013. (p. 45.)
[78] A. B. Duncan, N. Nüsken, and G. A. Pavliotis. Using Perturbed Underdamped Langevin Dynamics to Ef-
ficiently Sample from Probability Distributions. Journal of Statistical Physics, 169(6):1098–1131, December
2017. (p. 46.)
[79] Thomas G. Kurtz. Equivalence of Stochastic Equations and Martingale Problems. In Dan Crisan, editor,
Stochastic Analysis 2010, pages 113–130. Springer, Berlin, Heidelberg, 2011. (p. 50.)
[80] Hannes Risken and Till Frank.The Fokker-Planck Equation: Methods of Solution and Applications. Springer
Series in Synergetics. Springer-Verlag, Berlin Heidelberg, 2 edition, 1996. (p. 52 and 85.)
[81] Real analysis - Every divergence-free vector field generated from skew-symmetric matrix.
https://math.stackexchange.com/questions/578898/every-divergence-free-vector-field-generated-from-skew-
symmetric-matrix. (p. 52.)
[82] Ying-Jen Yang and Hong Qian. Bivectorial Nonequilibrium Thermodynamics: Cycle Affinity, Vorticity Poten-
tial, and Onsager’s Principle.Journal of Statistical Physics, 182(3):46, February 2021. (p. 52.)
[83] Giuseppe Da Prato and Jerzy Zabczyk.Stochastic Equations in Infinite Dimensions. Cambridge University
Press, April 2014. (p. 55, 58, and 59.)
[84] Johannes Ruf. A new proof for the conditions of Novikov and Kazamaki. Stochastic Processes and their
Applications, 123(2):404–421, February 2013. (p. 59.)
[85] Claude Dellacherie and Paul-Andre Meyer.Probabilities and Potential: Theory of Martingales Pt. B. Elsevier
Science Ltd, Amsterdam, December 1982. (p. 59.)
[86] Simon Prokop. Topological Support of Solutions to Stochastic Differential Equations. Master’s thesis, Charles
University in Prague, Prague, 2016. (p. 60.)
[87] John Duchi. Derivations for Linear Algebra and Optimization. (p. 61.)
[88] Casper Hesp, Maxwell Ramstead, Axel Constant, Paul Badcock, Michael Kirchhoff, and Karl Friston. A Multi-
scale View of the Emergent Complexity of Life: A Free-Energy Proposal. In Georgi Yordanov Georgiev, John M.
Smart, Claudio L. Flores Martinez, and Michael E. Price, editors,Evolution, Development and Complexity,
Springer Proceedings in Complexity, pages 195–227, Cham, 2019. Springer International Publishing. (p. 63,
64, and 80.)
[89] Michael Kirchhoff, Thomas Parr, Ensor Palacios, Karl Friston, and Julian Kiverstein. The Markov blankets
of life: Autonomy, active inference and the free energy principle. Journal of The Royal Society Interface,
15(138):20170792, January 2018. (p. 63 and 64.)
122
[90] Judea Pearl. Graphical Models for Probabilistic and Causal Reasoning. In Philippe Smets, editor,Quantified
Representation of Uncertainty and Imprecision, Handbook of Defeasible Reasoning and Uncertainty Manage-
ment Systems, pages 367–389. Springer Netherlands, Dordrecht, 1998. (p. 63, 65, 79, and 87.)
[91] Christopher M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics.
Springer, New York, 2006. (p. 63, 65, 93, and 94.)
[92] G. Nicolis and I. Prigogine.Self-Organization in Nonequilibrium Systems: From Dissipative Structures to Order
Through Fluctuations. Wiley-Blackwell, New York, June 1977. (p. 64, 88, and 98.)
[93] Albert Goldbeter. Dissipative structures in biological systems: Bistability, oscillations, spatial patterns and
waves. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences,
376(2124):20170376, July 2018. (p. 64.)
[94] Hermann Haken. Synergetics: An Introduction Nonequilibrium Phase Transitions and Self-Organization in
Physics, Chemistry and Biology. Springer Series in Synergetics. Springer-Verlag, Berlin Heidelberg, 2 edition,
1978. (p. 64, 85, 88, 98, and 113.)
[95] Nikolay Perunov, Robert A. Marsland, and Jeremy L. England. Statistical Physics of Adaptation.Physical
Review X, 6(2):021036, June 2016. (p. 64.)
[96] Kate Jeffery, Robert Pollack, and Carlo Rovelli. On the statistical mechanics of life: Schr\"odinger revisited.
arXiv:1908.08374 [physics], August 2019. (p. 64.)
[97] Jeremy L. England. Statistical physics of self-replication.The Journal of Chemical Physics, 139(12):121923,
August 2013. (p. 64.)
[98] Dominic J. Skinner and Jörn Dunkel. Improved bounds on entropy production in living systems.Proceedings
of the National Academy of Sciences, 118(18), May 2021. (p. 64.)
[99] Benjamin Dunn and Yasser Roudi. Learning and inference in a nonequilibrium Ising model with hidden nodes.
Physical Review E, 87(2):022127, February 2013. (p. 64.)
[100] Susanne Still. Thermodynamic Cost and Benefit of Memory.Physical Review Letters, 124(5):050601, February
2020. (p. 64.)
[101] Susanne Still, David A. Sivak, Anthony J. Bell, and Gavin E. Crooks. Thermodynamics of Prediction.Physical
Review Letters, 109(12):120604, September 2012. (p. 64 and 113.)
[102] Kai Ueltzhöffer. On the thermodynamics of prediction under dissipative adaptation.arXiv:2009.04006 [cond-
mat, q-bio], September 2020. (p. 64.)
[103] David H. Wolpert. Minimal entropy production rate of interacting systems. New Journal of Physics,
22(11):113013, November 2020. (p. 64.)
[104] David H. Wolpert. Uncertainty Relations and Fluctuation Theorems for Bayes Nets.Physical Review Letters,
125(20):200602, November 2020. (p. 64.)
[105] GavinE.CrooksandSusanneStill. Marginalandconditionalsecondlawsofthermodynamics. EPL (Europhysics
Letters), 125(4):40005, March 2019. (p. 64.)
[106] Jordan M. Horowitz and Massimiliano Esposito. Thermodynamics with Continuous Information Flow.Physical
Review X, 4(3):031015, July 2014. (p. 64.)
[107] Alexandre Pouget, Peter Dayan, and Richard S. Zemel. Inference and computation with population codes.
Annual Review of Neuroscience, 26(1):381–410, March 2003. (p. 64.)
[108] David C. Knill and Alexandre Pouget. The Bayesian brain: The role of uncertainty in neural coding and
computation. Trends in Neurosciences, 27(12):712–719, December 2004. (p. 64.)
[109] Rajesh P. N. Rao and Dana H. Ballard. Predictive coding in the visual cortex: A functional interpretation of
some extra-classical receptive-field effects.Nature Neuroscience, 2(1):79–87, January 1999. (p. 64 and 72.)
123
[110] Karl J. Friston, Jean Daunizeau, James Kilner, and Stefan J. Kiebel. Action and behavior: A free-energy
formulation. Biological Cybernetics, 102(3):227–260, March 2010. (p. 64, 76, 81, and 96.)
[111] Karl J. Friston, Erik D. Fagerholm, Tahereh S. Zarghami, Thomas Parr, Inês Hipólito, Loïc Magrou, and Adeel
Razi. Parcels and particles: Markov blankets in the brain.arXiv:2007.09704 [q-bio], July 2020. (p. 64.)
[112] Thomas Parr, Lancelot Da Costa, and Karl Friston. Markov blankets, information geometry and stochastic
thermodynamics. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering
Sciences, 378(2164):20190159, February 2020. (p. 64, 75, 93, and 96.)
[113] Karl Friston. A free energy principle for a particular physics.arXiv:1906.10184 [q-bio], June 2019. (p. 64, 75,
84, 85, 96, 106, and 114.)
[114] Martin J. Wainwright and Michael I. Jordan. Graphical Models, Exponential Families, and Variational Infer-
ence. Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2007. (p. 65 and 79.)
[115] Morris L Eaton.Multivariate Statistics: A Vector Space Approach. Institute of mathematical statistics, Beach-
wood, Ohio, 2007. (p. 66.)
[116] Thomas Parr.The Computational Neurology of Active Vision. PhD thesis, University College London, London,
2019. (p. 67.)
[117] Markus Meister and Michael J. Berry. The Neural Code of the Retina.Neuron, 22(3):435–450, March 1999.
(p. 67.)
[118] M. James. The generalised inverse.The Mathematical Gazette, 62(420):109–114, June 1978. (p. 68.)
[119] Miguel Aguilera, Beren Millidge, Alexander Tschantz, and Christopher L. Buckley. How particular is the physics
of the free energy principle?Physics of Life Reviews, 40:24–50, March 2022. (p. 68 and 71.)
[120] L. C. G. Rogers and David Williams.Diffusions, Markov Processes, and Martingales: Volume 1: Foundations,
volume 1 ofCambridge Mathematical Library. Cambridge University Press, Cambridge, 2 edition, 2000. (p. 71.)
[121] Joris Bierkens, Paul Fearnhead, and Gareth Roberts. The Zig-Zag process and super-efficient sampling for
Bayesian analysis of big data.The Annals of Statistics, 47(3):1288–1320, June 2019. (p. 71.)
[122] Joris Bierkens and Gareth Roberts. A piecewise deterministic scaling limit of lifted Metropolis–Hastings in the
Curie–Weiss model.The Annals of Applied Probability, 27(2):846–882, April 2017. (p. 71.)
[123] Alexandre Bouchard-Côté, Sebastian J. Vollmer, and Arnaud Doucet. The Bouncy Particle Sampler: A Non-
reversible Rejection-Free Markov Chain Monte Carlo Method.Journal of the American Statistical Association,
113(522):855–867, April 2018. (p. 71.)
[124] Carl Edward Rasmussen. Gaussian Processes in Machine Learning. In Olivier Bousquet, Ulrike von Luxburg,
and Gunnar Rätsch, editors,Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra,
Australia, February 2 - 14, 2003, Tübingen, Germany, August 4 - 16, 2003, Revised Lectures, Lecture Notes in
Computer Science, pages 63–71. Springer, Berlin, Heidelberg, 2004. (p. 71.)
[125] Ludwig Arnold. Random Dynamical Systems. Springer Monographs in Mathematics. Springer-Verlag, Berlin
Heidelberg, 1998. (p. 71, 84, 85, and 87.)
[126] Martin Biehl, Felix A. Pollock, and Ryota Kanai. A Technical Critique of Some Parts of the Free Energy
Principle. Entropy, 23(3):293, March 2021. (p. 71.)
[127] Karl J. Friston, Lancelot Da Costa, and Thomas Parr. Some Interesting Observations on the Free Energy
Principle. Entropy, 23(8):1076, August 2021. (p. 71.)
[128] Haider Hasan Jafri, R. K. Brojen Singh, and Ramakrishna Ramaswamy. Generalized synchrony of coupled
stochastic processes with multiplicative noise.Physical Review E, 94(5):052216, November 2016. (p. 71 and 96.)
[129] D. Cumin and C. P. Unsworth. Generalising the Kuramoto model for the study of neuronal synchronisation in
the brain. Physica D: Nonlinear Phenomena, 226(2):181–196, February 2007. (p. 71.)
124
[130] Ensor Rafael Palacios, Takuya Isomura, Thomas Parr, and Karl Friston. The emergence of synchrony in
networks of mutually inferring neurons.Scientific Reports, 9(1):6412, April 2019. (p. 71.)
[131] Stephen D. Bartlett, Terry Rudolph, and Robert W. Spekkens. Reconstruction of Gaussian quantum mechanics
from Liouville mechanics with an epistemic restriction.Physical Review A, 86(1):012103, July 2012. (p. 72.)
[132] S. Kullback and R. A. Leibler. On Information and Sufficiency. The Annals of Mathematical Statistics,
22(1):79–86, March 1951. (p. 72.)
[133] Rafal Bogacz. A tutorial on the free-energy framework for modelling perception and learning. Journal of
Mathematical Psychology, 76:198–211, February 2017. (p. 72, 81, and 98.)
[134] Karl Friston and Stefan Kiebel. Predictive coding under the free-energy principle.Philosophical Transactions
of the Royal Society B: Biological Sciences, 364(1521):1211–1221, May 2009. (p. 72 and 81.)
[135] Zenas C. Chao, Kana Takaura, Liping Wang, Naotaka Fujii, and Stanislas Dehaene. Large-Scale Cortical
Networks for Hierarchical Prediction and Prediction Error in the Primate Brain.Neuron, 100(5):1252–1266.e3,
May 2018. (p. 72.)
[136] SandraIglesias, ChristophMathys, KayH.Brodersen, LarsKasper, MarcoPiccirelli, HannekeE.M.denOuden,
and Klaas E. Stephan. Hierarchical Prediction Errors in Midbrain and Basal Forebrain during Sensory Learning.
Neuron, 80(2):519–530, October 2013. (p. 72.)
[137] Nathaniel D. Daw, Samuel J. Gershman, Ben Seymour, Peter Dayan, and Raymond J. Dolan. Model-Based
Influences on Humans’ Choices and Striatal Prediction Errors.Neuron, 69(6):1204–1215, March 2011. (p. 72.)
[138] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational Inference: A Review for Statisticians.
Journal of the American Statistical Association, 112(518):859–877, April 2017. (p. 72 and 75.)
[139] Christopher L. Buckley, Chang Sub Kim, Simon McGregor, and Anil K. Seth. The free energy principle for
action and perception: A mathematical review.Journal of Mathematical Psychology, 81:55–79, December 2017.
(p. 72, 76, 81, 96, and 98.)
[140] Matthew James Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, University of
London, 2003. (p. 75, 94, and 104.)
[141] Odelia Schwartz, Jonathan W. Pillow, Nicole C. Rust, and Eero P. Simoncelli. Spike-triggered neural charac-
terization. Journal of Vision, 6(4):484–507, July 2006. (p. 75.)
[142] R. J. Sayer, M. J. Friedlander, and S. J. Redman. The time course and amplitude of EPSPs evoked at synapses
between pairs of CA3/CA1 neurons in the hippocampal slice. The Journal of Neuroscience: The Official
Journal of the Society for Neuroscience, 10(3):826–836, March 1990. (p. 75.)
[143] Steven J. Luck.An Introduction to the Event-Related Potential Technique. A Bradford Book, Cambridge, MA,
USA, 2 edition, May 2014. (p. 75.)
[144] Kai Ueltzhöffer. Deep Active Inference. Biological Cybernetics, 112(6):547–573, December 2018. (p. 76, 96,
and 114.)
[145] Beren Millidge. Deep active inference as variational policy gradients. Journal of Mathematical Psychology,
96:102348, June 2020. (p. 76.)
[146] R. Conor Heins, M. Berk Mirza, Thomas Parr, Karl Friston, Igor Kagan, and Arezoo Pooresmaeili. Deep Active
Inference and Scene Construction.Frontiers in Artificial Intelligence, 3:81, 2020. (p. 76.)
[147] Pablo Lanillos, Jordi Pages, and Gordon Cheng. Robot self/other distinction: Active inference meets neural
networks learning in a mirror. InEuropean Conference on Artificial Intelligence. IOS press, April 2020. (p. 76.)
[148] Tim Verbelen, Pablo Lanillos, Christopher Buckley, and Cedric De Boom, editors.Active Inference: First
International Workshop, IWAI 2020, Co-located with ECML/PKDD 2020, Ghent, Belgium, September 14,
2020, Proceedings. Communications in Computer and Information Science. Springer International Publishing,
2020. (p. 76.)
125
[149] Rick A. Adams, Stewart Shipp, and Karl J. Friston. Predictions not commands: Active inference in the motor
system. Brain Structure & Function, 218(3):611–643, May 2013. (p. 76 and 81.)
[150] Corrado Pezzato, Riccardo Ferrari, and Carlos Hernández Corbato. A Novel Adaptive Controller for Robot
Manipulators Based on Active Inference.IEEE Robotics and Automation Letters, 5(2):2973–2980, April 2020.
(p. 76.)
[151] Guillermo Oliver, Pablo Lanillos, and Gordon Cheng. An empirical study of active inference on a humanoid
robot. IEEE Transactions on Cognitive and Developmental Systems, pages 1–1, 2021. (p. 76.)
[152] Magnus T. Koudahl and Bert de Vries. A Worked Example of Fokker-Planck-Based Active Inference. In Tim
Verbelen, Pablo Lanillos, Christopher L. Buckley, and Cedric De Boom, editors,Active Inference, Communi-
cations in Computer and Information Science, pages 28–34, Cham, 2020. Springer International Publishing.
(p. 76 and 96.)
[153] Karl Friston. What Is Optimal about Motor Control?Neuron, 72(3):488–498, November 2011. (p. 76.)
[154] Cansu Sancaktar, Marcel van Gerven, and Pablo Lanillos. End-to-End Pixel-Based Deep Active Inference for
Body Perception and Action.arXiv:2001.05847 [cs, q-bio], May 2020. (p. 76.)
[155] Manuel Baltieri and Christopher L. Buckley. PID Control as a Process of Active Inference with Linear Gener-
ative Models. Entropy, 21(3):257, March 2019. (p. 76, 77, 79, and 80.)
[156] R. Kosinski. A Literature Review on Reaction Time Kinds of Reaction Time Experiments. 2012. (p. 78.)
[157] Tony Roskilly and Dr Rikard Mikalsen.Marine Systems Identification, Modeling and Control. Butterworth-
Heinemann, Amsterdam ; Boston, illustrated edition edition, March 2015. (p. 78.)
[158] Karl Johan Åström.Pid Controllers. International Society for Measurement and Control, January 1995. (p. 77
and 79.)
[159] Sanjoy Mitter, Giorgio Picci, and Anders Lindquist. Toward a theory of nonlinear stochastic realization. In
Feedback and Synthesis of Linear and Nonlinear Systems, 1981. (p. 77, 79, and 97.)
[160] Anders Lindquist and Giorgio Picci.Linear Stochastic Systems: A Geometric Approach to Modeling, Estimation
and Identification. Series in Contemporary Mathematics. Springer-Verlag, Berlin Heidelberg, 2015. (p. 78.)
[161] Anders Lindquist and Giorgio Picci. Realization Theory for Multivariate Stationary Gaussian Processes.SIAM
Journal on Control and Optimization, 23(6):809–857, November 1985. (p. 78 and 97.)
[162] J. L. Doob. The Brownian Movement and Stochastic Equations.Annals of Mathematics, 43(2):351–369, 1942.
(p. 78.)
[163] Ming Chen Wang and G. E. Uhlenbeck. On the Theory of the Brownian Motion II. InSelected Papers on Noise
and Stochastic Processes. Dover, 2014. (p. 78.)
[164] Mikhail Kryachkov, Andrey Polyakov, and Vadim Strygin. Finite-time stabilization of an integrator chain using
only signs of the state variables. In2010 11th International Workshop on Variable Structure Systems (VSS),
pages 510–515, June 2010. (p. 78.)
[165] Konstantin Zimenko, Andrey Polyakov, Denis Efimo, and Wilfrid Perruquetti. Finite-time and fixed-time
stabilization for integrator chain of arbitrary order*. In 2018 European Control Conference (ECC), pages
1631–1635, June 2018. (p. 78.)
[166] K. J. Friston. Variational filtering.NeuroImage, 41(3):747–766, July 2008. (p. 78, 79, and 101.)
[167] K. J. Friston, N. Trujillo-Barreto, and J. Daunizeau. DEM: A variational treatment of dynamic systems.
NeuroImage, 41(3):849–885, July 2008. (p. 78, 101, and 103.)
[168] Thomas Parr, Jakub Limanowski, Vishal Rawji, and Karl Friston. The computational neurology of movement
under active inference.Brain, 144(6):1799–1818, June 2021. (p. 78 and 80.)
126
[169] S. N. Gomes, G. A. Pavliotis, and U. Vaes. Mean Field Limits for Interacting Diffusions with Colored Noise:
Phase Transitions and Spectral Numerical Methods.Multiscale Modeling & Simulation, 18(3):1343–1370, Jan-
uary 2020. (p. 78 and 79.)
[170] T. J. S. Tayor and M. Pavon. On the nonlinear stochastic realization problem.Stochastics and Stochastic
Reports, 26(2):65–79, February 1989. (p. 79.)
[171] A. E. Frazho. On stochastic realization theory.Stochastics, 7(1-2):1–27, January 1982. (p. 79.)
[172] Karl J. Friston, Thomas Parr, and Bert de Vries. The graphical brain: Belief propagation and active inference.
Network Neuroscience, 1(4):381–414, December 2017. (p. 80, 110, and 111.)
[173] Michael Chevalier, Mariana Gómez-Schiavon, Andrew H. Ng, and Hana El-Samad. Design and Analysis of a
Proportional-Integral-Derivative Controller with Biological Molecules.Cell Systems, 9(4):338–353.e10, October
2019. (p. 80.)
[174] Tau-Mu Yi, Yun Huang, Melvin I. Simon, and John Doyle. Robust perfect adaptation in bacterial chemotaxis
through integral feedback control. Proceedings of the National Academy of Sciences, 97(9):4649–4653, April
2000. (p. 80.)
[175] Karl Friston. Hierarchical Models in the Brain.PLoS Computational Biology, 4(11):e1000211, November 2008.
(p. 80 and 81.)
[176] R. E. Kalman. A New Approach to Linear Filtering and Prediction Problems.Journal of Basic Engineering,
82(1):35–45, March 1960. (p. 80 and 109.)
[177] Karl Friston, Jérémie Mattout, Nelson Trujillo-Barreto, John Ashburner, and Will Penny. Variational free
energy and the Laplace approximation.NeuroImage, 34(1):220–234, January 2007. (p. 81 and 101.)
[178] Karl Friston. A theory of cortical responses. Philosophical Transactions of the Royal Society B: Biological
Sciences, 360(1456):815–836, April 2005. (p. 81.)
[179] Giovanni Pezzulo. An Active Inference view of cognitive control.Frontiers in Psychology, 3, 2012. (p. 81.)
[180] Jean-Philippe Pellet and André Elisseeff. Using Markov Blankets for Causal Structure Learning.Journal of
Machine Learning Research, 9(43):1295–1342, 2008. (p. 81.)
[181] Belinda Tzen and M. Raginsky. Neural Stochastic Differential Equations: Deep Latent Gaussian Models in the
Diffusion Limit. ArXiv, 2019. (p. 81.)
[182] Roger A. Horn. Matrix Analysis: Second Edition. Cambridge University Press, New York, NY, 2nd edition
edition, December 2012. (p. 82.)
[183] Andy Clark. Whatever next? Predictive brains, situated agents, and the future of cognitive science. The
Behavioral and Brain Sciences, 36(3):181–204, June 2013. (p. 84.)
[184] Hans Crauel and Franco Flandoli. Attractors for random dynamical systems.Probability Theory and Related
Fields, 100(3):365–393, September 1994. (p. 84, 85, and 87.)
[185] Jakob Hohwy. The Self-Evidencing Brain.Noûs, 50(2):259–285, June 2016. (p. 84 and 96.)
[186] E. Noether. Invarianten beliebiger Differentialausdrücke.Nachrichten von der Gesellschaft der Wissenschaften
zu Göttingen, Mathematisch-Physikalische Klasse, 1918:37–44, 1918. (p. 84.)
[187] Hans Crauel. Global random attractors are uniquely determined by attracting deterministic compact sets.
Annali di Matematica Pura ed Applicata, 176(1):57–72, December 1999. (p. 85.)
[188] C. Jarzynski. Nonequilibrium Equality for Free Energy Differences.Physical Review Letters, 78(14):2690–2693,
April 1997. (p. 85.)
[189] Jack Carr. Applications of Centre Manifold Theory. 1982. (p. 85, 93, and 94.)
127
[190] T. Koide. Perturbative expansion of irreversible work in Fokker–Planck equation$\less$i$\greater$à
la$\less$/i$\greater$quantum mechanics.Journal of Physics A: Mathematical and Theoretical, 50(32):325001,
July 2017. (p. 85.)
[191] Detlef Dürr and Alexander Bach. The Onsager-Machlup function as Lagrangian for the most probable path of
a diffusion process.Communications in Mathematical Physics, 60(2):153–170, June 1978. (p. 86.)
[192] ErwinSchrodinger. What Is Life?: With Mind and Matter and Autobiographical Sketches. CambridgeUniversity
Press, Cambridge ; New York, reprint edition edition, March 2012. (p. 87.)
[193] Judea Pearl. Causality. Cambridge University Press, Cambridge, U.K. ; New York, 2nd edition edition,
September 2009. (p. 87.)
[194] Jianghong Shi, Tianqi Chen, Ruoshi Yuan, Bo Yuan, and Ping Ao. Relation of a New Interpretation of
Stochastic Differential Equations to Ito Process.Journal of Statistical Physics, 148(3):579–590, August 2012.
(p. 88.)
[195] Lancelot Da Costa and Grigorios A. Pavliotis. The entropy production of stationary diffusions, 2022. (p. 88
and 106.)
[196] Ruoshi Yuan, Yian Ma, Bo Yuan, and Ping Ao. Potential function in dynamical systems and the relation
with Lyapunov function. InProceedings of the 30th Chinese Control Conference, pages 6573–6580, July 2011.
(p. 88.)
[197] Mark Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo methods.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2):123–214, 2011. (p. 88.)
[198] Shun-ichi Amari. Natural Gradient Works Efficiently in Learning. page 36, 1998. (p. 88.)
[199] W.C. Kerr and A.J. Graham. Generalized phase space version of Langevin equations and associated Fokker-
Planck equations.The European Physical Journal B - Condensed Matter and Complex Systems, 15(2):305–311,
May 2000. (p. 92.)
[200] J. M. Lee.Introduction to Topological Manifolds. Springer, New York, NY, 2011. (p. 92.)
[201] Nihat Ay, Jürgen Jost, Hông Vân Lê, and Lorenz Schwachhöfer.Information Geometry, volume 64 ofErgebnisse
Der Mathematik Und Ihrer Grenzgebiete 34. Springer International Publishing, Cham, 2017. (p. 93.)
[202] S. Amari. Information Geometry and Its Applications. Springer, 2016. (p. 93.)
[203] Lancelot Da Costa, Thomas Parr, Biswa Sengupta, and Karl Friston. Neural Dynamics under Active Inference:
Plausibility and Efficiency of Information Processing.Entropy, 23(4):454, April 2021. (p. 93, 111, and 116.)
[204] Shun-ichi Amari and Hiroshi Nagaoka. Methods of Information Geometry, volume 191 of Translations of
Mathematical Monographs. American Mathematical Society, April 2007. (p. 93.)
[205] Kai Ueltzhöffer, Lancelot Da Costa, and Karl J. Friston. Variational free energy, individual fitness, and pop-
ulation dynamics under acute stress: Comment on “Dynamic and thermodynamic models of adaptation” by
Alexander N. Gorban et al.Physics of Life Reviews, 37:111–115, July 2021. (p. 94.)
[206] E. T. Jaynes. Information Theory and Statistical Mechanics. Physical Review, 106(4):620–630, May 1957.
(p. 94, 103, and 109.)
[207] Andrzej Lasota and Michael C. MacKey.Chaos, Fractals, and Noise: Stochastic Aspects of Dynamics. Springer-
Verlag, 1994. (p. 94 and 109.)
[208] John Winn and Christopher M Bishop. Variational Message Passing.Journal of Machine Learning Research,
page 34, 2005. (p. 94, 96, and 104.)
[209] Christoph J. G. Lang, O. Kneidl, M. Hielscher-Fastabend, and J. G. Heckmann. Voice recognition in aphasic
and non-aphasic stroke patients.Journal of Neurology, 256(8):1303–1306, August 2009. (p. 94.)
128
[210] Peter E. Kloeden and Eckhard Platen. Numerical Solution of Stochastic Differential Equations. Stochastic
Modelling and Applied Probability. Springer-Verlag, Berlin Heidelberg, 1992. (p. 94.)
[211] Steven J. Schiff and Tim Sauer. Kalman filter control of a model of spatiotemporal cortical dynamics.Journal
of Neural Engineering, 5(1):1–8, March 2008. (p. 96.)
[212] Brian R. Hunt, Edward Ott, and James A. Yorke. Differentiable generalized synchronization of chaos.Physical
Review E, 55(4):4029–4034, April 1997. (p. 96.)
[213] Victor Buendía, Pablo Villegas, Raffaella Burioni, and Miguel A. Muñoz. The broad edge of synchronization:
Griffiths effects and collective phenomena in brain networks.Philosophical Transactions of the Royal Society
A: Mathematical, Physical and Engineering Sciences, 380(2227):20200424, July 2022. (p. 96.)
[214] Karl J. Friston and Christopher D. Frith. Active inference, communication and hermeneutics.Cortex; a Journal
Devoted to the Study of the Nervous System and Behavior, 68:129–143, July 2015. (p. 96 and 104.)
[215] Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. Reinforcement Learning or Active Inference?PLoS
ONE, 4(7):e6421, July 2009. (p. 96.)
[216] Andrew Barto and Richard Sutton.Reinforcement Learning: An Introduction. 1992. (p. 98, 109, and 110.)
[217] Emanuel Todorov and Michael I. Jordan. Optimal feedback control as a theory of motor coordination.Nature
Neuroscience, 5(11):1226–1235, November 2002. (p. 98.)
[218] Peter Bossaerts and Carsten Murawski. From behavioural economics to neuroeconomics to decision neuro-
science: The ascent of biology in research on human decision making.Current Opinion in Behavioral Sciences,
5:37–42, October 2015. (p. 98 and 113.)
[219] J. Von Neumann and O. Morgenstern. Theory of Games and Economic Behavior. Theory of Games and
Economic Behavior. Princeton University Press, Princeton, NJ, US, 1944. (p. 98, 109, and 110.)
[220] L. M. Optican and B. J. Richmond. Temporal encoding of two-dimensional patterns by single units in primate
inferiortemporalcortex.III.Informationtheoreticanalysis. Journal of Neurophysiology, 57(1):162–178, January
1987. (p. 98 and 109.)
[221] R Linsker. Perceptual Neural Organization: Some Approaches Based on Network Models and Information
Theory. Annual Review of Neuroscience, 13(1):257–281, 1990. (p. 98 and 109.)
[222] H. B. Barlow.Possible Principles Underlying the Transformations of Sensory Messages. The MIT Press, 1961.
(p. 98 and 109.)
[223] Karl Friston, James Kilner, and Lee Harrison. A free energy principle for the brain.Journal of Physiology-Paris,
100(1-3):70–87, July 2006. (p. 98.)
[224] Stuart A. Kauffman. The Origins of Order: Self-organization and Selection in Evolution. Oxford University
Press, 1993. (p. 98.)
[225] W. R. Ashby. Principles of the Self-Organizing Dynamic System.The Journal of General Psychology, 37(2):125–
128, October 1947. (p. 98.)
[226] Roger C Conant and W. R. Ashby. Every good regulator of a system must be a model of that system.Int. J.
Systems Sci., 1(2):89–97, 1970. (p. 98.)
[227] Claude Bernard. Lectures on the Phenomena of Life Common to Animals and Plants. Thomas, 1974. (p. 98.)
[228] D. J. C. MacKay. Free energy minimisation algorithm for decoding and cryptanalysis.Electronics Letters,
31(6):446–447, March 1995. (p. 98.)
[229] David J. C. MacKay.Information Theory, Inference and Learning Algorithms. Cambridge University Press,
Cambridge, UK ; New York, sixth printing 2007 edition edition, September 2003. (p. 98, 109, and 113.)
129
[230] Hermann von Helmholtz and James P. C Southall.Helmholtz’s Treatise on Physiological Optics.Dover Publi-
cations, New York, 1962. (p. 98.)
[231] R. L. Gregory. Perceptions as hypotheses.Philosophical Transactions of the Royal Society of London. Series
B, Biological Sciences, 290(1038):181–197, July 1980. (p. 98.)
[232] Peter Dayan, Geoffrey E. Hinton, Radford M. Neal, and Richard S. Zemel. The Helmholtz Machine.Neural
Computation, 7(5):889–904, September 1995. (p. 98.)
[233] R. Landauer. Irreversibility and Heat Generation in the Computing Process.IBM Journal of Research and
Development, 5(3):183–191, July 1961. (p. 98.)
[234] Eugene Wong and Moshe Zakai. On the relation between ordinary and stochastic differential equations.Inter-
national Journal of Engineering Science, 3(2):213–229, July 1965. (p. 97.)
[235] Bhashyam Balaji and Karl Friston. Bayesian state estimation using generalized coordinates. In Ivan Kadar,
editor, SPIE Defense, Security, and Sensing, page 80501Y, Orlando, Florida, United States, May 2011. (p. 97.)
[236] R. Biscay, J. C. Jimenez, J. J. Riera, and P. A. Valdes. Local Linearization method for the numerical solution
of stochastic differential equations.Annals of the Institute of Statistical Mathematics, 48(4):631–644, December
1996. (p. 97.)
[237] D. R. Cox and H. D. Miller.The Theory of Stochastic Processes. Chapman and Hall/CRC, London, February
1977. (p. 97.)
[238] Thomas Parr, Giovanni Pezzulo, and Karl J. Friston.Active Inference: The Free Energy Principle in Mind,
Brain, and Behavior. MIT Press, Cambridge, MA, USA, March 2022. (p. 99 and 110.)
[239] H.-A. Loeliger. Least Squares and Kalman Filtering on Forney Graphs. In Richard E. Blahut and Ralf Koetter,
editors, Codes, Graphs, and Systems: A Celebration of the Life and Career of G. David Forney, Jr. on the
Occasion of His Sixtieth Birthday, The Kluwer International Series in Engineering and Computer Science, pages
113–135. Springer US, Boston, MA, 2002. (p. 101.)
[240] H. J. Kappen. Path integrals and symmetry breaking for optimal control theory.Journal of Statistical Me-
chanics: Theory and Experiment, 2005(11):P11011–P11011, November 2005. (p. 101 and 113.)
[241] Emanuel Todorov. General duality between optimal control and estimation. In2008 47th IEEE Conference on
Decision and Control, pages 4286–4292, December 2008. (p. 101 and 109.)
[242] Bart van den Broek, Wim Wiegerinck, and Bert Kappen. Risk sensitive path integral control.UAI, 2010.
(p. 101 and 113.)
[243] Hazem Toutounji and Gordon Pipa. Spatiotemporal Computations of an Excitable and Plastic Brain: Neu-
ronal Plasticity Leads to Noise-Robust and Noise-Constructive Computations.PLOS Computational Biology,
10(3):e1003512, March 2014. (p. 102.)
[244] Karl Friston, Jérémie Mattout, and James Kilner. Action understanding and active inference. Biological
Cybernetics, 104(1-2):137–160, February 2011. (p. 104 and 105.)
[245] Anatol G. Feldman. New insights into action-perception coupling.Experimental Brain Research, 194(1):39–58,
March 2009. (p. 104.)
[246] Warren Mansell. Control of Perception Should be Operationalized as a Fundamental Property of the Nervous
System. Topics in Cognitive Science, 3(2):257–261, 2011. (p. 104.)
[247] Vittorio Gallese and Alvin Goldman. Mirror neurons and the simulation theory of mind-reading.Trends in
Cognitive Sciences, 2(12):493–501, December 1998. (p. 105.)
[248] Giacomo Rizzolatti and Laila Craighero. The mirror-neuron system.Annual Review of Neuroscience, 27:169–
192, 2004. (p. 105.)
130
[249] James M. Kilner, Karl J. Friston, and Chris D. Frith. Predictive coding: An account of the mirror neuron
system. Cognitive Processing, 8(3):159–166, September 2007. (p. 105.)
[250] Justin Dauwels. On Variational Message Passing on Factor Graphs. In2007 IEEE International Symposium
on Information Theory, pages 2546–2550, Nice, June 2007. IEEE. (p. 104.)
[251] Richard Feynman.Statistical Mechanics: A Set Of Lectures. Westview Press, Boulder, Colo, 1st edition edition,
March 1998. (p. 104.)
[252] Daniela Andres. On the Motion of Spikes: Turbulent-Like Neuronal Activity in the Human Basal Ganglia.
Frontiers in Human Neuroscience, 12, 2018. (p. 106.)
[253] Gustavo Deco and Morten L. Kringelbach. Turbulent-like Dynamics in the Human Brain. Cell Reports,
33(10):108471, December 2020. (p. 106.)
[254] F. Lopes da Silva. Neural mechanisms underlying brain waves: From neural membranes to networks.Elec-
troencephalography and Clinical Neurophysiology, 79(2):81–93, August 1991. (p. 106.)
[255] N. Kopell, M. A. Whittington, and M. A. Kramer. Neuronal assembly dynamics in the beta1 frequency range
permits short-term memory.Proceedings of the National Academy of Sciences, 108(9):3779–3784, March 2011.
(p. 106.)
[256] Luc H. Arnal and Anne-Lise Giraud. Cortical oscillations and sensory predictions.Trends in Cognitive Sciences,
16(7):390–398, July 2012. (p. 106.)
[257] György Buzsáki, Nikos Logothetis, and Wolf Singer. Scaling brain size, keeping timing: Evolutionary preser-
vation of brain rhythms.Neuron, 80(3):751–764, October 2013. (p. 106.)
[258] Edward Ott, Celso Grebogi, and James A. Yorke. Controlling chaos.Physical Review Letters, 64(11):1196–1199,
March 1990. (p. 106.)
[259] Chii-Ruey Hwang, Shu-Yin Hwang-Ma, and Shuenn-Jyi Sheu. Accelerating Gaussian Diffusions.The Annals
of Applied Probability, 3(3):897–913, August 1993. (p. 106.)
[260] N. Aslimani and R. Ellaia. A new hybrid algorithm combining a new chaos optimization approach with gradient
descent for high dimensional optimization problems.Computational and Applied Mathematics, 37(3):2460–2488,
July 2018. (p. 106.)
[261] Joachim Gross, Nienke Hoogenboom, Gregor Thut, Philippe Schyns, Stefano Panzeri, Pascal Belin, and Si-
mon Garrod. Speech rhythms and multiplexed oscillatory sensory coding in the human brain.PLoS biology,
11(12):e1001752, December 2013. (p. 106.)
[262] György Buzsáki and Andreas Draguhn. Neuronal oscillations in cortical networks.Science (New York, N.Y.),
304(5679):1926–1929, June 2004. (p. 106.)
[263] Mihaly Csikszentmihalyi. Flow: The Psychology of Optimal Experience. HarperCollins e-books, 1st edition
edition, August 2008. (p. 106.)
[264] Hagai Attias. Planning by Probabilistic Inference. In9th Int. Workshop on Artificial Intelligence and Statistics,
page 8, 2003. (p. 109.)
[265] Matthew Botvinick and Marc Toussaint. Planning as inference.Trends in Cognitive Sciences, 16(10):485–488,
October 2012. (p. 109.)
[266] Raphael Kaplan and Karl J. Friston. Planning and navigation as active inference. Biological Cybernetics,
112(4):323–343, August 2018. (p. 109.)
[267] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pezzulo. Active
Inference: A Process Theory.Neural Computation, 29(1):1–49, January 2017. (p. 109, 110, and 111.)
131
[268] Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and Karl Friston. Active
inference on discrete state-spaces: A synthesis.Journal of Mathematical Psychology, 99:102447, December 2020.
(p. 109, 110, 111, 114, and 115.)
[269] Alessandro Barp, Lancelot Da Costa, Guilherme França, Karl Friston, Mark Girolami, Michael I. Jordan,
and Grigorios A. Pavliotis. Geometric Methods for Sampling, Optimisation, Inference and Adaptive Agents.
volume 46, pages 21–78. 2022. (p. 109 and 110.)
[270] Daniel Kahneman and Amos Tversky. Prospect Theory: An Analysis of Decision under Risk.Econometrica,
47(2):263–291, 1979. (p. 109.)
[271] Sergey Levine. Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review.
arXiv:1805.00909 [cs, stat], May 2018. (p. 109 and 110.)
[272] Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On Stochastic Optimal Control and Reinforcement
Learning by Approximate Inference. InTwenty-Third International Joint Conference on Artificial Intelligence,
June 2013. (p. 109.)
[273] Marc Toussaint. Robot trajectory optimization using approximate inference. InProceedings of the 26th Annual
International Conference on Machine Learning, ICML ’09, pages 1049–1056, Montreal, Quebec, Canada, June
2009. Association for Computing Machinery. (p. 109.)
[274] Hilbert J. Kappen, Vicenç Gómez, and Manfred Opper. Optimal control as a graphical model inference problem.
Machine Learning, 87(2):159–182, May 2012. (p. 109 and 113.)
[275] B. Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy.PhD
thesis, Carnegie Mellon University, Pittsburgh, 2010. (p. 109 and 110.)
[276] Alexander S. Klyubin, Daniel Polani, and Chrystopher L. Nehaniv. Keep Your Options Open: An Information-
Based Driving Principle for Sensorimotor Systems.PLOS ONE, 3(12):e4018, December 2008. (p. 109.)
[277] D. V. Lindley. On a Measure of the Information Provided by an Experiment.The Annals of Mathematical
Statistics, 27(4):986–1005, 1956. (p. 108, 109, and 113.)
[278] David J. C. MacKay. Information-Based Objective Functions for Active Data Selection.Neural Computation,
4(4):590–604, July 1992. (p. 109.)
[279] Pierre-Yves Oudeyer and Frederic Kaplan. What is Intrinsic Motivation? A Typology of Computational
Approaches. Frontiers in Neurorobotics, 1:6, November 2007. (p. 109 and 113.)
[280] Jürgen Schmidhuber. Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990–2010).IEEE Trans-
actions on Autonomous Mental Development, 2(3):230–247, September 2010. (p. 109 and 113.)
[281] Andrew Barto, Marco Mirolli, and Gianluca Baldassarre. Novelty or Surprise? Frontiers in Psychology, 4,
2013. (p. 109 and 113.)
[282] Yi Sun, Faustino Gomez, and Juergen Schmidhuber. Planning to Be Surprised: Optimal Bayesian Exploration
in Dynamic Environments.arXiv:1103.5708 [cs, stat], March 2011. (p. 109.)
[283] Edward Deci and Richard M. Ryan.Intrinsic Motivation and Self-Determination in Human Behavior. Per-
spectives in Social Psychology. Springer US, New York, 1985. (p. 109.)
[284] Laurent Itti and Pierre Baldi. Bayesian surprise attracts human attention.Vision research, 49(10):1295–1306,
May 2009. (p. 109.)
[285] Thomas Parr, Noor Sajid, Lancelot Da Costa, M. Berk Mirza, and Karl J. Friston. Generative Models for
Active Vision.Frontiers in Neurorobotics, 15, 2021. (p. 109.)
[286] Richard E. Bellman.Dynamic Programming. Princeton University Press, Princeton, NJ, US, 1957. (p. 109.)
[287] K. J Åström. Optimal control of Markov processes with incomplete state information.Journal of Mathematical
Analysis and Applications, 10(1):174–205, February 1965. (p. 109.)
132
[288] James O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer Series in Statistics. Springer-
Verlag, New York, 2 edition, 1985. (p. 109 and 113.)
[289] Thomas Parr and Karl J. Friston. Generalised free energy and active inference. Biological Cybernetics,
113(5):495–513, December 2019. (p. 108.)
[290] Lancelot Da Costa, Noor Sajid, Thomas Parr, Karl Friston, and Ryan Smith. Reward Maximization Through
Discrete Active Inference.Neural Computation, 35(5):807–852, April 2023. (p. 110 and 114.)
[291] Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, and Thomas Parr. Sophisticated Inference.
Neural Computation, 33(3):713–763, February 2021. (p. 110 and 113.)
[292] Noor Sajid, Lancelot Da Costa, Thomas Parr, and Karl Friston. Active inference, Bayesian optimal design,
and expected utility. InThe Drive for Knowledge: The Science of Human Information Seeking. 2022. (p. 110
and 113.)
[293] Danijar Hafner, Pedro A. Ortega, Jimmy Ba, Thomas Parr, Karl Friston, and Nicolas Heess. Action and
Perception as Divergence Minimization.arXiv:2009.01791 [cs, math, stat], October 2020. (p. 110, 113, and 114.)
[294] Beren Millidge, Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. On the Relationship Between
Active Inference and Control as Inference. In Tim Verbelen, Pablo Lanillos, Christopher L. Buckley, and Cedric
De Boom, editors,Active Inference, Communications in Computer and Information Science, pages 3–11, Cham,
2020. Springer International Publishing. (p. 110.)
[295] Thomas H. B. FitzGerald, Philipp Schwartenbeck, Michael Moutoussis, Raymond J. Dolan, and Karl Friston.
Active inference, evidence accumulation, and the urn task.Neural Computation, 27(2):306–328, February 2015.
(p. 111.)
[296] Philipp Schwartenbeck, Thomas H. B. FitzGerald, Christoph Mathys, Ray Dolan, Martin Kronbichler, and Karl
Friston. Evidence for surprise minimization over value maximization in choice behavior.Scientific Reports,
5:16575, November 2015. (p. 110 and 111.)
[297] Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. Active Inference: Demystified and Compared.
Neural Computation, 33(3):674–712, January 2021. (p. 111.)
[298] Karl J. Friston, Richard Rosch, Thomas Parr, Cathy Price, and Howard Bowman. Deep temporal models and
active inference.Neuroscience & Biobehavioral Reviews, 90:486–501, July 2018. (p. 111 and 113.)
[299] M. Berk Mirza, Rick A. Adams, Christoph D. Mathys, and Karl J. Friston. Scene Construction, Visual Foraging,
and Active Inference.Frontiers in Computational Neuroscience, 10, June 2016. (p. 111.)
[300] Jelle Bruineberg and Erik Rietveld. Self-organization, free energy minimization, and optimal grip on a field of
affordances. Frontiers in Human Neuroscience, 8, 2014. (p. 111.)
[301] Ryan Smith, Karl J. Friston, and Christopher J. Whyte. A step-by-step tutorial on active inference and its
application to empirical data.Journal of Mathematical Psychology, 107:102632, April 2022. (p. 110.)
[302] J.S. Yedidia, W.T. Freeman, and Y. Weiss. Constructing Free-Energy Approximations and Generalized Belief
Propagation Algorithms. IEEE Transactions on Information Theory, 51(7):2282–2312, July 2005. (p. 110.)
[303] Thomas Parr, Dimitrije Markovic, Stefan J. Kiebel, and Karl J. Friston. Neuronal message passing using
Mean-field, Bethe, and Marginal approximations.Scientific Reports, 9(1):1889, December 2019. (p. 110.)
[304] Karl Friston, Rick Adams, Laurent Perrinet, and Michael Breakspear. Perceptions as Hypotheses: Saccades as
Experiments. Frontiers in Psychology, 3, 2012. (p. 112.)
[305] Stefano Balietti, Brennan Klein, and Christoph Riedl. Optimal design of experiments to identify latent behav-
ioral types. Experimental Economics, 24(3):772–799, September 2021. (p. 113.)
[306] Abraham Wald. An Essentially Complete Class of Admissible Decision Functions.The Annals of Mathematical
Statistics, 18(4):549–555, December 1947. (p. 113.)
133
[307] Lawrence D. Brown. A Complete Class Theorem for Statistical Problems with Finite Sample Spaces.The
Annals of Statistics, 9(6):1289–1300, November 1981. (p. 113.)
[308] Izzet B. Yildiz, Katharina von Kriegstein, and Stefan J. Kiebel. From Birdsong to Human Speech Recog-
nition: Bayesian Inference on a Hierarchy of Nonlinear Dynamical Systems.PLOS Computational Biology,
9(9):e1003219, September 2013. (p. 113.)
[309] HermannHakenandJuvalPortugali. InformationandSelforganization: AUnifyingApproachandApplications.
Entropy, 18(6):197, June 2016. (p. 113.)
[310] Ilya Prigogine. Time, Structure, and Fluctuations.Science, 201(4358):777–785, September 1978. (p. 113.)
[311] Dalton A. R. Sakthivadivel. Entropy-Maximising Diffusions Satisfy a Parallel Transport Law, January 2023.
(p. 113.)
[312] A.S. Klyubin, D. Polani, and C.L. Nehaniv. Empowerment: A universal agent-centric measure of control. In
2005 IEEE Congress on Evolutionary Computation, volume 1, pages 128–135 Vol.1, September 2005. (p. 113.)
[313] Naftali Tishby, Fernando C. Pereira, and W. Bialek. The information bottleneck method.ArXiv, 2000. (p. 113.)
[314] Susanne Still and Doina Precup. An information-theoretic approach to curiosity-driven reinforcement learning.
Theory in Biosciences = Theorie in Den Biowissenschaften, 131(3):139–148, September 2012. (p. 113.)
[315] Matthew M. Botvinick, Yael Niv, and Andew G. Barto. Hierarchically organized behavior and its neural
foundations: A reinforcement learning perspective.Cognition, 113(3):262–280, December 2009. (p. 113.)
[316] Santosh Manicka and Michael Levin. Modeling somatic computation with non-neural bioelectric networks.
Scientific Reports, 9(1):18612, December 2019. (p. 113.)
[317] Ensor Rafael Palacios, Adeel Razi, Thomas Parr, Michael Kirchhoff, and Karl Friston. On Markov blankets
and hierarchical self-organisation.Journal of Theoretical Biology, 486:110089, February 2020. (p. 113.)
[318] J. A. Scott Kelso. Dynamic Patterns: The Self-Organization of Brain and Behavior. Complex Adaptive
Systems. A Bradford Book, Cambridge, MA, USA, April 1995. (p. 114.)
[319] Gustavo Deco, Viktor K. Jirsa, Peter A. Robinson, Michael Breakspear, and Karl Friston. The Dynamic Brain:
From Spiking Neurons to Neural Masses and Cortical Fields. PLoS Computational Biology, 4(8):e1000092,
August 2008. (p. 114.)
[320] J. A. Scott Kelso. Unifying Large- and Small-Scale Theories of Coordination.Entropy, 23(5):537, May 2021.
(p. 114.)
[321] Karl Friston, Michael Levin, Biswa Sengupta, and Giovanni Pezzulo. Knowing one’s place: A free-energy
approach to pattern regulation.Journal of The Royal Society Interface, 12(105):20141383, April 2015. (p. 114.)
[322] MaxwellJamesDésormeauRamstead, PaulBenjaminBadcock, andKarlJohnFriston. AnsweringSchrödinger’s
question: A free-energy formulation.Physics of Life Reviews, 24:1–16, March 2018. (p. 114.)
[323] Alexander Tschantz, Manuel Baltieri, Anil K. Seth, and Christopher L. Buckley. Scaling active inference.
arXiv:1911.10601 [cs, eess, math, stat], November 2019. (p. 114.)
[324] Zafeirios Fountas, Noor Sajid, Pedro A. M. Mediano, and Karl Friston. Deep active inference agents using
Monte-Carlo methods. arXiv:2006.04176 [cs, q-bio, stat], June 2020. (p. 114 and 116.)
[325] Ozan Çatal, Tim Verbelen, Toon Van de Maele, Bart Dhoedt, and Adam Safron. Robot navigation as hierar-
chical active inference.Neural Networks, 142:192–204, October 2021. (p. 114.)
[326] Sergio Rubin, Thomas Parr, Lancelot Da Costa, and Karl Friston. Future climates: Markov blankets and active
inference in the biosphere.Journal of The Royal Society Interface, 17(172):20200503, November 2020. (p. 115.)
[327] Takuya Isomura, Hideaki Shimazaki, and Karl J. Friston. Canonical neural networks perform active inference.
Communications Biology, 5(1):1–15, January 2022. (p. 116.)
134
[328] Pablo Lanillos, Cristian Meo, Corrado Pezzato, Ajith Anil Meera, Mohamed Baioumy, Wataru Ohata, Alexan-
der Tschantz, Beren Millidge, Martijn Wisse, Christopher L. Buckley, and Jun Tani. Active Inference in
Robotics and Artificial Agents: Survey and Challenges.arXiv:2112.01871 [cs], December 2021. (p. 116.)
135