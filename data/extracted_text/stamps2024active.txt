Active Inference and Artificial Spin Ice:
Control Processes and State Selection
Robert L. Stamps,∗ Rehana Begum Popy, and Johan van Lierop∗
Department of Physics & Astronomy, University of Manitoba, Winnipeg, Canada
E-mail: Robert.Stamps@umanitoba.ca; Johan.van.Lierop@umanitoba.ca
Abstract
A numerical model of interacting nanomagnetic elements is used to demonstrate
active inference with a three dimensional Artificial Spin Ice structure. It is shown that
thermal fluctuations can drive this magnetic spin system to evolve under dynamic con-
straints imposed through interactions with an external environment as predicted by
the neurological free energy principle and active inference. The structure is defined
by two layers of magnetic nanoelements where one layer is a square Artificial Spin Ice
geometry. The other magnetic layer functions as a sensory filter that mediates interac-
tion between the external environment and the “hidden” Artificial Spin Ice layer. Spin
dynamics displayed by the bilayer structure are shown to be well described using a con-
tinuous form of a neurological free energy principle that has been previously proposed
as a high level description of certain biological neural processes. Numerical simulations
demonstrate that this proposed bilayer geometry is able to reproduce theoretical results
derived previously for examples of active inference in neurological contexts.
1
arXiv:2401.12211v4  [cond-mat.mes-hall]  27 Feb 2024
Keywords
artificial spin ice, bilayer, active inference, free energy principal, action and perception,
variational Bayesian, Monte Carlo
In recent years new insights into how a brain might processes and respond to information
have emerged from a neurological theory whose primary components are called active infer-
ence and the free energy principle. The theory provides a high-level mathematical description
of neural processes where comparisons are made between predictions and sensory percep-
tions resulting in adjustments that affect future sensory input. The theory is built from
simple general assumptions and has been proposed as a biologically plausible mechanism
that can describe key aspects of motor control and movement regulation.1–5 A fundamental
hypothesis is that many brain functions can be described by Bayesian inference that can be
expressed mathematically using a variational Bayes technique.6–8
The purpose of the present paper is to show that the same theory can describe dynamics
of a non-biological system and how this system might be studied experimentally. Moreover,
we show that active inference in a physical device that permits detailed experimental study
would open up new avenues of investigation that provide a new methodology for probing
complex out-of-equilibrium dynamics and would also have significant potential for a new
type of physical neuromorphic computing.9
We demonstrate these ideas with a model based on Artificial Spin Ice (ASI). Research
into ASI was initially directed towards investigations of complex frustrated systems with an
emphasis on experiment.10,11 One of the benefits has been to provide experimental systems
that can be probed with unprecedented detail on time- and length-scales that are not oth-
erwise amenable to study.12 Some of the most recent developments have expanded the field
to include neuromorphic applications for machine learning13–15 and fabrication of non-trivial
three dimensional structures in complex geometries.16,17
2
Nanomagnet Model
We take inspiration from the long history of using binary spins in models for neural opera-
tions18 and theories for their operation in neural networks (examples are discussed in Ref.19
and references therein). Likewise, Ising spin models have been shown to be well suited to
correctly predict the overall physics of ASIs because the spin’s binary ‘up’ or ‘down’ de-
scribes the nanomagnet’s north-south pole alignment. In the following, we lay out how our
proposed model behaves magnetically, and then map out how that behaviour is a physical
implementation of an active inference agent. In our model, an Ising spin is represented by a
nanometer scale magnet shaped approximately like a rectangular needle. As single-domain
nano-sized magnetic particles, each nanomagnet presents a magnetization aligned along the
needle axis below a critical temperatureTc without the application of an external magnetic
field. At their simplest, nanomagnets can be considered to behave like compass needles in
that they attempt to align their north and south poles according to the direction of magnetic
fields (but constrained by the needle geometry).
We propose two arrays of nanomagnets in a bilayer configuration arranged in a three
dimensional geometry as sketched in figure 1a. This structure is a modification of a geometry
suggestedinBegumPopy et al.20. Thetoplayerismadeofwell-separatedsuperparamagnetic
nanomagnets whose magnetization can flip direction randomly and independently by thermal
excitation as superparamagnets. However, also as with superparamagnets, the nanomagnets
will try to align their magnetizations as parallel as possible to a sufficiently strong external
magnetic field. The bottom layer is an array of nanomagnets arranged in an ASI geometry
whose nanoelements are placed close enough to interact strongly with one another through
their stray magnetic fields. They also respond to the stray fields produced by the top layer
of superparamagnetic nanomagnets.
Effects due to the magnetic field acting on the top superparamagnetic layer are assumed
to be negligible at the bottom square ASI layer. In this sense the top layer acts as a
“sensor” of magnetic fields from an external environment that transmits information to the
3
“hidden” (from the environment fields) bottom ASI layer. The ASI nanomagnets produce
an average magnetization that can be measured and used to provide a signal which is fed
back to the environment to control future input to the top-layer sensory nanomagnets. We
find that when viewed on the time-scale of the environment, the average magnetization
produced by the hidden layer follows a trajectory described by the magnetic response that
can converge to the neighbourhood of a preset value. In what follows we will show that
this behaviour is described accurately by the neurological free energy principle with active
inference. This formalism provides a useful means of describing complex out-of-equilibrium
non-linear dynamics that evolves on internal timescales that can differ from the timescale
over which the environment changes.
The bottom hidden ASI layer of figure 1a is made of16 spins in a so-called “square” ice
geometry. This geometry is known to have a two-fold degenerate ground state where the two
lowest energy spin configurations produce a zero average magnetization for the array.11 These
ground states are characterized by “ice rules” that require two spins at each vertex to point
inwards towards the vertex centre with the other two spins pointing out from the vertex.
The stability of vertex spin configurations depends on temperature as thermal fluctuations
have a finite probability to reverse spins randomly. The configurations of vertex spins can
form topological excitations that propogate through the ASI and govern the emergence of
global metastable nanomagnet spin configurations.10
The local fields acting on the ASI, produced by the top sensory layer, affect directly the
creation and propagation of vertex excitations. We show below how these local fields ma-
nipulate relaxation dynamics in the ASI. Note that our requirements for spins in the sensory
layer are that they do not reverse as easily as the spins in the ASI bottom hidden layer and
are assumed to align independently of both the ASI and of one another. These requirements
present challenges for practical design which are not unrelated to those addressed by mag-
netic data storage technologies. Some considerations are discussed more fully at the end of
this paper.
4
Figure 1: a) Geometry of the bilayer with Artificial Spin Ice for use in active inference. The
top layer of spins (shaded blue in the figure) are placed above the vertices of the so-called
‘square’ ASI geometry (unshaded). The top layer elements interact with the bottom layer
through their individual (stray) magnetic fields as discussed in the text.
b) and c) An external field is applied directly to the hidden spin square ice layer for numerical
simulation of magnetic hysteresis taken over700 field steps. The field is applied uniformly
to all of the hidden spins and response is simulated with Monte Carlo usingNh = 100 and
Th = 0.4 in b) andTh = 1.0 in c). The states of the hidden spins are distributed as shown in
the main plot at small fields for three neighbouring field steps nearH = 0 for the hysteresis
shown in the insets. Only two significant peaks appear at low temperature, one atM = 0
and the other atM = 0.4. The hysteresis apparent in the b)M(H) indicates relaxation into
long lived metastable states.
d and e) Hidden state response to sensory spins driven by an external field are shown in
(inset to d) forTh = 0.4 and in (inset to e) forTh = 1.0. Values Ts = 1.0 and Ns = 1 are
used in both cases. Hysteresis appears for eachTh and is widest at the higher temperature.
The hidden spin states occupied at small external fields are shown in the main plot (d)
for the low Th case, and (e) for the higher Th case. The wide distributions are because
sensory spin response to the external field creates local fields in the hidden spin array that
access configuration states which are not possible if the external field is applied directly and
uniformly to the entire hidden spin array.
Properties of the Sensory Spin Layer
We use numerical simulations to illustrate the importance of how the sensory layer facilitates
sampling of hidden layer spin configurations. For the numerical simulations a36 element
square ice is used with nine sensory control elements placed above the vertices internal to
the lattice in the manner depicted in figure 1 (for relevant parameters and other definitions
see Methodology). For simplicity, mh is set to unity and ms then appears only in the
environment field energy and the magnetic interaction between sensory and hidden spins.
Monte Carlo sampling is made for an ensemble of 100 identical bilayer array replicas sampled
5
independently. ThenumberofMonteCarloSteps(MCS)performedduringeachtimeinterval
are specified separately for the sensory and hidden spins of numbersNs and Nh, respectively.
The algorithm used for sampling is Glauber dynamics.
The stray magnetic field produced by a sensory spin affects most strongly the four spins
that comprise the ASI vertex directly below. These additional fields on the ASI spins impacts
the stability of each four-spin vertex and will bias the probability of spin reversals for each of
the vertex spins. In this way the sensory layer embodies some aspects of a Markov Blanket in
that it mediates information that the hidden array can receive from the environment.7,22,23
Information flow between the sensory and hidden spin arrays is directed in that the sensory
array is designed to respond to the environment external field whereas the hidden spins
respond only to the individual sensory spin fields.
The importance of this sensory ‘blanket’ design can be appreciated from two aspects of
how spin states in the hidden layer are accessed by the external field. To illustrate this
point, in figures 1b-e comparisons of state access are made between the case of an external
environment field applied directly to the hidden spins without sensory spin fields and the
case of only sensory spin fields acting on the hidden spins.
Weconsiderfirstthecasewithoutsensoryspinmediationoftheexternalfield. Infigure1b
magnetization states accessed in hidden spin array by a magnetic field applied directly to
the ASI are shown for fields sampled during a hysteresis loop. The complete hysteresis loop
is run with 700 field steps as shown in the insetM − H plot. In the hysteresis plot the
dark line represents the average taken ofM over all replicas and the lightly shaded lines
are the individual replicaM hysteresis loops. The magnetic field is applied uniformly across
all spins in the hidden layerwithout mediation from sensory spins. The simulation is run
for Nh = 100 MCS which is long enough to allow the hidden spin system to relax towards
long-lived meta-stable states at each applied field and the distribution of these states for
three small applied fields are shown in the main plot.
The temperature in figure 1b is Th = 0 .4 (in units of dumbbell strength) and state
6
occupation is shown in the main plot for the three closely spaced fields chosen nearH = 0.
ThedistributionisdiscreteduetotheASIgeometrywhichconstrainspossibleconfigurational
states. Only two of the possible states are occupied significantly which is consistent with
the low open loop fields in theM − H hysteresis. Small changes in the applied field do not
excite additional states as seen by the close proximity of the three neighbouring sampled
field strengths. Changes in the distribution appear at higher the temperature Th = 1 .0
as illustrated in figure 1c. More states are accessed at this higher temperature but the
broadening is limited to states near the two accessed at lower temperature. Note also that
the M −H loop is closed so that the state distribution is now peaked significantly only about
H = 0.
Figures ??d and 1e represent the case where the external environment field only acts on
the sensor spin layer, and the ASI hidden layer experiences only fields generated by the sensor
spins. The temperatures areTs = 1.0 with Th = 0.4 in figure??d andTh = 1.0 in figure 1e.
A small hysteresis appears in the insetM − H loops, but the ASIM has large fluctuations
in both cases as evidenced by the replica loops. The main plots show the occupation of ASI
states for (external environment) fields nearH = 0, again at three neighbouring values. The
low temperature distribution has six significant peaks spread across all possible values (±1).
The spread associated with the three neighbouringH values is broader than what occurred
for the directly applied field case shown in figure 1b. The higher temperature example in
figure 1e shows a much broader distribution of accessed states as well as a broader loop in
the hysteresis plot inset.
The differences between how the fields on the hidden layer are generated shown in figure 1
can be understood in terms the spin flip processes responsible for changingM values. The
nanomagnet elements in the hidden layer interact strongly, and alignment of these elements
occurs generally in square ASI via a type of avalanche process. When the external field is
applied directly to all the square ASI elements, excitation of these processes will most likely
begin at array edges where the total local interaction field acting on nanomagnets is weakest.
7
Figure 2: Pulse field profile and averagedM response are shown in the left panels. Cross-
correlation is shown in the middle panels, and the distribution of hidden spin states is shown
in the rightmost panels. In (a) a 20 time step wide pulse is applied as directly without
sensory spin mediation. In (b) the pulse is delivered to the hidden spin layer via the sensor
layer. The sensory spins are sampled atTs = 0.1 with Ns = 1. In (c) the 20 time step pulse
width is delivered to the hidden spin layer through the sensory spin layer withTs = 0.1 and
Ns = 10. In (d)Ts = 0.1, Ns = 10 and the pulse width is4.
Avalanches for an ASI directly responding to a uniform applied field will typically begin only
at the edges. When the sensor layer is present, sensor generated local fields can destabilize
orientations within the hidden layer ASI at sites away from the array edges. This leads to
a greater probability of nanomagnet reversal avalanches nucleated at sites within the ASI
array closest to the sensor spins.
From these examples, one can see that the top sensor layer provides local fields that fa-
cilitate broader sampling of states than a globally applied field can. This is further enhanced
by spatial differences of local fields that will appear because each sensory spin fluctuates
randomly due to thermally driven reversal but in a manner that the average remains con-
sistent with the value of the external field. The ability to leverage multiple states through
defects or direct control of ASI states was noted several years ago by Budrikis using graph
theoretical methods.24,25 The behaviour observed in the present paper can be understood
similarly in that local fields acting on a subset of spins can open pathways for avalanche
processes that would otherwise have a low probability of occurring. This gives the overall
hidden layer ASI access to a larger portion of configurational phase space through the top
sensor layer than is possible when a field is applied directly to the ASI. Another perspective
is to consider that the spread of states sampled through the ensemble of hidden spins is wide
8
because there exists a multitude of possible trajectories available to the correlated hidden
spins as they evolve in time towards a lower energy configuration. This distinguishes the
hidden spin system from a purely random Markov sampling of states. A range of possible
trajectories towards some minimal energy is fundamental to the operation of the system in
the active inference applications that will be discussed later.
The temporal duration of a signal (i.e. the time-span or pulse-length of the applied field
of the environment) is also important, and here too the bottom hidden layer’s response is
facilitated by the top sensor layer. To understand this, we examine the bottom hidden layer
response to a pulsed environment field without a top sensor layer. Figure 2a contains results
for a pulsed external field whereH lasts for 20 time steps (with no sensory spin mediation)
at Th = 1 .0. The left panel of figure 2a shows H for a 20 time-step wide pulse profile
(black line), while theMt response averaged over all replicas of the system is shown in blue.
The middle panel of figure. 2a is the cross-correlation response between the pulse and the
Mt response to the pulse for delay times ranging up to 15 time steps. The correlation is
normalized and calculated as described in Honet al.14 using the definition
R2(d) =
"X
t
(Mt − ⟨Mt⟩t){pt(d) − ⟨pt−d⟩t}
(N − 1)σM σp
#2
. (1)
Theaveragesateachtimesteparetakenoverthereplica-averagedhiddenlayermagnetization
response Mt calculated at each time stept, and the pulse input to the sensory spinspt. The
averages are over the time interval sampled, andσM and σp are the corresponding standard
deviations. The delay betweenMt and pt is d. The cross-correlation presented in figure 2a
reaches a maximum at around six time steps into the pulse, and decays rapidly thereafter.
The states accessed throughout all times are tightly grouped aroundMt = 0 as shown in the
right panel of figure 2a.
The effects of sensor layer mediation are shown in figure 2b whereTs = 0.1 and Ns = 1.
Clearly, sensor-layer-mediated input to the hidden spins increases dramatically the hidden
9
layer’s sensitivity and response to the field pulse. The correlationR2 and distribution of
states (middle and right panels of figure 2b, respectively) present a delayed, but significant,
M(t) response to the onset of the pulse, with a clear asymmetry forM = −1 in the state
distribution that corresponds to the pulse.
The results of a larger sensory spin MCS, withNs = 10 , is shown in figure 2c with
Ts = 0.1. The Mt response is now better synchronized with the pulse as can be seen from
the response profile and theR2 correlation. The distribution of states has states spread away
from the Mt = −1 value. Lastly, figure 2d illustrates sensitivity to pulses through sensor
layer mediation. Here the field pulse width is reduced from 20 to 4 time steps. We find
that when there is no sensor layer, theM(t) response of the hidden layer to the significantly
shorter field pulse is very weak (not shown). The left panel of figure 2d shows that the
hidden layer M response to the shorter field pulse is substantial when done through the
sensory spins local fields (right panel). Also, strong correlation with the pulse remains, and
the distribution of states is similar to that in figure 2c.
This sensitivity to changes in environment external field and the ability to activate a
range of states is perhaps central to the operation of the system for active inference. As
discussed earlier, the sensor layer creates local fields that, through avalanche dynamics,
instigate configurational changes that drive the hidden system through its spin state phase
space. In what follows, it will be seen that this sensitivity enables the system to search widely
for configurations that, when active inference is enabled, direct the hidden spin evolution
trajectory towards targets.
Sampling the Variational Free Energy and Implementation
of Active Inference
A few definitions are required before we can describe how the above bilayer system is able to
perform active inference. The theory we describe is a formulation in terms of stochastic dif-
10
ferential equations proposed by Friston.26 Only a summary of the essential points is presented
here as the complete theory is well described in numerous other papers. The formulation
and examples we use largely follow the description presented by Buckleyet al.27.
Theexternalenvironmentdefinesatimesequencethatissampledregularlybythesensory
spins. The variables sampled consist of data encoded as time varying fields and can represent
differentcomponentssuchaspositions, velocitiesandaccelerations, andotherratesofchange.
In the formalism used here, variables are defined as generalized coordinates which facilitate
the multiple timescale feature highlighted earlier and enables definition of continuous time
evolution of otherwise stochastic quantities. A generalizedx(t) coordianate is defined byx(t)
and its derivatives. These can be represented as a list˜x of data values where each component
is a derivative with respect tot. To simplify the notation, a component of˜x is denoted by
xα = dαx/dtα. A notable feature of this representation is that the action of a derivative on
˜x is defined as the promotion of a component to its next higher order derivative component.
This operation is denoted by the operatorD where Dxα → xα+1 (within the same vector
component set˜x).
The configurational states of the sensory spins create a mapping of the environment with
a component of ˜ϕ corresponding to a component of˜x at time t. Here, a sensory stateϕα
is sampled during a MCS over the sensory spins in the presence of an external environment
magnetic field, representing a component of˜x. The value assigned toϕ is an average overNs
MCS for the ensemble of system replicas. An average generated this way is made for each
member of˜x, and thereby mapped to the corresponding values of˜ϕ.
In a similar manner, state averages˜µ are defined for the hidden spins where an average
of an ensemble of hidden spin states is mapped to the˜ϕ (which are in turn mapped to the
˜x). Note that the timescale for this mapping is not specified by a particulart, but depends
on the number of MCS taken during a particulart. This provides an interesting separation
of time scales between the changing values of the˜x that occur on the environment time, and
the response of the bilayer components˜µ and ˜ϕ that are changing during stochastic thermal
11
relaxation processes.
The probability for a thermal reversal of a nanomagnet element ‘spin’ during a time
interval ∆t depends on the energy difference between an initial and possible final state and
the attempt frequency. This can be described by an Arrhenius relation,ν exp(−βϵ) where β
is the inverse temperature andϵ the energy difference between states. The attempt frequency
ν measures the number of pathways to an energy barrier saddle point relative to the number
of pathways over that barrier. The Monte Carlo algorithm used herein does not takeν
into account directly. However the number of MCS determine to what degree configurations
sampled during one∆t time interval are correlated with those of the previous interval. This
provides a very rough analogy to the prefactorν, and captures some sense of the different
timescales for relaxation that the spins in the two layers experience. Larger MCS values
correspond to less ‘memory’ between time steps (determined by the environment applied
field). The parameters Ns and Nh thereby determine the rates at which the spins in the
layers remain coherent over time with respect to the ‘environment clock’ determined by∆t.
As will be seen below, judicious choice ofNs and Nh are important to optimize performance
of the bilayer system for active inference.
During thermal relaxation, transitions change states˜µ to new average values. A varia-
tional Bayes relaxation is used to infer the best estimate of the distribution describing the
joint probabilityP(µ, ϕ) of having a stateµ when the sensory spins are in a stateϕ. The
algorithm for this describes the evolution of a trajectory for each component of˜µ defined by
dµα
dt − Dµα = −κ∂E(˜µ, ˜ϕ)
∂µα
. (2)
The quantityE(˜µ, ˜ϕ) plays the role of a negative entropy as defined in ensemble learning27
which assignsE to a distributionP(µ, ϕ) ∼ exp{−βE(˜µ, ˜ϕ)} that represents the unknown
joint probability relating hidden states to sensory states.
As key point of the present paper,E(˜µ, ˜ϕ) is here viewed as a thermodynamic energy
associated with the bilayer system. The gradient in Eqn. 2 suggests an interpretation of the
12
right hand side as a conjugate field toµα driving states of the hidden spins along a trajectory
that leads to the conditiondµα/dt−Dµα = 0. This condition describes a trajectory through
state phase space where the time evolution of the average˜µ is directed towards the maximum
of the distribution that is being inferred.28
These ideas are illustrated for the ASI bilayer through the results presented below. The
first example is for tracking of the magnetizationM to a target parameter value subject to
unknown (to the bilayer spins) external constraints imposed by the environment.
This example is set up with a model adapted from Baltieri29. Here, the environment is
defined by a one dimensional equation of motion specified as
dx(t)
dt = −γx(t) + a(t). (3)
This equation describes dissipative motion alongx that would decay to zero via a friction
γ unless offset by a velocity terma(t). The target specifies a fixed velocitydx/dt = vd the
system should arrive to as it relaxes to the conditiondµα/dt − Dµα = 0.
The sensory spins’ response to the environment field are mapped to position and velocity
information through generalized coordinate vectors ˜x = {x(t), dx(t)/dt} at regular time
steps.28,30 An important note is that here the same bilayer is used for each component of˜x
with each component updated sequentially as discussed below. Each of the bilayer’s hidden
spins will respond with an overall magnetizationMα that is defined by an ensemble average
over a quantityχ(Bα) via
Mα = ⟨χ(Bα)⟩T , (4)
where χ is the magnetization measured along one direction of the spin lattice array that
experiences the fieldBα. The averageMα replaces ∂E(˜µ, ˜ϕ)/∂µα in Eqn. 2.
The quantity Bα is a field corresponding to a linear combinations of˜µα and ˜ϕα com-
ponents that are determined in the following way:χ(Bα) samples a component of the log
joint probability P(˜µ, ˜ϕ). Following Friston et al.31, under the assumption of statistical
13
independence, one arrives at the factorizations
P(˜µ|˜ϕ) =
Y
α
P(µα|ϕα), (5)
and
P(˜µ) =
Y
α
P(µα+1|µα) (6)
Sampling distributions produced by the log of these probabilities then reduces the problem to
a summation ofMα terms where theBα in Eqn. 4 are determined by the conditional variables
appearing in the P(˜µ|˜ϕ). For example, the ensemble average oflog P(µα|ϕα) corresponds
to evaluating⟨χ(ϕα − µα)⟩T . Note that when using the Laplace approximation of Gaussian
distributions for eachP, the precision of the Gaussians enter as adjustable parameters that
can optimize the ability of the system to relax towards target values of sensory input. In the
bilayer, the inverse temperaturesβs and βh become the corresponding adjustable parameters,
although Ns and Nh also play a role.
The final step is to include active inference. Active inference provides a feedback from
the hidden spins to the world environment, and in the present context is represented by the
a(t) appearing in Eqn. 3. This term is defined by imposing a functional dependence forϕ(a)
such that it appears as a time dependent constraint on minimization to the steady state
condition of Eqn. 2. A time evolution defined as a force
da(t)
dt = −κdϕ
da
dE(˜µ, ˜ϕ)
dϕ (7)
is assumed. Proceeding as above, the bilayer equivalent replacesE with ⟨χ(Bα)⟩T to deter-
mine the form of Eqn. 7. The manner in whicha(t) enters ˜µ is through definition of a target
probability PS. In general this corresponds to assigninga(t) to a particular component (or
components) of˜µ.
In this particular tracking example, the factorization requires only two generalized com-
14
Figure 3: a) and b) present two example applications of active inference and the variational
free energy principle. In (a), sensory layer spins receive environment position and velocity
information and active inference generated by hidden layer spin states drive the system
towards a target velocity vd = 0 .6. In (b) environment measures are determined by a
function T(x) and its derivativedT(x)/dx, but the sensory spins receive only the valuesT
and dT/dx without information ofx or the functional form ofT(x). Active inference drives
the system to a specified targetT(x) = Td. The trajectory of the sensory spin states is shown
in (c) for the example in (b). Active inference feeds information derived from the hidden spin
state averages to the environment that causes changes in subsequent sensory input. In this
way the trajectory of states within the hidden layer are directed toward the target goal by
the sensory spins in response to changes in the environment. The corresponding trajectory
for the hidden spins is shown in (d).
ponents for each of the sensory and hidden spin states. They areϕ0 and ϕ1 for the spin
layer, andµ0 and µ1 for the hidden layer. The target appears in theµ1 component via the
transition probabilityPS(µ1|vd − αµ0). Note that the factorization ofP(˜µ) is truncated by
requiring µ2 = 0.
The resulting evolution equations for the first example shown in figure 3 are
dµ0
dt = µ1 + ⟨χ(βz(ϕ0 − µ0))⟩T − α⟨χ(βw(µ1 + αµ0 − vd))⟩T , (8)
dµ1
dt = ⟨χ(βz(ϕ1 − µ1))⟩T − ⟨χ(βw(µ1 + αµ0 − vd))⟩T , (9)
da
dt = −⟨χ(βz(ϕ1 − µ1))⟩T . (10)
In this expression, a generalization of the effective temperature is introduced which al-
15
lows the temperature of the hidden layer for each bilayer to be treated as a parameter.
In Baltieri29 separate precisions are defined for the Gaussian distributions that are instead
here sampled from the ASI. Here we define corresponding parameters asβz = βhΠz and
βw = βhΠw with additional parametersΠz and Πw that are used to adjust the probabilities
affecting theϕα and µα terms. This generalization provides a functionality analogous to the
precisions used in Baltieri29. Unless specified otherwise, for simplicity we specify onlyTh (so
that βz = βw) orβz and βw separately. We also note thatTs = 0.5 for all simulations that
follow.
In order to confirm minimization, a measure of the free energy is defined as the lowest
order contribution to the factorizedlog P(˜µ, ˜ϕ):
F ≈ 1
2[βz(ϕ0 − µ0)2 + βz(ϕ1 − µ1)2 + βw(µ1 + αµ0 − vd)2]. (11)
The panels in figure 3a are the result of iterating Eqs. 8 through 10. The top panel of
figure 3a shows howµ0 and ϕ0 evolve in time with reference to the targetvd. The middle
panel shows the time evolution ofµ1, ϕ1 and a. The corresponding free energy is in the
bottom panel. The system evolves to the target value ofvd = 0.6 using βz = 10 and βw = 1.
Here Ns = 1, andNh = 1 at each time step. This system still arrives at the target for larger
values ofNs or Nh with generally less noise. The ability of the system to reach the target
is sensitive to the values of the inverse temperatures in analogy to sensitivity to precisions
noted in Baltieri29.
In the above example a model of the environment was encoded in the hidden spin system
throughPS by inclusion of theα dissipation term. This next example illustrates that only the
target need be encoded in the hidden states, and additional information can be inferred from
the environment at the sensory level. The basis for this example is the thermostat problem
presented in Buckley27 and discussed in the context of proportional integral derivative (PID)
16
control in Baltieri29,32. The sensory input is from two functions of positionx defined by
T(x) = 1
1 + x2 , (12)
and
dT(x)
dx = − 2x
(1 + x2)2 . (13)
Action is included indx/dt:
dx(t)
dt = a(t), (14)
In this model, target informationTd is included throughPS(µ1|µ0 −Td), but no information
about ˜x is passed to the bilayer. The equations of motion are truncated by requiringµ3 =
0. The equations for ˜µ are of the same form as in Buckley et al.27, repeated here for
completeness:
dµ0
dt = µ1 + ⟨χ(βz(ϕ0 − µ0))⟩T − ⟨χ(βw(µ0 + µ1 − Td))⟩T , (15)
dµ1
dt = µ2 + ⟨χ(βz(ϕ1 − µ1))⟩T − ⟨χ(βw(µ0 + µ1 − Td))⟩T + ⟨χ(βw(µ1 + µ2))⟩T , (16)
dµ2
dt = −⟨χ(βw(µ1 + µ2))⟩T . (17)
The time dependence of the action for this example given by
da
dt = −⟨χ(βz(ϕ1 − µ1))⟩T
dT(x)
dx . (18)
Results are shown in figure 3b for the same temperatures as above but withNs = 10 (Nh
remains 1). The increased value ofNs helps optimize the trajectory to arrive at the target
value Td = 0.3. The µ0 and ϕ0 time evolutions are shown in the top panel,µ1, ϕ1 and a
are in the middle panel, and the free energy is in the bottom panel; it is clearly minimized
as the system evolves. The decaying oscillations observed in the˜µ0 and ˜ϕ0 are strikingly
17
reminiscent of PID control behaviour as discussed by Baltieri32. The authors therein note
that optimization of parameters can be performed to enhance the decay toward the target,
and we find the same.
It is illuminating to track the evolution throughϕ0 and ϕ1 when viewed as a phase space
trajectory. The trajectory corresponding to the evolutions of the sensory input shown in
figure 3c is for the time evolution of hidden states shown in figure 3d. The corresponding
trajectory for the hidden state˜µ is quite different as shown in figure 3d. The sensory inputs
circle through˜ϕ to oscillate around the target value. The hidden states˜µ lie roughly along a
quasi-linear line centred about the neighbourhood of the target value. Gradients of the free
energy direct evolution of the hidden states towards a most likely maximum that lies along
this line at the target value. There is a strong dependence on the number of sensory spins,
Ns, where the system has difficulty finding the target for small values causing it instead to
relax toϕ0 = 0. The optimal parameter values for achieving the target also depend on the
magnitude of the targetTd. As noted earlier, thermal evolution during environment time
scales within the hidden spin system depend on temperatures and MCS.
Details of how these parameters affect the trajectories and relaxation are not well un-
derstood at present, but some qualitative observations can be reported. The results shown
in figure 3 require an amount of environmental time to relax towards the target, and also
depend onTh and Nh. Increasing Nh generally appears to increase the correlation between
˜µ and ˜ϕ, while introducing more noise into˜µ and the free energy.Th (and its generalization
to βz and βw) is an important optimization parameter for achieving the lowest free energy
for differentTd.
A very interesting aspect is the dependence on βz and βw of the time evolution for
˜ϕ and ˜µ. Unusual oscillations can appear for different temperatures and targets that may
contain information about internal state selection within the hidden layer. These are aspects
currently under study that are outside the scope of the present work, but an example is shown
in figure 4.
18
Figure 4: The oscillations of˜µ and ˜ϕ relax toward a targetTd = 0.5 in (a) for parameters
βz = 3 .0 and βw = 0 .75. Similar oscillations are found using the same parameters when
the ⟨χ⟩T is replaced with a Gaussian distribution approixmation. A very different quasi-
periodic oscillation arising from the ASI through⟨χ⟩T is found forβz = 2.0 and βw = 0.5 as
shown in (b). A Gaussian approximation for theseβα values is instead similar to the results
shown in (a). The oscillations correspond to limit cycles as seen with the trajectories for the
corresponding ˜x and ˜ϕ displayed in (c).
Results for aTd = 0.5 with βz = 3.0 and βw = 0.75 are shown in figure 4a. These are
qualitatively similar to those shown in 3b although with noticeable differences that illustrate
sensitivity of the nanomagnet dynamics to temperature. We note that the results can be
fit using hyperparameters of a Gaussian distribution approximation for the˜µ distributions
(which is done by replacing the⟨χ⟩T with Gaussians as in Buckleyet al.27). Using instead
βz = 2 .0 and βw = 0 .5 we find the results shown in figure 4b. Quasiperidic oscillations
in ˜ϕ and ˜µ which we were not able to reproduce using the Gaussian approximation. The
corresponding trajectories for the sensory and hidden averages are shown in figure 4c. The
oscillations appear similarly for longer time integrations suggesting a stable limit cycle.
In any case, this strong dependence on temperature and precision weightings suggest that
non-trivial dynamics can arise under certain circumstances. Moreover, there appears to be
sensitivity to details of the ASI design as we find different behaviour when we change the
lattice geometry from square to pinwheel by rotating the nanoelements.
19
Conclusions
Perhaps the most interesting aspect demonstrated in this paper is the possibility of studying
transition dynamics in experimentally accessible magnetic and nonmagnetic systems using
active inference. Viewed as a methodology for probing system dynamics, the mathematical
treatment and intrinsic separation of timescales underlying the variational free energy and
active inference theory may inspire new ways of studying experimentally a range of complex
systems. There are few reports in the literature of applications of the neurological free energy
principle in physical experiments. One example is saccadic eye motion2,33 and another is
a recent report on small assemblies of neurons.34 Trajectories of quantities defined for this
paper (˜µ and ˜ϕ) are sensitive to parameters of the theory that can be associated with
measurablepropertiesofthesystemunderstudy. Ourpurposehasbeentosuggestarelatively
simple, non-biological system that can serve as a testing ground for exploring this approach
to active inference theory in contexts that can be studied experimentally.
The essential component of our approach is the sensor layer and how it provides an
interface to the measured environment in a manner analogous to a Markov Blanket (as
discussed by Kirchhoff et al.23). The mechanisms at play in the bilayer system involve local
fields generated by the sensory spins in response to input from an environment. These local
fields act on the spins in the hidden layer, enabling expedited relaxation towards a minimum
free energy defined by a target constraint imposed through active inference. The ability to
sampleregionsofhiddenspinstatespaceinthiswayisreminiscentof, butquitedistinctfrom,
other strategies sometimes used for escaping long lived metastable states in computations,
such as parallel tempering.35 It is interesting to speculate that the need to access a range
of states relevant for achieving a target may be analogous to the requirement of sufficient
memory and computational capacity for a system to be suitable for reservoir computing.14
It is also interesting that the trajectory through states of the hidden spins that satisfies
Eqn. 2 can be described as a temporally shifted reference frame under the Fokker-Planck
Kolmogorovformalism. 28,36,37 Fromthisperspective, theconditionforastationarytrajectory,
20
dµα/dt −Dµα = 0, matches the environment time scale governing input to the sensory spins
with the time scale over which internal hidden state processes are thermally averaged by state
transitions. As noted by Balaji et al.,30 the stationary trajectory condition means the hidden
spin states evolve along time dependent free energy gradients that lead toward target values.
In the simulations discussed above, this condition assigns a physical time interval to thermal
transition probabilities sampled using Glauber dynamics. It may be possible to exploit this
feature to probe timescales for state transitions processes in experiments with systems whose
complex energy landscape are being measured, such as spin- or structural glasses. We note
that spin glasses have been explored somewhat in regards to active inference from a different
direction by Heinset al.38.
The spin models we used are simple yet appear sufficient to capture the essential features
displayedin experimentsusing actualnanomagnets. Some implementation ofactive inference
may be possible to observe with suitably designed nanomagnet arrays using fabrication
technology that is already available. For simplicity, we have here considered only magnetic
interactionsfromtheexternalappliedfieldandfromthenanomagnetsinthebilayer. However
ourmodelcreatespracticalchallengesforimplementationbecauseofourrequirementthatthe
environmentmagneticfieldaffectdirectlyonlythetoplayerofmagneticnanoelements. There
are a number of different strategies that would meet this requirement. For example, one
could use vertical magnetic sensor elements with horizontal environment fields or variations
of techniques for domain switching as used in magnetic data storage. Alternatively, a voltage
actuated sensor layer using multiferroic nanomagnets such asϵ-Fe2O3 39 would create local
magnetic fields. Many of these technologies also permit integration into other spintronic and
CMOS-based devices.
Concerning possible applications, a great potential of creating energy efficient platforms
for machine learning algorithms using nanomagnetic arrays have been identified and some
in cases demonstrated in ASI13,14 as well as other magnetic material platforms.40,41 Most re-
cently, magnetic nanoparticle artificial spin ice configurations have been studied experimen-
21
tally15 whose state configurations are manipulated locally and detected using microwaves.
The energy efficiency of nanomagnetic systems for machine learning applications, as well
as the microwave properties that can be associated with configurational states, make these
systems attractive for practical use. The ability to incorporate active inference into architec-
tures based on nanomagnets would be a new aspect to pursue. A very interesting direction
to explore would be implementation of hierarchical structures42 where multiple sensory and
hidden layers could be connected, and features associated with learning and other complex
tasks explored. Although not reported here, stacked bilayers of the form presented above
appear able to mimic some features of learning at different timescales that can facilitate
optimizations toward target values.
Finally, the nanomagnet architecture used here is only one example ASI geometry. Al-
though not reported here, some variants of the square ASI geometry also display analogous
properties under active inference. An advantage to using ASI as a model platform is the free-
dom to impose constraints and introduce frustration that can be studied as pathways through
configurational states. These can be analysed in a variety of ways including graph theoretical
methods as discussed for athermal ASI by Budrikis43. Moreover, pathways through state
configurations in these systems can often be understood via topological excitations with rules
determined by allowed transitions. Active inference methodologies may be helpful in this
context to understand how trajectories through configuration space can be manipulated via
environmental control of topological excitations. For this reason it would be particularly
exciting to examine in non-Ising spin systems where topological excitations arise through
competing interactions. A prime example is magnetic skyrmions in thin film geometries
which enable control and detection via local electric potentials. Skyrmions are topologically
protected spin textures and display non-linear response to applied fields and undergo tran-
sitions through mediating metastable states.44,45 These systems can display a sufficiently
complex state space for applications in reservoir computing.46,47
22
Methods
Interactions in our numerical model of the bilayer system are provided by magnetic fields
produced by the nanomagnets. The geometry used for the hidden spins is defined by a square
lattice of lattice constanta with elements of lengthℓ = a/2 aligned diagonally within each
unit cell. The fields produced by the sensory spins have strengths in the hidden spin layer
that depend on the layer separation distance. The separation assumed throughout isa/10.
Sensory spin nanomagnet elements have a magnetic ‘spin’ momentms while ASI hidden
elements have amh ‘spin’ moment for each element. Each nanomagnet spin is assumed to
be independent of temperature for the ranges considered here. Nanomagnet generated fields
acting on a single nanomagnet are represented as a sum over all other nanomagnet spins at
distances rij with directions specified by unit vectors for each spinˆϵi, ˆϵj. The corresponding
interaction energy in the dipole limit is given by
D = D
X
i,j
ˆϵi · ˆϵj
|rij|3 − 3(ˆϵi · ⃗ rij))(ˆϵj · ⃗ rij))
|rij|5

(19)
The prefactor contains the nanomagnet magnetization and volume asD = µ0/4πa3 . In
our model all dipole sums are approximated by dumbbell charges as outlined in Castelnovo
et al.21 which takes into account the length of the nanomagnet elements.
The sensor layer nanomagnets’ alignment with the external field is stochastic in that each
nanomagnet’s spin has a finite probability of aligning with the external field. In this regard,
the sensory and hidden layer spins experience different temperaturesTs and Th due to the
different magnetic properties of each layer. The probability of a magnetization reversal for
a nanomagnet at a temperature in a field specific to the layer is described by an effective
temperature. Spinsinthesensorylayerareataneffectivetemperature βs
−1 = kBTs/ms while
the effective temperature of the hidden spins isβh
−1 = kBTh/mh, wherekB is Boltzmann’s
constant.
The energy used for Monte Carlo simulations includes field and dipole interaction terms
23
acting at each spin, and is used to determine the energy change∆E for a possible spin
reversal at each site. With a spinσs(rs) at site rs in the sensory layer, and spinσh(rh) at
site rh in the hidden layer, the approximate∆E for a sensory spinσs(rs) experiencing an
external applied fieldhe is: (note thatσ = ±1)
∆E(rs) =2βs
(
− msheσs(rs)
+ msσs(rs)
X
r′
s
D(rs − r
′
s)msσs(r
′
s) + msσs(rs)
X
r′
h
D(rs − r
′
h)mhσh(r
′
h)
)
≈2βs
(
− msheσs(rs) + msσs(rs)
X
r′
h
D(rs − r
′
h)mhσh(r
′
h)
)
. (20)
The first term in Eqn. 20 is the interaction energy of the sensory spins with the environment
field and is proportional toms of a sensory element. The second term represents the dipole
field interactions between the sensory spins.
The first dipole term in the top part of Eqn. (20) is neglected under the assumption
that the environment field energy is much larger than the dipole interaction between sensory
spins. An additional assumption is that the sensory spinms is much larger than the hidden
spin mh. This means the he term dominates (except for very small applied fields) and
the D energy contributes effectively as a small noise term; it is neglected in the numerical
calculations.
A spin in the hidden layer is described by the energy change:
∆E(rh) =2βh
(
− mhheσh(rh)
+ mhσh(rh)
X
r′
h
Dβh (rh − r
′
h)mhσh(r
′
h) + mhσh(rh)
X
r‘s
D(rh − r
′
s)msσs(r
′
s)
)
≈2βh
(
mhσh(rh)
X
r′
h
D(rh − r
′
h)mhσh(r
′
h) + mhσh(rh)
X
rs
D(rs − r
′
h)msσs(r
′
s)
)
(21)
24
Here, the first term in Eqn. 21 is the effect of the external environment fieldhe on the hidden
spins. The second term represents interactions between all spins in the hidden layer. This
external field is assumed to be applied locally to the sensor spins and is much weaker than
the hidden spin interaction terms and therefore neglected. The third term represents the
fields generated by sensory spins acting on the hidden layer spins.
An important aspect of this model is that because the effective temperatures for the sen-
sory and hidden layer spins are different, timescales for reversal dynamics are also different.
These are modelled such that the sensory spins relax with respect to the instantaneous value
of the external field, while the hidden spins relax in accord to the fields generated by the
sensory spins in their states. The conditionTh > Ts describes sensory spins that sample
the local input field less frequently than the hidden spins sample the sensory spin configu-
ration. We find in the active inference experiments discussed above that the ratio of these
temperatures affects the sampling of ASI configuration states.
Acknowledgement
The authors thank M. Falconbridge for helpful and insightful discussions. This work was
supported from the University of Manitoba, the Natural Sciences and Engineering Research
Council of Canada (NSERC RGPIN 05011-18 and RGPIN-2018-05012), and the Canadian
Foundation for Innovation (CFI) John R. Evans Leaders Fund.
References
1. Friston, K.; Kilner, J.; Harrison, L. A free energy principle for the brain.Journal of
Physiology-Paris 2006, 100, 70–87.
2. Adams, R. A.; Aponte, E.; Marshall, L.; Friston, K. J. Active inference and oculomo-
25
tor pursuit: The dynamic causal modelling of eye movements.Journal of Neuroscience
Methods 2015, 242, 1–14.
3. Friston,K.; Trujillo-Barreto,N.; Daunizeau,J.DEM:Avariationaltreatmentofdynamic
systems. NeuroImage 2008, 41, 849–885.
4. Friston, K.; Stephan, K.; Li, B.; Daunizeau, J. Generalised Filtering.Mathematical Prob-
lems in Engineering2010, 2010, 1–34.
5. Friston, K. The free-energy principle: a unified brain theory?Nat Rev Neurosci2010,
11, 127–138.
6. Friston, K. The free-energy principle: a rough guide to the brain?Trends in Cognitive
Sciences 2009, 13, 293–301.
7. Aguilera, M.; Millidge, B.; Tschantz, A.; Buckley, C. L. How particular is the physics of
the free energy principle?Physics of Life Reviews2022, 40, 24–50.
8. FitzGerald, T. H. B.; Schwartenbeck, P.; Moutoussis, M.; Dolan, R. J.; Friston, K. Active
Inference, Evidence Accumulation, and the Urn Task.Neural Computation 2015, 27,
306–328.
9. Nakajima, K. Physical reservoir computing—an introductory perspective. Japanese
Journal of Applied Physics2020, 59, 060501.
10. Skjærvø, S. H.; Marrows, C. H.; Stamps, R. L.; Heyderman, L. J. Advances in artificial
spin ice.Nature Reviews Physics2020, 2, 13–28.
11. Nisoli, C.; Moessner, R.; Schiffer, P. Colloquium: Artificial spin ice: Designing and
imaging magnetic frustration.Reviews of Modern Physics2013, 85, 1473–1490.
12. Marrows, C. H. InSpin Ice; Udagawa, M., Jaubert, L., Eds.; Springer Series in Solid-
State Sciences; Springer International Publishing: Cham, 2021; pp 455–478.
26
13. Jensen, J. H.; Tufte, G. Reservoir Computing in Artificial Spin Ice. 2020; pp 376–383.
14. Hon, K.; Kuwabiraki, Y.; Goto, M.; Nakatani, R.; Suzuki, Y.; Nomura, H. Numerical
simulation of artificial spin ice for reservoir computing.Applied Physics Express2021,
14, 033001.
15. Gartside, J. C.; Stenning, K. D.; Vanstone, A.; Holder, H. H.; Arroo, D. M.; Dion, T.;
Caravelli, F.; Kurebayashi, H.; Branford, W. R. Reconfigurable training and reservoir
computing in an artificial spin-vortex ice via spin-wave fingerprinting.Nature Nanotech-
nology 2022, 17, 460–469.
16. May, A.; Saccone, M.; van den Berg, A.; Askey, J.; Hunt, M.; Ladak, S. Magnetic charge
propagation upon a 3D artificial spin-ice.Nature Communications2021, 12, 3217.
17. Saccone,M.; VandenBerg,A.; Harding,E.; Singh,S.; Giblin,S.R.; Flicker,F.; Ladak,S.
Exploring the phase diagram of 3D artificial spin-ice.Communications Physics2023, 6,
1–9.
18. Hopfield, J. J. Neural networks and physical systems with emergent collective computa-
tional abilities.Proceedings of the National Academy of Sciences1982, 79, 2554–2558,
Publisher: Proceedings of the National Academy of Sciences.
19. Coolen, A. C. C.; Kuehn, R.; Sollich, P.Theory of Neural Information Processing Sys-
tems; Oxford University Press, Oxford, 2005.
20. Begum Popy, R.; Frank, J.; Stamps, R. L. Magnetic field driven dynamics in twisted
bilayer artificial spin ice at superlattice angles.Journal of Applied Physics2022, 132,
133902.
21. Castelnovo, C.; Moessner, R.; Sondhi, S. L. Magnetic monopoles in spin ice.Nature
2008, 451, 42–45.
27
22. Friston, K. Life as we know it. Journal of The Royal Society Interface 2013, 10,
20130475.
23. Kirchhoff, M.; Parr, T.; Palacios, E.; Friston, K.; Kiverstein, J. The Markov blankets of
life: autonomy, active inference and the free energy principle.J. R. Soc. Interface. 15,
20170792.
24. Budrikis, Z.; Politi, P.; Stamps, R. L. Diversity Enabling Equilibration: Disorder and
the Ground State in Artificial Spin Ice. Physical Review Letters 2011, 107, 217204,
Publisher: American Physical Society.
25. Budrikis, Z.; Morgan, J. P.; Akerman, J.; Stein, A.; Politi, P.; Langridge, S.; Mar-
rows, C. H.; Stamps, R. L. Disorder Strength and Field-Driven Ground State Domain
Formation in Artificial Spin Ice: Experiment, Simulation, and Theory.Physical Review
Letters 2012, 109, 037203.
26. Friston, K. J.; Stephan, K. E. Free-energy and the brain.Synthese 159, 417–458.
27. Buckley, C. L.; Kim, C. S.; McGregor, S.; Seth, A. K. The free energy principle for action
and perception: A mathematical review.Journal of Mathematical Psychology 81, 55–79.
28. Balaji, B. Continuous-Discrete Path Integral Filtering.Entropy 11, 402–430.
29. Baltieri, M.Active Inference: Building a New Bridge Between Control Theory and Em-
bodied Cognitive Science; University of Sussex, 2019.
30. Balaji, B.; Friston, K. Bayesian state estimation using generalized coordinates. p 80501Y.
31. Friston, K.; Mattout, J.; Trujillo-Barreto, N.; Ashburner, J.; Penny, W. Variational free
energy and the Laplace approximation.NeuroImage 34, 220–234.
32. Baltieri, M. A Bayesian perspective on classical control. 2020 International Joint Con-
ference on Neural Networks (IJCNN). 2020; pp 1–8, ISSN: 2161-4407.
28
33. Adams, R. A.; Bauer, M.; Pinotsis, D.; Friston, K. J. Dynamic causal modelling of eye
movements during pursuit: Confirming precision-encoding in V1 using MEG.NeuroIm-
age 2016, 132, 175–189.
34. Isomura, T.; Kotani, K.; Jimbo, Y.; Friston, K. J. Experimental validation of the free-
energy principle with in vitro neural networks.Nat Commun 14, 4547.
35. Baños, R. A.; Cruz, A.; Fernandez, L. A.; Gil-Narvion, J. M.; Gordillo-Guerrero, A.;
Guidetti, M.; Maiorano, A.; Mantovani, F.; Marinari, E.; Martin-Mayor, V.; Monforte-
Garcia, J.; Sudupe, A. M.; Navarro, D.; Parisi, G.; Perez-Gaviro, S.; Ruiz-Lorenzo, J. J.;
Schifano, S. F.; Seoane, B.; Tarancon, A.; Tripiccione, R.et al.Nature of the spin-glass
phase at experimental length scales.J. Stat. Mech.2010, 2010, P06026.
36. Koudahl, M. T.; de Vries, B. A Worked Example of Fokker-Planck-Based Active Infer-
ence. Active Inference. pp 28–34.
37. Friston, K.; Da Costa, L.; Sajid, N.; Heins, C.; Ueltzhöffer, K.; Pavliotis, G. A.; Parr, T.
The free energy principle made simpler but not too simple.Physics Reports2023, 1024,
1–29.
38. Heins, C.; Klein, B.; Demekas, D.; Aguilera, M.; Buckley, C. L. Spin Glass Systems
as Collective Active Inference. Active Inference. Cham, 2023; pp 75–98.
39. Nickel, R.; Gibbs, J.; Burgess, J.; Shafer, P.; Motta Meira, D.; Sun, C.; van Lierop, J.
Nanoscale size effects on push-pull Fe-O hybridization through the multiferroic transition
of perovskiteϵ-Fe2O3. Nano Letters2023, 23, 7845.
40. Torrejon,J.; Riou,M.; Araujo,F.A.; Tsunegi,S.; Khalsa,G.; Querlioz,D.; Bortolotti,P.;
Cros, V.; Yakushiji, K.; Fukushima, A.; Kubota, H.; Yuasa, S.; Stiles, M. D.; Grollier, J.
Neuromorphic computing with nanoscale spintronic oscillators.Nature 2017, 547, 428–
431.
29
41. Grollier, J.; Querlioz, D.; Camsari, K. Y.; Everschor-Sitte, K.; Fukami, S.; Stiles, M. D.
Neuromorphic spintronics.Nature Electronics2020, 3, 360–370.
42. Lee, T. S.; Mumford, D. Hierarchical Bayesian inference in the visual cortex.J. Opt.
Soc. Am. A2003, 20, 1434.
43. Budrikis, Z. InChapter Two - Disorder, Edge, and Field Protocol Effects in Athermal
Dynamics of Artificial Spin Ice; Camley, R. E., Stamps, R. L., Eds.; Solid State Physics;
Academic Press, 2014; Vol. 65; pp 109–236.
44. Desplat, L.; Suess, D.; Kim, J.-V.; Stamps, R. L. Thermal stability of metastable mag-
netic skyrmions: Entropic narrowing and significance of internal eigenmodes.Physical
Review B2018, 98, 134407.
45. Desplat, L.; Kim, J.-V.; Stamps, R. L. Paths to annihilation of first- and second-order
(anti)skyrmions via (anti)meron nucleation on the frustrated square lattice.Physical
Review B2019, 99, 174409.
46. Pinna, D.; Bourianoff, G.; Everschor-Sitte, K. Reservoir Computing with Random
Skyrmion Textures.Phys. Rev. Appl.2020, 14, 054020.
47. Raab, K.; Brems, M. A.; Beneke, G.; Dohi, T.; Rothörl, J.; Kammerbauer, F.;
Mentink, J. H.; Kläui, M. Brownian reservoir computing realized using geometrically
confined skyrmion dynamics.Nat Commun 2022, 13, 6982.
30
TOC Graphic
31