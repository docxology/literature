ON THE RELATIONSHIP OF ACTIVE INFERENCE AND CONTROL
AS INFERENCE
BerenMillidge AlexanderTschantz
SchoolofInformatics SacklerCenterforConsciousnessScience
UniversityofEdinburgh SchoolofEngineeringandInformatics
beren@millidge.name UniversitySussex
tschantz.alec@gmail.com
AnilKSeth ChristopherLBuckley
SacklerCenterforConsciousnessScience EvolutionaryandAdaptiveSystemsResearchGroup
EvolutionaryandAdaptiveSystemsResearchGroup SchoolofEngineeringandInformatics
SchoolofEngineeringandInformatics UniversityofSussex
UniversityofSussex C.L.Buckley@sussex.ac.uk
A.K.Seth@sussex.ac.uk
June30,2020
ABSTRACT
ActiveInference(AIF)isanemergingframeworkinthebrainscienceswhichsuggeststhatbiological
agents act to minimise a variational bound on model evidence. Control-as-Inference (CAI) is a
frameworkwithinreinforcementlearningwhichcastsdecisionmakingasavariationalinference
problem. While these frameworks both consider action selection through the lens of variational
inference,theirrelationshipremainsunclear. Here,weprovideaformalcomparisonbetweenthem
anddemonstratethattheprimarydifferencearisesfromhowvalueisincorporatedintotheirrespective
generative models. In the context of this comparison, we highlight several ways in which these
frameworkscaninformoneanother.
ActiveInference(AIF)isanemergingframeworkfromtheoreticalneurosciencewhichproposesaunifiedaccountof
perception,learning,andaction(K.Friston,2008,2010;K.Friston,Kilner,&Harrison,2006). Thisframeworkposits
thatagentsembodyagenerativemodeloftheirenvironmentandperceptionandlearningtakeplacethroughaprocess
ofvariationalinferenceonthisgenerativemodelbyminimizinganinformation-theoreticquantityâ€“thevariational
freeenergy(Beal,2003;K.J.Friston,Parr,&deVries,2017;Wainwright&Jordan,2008). Withinthisframework,
actionselectioncanbecastasaprocessofinference,underwrittenbythesamemechanismswhichperformperceptual
inferenceandlearning(K.Friston,FitzGerald,Rigoli,Schwartenbeck,&Pezzulo,2017;K.J.Friston,Daunizeau,
&Kiebel,2009;Tschantz,Millidge,Seth,&Buckley,2020). Implementationsofactiveinferencehaveadegreeof
biologicalplausibility(K.Fristonetal.,2017)andaresupportedbyconsiderableempiricalevidence(Walsh,McGovern,
Clark,&Oâ€™Connell,2020). Moreover,recentworkhasshownthatactiveinferencecanbeappliedtohigh-dimensional
tasksandenvironments(Fountas,Sajid,Mediano,&Friston,2020;Millidge,2019a,2019b,2020;Tschantz,Baltieri,
Seth,Buckley,etal.,2019;Tschantz,Millidge,etal.,2020;UeltzhÃ¶ffer,2018).
Thefieldofreinforcementlearning(RL)(Sutton&Barto,2018)isalsoconcernedwithunderstandingadaptiveaction
selection. RLassumesthatagentsseektomaximisetheexpectedsumofrewards(whicharegenerallyassumedtobe
exogenous,notintrinsictotheagent),andthenwillselecttheactionsthatwillmaximizereward. Inrecentyears,the
frameworkofcontrolasinference(CAI)(Abdolmalekietal.,2018;Attias,2003;Levine,2018;K.Rawlik,Toussaint,&
Vijayakumar,2010;K.C.Rawlik,2013)hasrecasttheproblemofRLinthelanguageofvariationalinference. Instead
ofmaximizingrewards,agentsmustinferactionsthatleadtooptimaltrajectories. Thisreformulationenablestheuseof
powerfulinferencealgorithmsinRL,whilealsoprovidinganaturalmethodofexploration(Abdolmalekietal.,2018;
Haarnoja,Zhou,Abbeel,&Levine,2018;Haarnoja,Zhou,Hartikainen,etal.,2018).
0202
nuJ
92
]IA.sc[
3v46921.6002:viXra
APREPRINT-JUNE30,2020
Both AIF and CAI view adaptive action selection as a problem of inference. However, despite these similarities,
theformalrelationshipbetweenthetwoframeworksremainsunclear. Inthiswork,weattempttoshedlightonthis
relationship. WepresentbothAIFandCAIinacommonlanguage,highlightingconnectionsbetweentheframeworks
whichmayhaveotherwisebeenoverlooked. Wethenconsiderthekeydistinctionbetweentheframeworks,namely,
howâ€˜valueâ€™orâ€˜goalsâ€™areencodedintothegenerativemodel. Wediscusshowthisdistinctionleadstosubtledifferences
intheobjectivesthatbothschemesoptimize,andsuggesthowthesedifferencesmayimpactbehaviour.
1 Formalism
ğ’ª 1 ğ’ª 2 ğ’ª 3 ğ’ªT a a a â€¦ a
1 2 3 T
a a a â€¦ a
1 2 3 T s s s â€¦ s
1 2 3 T
s s s â€¦ s
1 2 3 T oËœ
1
oËœ
2
oËœ
3
â€¦ oËœT
o o o â€¦ o
1 2 3 T
(a)Control-as-Inference (b)ActiveInference
BothAIFandCAIcanbeformalisedinthecontextofapartiallyobservedMarkovDecisionProcess(POMDP).Let
adenoteactions,sdenotestatesandodenoteobservations. InaPOMDPsetting,statetransitionsaregovernedby
s â€p ps |s ,a qwhereasobservationsaregovernedbyo â€p po |s q. Wealsoassumethattheenvironment
t`1 env t`1 t t t env t t
possessesarewardfunctionr :SË†AÃ‘R1 whichmapsfromstate-actionpairstoascalarreward. Agentsencode
(andpotentiallylearn)agenerativemodelpps ,a ,o qthatdescribestherelationshipbetweenstates,actions,
t:T t:T t:T
observationsuptoatimehorizonT.AIFandCAIarebothconcernedwithinferringtheposteriordistributionoverlatent
variablesppa ,s |o q. However,solvingthisâ€˜value-freeâ€™inferenceproblemwillnotbyitselfleadtoadaptive
t:T t:T t:T
behaviour. Additionalassumptionsarerequiredtobiasinferencetowardsinferringactionsthatleadtoâ€˜valuableâ€™states.
2 ControlasInference
CAIincorporatesthenotionofâ€˜valueâ€™byintroducinganadditionalâ€˜optimalityâ€™variableO ,whereO â€œ1implies
t t
that time step t was optimal, meaning that given that the later timesteps t ` 1 : T are also optimal, the whole
trajectory t : T is optimal. In what follows, we simplify notation by assuming ppO q â€œ ppO â€œ 1q. The goal of
t t
CAI then to recover the posterior over states and actions, given the belief that the agent will observe itself being
optimal, i.e. pps ,a |o ,O q. By including the optimality variable we can write the agentâ€™s generative model as
t t t Å›t
pps ,a ,o ,O q â€œ T ppO |s ,a qppo |s qppa |s qpps |s ,a q1. Inferringtheposteriorpps ,a |o ,O q
t:T t:T t:T t t t t t t t t t t tÂ´1 tÂ´1 t t t t
is generally intractable, but it can approximated by introducing an auxillary variational distribution q ps ,a q â€œ
Ï† t t
q pa |s qqps qandoptimisingthevariationalboundLpÏ†q:
Ï† t t t
Â´ Â¯
LpÏ†qâ€œD q ps ,a q}pps ,a ,o ,O q
KL Ï† t t t t t t
â€œ â€° Â´ Â¯
â€œlÂ´o E oooqo Ï† oposoo t o,oao t oqooolmnopopooOooto|osooto,oaootooqn`D loKoooLooo q oo p o s ooto q o } oo p o p m s to | o s ootoÂ´oo1o , oo a otoÂ´oo1oo q on
(1)
â€œ Extrins Â´ ic Value Â¯â€° State dive â€œ rgence â€°
`E loqoposootoqoo D oooKooLoooo q oÏ†oo p m a too |s ooto q o } oo p oo p o a ooto | o s oto q ooon Â´E loqo Ï† ooposo t o,oao t oqoomlnoopoopooooto|osotooqn
Action Divergence Observation Ambiguity
Minimising Eq. 1 â€“ a process known as variational inference â€“ will cause the approximate posterior q ps ,a q to
Ï† t t
tendtowardsthetrueposteriorpps ,a |o ,O q,andwillalsocausethemarginal-likelihoodofoptimalityppO qtobe
t t t t t
maximised.
1NotethatCAIisusuallyformulatedinthecontextofanMDPratherthanaPOMDP.WehavepresentedthePOMDPcaseto
maintainconsistencywithAIF,butbothframeworkscanbeappliedinbothMDPsandPOMDPs.
2
APREPRINT-JUNE30,2020
ThesecondequalityinEq. 1demonstratesthatthisvariationalboundcanbedecomposedintofourterms. Thefirstterm
(extrinsicvalue)quantifiesthelikelihoodthatsomestate-actionpairisoptimal. IntheCAIliterature,thelikelihood
ofoptimalityisusuallydefinedasppO
t
|s
t
,a
t
q:â€œerpst,atq,suchthatlnppO
t
|s
t
,a
t
qâ€œrps
t
,a
t
q. Extrinsicvaluethus
quantifiestheexpectedrewardofsomestate-actionpair,suchthatminimisingLpÏ†qmaximisesexpectedreward. 2
Thestatedivergenceandactiondivergencetermsquantifythedegreetowhichstatesandactionsdivergefromtheir
respectivepriors. Theapproximateposterioroverstatesandtheagentâ€™smodelofstatedynamicsareassumedtobeequal
qps q:â€œpps |s ,a q,suchthattheagentbelievesithasnocontroloverthedynamicsexceptthroughaction. Ifthis
t t tÂ´1 tÂ´1
assumptionisnotmade,thistypicallyleadstorisk-seekingpolicies,astheagentassumesitcanalterthedynamics
arbitrarilytoavoidbadoutcomes(Levine,2018). Thisassumptioneliminatesthesecondterm(statedivergence)from
thebound. Moreover,undertheassumptionthattheactionpriorisuniformppa |s q:â€œ 1 ,theactiondivergenceterm
t t |A|
reducestothenegativeentropyofactions. Maximisinganactionentropytermprovidesseveralbenefits,includinga
mechanismforofflinelearning,improvedexplorationandincreasedalgorithmicstability(Haarnoja,Zhou,Abbeel,&
Levine,2018;Haarnoja,Zhou,Hartikainen,etal.,2018). Thefourthterm(observationambiguity)encouragesagents
toseekoutstateswhichhaveaprecisemappingtoobservations,andonlyarisesinaPOMDPsetting. Ineffectthis
leadstoagentsthatimplicitlytrytominimizetheoverheadofaPOMDPcomparedtoanMDPbytryingtostaywithin
regionsofthestate-spacewhichhavealow-entropymappingtoobservations.
2.1 Inferringplans
Traditionally,CAIhasbeenconcernedwithinferringpolicies,ortime-dependentstate-actionmappings. Here,we
reformulatethestandardCAIapproachtoinsteadinferfixedactionsequences,orplansÏ€ â€œta ,...,a u. Specifically,
t T
we derive a novel variational bound for CAI and show that it can be used to derive an expression for the optimal
time-independentplan. Weadaptthegenerativemodelandapproximateposteriortoaccountforatemporalsequenceof
Å› Å›
variablespps ,Ï€,o ,O q â€œ T ppO |s ,Ï€qppo |s qpps |s ,Ï€qppÏ€qandqps ,Ï€q â€œ T qps |Ï€qqpÏ€q. The
t:T t:T t:T t t t t t t tÂ´1 t:T t t
optimalpolicycanthenberetrievedas:
Â´ Â¯
Lâ€œD qps ,Ï€q}pps ,Ï€,o ,O q
KL t:T t:T t:T t:T
Â´ Ã¿T Â¯ Â´ Ã¿T Â¯ (2)
â€œD qpÏ€q}ppÏ€qexppÂ´ L pÏ€qq Ã¹Ã± qËšpÏ€qâ€œÏƒ ppÏ€qÂ´ L pÏ€q
KL t t
t t
TheoptimalpolicyisthusapathintegraloftheL pÏ€qwhichcanbewrittenas:
â€œ t
L pÏ€qâ€œE lnqps |Ï€qÂ´lnpps ,Ï€,o ,O qs
t qpst|Ï€q â€œ t â€° t tÂ´ t Â¯ â€œ â€°
â€œÂ´l E oqoposoo t o|Ï€ooqooolonomppooOooto|osooto,oÏ€oooqn`D looKooLoooo q o p oo s oto | o Ï€ ooo q m }p o p o s ooto | o s ootoÂ´oo1o , oo Ï€ oo q on Â´E loqoposoo t o|oÏ€oqooolmnopopoooooto|osotooqn (3)
Extrinsic Value State divergence Observation Ambiguity
WhichisequivalenttoEq. 1exceptthatitomitstheaction-divergenceterm.
3 ActiveInference
UnlikeCAI,AIFdoesnotintroduceadditionalvariablesincorporateâ€˜valueâ€™intothegenerativemodel. Instead,AIF
assumesthatthegenerativemodelisintrinsicallybiasedtowardsvaluablestatesorobservations. Forinstance,wemight
assumethatthepriordistributionoverobservationsisbiasedtowardsobservingrewards,lnpËœpo
t:T
q9erpot:Tq,wherewe
usenotationpËœpÂ¨qtodenoteabiaseddistribution3. Lettheagentâ€™sgenerativemodelbedefinedaspËœps ,o ,Ï€q â€œ
Å› Å› t:T t:T
ppÏ€q T pps |o ,Ï€qpËœpo |Ï€q,andtheapproximateposteriorasqps ,Ï€qâ€œqpÏ€q T qps |Ï€q.
t t t t t:T t t
Itisthenpossibletoderiveananalyticalexpressionfortheoptimalplan:
â€œ â€°
Â´FpÏ€qâ€œE lnqps ,Ï€qÂ´lnpËœpo ,s ,Ï€q
qpot:T,st:T,Ï€q t:T t:T t:T
` Ã¿T Ë˜ (4)
Ã¹Ã± qËšpÏ€qâ€œÏƒ lnppÏ€qÂ´ F pÏ€q
t
t
2Anadditional,butminordifferencebetweentheframeworksisthatCAItypicallyassumesthatrewardscomefromstate-action
pairs r â€ rps t ,a tq while AIF typically assumes rewards are a function of observations r â€ rpo tq. This difference can be
straightforwardlyfinessedbyeitherreparametrisingCAI-rewardsintermsofobservationsorAIF-rewardsintermsofstatesand
actions.
3AIFisusuallyformulatedonlyintermsofobservationswheresomeobservationsaremoredesiredthanothers.Weintroduced
rewardstoretainconsistencywithCAI.
3
APREPRINT-JUNE30,2020
whereÂ´F pÏ€qisreferredtoastheexpectedfreeenergy(notethatotherfunctionalsareconsistentwithAIF(Millidge,
t
Tschantz,&Buckley,2020)). Givenauniformprioroverpolicies,behaviourisdeterminedbytheexpectedfreeenergy
functional,whichdecomposesinto:
â€œ â€°
Â´F pÏ€qâ€œÂ´E lnqps |Ï€qÂ´lnpËœpo ,s |Ï€q
t qpot,st|Ï€q â€œ t â€° t â€t ` Ë˜Ä±
â€œlÂ´oo E ooqopoooo t o,oso t o|oÏ€omq olonoopoËœopooooto|oÏ€oooqnÂ´ l E oqoopoooto|oÏ€ooqoo D oooKooLoooo q o p om s t | oo o ooto , o Ï€ oo q oo } o q oo p o s otoo |Ï€ oo q ooon (5)
Extrinsic Value Intrinsic Value
wherewehavemadetheassumptionthattheinferenceprocedureisapproximatelycorrect,suchthatqps |o ,Ï€q Â«
t t
pps |o ,Ï€q. As agents are required to minimise Eq. 5, they are required to maximise both extrinsic and intrinsic
t t
value. Extrinsic value measures the degree to which expected observations are consistent with prior beliefs about
favourableobservations. UndertheassumptionthatlnpËœpo
t:T
q9erpot:Tq,thisisequivalenttoseekingoutrewarding
observations. Intrinsicvalueisequivalenttotheexpectedinformationgainoverstates,whichcompelsagentstoseek
informativeobservationsthatmostreduceposterior-stateuncertaintyandwhichleadstolargeupdatesbetweentheprior
andposteriorbeliefsaboutstates.
3.1 Inferringpolicies
WhileAIFisusuallyformulatedintermsoffixedactionsequences,itcanalsobeformulatedintermsofpolicies(i.e.
state-actionmappings). Lettheagentâ€™sgenerativemodelbedefinedaspËœps ,o ,a q â€œ pps |o ,a qppa |s qpËœpo |a q,
t t t t t t t t t t
andtheapproximateposteriorasq ps ,a qâ€œq pa |s qqps q. Wecannowwritetheexpectedfreeenergyfunctionalin
Ï† t t Ï† t t t
termsofthepolicyparametersÏ†:
â€ Ä±
Â´F pÏ†qâ€œE lnq pa ,s qÂ´lnpËœps ,o ,a q
t qpot,st,atq
â€œ
Ï† t t
â€°
t t
â€
t
` Ë˜Ä± â€ ` Ë˜Ä±
â€œÂ´l E oqoopooo t o|oao t ooqoomlnopoËœopooooto|oaootooqnÂ´ l E oqoopoooto,oaooto|sootoqoo D oooKooLoooo q o p m s too | o o ooto , o a ooto q oo } o q oo p o s otoo |a ootoo q oon `E loqooposotoqooo D ooKooLoooo q oÏ†oo p m a too | o s oto q oo } o p oo p o a ooto | o s otoo q oon
Extrinsic Value Intrinsic Value Action Divergence
(6)
InferringpolicieswithAIFthusrequiresminimizinganactiondivergencetermwhichisnotpresentwheninferring
plansbutisdirectlyequivalenttotheaction-divergencetermintheCAIformulation.
4 EncodingValue
WehaveshownthatbothAIFandCAIcanbeformulatedasvariationalinference,forbothfixedactionsequences
(i.e. plans)andpolicies(i.e. state-actionmappings). Wenowmoveontoconsiderthekeydifferencebetweenthese
frameworksâ€“howtheyencodeâ€˜valueâ€™.AIFencodesvaluedirectlyintothegenerativemodelasaprioroverobservations,
whereasinCAItheextrinsicvalueiseffectivelyencodedintothelikelihoodwhich,byBayesrule,relatestotheprioras
ppo |s qâ€œppo q ppstq . WhenappliedwithinaKLdivergence,thisfractionbecomesanegativeinformationgain. We
t t t ppst|otq
elucidatethisdistinctionbymodellingafurthervariantofactiveinference,whichherewecalllikelihood-AIF,where
insteadofabiasedprioroverrewardstheagenthasabiasedlikelihoodpËœpo ,s qâ€œpËœpo |s qpps q. Thelikelihood-AIF
t t t t t
objectivefunctionalFË† pÏ†qbecomes:
â€œ â€°
Â´FË† pÏ†qâ€œE lnq ps ,a qÂ´lnpËœpo ,s ,a q
t qÏ†pst,ot,atq â€œ Ï† t â€° t Â´ t t t Â¯ Â´ Â¯
â€œlÂ´oo E ooqo Ï† ooposo t o,oao t ooqmlonooopËœopooootoo|osotooqn`D loKoooLooo q oo p o s ooto q o } oo p o p m s to | o s ootoÂ´oo1o , oo a otoÂ´oo1oo q on ` l D ooKooLoooo q oÏ†oo p o a ooto | m s to q oo } o p oo p o a ooto | o s otoo q on
Extrinsic Value State divergence Action Divergence
IfwesetlnpËœpo |s qâ€œlnppO |s ,a q,thisisexactlyequivalenttotheCAIobjectiveinthecaseofMDPs. Thefact
t t t t t
thatlikelihood-AIFonPOMDPsisequivalenttoCAIonMDPsisduetothefactthattheobservationmodalityinAIFis
â€˜hijackedâ€™bytheencodingofvalue,andthuseffectivelycontainsonelessdegree-of-freedomcomparedtoCAI,which
maintainsaseparateveridicalrepresentationofobservationlikelihoods. AfurtherconnectionisthatAIFonMDPsis
equivalenttoKLcontrol(K.Rawliketal.,2010;K.Rawlik,Toussaint,&Vijayakumar,2013;K.C.Rawlik,2013;
vandenBroek,Wiegerinck,&Kappen,2010),andtherecentlyproposedstate-marginal-matching(Lee,Eysenbach,
Parisotto,Xing,&Levine,2019)objectives. Weleavefurtherexplorationofthesesimilaritiestofuturework.
4
APREPRINT-JUNE30,2020
5 Discussion
Inthiswork,wehavehighlightedthelargedegreeofoverlapbetweentheframeworksofactiveinference(AIF)and
controlasinference(CAI)andhaveexploredhoweachframeworkencodesvalueintothegenerativemodel,thereby
turning a value-free inference problem into one that can serve the purposes of adaptive action. CAI augments the
â€˜naturalâ€™probabilisticgraphicalmodelwithexogenousoptimalityvariables. 4. Incontrast,AIFleavesthestructureof
thegraphicalmodelunalteredandinsteadencodesvalueintothegenerativemodeldirectly. Thesetwoapproacheslead
tosignificantdifferencesbetweentheirrespectivefunctionals. AIF,bycontaminatingtheveridicalgenerativemodel
withvalue-imbuingbiases,losesadegreeoffreedomcomparedtoCAIwhichmaintainsastrictseparationbetweenthe
veridicalgenerativemodeloftheenvironmentanditsgoals. InPOMDPs,thisapproachresultsinCAIbeingsensitiveto
anâ€˜observation-ambiguityâ€™termwhichisabsentintheAIFformulation. Secondly,thedifferentmethodsforencoding
theprobabilityofgoalsâ€“likelihoodsinCAIandpriorsinAIFâ€“leadtodifferentexploratorytermsintheobjective
functionals. Specifically,AIFisendowedwithanexpectedinformationgainthatCAIlacks. AIFapproachesthuslend
themselvesnaturallytogoal-directedexplorationwhereasCAImandatesonlyrandom,entropy-maximizingexploration.
Thesedifferentwaysofencodinggoalsintoprobabilisticmodelsalsolendthemselvestomorephilosophicalinterpreta-
tions. CAI,byviewinggoalsasanadditionalexogenousfactorinanotherwiseunbiasedinferenceprocess,maintainsa
cleanseparationbetweenveridicalperceptionandcontrol,thusmaintainingthemodularitythesisofseparateperception
andactionmodules(Baltieri&Buckley,2018). ThismakesCAIapproachesconsonantwithmainstreamviewsin
machinelearningthatseethegoalofperceptionasrecoveringveridicalrepresentationsoftheworld,andcontrolas
usingthisworld-modeltoplanactions. Incontrast,AIFelidesthesecleanboundariesbetweenunbiasedperceptionand
actionbyinsteadpositingthatbiasedperception(Tschantz,Seth,&Buckley,2020)iscrucialtoadaptiveaction. Rather
thanmaintaininganunbiasedworldmodelthatpredictslikelyconsequences,AIFinsteadmaintainsabiasedgenerative
modelwhichpreferentiallypredictsourpreferencesbeingfulfilled. Active-inferencethusalignscloselywithenactive
andembodiedapproaches(Baltieri&Buckley,2019;Clark,2015)tocognition,whichviewtheaction-perceptionloop
asacontinualflowratherthanasequenceofdistinctstages.
We have thus seen how two means of encoding preferences into inference problems leads to two distinct families
ofalgorithms,eachoptimisingsubtlydifferentfunctionals,resultingindifferingbehaviour. Thisraisesthenatural
questionsofwhichmethodshouldbepreferred,andwhetherthesearetheonlytwopossiblemethods.Onecanimagine
explicitlymodellingtheexpectedreward,andbiasinginferenceswithpriorsovertheexpectedreward. Alternatively,
agentscouldmaintaindesireddistributionsoverallofstates,observations,andactions,whichwouldmaximizethe
flexibilityinspecifyinggoalsintrinsictotheframework. Futureresearchwillexplorethesepotentialextensionstothe
framework,theirrelationtooneanother,andtheobjectivefunctionalstheyinduce.
References
Abdolmaleki,A.,Springenberg,J.T.,Tassa,Y.,Munos,R.,Heess,N.,&Riedmiller,M. (2018). Maximumaposteriori
policyoptimisation. arXivpreprintarXiv:1806.06920.
Attias,H. (2003). Planningbyprobabilisticinference. InAistats.
Baltieri,M.,&Buckley,C.L. (2018). Themodularityofactionandperceptionrevisitedusingcontroltheoryandactive
inference. InArtificiallifeconferenceproceedings(pp.121â€“128).
Baltieri,M.,&Buckley,C.L. (2019). Generativemodelsasparsimoniousdescriptionsofsensorimotorloops. arXiv
preprintarXiv:1904.12937.
Beal,M.J. (2003). Variationalalgorithmsforapproximatebayesianinference(Unpublisheddoctoraldissertation).
UCL(UniversityCollegeLondon).
Clark,A. (2015). Radicalpredictiveprocessing. TheSouthernJournalofPhilosophy,53,3â€“27.
Fountas,Z.,Sajid,N.,Mediano,P.A.,&Friston,K. (2020). Deepactiveinferenceagentsusingmonte-carlomethods.
arXivpreprintarXiv:2006.04176.
Friston,K. (2008). Hierarchicalmodelsinthebrain. PLoScomputationalbiology,4(11).
Friston,K. (2010). Thefree-energyprinciple: aunifiedbraintheory? Naturereviewsneuroscience,11(2),127â€“138.
Friston,K.,FitzGerald,T.,Rigoli,F.,Schwartenbeck,P.,&Pezzulo,G. (2017). Activeinference: aprocesstheory.
Neuralcomputation,29(1),1â€“49.
Friston, K., Kilner, J.,&Harrison, L. (2006). Afreeenergyprincipleforthebrain. JournalofPhysiology-Paris,
100(1-3),70â€“87.
Friston,K.J.,Daunizeau,J.,&Kiebel,S.J. (2009). Reinforcementlearningoractiveinference? PloSone,4(7).
4Utilising optimality variables is not strictly necessary for CAI. In the case of undirected graphical models, an additional
undirectedfactorcanbeappendedtoeachnode(Ziebart,2010). Interestingly,thisapproachbearssimilaritiestotheprocedure
adoptedinParrandFriston(2019),suggestingafurtherconnectionbetweengeneralisedfreeenergyandCAI.
5
APREPRINT-JUNE30,2020
Friston,K.J.,Parr,T.,&deVries,B. (2017). Thegraphicalbrain: beliefpropagationandactiveinference. Network
Neuroscience,1(4),381â€“414.
Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep
reinforcementlearningwithastochasticactor. arXivpreprintarXiv:1801.01290.
Haarnoja,T.,Zhou,A.,Hartikainen,K.,Tucker,G.,Ha,S.,Tan,J.,... others (2018). Softactor-criticalgorithmsand
applications. arXivpreprintarXiv:1812.05905.
Lee,L.,Eysenbach,B.,Parisotto,E.,Xing,E.,&Levine,S. (2019). Efficientexplorationviastatemarginalmatching.
arXivpreprintarXiv:1906.05274.
Levine,S. (2018). Reinforcementlearningandcontrolasprobabilisticinference: Tutorialandreview. arXivpreprint
arXiv:1805.00909.
Millidge,B. (2019a). Combiningactiveinferenceandhierarchicalpredictivecoding: Atutorialintroductionandcase
study.
Millidge,B. (2019b). Implementingpredictiveprocessingandactiveinference: Preliminarystepsandresults.
Millidge,B. (2020). Deepactiveinferenceasvariationalpolicygradients. JournalofMathematicalPsychology,96,
102348.
Millidge, B., Tschantz, A., & Buckley, C. L. (2020). Whence the expected free energy? arXiv preprint
arXiv:2004.08128.
Parr, T., & Friston, K. J. (2019). Generalised free energy and active inference. Biological cybernetics, 113(5-6),
495â€“513.
Rawlik,K.,Toussaint,M.,&Vijayakumar,S. (2010). Approximateinferenceandstochasticoptimalcontrol. arXiv
preprintarXiv:1009.3958.
Rawlik,K.,Toussaint,M.,&Vijayakumar,S. (2013). Onstochasticoptimalcontrolandreinforcementlearningby
approximateinference. InTwenty-thirdinternationaljointconferenceonartificialintelligence.
Rawlik,K.C. (2013). Onprobabilisticinferenceapproachestostochasticoptimalcontrol.
Sutton,R.S.,&Barto,A.G. (2018). Reinforcementlearning: Anintroduction. MITpress.
Tschantz, A., Baltieri, M., Seth, A., Buckley, C. L., et al. (2019). Scaling active inference. arXiv preprint
arXiv:1911.10601.
Tschantz,A.,Millidge,B.,Seth,A.K.,&Buckley,C.L. (2020). Reinforcementlearningthroughactiveinference.
arXivpreprintarXiv:2002.12636.
Tschantz,A.,Seth,A.K.,&Buckley,C.L. (2020). Learningaction-orientedmodelsthroughactiveinference. PLoS
computationalbiology,16(4),e1007805.
UeltzhÃ¶ffer,K. (2018). Deepactiveinference. Biologicalcybernetics,112(6),547â€“573.
vandenBroek,L.,Wiegerinck,W.,&Kappen,H.J. (2010). Risksensitivepathintegralcontrol.
Wainwright,M.J.,&Jordan,M.I. (2008). Graphicalmodels,exponentialfamilies,andvariationalinference. Now
PublishersInc.
Walsh,K.S.,McGovern,D.P.,Clark,A.,&Oâ€™Connell,R.G. (2020). Evaluatingtheneurophysiologicalevidencefor
predictiveprocessingasamodelofperception. AnnalsoftheNewYorkAcademyofSciences,1464(1),242.
Ziebart,B.D. (2010). Modelingpurposefuladaptivebehaviorwiththeprincipleofmaximumcausalentropy.
6