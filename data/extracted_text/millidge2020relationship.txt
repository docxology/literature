ON THE RELATIONSHIP OF ACTIVE INFERENCE AND CONTROL
AS INFERENCE
Beren Millidge
School of Informatics
University of Edinburgh
beren@millidge.name
Alexander Tschantz
Sackler Center for Consciousness Science
School of Engineering and Informatics
University Sussex
tschantz.alec@gmail.com
Anil K Seth
Sackler Center for Consciousness Science
Evolutionary and Adaptive Systems Research Group
School of Engineering and Informatics
University of Sussex
A.K.Seth@sussex.ac.uk
Christopher L Buckley
Evolutionary and Adaptive Systems Research Group
School of Engineering and Informatics
University of Sussex
C.L.Buckley@sussex.ac.uk
June 30, 2020
ABSTRACT
Active Inference (AIF) is an emerging framework in the brain sciences which suggests that biological
agents act to minimise a variational bound on model evidence. Control-as-Inference (CAI) is a
framework within reinforcement learning which casts decision making as a variational inference
problem. While these frameworks both consider action selection through the lens of variational
inference, their relationship remains unclear. Here, we provide a formal comparison between them
and demonstrate that the primary difference arises from how value is incorporated into their respective
generative models. In the context of this comparison, we highlight several ways in which these
frameworks can inform one another.
Active Inference (AIF) is an emerging framework from theoretical neuroscience which proposes a uniï¬ed account of
perception, learning, and action (K. Friston, 2008, 2010; K. Friston, Kilner, & Harrison, 2006). This framework posits
that agents embody a generative model of their environment and perception and learning take place through a process
of variational inference on this generative model by minimizing an information-theoretic quantity â€“ the variational
free energy (Beal, 2003; K. J. Friston, Parr, & de Vries, 2017; Wainwright & Jordan, 2008). Within this framework,
action selection can be cast as a process of inference, underwritten by the same mechanisms which perform perceptual
inference and learning (K. Friston, FitzGerald, Rigoli, Schwartenbeck, & Pezzulo, 2017; K. J. Friston, Daunizeau,
& Kiebel, 2009; Tschantz, Millidge, Seth, & Buckley, 2020). Implementations of active inference have a degree of
biological plausibility (K. Friston et al., 2017) and are supported by considerable empirical evidence (Walsh, McGovern,
Clark, & Oâ€™Connell, 2020). Moreover, recent work has shown that active inference can be applied to high-dimensional
tasks and environments (Fountas, Sajid, Mediano, & Friston, 2020; Millidge, 2019a, 2019b, 2020; Tschantz, Baltieri,
Seth, Buckley, et al., 2019; Tschantz, Millidge, et al., 2020; UeltzhÃ¶ffer, 2018).
The ï¬eld of reinforcement learning (RL) (Sutton & Barto, 2018) is also concerned with understanding adaptive action
selection. RL assumes that agents seek to maximise the expected sum of rewards (which are generally assumed to be
exogenous, not intrinsic to the agent), and then will select the actions that will maximize reward. In recent years, the
framework of control as inference (CAI) (Abdolmaleki et al., 2018; Attias, 2003; Levine, 2018; K. Rawlik, Toussaint, &
Vijayakumar, 2010; K. C. Rawlik, 2013) has recast the problem of RL in the language of variational inference. Instead
of maximizing rewards, agents must infer actions that lead to optimal trajectories. This reformulation enables the use of
powerful inference algorithms in RL, while also providing a natural method of exploration (Abdolmaleki et al., 2018;
Haarnoja, Zhou, Abbeel, & Levine, 2018; Haarnoja, Zhou, Hartikainen, et al., 2018).
arXiv:2006.12964v3  [cs.AI]  29 Jun 2020
A PREPRINT - JUNE 30, 2020
Both AIF and CAI view adaptive action selection as a problem of inference. However, despite these similarities,
the formal relationship between the two frameworks remains unclear. In this work, we attempt to shed light on this
relationship. We present both AIF and CAI in a common language, highlighting connections between the frameworks
which may have otherwise been overlooked. We then consider the key distinction between the frameworks, namely,
how â€˜valueâ€™ or â€˜goalsâ€™ are encoded into the generative model. We discuss how this distinction leads to subtle differences
in the objectives that both schemes optimize, and suggest how these differences may impact behaviour.
1 Formalism
ğ’ª1 ğ’ª2 ğ’ª3
a1 a2 a3
s1 s2 s3
o1 o2 o3 â€¦
â€¦
â€¦
ğ’ªT
aT
sT
oT
(a) Control-as-Inference
a1 a2 a3
s1 s2 s3
â€¦
â€¦
â€¦ aT
sT
Ëœo1 Ëœo2 Ëœo3 ËœoT (b) Active Inference
Both AIF and CAI can be formalised in the context of a partially observed Markov Decision Process (POMDP). Let
a denote actions, s denote states and o denote observations. In a POMDP setting, state transitions are governed by
st`1 â€penvpst`1|st,atqwhereas observations are governed by ot â€penvpot|stq. We also assume that the environment
possesses a reward function r: S Ë†A Ã‘R1 which maps from state-action pairs to a scalar reward. Agents encode
(and potentially learn) a generative model ppst:T,at:T,ot:Tqthat describes the relationship between states, actions,
observations up to a time horizonT. AIF and CAI are both concerned with inferring the posterior distribution over latent
variables ppat:T,st:T|ot:Tq. However, solving this â€˜value-freeâ€™ inference problem will not by itself lead to adaptive
behaviour. Additional assumptions are required to bias inference towards inferring actions that lead to â€˜valuableâ€™ states.
2 Control as Inference
CAI incorporates the notion of â€˜valueâ€™ by introducing an additional â€˜optimalityâ€™ variableOt, where Ot â€œ1 implies
that time step t was optimal, meaning that given that the later timesteps t`1 : T are also optimal, the whole
trajectory t : T is optimal. In what follows, we simplify notation by assuming ppOtq â€œppOt â€œ1q. The goal of
CAI then to recover the posterior over states and actions, given the belief that the agent will observe itself being
optimal, i.e. ppst,at|ot,Otq. By including the optimality variable we can write the agentâ€™s generative model as
ppst:T,at:T,ot:T,Otqâ€œ Å›T
t ppOt|st,atqppot|stqppat|stqppst|stÂ´1,atÂ´1q1. Inferring the posterior ppst,at|ot,Otq
is generally intractable, but it can approximated by introducing an auxillary variational distribution qÏ†pst,atq â€œ
qÏ†pat|stqqpstqand optimising the variational bound LpÏ†q:
LpÏ†qâ€œ DKL
Â´
qÏ†pst,atq}ppst,at,ot,Otq
Â¯
â€œÂ´EqÏ†pst,atq
â€œ
ln ppOt|st,atq
â€°
loooooooooooooooomoooooooooooooooon
Extrinsic Value
`DKL
Â´
qpstq}ppst|stÂ´1,atÂ´1q
Â¯
looooooooooooooooomooooooooooooooooon
State divergence
`Eqpstq
â€œ
DKL
Â´
qÏ†pat|stq}ppat|stq
Â¯â€°
loooooooooooooooooooomoooooooooooooooooooon
Action Divergence
Â´EqÏ†pst,atq
â€œ
ln ppot|stq
â€°
loooooooooooomoooooooooooon
Observation Ambiguity
(1)
Minimising Eq. 1 â€“ a process known as variational inference â€“ will cause the approximate posterior qÏ†pst,atqto
tend towards the true posterior ppst,at|ot,Otq, and will also cause the marginal-likelihood of optimality ppOtqto be
maximised.
1Note that CAI is usually formulated in the context of an MDP rather than a POMDP. We have presented the POMDP case to
maintain consistency with AIF, but both frameworks can be applied in both MDPs and POMDPs.
2
A PREPRINT - JUNE 30, 2020
The second equality in Eq. 1 demonstrates that this variational bound can be decomposed into four terms. The ï¬rst term
(extrinsic value) quantiï¬es the likelihood that some state-action pair is optimal. In the CAI literature, the likelihood
of optimality is usually deï¬ned as ppOt|st,atq:â€œerpst,atq, such that ln ppOt|st,atqâ€œ rpst,atq. Extrinsic value thus
quantiï¬es the expected reward of some state-action pair, such that minimising LpÏ†qmaximises expected reward. 2
The state divergence and action divergence terms quantify the degree to which states and actions diverge from their
respective priors. The approximate posterior over states and the agentâ€™s model of state dynamics are assumed to be equal
qpstq:â€œppst|stÂ´1,atÂ´1q, such that the agent believes it has no control over the dynamics except through action. If this
assumption is not made, this typically leads to risk-seeking policies, as the agent assumes it can alter the dynamics
arbitrarily to avoid bad outcomes (Levine, 2018). This assumption eliminates the second term (state divergence) from
the bound. Moreover, under the assumption that the action prior is uniform ppat|stq:â€œ 1
|A|, the action divergence term
reduces to the negative entropy of actions. Maximising an action entropy term provides several beneï¬ts, including a
mechanism for ofï¬‚ine learning, improved exploration and increased algorithmic stability (Haarnoja, Zhou, Abbeel, &
Levine, 2018; Haarnoja, Zhou, Hartikainen, et al., 2018). The fourth term (observation ambiguity) encourages agents
to seek out states which have a precise mapping to observations, and only arises in a POMDP setting. In effect this
leads to agents that implicitly try to minimize the overhead of a POMDP compared to an MDP by trying to stay within
regions of the state-space which have a low-entropy mapping to observations.
2.1 Inferring plans
Traditionally, CAI has been concerned with inferring policies, or time-dependent state-action mappings. Here, we
reformulate the standard CAI approach to instead infer ï¬xed action sequences, or plans Ï€â€œtat,..., aTu. Speciï¬cally,
we derive a novel variational bound for CAI and show that it can be used to derive an expression for the optimal
time-independent plan. We adapt the generative model and approximate posterior to account for a temporal sequence of
variables ppst:T,Ï€, ot:T,Ot:Tqâ€œ Å›T
t ppOt|st,Ï€qppot|stqppst|stÂ´1,Ï€qppÏ€qand qpst:T,Ï€qâ€œ Å›T
t qpst|Ï€qqpÏ€q. The
optimal policy can then be retrieved as:
L â€œDKL
Â´
qpst:T,Ï€q}ppst:T,Ï€, ot:T,Ot:Tq
Â¯
â€œDKL
Â´
qpÏ€q}ppÏ€qexppÂ´
TÃ¿
t
LtpÏ€qq
Â¯
Ã¹Ã± qËšpÏ€qâ€œ Ïƒ
Â´
ppÏ€qÂ´
TÃ¿
t
LtpÏ€q
Â¯ (2)
The optimal policy is thus a path integral of the LtpÏ€qwhich can be written as:
LtpÏ€qâ€œ Eqpst|Ï€q
â€œ
ln qpst|Ï€qÂ´ ln ppst,Ï€, ot,Otqs
â€œÂ´ Eqpst|Ï€q
â€œ
ln ppOt|st,Ï€q
â€°
looooooooooooomooooooooooooon
Extrinsic Value
`DKL
Â´
qpst|Ï€q}ppst|stÂ´1,Ï€q
Â¯
looooooooooooooooomooooooooooooooooon
State divergence
Â´Eqpst|Ï€q
â€œ
ln ppot|stq
â€°
looooooooooomooooooooooon
Observation Ambiguity
(3)
Which is equivalent to Eq. 1 except that it omits the action-divergence term.
3 Active Inference
Unlike CAI, AIF does not introduce additional variables incorporate â€˜valueâ€™ into the generative model. Instead, AIF
assumes that the generative model is intrinsically biased towards valuable states or observations. For instance, we might
assume that the prior distribution over observations is biased towards observing rewards,ln Ëœppot:Tq9erpot:Tq, where we
use notation ËœppÂ¨qto denote a biased distribution3. Let the agentâ€™s generative model be deï¬ned as Ëœppst:T,ot:T,Ï€qâ€œ
ppÏ€qÅ›T
t ppst|ot,Ï€qËœppot|Ï€q, and the approximate posterior as qpst:T,Ï€qâ€œ qpÏ€qÅ›T
t qpst|Ï€q.
It is then possible to derive an analytical expression for the optimal plan:
Â´FpÏ€qâ€œ Eqpot:T,st:T,Ï€q
â€œ
ln qpst:T,Ï€qÂ´ ln Ëœppot:T,st:T,Ï€q
â€°
Ã¹Ã± qËšpÏ€qâ€œ Ïƒ
`
ln ppÏ€qÂ´
TÃ¿
t
FtpÏ€q
Ë˜ (4)
2An additional, but minor difference between the frameworks is that CAI typically assumes that rewards come from state-action
pairs r â€ rpst, atqwhile AIF typically assumes rewards are a function of observations r â€ rpotq. This difference can be
straightforwardly ï¬nessed by either reparametrising CAI-rewards in terms of observations or AIF-rewards in terms of states and
actions.
3AIF is usually formulated only in terms of observations where some observations are more desired than others. We introduced
rewards to retain consistency with CAI.
3
A PREPRINT - JUNE 30, 2020
where Â´FtpÏ€qis referred to as the expected free energy (note that other functionals are consistent with AIF (Millidge,
Tschantz, & Buckley, 2020)). Given a uniform prior over policies, behaviour is determined by the expected free energy
functional, which decomposes into:
Â´FtpÏ€qâ€œÂ´ Eqpot,st|Ï€q
â€œ
ln qpst|Ï€qÂ´ ln Ëœppot,st|Ï€q
â€°
â€œÂ´Eqpot,st|Ï€q
â€œ
ln Ëœppot|Ï€q
â€°
loooooooooooooomoooooooooooooon
Extrinsic Value
Â´Eqpot|Ï€q
â€
DKL
`
qpst|ot,Ï€q}qpst|Ï€q
Ë˜Ä±
loooooooooooooooooooooomoooooooooooooooooooooon
Intrinsic Value
(5)
where we have made the assumption that the inference procedure is approximately correct, such that qpst|ot,Ï€qÂ«
ppst|ot,Ï€q. As agents are required to minimise Eq. 5, they are required to maximise both extrinsic and intrinsic
value. Extrinsic value measures the degree to which expected observations are consistent with prior beliefs about
favourable observations. Under the assumption that ln Ëœppot:Tq9erpot:Tq, this is equivalent to seeking out rewarding
observations. Intrinsic value is equivalent to the expected information gain over states, which compels agents to seek
informative observations that most reduce posterior-state uncertainty and which leads to large updates between the prior
and posterior beliefs about states.
3.1 Inferring policies
While AIF is usually formulated in terms of ï¬xed action sequences, it can also be formulated in terms of policies (i.e.
state-action mappings). Let the agentâ€™s generative model be deï¬ned as Ëœppst,ot,atqâ€œ ppst|ot,atqppat|stqËœppot|atq,
and the approximate posterior as qÏ†pst,atqâ€œ qÏ†pat|stqqpstq. We can now write the expected free energy functional in
terms of the policy parameters Ï†:
Â´FtpÏ†qâ€œ Eqpot,st,atq
â€
ln qÏ†pat,stqÂ´ ln Ëœppst,ot,atq
Ä±
â€œÂ´ Eqpot|atq
â€œ
ln Ëœppot|atq
â€°
loooooooooooomoooooooooooon
Extrinsic Value
Â´Eqpot,at|stq
â€
DKL
`
qpst|ot,atq}qpst|atq
Ë˜Ä±
looooooooooooooooooooooooomooooooooooooooooooooooooon
Intrinsic Value
`Eqpstq
â€
DKL
`
qÏ†pat|stq}ppat|stq
Ë˜Ä±
loooooooooooooooooooomoooooooooooooooooooon
Action Divergence
(6)
Inferring policies with AIF thus requires minimizing an action divergence term which is not present when inferring
plans but is directly equivalent to the action-divergence term in the CAI formulation.
4 Encoding Value
We have shown that both AIF and CAI can be formulated as variational inference, for both ï¬xed action sequences
(i.e. plans) and policies (i.e. state-action mappings). We now move on to consider the key difference between these
frameworks â€“ how they encode â€˜valueâ€™. AIF encodes value directly into the generative model as a prior over observations,
whereas in CAI the extrinsic value is effectively encoded into the likelihood which, by Bayes rule, relates to the prior as
ppot|stqâ€œ ppotq ppstq
ppst|otq. When applied within a KL divergence, this fraction becomes a negative information gain. We
elucidate this distinction by modelling a further variant of active inference, which here we call likelihood-AIF, where
instead of a biased prior over rewards the agent has a biased likelihood Ëœppot,stqâ€œ Ëœppot|stqppstq. The likelihood-AIF
objective functional Ë†FpÏ†qbecomes:
Â´Ë†FtpÏ†qâ€œ EqÏ†pst,ot,atq
â€œ
ln qÏ†pst,atqÂ´ ln Ëœppot,st,atq
â€°
â€œÂ´EqÏ†pst,atq
â€œ
ln Ëœppot|stq
â€°
loooooooooooooomoooooooooooooon
Extrinsic Value
`DKL
Â´
qpstq}ppst|stÂ´1,atÂ´1q
Â¯
looooooooooooooooomooooooooooooooooon
State divergence
`DKL
Â´
qÏ†pat|stq}ppat|stq
Â¯
looooooooooooooomooooooooooooooon
Action Divergence
If we set ln Ëœppot|stqâ€œ ln ppOt|st,atq, this is exactly equivalent to the CAI objective in the case of MDPs. The fact
that likelihood-AIF on POMDPs is equivalent to CAI on MDPs is due to the fact that the observation modality in AIF is
â€˜hijackedâ€™ by the encoding of value, and thus effectively contains one less degree-of-freedom compared to CAI, which
maintains a separate veridical representation of observation likelihoods. A further connection is that AIF on MDPs is
equivalent to KL control (K. Rawlik et al., 2010; K. Rawlik, Toussaint, & Vijayakumar, 2013; K. C. Rawlik, 2013;
van den Broek, Wiegerinck, & Kappen, 2010), and the recently proposed state-marginal-matching (Lee, Eysenbach,
Parisotto, Xing, & Levine, 2019) objectives. We leave further exploration of these similarities to future work.
4
A PREPRINT - JUNE 30, 2020
5 Discussion
In this work, we have highlighted the large degree of overlap between the frameworks of active inference (AIF) and
control as inference (CAI) and have explored how each framework encodes value into the generative model, thereby
turning a value-free inference problem into one that can serve the purposes of adaptive action. CAI augments the
â€˜naturalâ€™ probabilistic graphical model with exogenous optimality variables.4. In contrast, AIF leaves the structure of
the graphical model unaltered and instead encodes value into the generative model directly. These two approaches lead
to signiï¬cant differences between their respective functionals. AIF, by contaminating the veridical generative model
with value-imbuing biases, loses a degree of freedom compared to CAI which maintains a strict separation between the
veridical generative model of the environment and its goals. In POMDPs, this approach results in CAI being sensitive to
an â€˜observation-ambiguityâ€™ term which is absent in the AIF formulation. Secondly, the different methods for encoding
the probability of goals â€“ likelihoods in CAI and priors in AIF â€“ lead to different exploratory terms in the objective
functionals. Speciï¬cally, AIF is endowed with an expected information gain that CAI lacks. AIF approaches thus lend
themselves naturally to goal-directed exploration whereas CAI mandates only random, entropy-maximizing exploration.
These different ways of encoding goals into probabilistic models also lend themselves to more philosophical interpreta-
tions. CAI, by viewing goals as an additional exogenous factor in an otherwise unbiased inference process, maintains a
clean separation between veridical perception and control, thus maintaining the modularity thesis of separate perception
and action modules (Baltieri & Buckley, 2018). This makes CAI approaches consonant with mainstream views in
machine learning that see the goal of perception as recovering veridical representations of the world, and control as
using this world-model to plan actions. In contrast, AIF elides these clean boundaries between unbiased perception and
action by instead positing that biased perception (Tschantz, Seth, & Buckley, 2020) is crucial to adaptive action. Rather
than maintaining an unbiased world model that predicts likely consequences, AIF instead maintains a biased generative
model which preferentially predicts our preferences being fulï¬lled. Active-inference thus aligns closely with enactive
and embodied approaches (Baltieri & Buckley, 2019; Clark, 2015) to cognition, which view the action-perception loop
as a continual ï¬‚ow rather than a sequence of distinct stages.
We have thus seen how two means of encoding preferences into inference problems leads to two distinct families
of algorithms, each optimising subtly different functionals, resulting in differing behaviour. This raises the natural
questions of which method should be preferred, and whether these are the only two possible methods.One can imagine
explicitly modelling the expected reward, and biasing inferences with priors over the expected reward. Alternatively,
agents could maintain desired distributions over all of states, observations, and actions, which would maximize the
ï¬‚exibility in specifying goals intrinsic to the framework. Future research will explore these potential extensions to the
framework, their relation to one another, and the objective functionals they induce.
References
Abdolmaleki, A., Springenberg, J. T., Tassa, Y ., Munos, R., Heess, N., & Riedmiller, M. (2018). Maximum a posteriori
policy optimisation. arXiv preprint arXiv:1806.06920.
Attias, H. (2003). Planning by probabilistic inference. In Aistats.
Baltieri, M., & Buckley, C. L. (2018). The modularity of action and perception revisited using control theory and active
inference. In Artiï¬cial life conference proceedings (pp. 121â€“128).
Baltieri, M., & Buckley, C. L. (2019). Generative models as parsimonious descriptions of sensorimotor loops. arXiv
preprint arXiv:1904.12937.
Beal, M. J. (2003). Variational algorithms for approximate bayesian inference(Unpublished doctoral dissertation).
UCL (University College London).
Clark, A. (2015). Radical predictive processing. The Southern Journal of Philosophy, 53, 3â€“27.
Fountas, Z., Sajid, N., Mediano, P. A., & Friston, K. (2020). Deep active inference agents using monte-carlo methods.
arXiv preprint arXiv:2006.04176.
Friston, K. (2008). Hierarchical models in the brain. PLoS computational biology, 4(11).
Friston, K. (2010). The free-energy principle: a uniï¬ed brain theory? Nature reviews neuroscience, 11(2), 127â€“138.
Friston, K., FitzGerald, T., Rigoli, F., Schwartenbeck, P., & Pezzulo, G. (2017). Active inference: a process theory.
Neural computation, 29(1), 1â€“49.
Friston, K., Kilner, J., & Harrison, L. (2006). A free energy principle for the brain. Journal of Physiology-Paris,
100(1-3), 70â€“87.
Friston, K. J., Daunizeau, J., & Kiebel, S. J. (2009). Reinforcement learning or active inference? PloS one, 4(7).
4Utilising optimality variables is not strictly necessary for CAI. In the case of undirected graphical models, an additional
undirected factor can be appended to each node (Ziebart, 2010). Interestingly, this approach bears similarities to the procedure
adopted in Parr and Friston (2019), suggesting a further connection between generalised free energy and CAI.
5
A PREPRINT - JUNE 30, 2020
Friston, K. J., Parr, T., & de Vries, B. (2017). The graphical brain: belief propagation and active inference. Network
Neuroscience, 1(4), 381â€“414.
Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep
reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290.
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., . . . others (2018). Soft actor-critic algorithms and
applications. arXiv preprint arXiv:1812.05905.
Lee, L., Eysenbach, B., Parisotto, E., Xing, E., & Levine, S. (2019). Efï¬cient exploration via state marginal matching.
arXiv preprint arXiv:1906.05274.
Levine, S. (2018). Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint
arXiv:1805.00909.
Millidge, B. (2019a). Combining active inference and hierarchical predictive coding: A tutorial introduction and case
study.
Millidge, B. (2019b). Implementing predictive processing and active inference: Preliminary steps and results.
Millidge, B. (2020). Deep active inference as variational policy gradients. Journal of Mathematical Psychology, 96,
102348.
Millidge, B., Tschantz, A., & Buckley, C. L. (2020). Whence the expected free energy? arXiv preprint
arXiv:2004.08128.
Parr, T., & Friston, K. J. (2019). Generalised free energy and active inference. Biological cybernetics, 113(5-6),
495â€“513.
Rawlik, K., Toussaint, M., & Vijayakumar, S. (2010). Approximate inference and stochastic optimal control. arXiv
preprint arXiv:1009.3958.
Rawlik, K., Toussaint, M., & Vijayakumar, S. (2013). On stochastic optimal control and reinforcement learning by
approximate inference. In Twenty-third international joint conference on artiï¬cial intelligence.
Rawlik, K. C. (2013). On probabilistic inference approaches to stochastic optimal control.
Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.
Tschantz, A., Baltieri, M., Seth, A., Buckley, C. L., et al. (2019). Scaling active inference. arXiv preprint
arXiv:1911.10601.
Tschantz, A., Millidge, B., Seth, A. K., & Buckley, C. L. (2020). Reinforcement learning through active inference.
arXiv preprint arXiv:2002.12636.
Tschantz, A., Seth, A. K., & Buckley, C. L. (2020). Learning action-oriented models through active inference. PLoS
computational biology, 16(4), e1007805.
UeltzhÃ¶ffer, K. (2018). Deep active inference. Biological cybernetics, 112(6), 547â€“573.
van den Broek, L., Wiegerinck, W., & Kappen, H. J. (2010). Risk sensitive path integral control.
Wainwright, M. J., & Jordan, M. I. (2008). Graphical models, exponential families, and variational inference. Now
Publishers Inc.
Walsh, K. S., McGovern, D. P., Clark, A., & Oâ€™Connell, R. G. (2020). Evaluating the neurophysiological evidence for
predictive processing as a model of perception. Annals of the New York Academy of Sciences, 1464(1), 242.
Ziebart, B. D. (2010). Modeling purposeful adaptive behavior with the principle of maximum causal entropy.
6