1202
yaM
21
]GL.sc[
1v88260.5012:viXra
Anomaly Detection via Controlled Sensing and
Deep Active Inference
Geethu Joseph, Chen Zhong, M. Cenk Gursoy, Senem Velipasalar, and Pramod K. Varshney
Department of of Electrical Engineering and Computer Science
Syracuse University
New York 13244, USA
Emails:{gjoseph,czhong03,mcgursoy,svelipas,varshney}@syr.edu.
Abstract—In this paper, we address the anomaly detection this incurs a large cost (e.g., higher energy consumption
problem where the objective is to find the anomalous processes that reduces the life span of the sensor network). Therefore,
amongagiven setofprocesses. Tothisend,thedecision-making
the agent uses the controlled sensing technique with which it
agent probes a subset of processes at every time instant and
probesasmallsubsetofprocessesateverytimeinstant.Inthis
obtains a potentially erroneous estimate of the binary variable
which indicates whether or not the corresponding process is context,weaddressthequestionofhowtheagentsequentially
anomalous. The agent continues to probe the processes until it chooses a subset of processes so that it accurately detects the
obtainsasufficientnumberofmeasurementstoreliablyidentify anomalies with a minimum delay and a minimum number of
theanomalousprocesses.Inthiscontext,wedevelopasequential
sensor measurements.
selectionalgorithmthatdecideswhichprocessestobeprobedat
Aclassicalapproachtosolvethesequentialsensorselection
everyinstanttodetecttheanomalieswithanaccuracyexceeding
adesiredvaluewhileminimizingthedelayinmakingthedecision problem is based on the active hypothesis testing frame-
and the total number of measurements taken. Our algorithm work [3], [4] where the decision-making agent constructs a
is based on active inference which is a general framework to hypothesiscorrespondingto each of the possible states of the
make sequential decisions in order to maximize the notion of
processesanddeterminewhichoneofthesehypothesesistrue.
freeenergy.Wedefinethefreeenergyusingtheobjectivesofthe
Activehypothesistestingisawell-studiedproblemandseveral
selection policy and implement the active inference framework
using a deep neural network approximation. Using numerical solutionstrategieshavebeenproposedintheliterature[5]–[9].
experiments,wecompareouralgorithmwiththestate-of-the-art However, these approaches provide model-based algorithms
method based on deep actor-critic reinforcement learning and which are designed under simplified modeling assumptions.
demonstrate the superior performance of our algorithm.
This has motivated the researchers to design data-driven
Index Terms—Active hypothesis testing, anomaly detection,
deep learning algorithms [3], [4], [10]. These algorithms are
active inference, quickest state estimation, sequential decision-
making, sequential sensing. not only more flexible than traditional algorithms, but they
also possess reduced computational complexity. The existing
I. INTRODUCTION literature along these lines relies on the mostfundamentalre-
inforcementlearning(RL)algorithmssuchasQ-learning[10]
In many practical applications such as remote health mon-
and actor-critic [3], [4]. However, recently a new framework
itoring using sensors, the goal is to identify the anomalies
called active inference has been shown to be a promising
among a given set of functionalities of a system [1], [2].
complement to the traditional RL approaches for several
Here, the system is equipped with multiple sensors and each
sequentialdecision-makingproblems[11]–[13].Therefore,in
sensor monitors a different, but not necessarily independent
thispaper,we developandimplementa novelpolicyto select
functionality (which we henceforth refer to as a process) of
processes to obtain measurements at each step, inspired by
the system. The sensor sends its observationsto the decision-
the active inference approach.
making agent over a communication link, and the received
The contributions of the paper are as follows: we first
observation may be distorted due to the unreliability in the
define the notion of free-energy based on the entropy as-
sensorhardwareand/orthenoisylink(e.g.,awirelesschannel)
sociated with the estimate of the states of the processes
between the sensor and the agent. Hence, the decision agent
and the cost of sensing. This allows us to reformulate the
needs to probe each process multiple times before it declares
anomaly detection problem as an active inference problem
oneormoreoftheprocessestobeanomalouswiththedesired
in which the goal is to minimize the free energy. We then
confidence. Repeatedly probing all the processes allows the
implement our algorithm using deep neural networks which
agent to quickly find any potential system malfunction, but
are relatively less explored in the context of active inference.
Our algorithm balances the model-based and the data-driven
Theinformation,data,orworkpresentedhereinwasfundedinpartbyNa-
tionalScienceFoundation(NSF)underGrant1618615,Grant1739748,Grant approachesofactiveinference.Specifically,weusethemodel-
1816732andbytheAdvancedResearchProjectsAgency-Energy(ARPA-E), based posterior updates to tackle the uncertainties in the
U.S.DepartmentofEnergy,underAwardNumberDE-AR0000940.Theviews
observations, and the data-driven neural network to handle
and opinions of authors expressed herein do not necessarily state or reflect
thoseoftheUnitedStates Government oranyagency thereof. the underlying statistical dependence between the processes.
The active inference approach has many similarities to the and therefore, it is biased towards the agent’s preferences.
reinforcement-basedalgorithms,suchaslearningprobabilistic Given a generative model, the agent inverts the model using
models, exploration and exploitation of various actions, and the method of approximate Bayesian inference. To this end,
efficient planning. So we compare our algorithm with the it defines a variational distribution q that the agent controls.
existing RL-based approach presented in [4] using numerical The distribution q is optimized by minimizing the Kullback-
simulations.Weobservethatthedelayinestimationissmaller Leibler (KL) divergence between the distributions q and Q.
for our method while the corresponding accuracy and cost Therefore, if we choose actions from the distribution q, they
of sensing are competitive to the performance of the RL- fulfill the agent’s preferences. The KL divergence between
based method given in [4]. This advantage makes our active the variational distribution and the generative model is called
inference-based approach a better alternative to the existing the variational free energy. In short, the goal of the active
RL-based method. inferenceagentis to minimize its expectedfree energy(EFE)
into the future up to the stopping time K. Next, we provide
II. ANOMALY DETECTION PROBLEM
thedetailsofthe activeinferenceframeworkinthe contextof
We consider N random processes that are potentially sta- anomaly detection.
tistically dependent.Each process is in one of the two states:
normal (denoted by 0) or anomalous (denoted by 1). The
A. Environment
states of these processes are denoted by a random vector
s∈{0,1}N. The goalof the workis to detectthe anomalous The environment of the active inference framework refers
processes out of the N processes, which is equivalent to to the set of states, actions, and observations. In the context
estimating the random vector s. The dependence pattern and of our anomaly detection problem, we define the state of the
the number of anomalous processes are unknown to the active inference framework at time k as the posterior belief
decision-making agent. π(k) on the random vector s ∈ {0,1}N. Since there are
To estimate s, the decision-making agent probes one or m = 2N possible values for s, the posterior belief is an
more processes at every time instant and obtains potentially m−dimensional vector π ∈[0,1]m. Further, the actions refer
erroneous observationsof the correspondingentries of s. Let to the selection of which processes to observe A ∈ P, and
k
c th o e rre se sp t o o n f di p n r g oc o e b ss s e e s rv p a r ti o o b n ed ve a c t to ti r m b e e k y b A e k ( A k) k ∈ ∈ P {0, a 1 n } d |A t k h | e . y A W k e de fi n r o st te n s o t t h e e t o h b a s t e a rv t a t t i i m on e s. k, the information available to
Here, P denotes the power set of {1,2,...,N} without the the agent is the set of processes observed till time k and the
nullset(|P|=2N−1).The observationcorrespondingto the k
ith process at time k, denoted by y
i
(k) ∈ {0,1}, obeys the correspondingobservationvectors: A j ,y Aj
j=1
.Usingthis
following probabilistic model: information,the posterior belief vecntor π(k)o∈[0,1]m can be
computed in closed form as follows [4]:
s with probability 1−p
i
y (k)= (1)
i (1−s i with probability p, π (k)= π i (k−1) a∈Ak (1−p)
where p ∈ [0,1] denotes the probability that the observation i Q h differs from the actual state of the process. We assume
that given s, the observations obtained across different time
instantsare jointly (conditionally)independent.Also, probing
each process incurs a cost of sensing of λ ≥ 0, i.e., the cost
of sensing at time k is |A |λ.
k
At each time k, the agent determines which processes to
observe(A )untilitdeclarestheestimateofswiththedesired
k
confidence. The selection policy is designed such that the
stopping time K and the total cost of sensing λ K |A |
k=1 k
are minimized.
P
III. ANOMALY DETECTIONUSING DEEPACTIVE
INFERENCE
Theactiveinferenceframeworkreliesonanormativetheory
of brain function based on its perception of the environment.
At a high level, the active inference agent maintains a gen-
erative model that represents its perception. The generative
model Q comprises a joint probability distribution on the
state of the environment, the actions, and the corresponding
observations. The generative model assigns higher probabili-
ties to the states and actions that are favorable to the agent,
1 +p Ea,k,i 1 Ec a,k,i
m π (k−1) (1−p) i i=1 i a∈Ak
h
P Q
1 +p Ea,k,i 1
,
Ec a,k,i
(2)
i
where
1
is the indicator function and the event E ,
a,k,i
{y (k)=s |H=i} denotes the event that the observation
a a
obtained and the corresponding state are the same, when the
indexcorrespondingtothetruevalueofsisH=i.Also, the
event Ec , {y (k)6=s |H=i} denotes the complement
a,k,i a a
of E . As a result, given the previous state π(k−1), the
a,k,i
action A and the observation y , we can exactly compute
k A
the updated posterior belief π(k) using (2). Therefore, the
generative modelthat learns the environmentis a distribution
on the actions and the observations: Q(A ,y |π(k−1)) . k Ak
B. Preferences
Inthissubsection,we considerthe preferencesof theagent
that defines the generative model. Recall that our goal is to
estimate the vector s with confidence exceeding a specific
level while minimizing the stopping time K and the cost of
sensing λ K |A |. Clearly, the best estimate of s based
k=1 k
ontheposteriorbeliefcorrespondstoi∗(k), argmaxπ (k),
i
P i=1,2,...,m
andtheconfidenceassociatedwiththeestimationisπ i∗(k) (k). The goal of the agent is to minimize the total free-energy of
Therefore, the agent terminates the detection algorithm when the expected trajectories into the future:
argmaxπ i (k)>π upper , (3) K
i=1,2,...,m G(A,π)= E{F(j)|A =A,π(k−1)=π}. (10)
k
where π is the desired level of confidence. In short, the
upper j=k
X
decisionmakingreliesonlyontheposteriorbeliefπ(k).Also,
In other words, the agent computes the expected free-energy
as k increases, we get more observations and the posterior
of all paths into the future and probabilistically chooses an
belief becomesmore accurate.Therefore,the selection policy
action that minimizes the expected free-energy. Therefore, a
µ is a function of the latest value of the posterior belief:
popular choice for the distribution over the actions assigned
µ(π(k−1))=A .
k
by the generative model is a Boltzmann distribution over the
Furtherexploringtheobjectiveofthepolicydesign,wenote
expected free energies [11], [14], [15]:
that minimizing the stopping time is identical to driving the
largestentryofπ(k)toπ assoonaspossible.Weachieve
upper Q(A|π(k−1))=σ(−G(A,π(k−1))), (11)
thisbyminimizingtheentropyH(π(K))ofπ(K)becausethe
entropyisminimizedwhenthelargestentryofπ(K)is1and
where σ(·) is again the softmax function, and G is given by
the remaining entries are zeros. Here, the entropy is given by
(10).
m So far, we have presented the conceptual aspects of our
H(π)=− π log(π ). (4)
i i algorithm. We next discuss how to compute the expressions
X i=1 in (9) and (10).
We note that this approachis differentfrom the Bayesian log
likelihood ratio based-approach in [3], [4], [10]. Therefore, D. Deep-learning based implementation
we define the instantaneous objective function that the agent
We implement our algorithm using deep neural networks.
aims to minimize at time k as follows:
We start with the computation of the free energy in (9):
r(k)=H(π(k))−H(π(k−1))+λ|A |. (5)
k
This definition ensures that the overall objective function is F =−H(q(A|π(k−1)))
given by − q(A|π(k−1))logQ(A|π(k−1)), (12)
K K A∈P
X
r(k)=H(π(K))−H(π(0))+ λ|A |, (6)
k
where the entropy term H(q(A|π(k − 1))) is a function
k=1 k=1
X X of the variational distribution q which is controlled by the
where minimizing H(π(K)) − H(π(0)) minimizes the en-
agent. We implement this distribution using a neural network
tropy in the posterior belief as H(π(0)) is a constant, and
which we refer to as the policy network. The policy neural
minimizing K λ|A | minimizes the total cost of sens-
k=1 k network takes the posterior belief π(k − 1) as the input
ing. The instantaneous objective function r(k) represents the and outputs stochastic selection policy q ∈ [0,1]m−1 which
P θ
preferences of the agent at time k and it is encoded into the
is a probability distribution on P and parameterized by θ.
generativemodelas the priorprobabilityonthe belief vector:
Therefore,the entropy term is computedusing the entropy of
the distribution outputted by the neural network. This neural
Q(y |A ,π(k−1))
Ak k networkalsogivesthepolicyimplementedbytheagent,which
=σ(−H(π(k))+H(π(k−1))−λ|A |), (7)
k is sampled from the distribution q learned at time k:
where σ(·) is the softmax function. Also, π(k) is a function
A =µ(π(k−1))∼q (π(k−1)). (13)
of π(k−1),A and y due to (2). We also note that k θ
k Ak
Q(y Ak ,A k |π(k−1))=Q(y Ak |A k ,π(k−1))Q(A k |π(k−1)). Further, the second term in (12) can be determined using
(8) (11) and (10). From (10), the EFE for a single time-step can
Therefore, the generative model is completely defined if we be approximated as follows [15]:
specify the distribution Q(A |π(k−1)). This distribution is
k
defined based on the EFE of the future as we discuss in the G(A ,π(k−1))≈−logQ(y |A ,π(k−1))
k Ak k
following subsection. +E {G(A,π(k))}. (14)
A∼Q(·|π(k)
C. Total expected free energy
Here, the first term is determined using (7). However, the
ThevariationalfreeenergyF istheKLdivergencebetween
second term in (14) involves explicit computation into the
the variational distribution q(A|π(k−1)) and the generative
future values. Therefore, we learn a bootstrap estimate of
model Q(A|π(k−1)). Thus,
this quantity using a neural network which we refer to as
q(A|π(k−1)) the bootstrappedEFE-network.LetG (A) denotethisneural
F(k)= q(A|π(k−1))log . (9) φ
Q(A|π(k−1)) network where φ is the parameter of the network. In other
A∈P
X
words, the estimate of the neural network is the predicted processesasN =3andthus,m=2N =8.Theprobabilityof
value of the free-energyof the system. Thus, (14) reduces to aprocessbeingnormalistakenasq =0.8.Here,thefirstand
second processes are assumed to be statistically dependent,
G(A k ,π(k−1))=H(π(k))−H(π(k−1)) and the third process is independent of the other two. The
+λ|A |+E {G (A ,π(k))}. (15) correlation between the dependent processes is captured by
k A∼Q(·|π(k) φ k+1
the parameter ρ∈[0,1]:
Substituting (15) and (11) into (12) completes the derivation
of the algorithm. P{s 1 =0,s 2 =0}=q2+ρq(1−q) (17)
To summarize, our solution involves two neural networks P{s =0,s =1}=q(1−q)(1−ρ) (18)
1 2
q θ and G φ which represent the policy and the expected free- P{s =1,s =0}=q(1−q)(1−ρ) (19)
1 2
energy, respectively. At every time instant, we sample an
P{s =1,s =1}=(1−q)2+ρq(1−q). (20)
action from the output distribution of the policy network 1 2
q θ and obtain the corresponding observation y Ak . Next, we Also, we assume that the crossover probability of the obser-
compute the bootstrapped EFE estimate and the variational vations is p = 0.8, and the maximum number of time slots
free energy using the neural networks and (12) and (15). for each episode (trial or run) is T =300.
max
Finally, the parameter θ of the policy network is modified Fortheactiveinferencealgorithm,weimplementthepolicy
byminimizingthevariationalfreeenergyF(k).Similarly,the neural network and the bootstrapped EFE-network with three
parameterφofthebootstrappedEFE-networkisoptimizedby layers and the ReLU activation function between consecutive
comparingEFE-networkoutputwiththevalueoftheexpected layers. To update the parameters of the neural networks, we
value G(A) calculated at time k. We use the ℓ 2 −norm of the apply the Adam Optimizer, and we set the learning rates of
difference between the two estimates: thepolicynetworkandthebootstrappedEFE-networkas10−6
L=kG (A)−G(A)k2. (16) and 5×10−6, respectively. The implementation of the actor-
φ
critic method is the same as that in [4] except that we use
The pseudo-code of the algorithm is summarized in Algo- the entropybased-rewardfunctionas defined in (5). Also, we
rithm 1 below. choose the learning rates of the actor and critic networks as
5×10−4 and 5×10−3, respectively.
Algorithm 1 Active inference for anomaly detection
The simulation results are presented in Figs. 1 to 3. Our
Initialization: • Policy network q θ (a|π) with parameters θ observations from the numerical results are as follows:
• BootstrappedEFE-networkG φ (π;a)withparametersφ • Success rate: In Fig. 1, we plot the success rates of
1: repeat the two algorithms as a function of the upper bound on
2: Initialize the prior state π 0 ∈ [0,1]m (can be learned the posterior π upper . The success rate is defined as the
from the training data) ratiobetweenthenumberoftimesthealgorithmcorrectly
3: Time index k =0 identifiesalltheanomalousprocessestothetotalnumber
4: while k <T and maxπ i >π upper and k <T max do of trials. We observe that the success rates achieved
i
5: Choose action A k ∼q θ (π(k−1)) by both algorithms are comparable in all the settings.
6: Generate observations y Ak,k Also, the success rate depends primarily on π upper and
7: Compute π(k+1) using (2) it is almost insensitive to λ and ρ. This is intuitive
8: Compute the bootstrapped EFE estimate G using because π upper sets the confidence level with which the
(15) algorithms identify the anomalies, and therefore, for the
9: Compute the variational free energy F using (11) same confidence level, the success rates achieved by the
and (12) algorithms are almost the same.
10: Update the policy network network by minimizing • Stopping time: In Fig. 2, we show the variation of the
the variational free energy F with respect to θ stopping time K with π upper . We see that the stopping
11: Update the bootstrapped EFE-network by minimiz- time increases with π upper in all cases, as a higher
ing the boostrapping loss in (16) with respect to φ value of π upper requires the algorithms to collect more
12: Increase time index k =k+1 observationsbeforetheymakethedecisionregardingthe
13: end while anomalousprocesses.Also, weobservethatthestopping
14: until time decreases with an increase in ρ for all values of λ
15: Declare the estimate corresponding to argmaxπ i andπ upper .Thisdecreaseisexpectedduetothefactthat
i asthecorrelationincreases,anobservationcorresponding
tooneofthedependentprocessesgivesmoreinformation
about the other. Consequently, the algorithms require
IV. NUMERICAL RESULTS
fewer observations, and thus, a smaller stopping time,
In thissection,we presentnumericalresultscomparingour to achieve the same confidence level. Finally, we notice
algorithm with the actor-critic method in [4]. The simulation that the stopping time for the active inference algorithm
setup is similar to that in [4]. We choose the number of is less than that of the actor-critic algorithm.
1
0.98
0.96
0.94
0.92
0.9
0.88
0.86
0.8 0.85 0.9 0.95 1
Upper bound on belief
upper
etar
sseccuS
1
Active Inference, = 0
Actor-Critic, = 0
Active Inference, = 0.3 0.98
Actor-Critic, = 0.3
Active Inference, = 0.8 0.96
Actor-Critic, = 0.8
0.94
0.92
0.9
0.88
0.86
0.8 0.85 0.9 0.95 1
Upper bound on belief
upper
(a) Cost per measurement λ=0.05
etar
sseccuS
1
Active Inference, = 0
Actor-Critic, = 0
Active Inference, = 0.3
Actor-Critic, = 0.3
Active Inference, = 0.8
Actor-Critic, = 0.8 0.95
0.9
0.85
0.8 0.85 0.9 0.95 1
Upper bound on belief
upper
(b) Cost per measurement λ=0.1
etar
sseccuS
Active Inference, = 0
Actor-Critic, = 0
Active Inference, = 0.3
Actor-Critic, = 0.3
Active Inference, = 0.8
Actor-Critic, = 0.8
(c) Cost per measurement λ=0.2
Fig. 1: Variation of the success rate of the active inference and the actor-critic algorithms when π ,λ and ρ are varied.
upper
22
20
18
16
14
12
10
8
6
4
0.8 0.85 0.9 0.95 1
Upper bound on belief
upper
K
emit
gnippotS
25
Active Inference, = 0
Actor-Critic, = 0
Active Inference, = 0.3
Actor-Critic, = 0.3 20
Active Inference, = 0.8
Actor-Critic, = 0.8
15
10
5
0
0.8 0.85 0.9 0.95 1
Upper bound on belief
upper
(a) Cost per measurement λ=0.05
K
emit
gnippotS
25
Active Inference, = 0
Actor-Critic, = 0
Active Inference, = 0.3
Actor-Critic, = 0.3 20
Active Inference, = 0.8
Actor-Critic, = 0.8
15
10
5
0
0.8 0.85 0.9 0.95 1
Upper bound on belief
upper
(b) Cost per measurement λ=0.1
K
emit
gnippotS
Active Inference, = 0
Actor-Critic, = 0
Active Inference, = 0.3
Actor-Critic, = 0.3
Active Inference, = 0.8
Actor-Critic, = 0.8
(c) Cost per measurement λ=0.2
Fig.2:VariationofthestoppingtimeK oftheactiveinferenceandtheactor-criticalgorithmswhenπ ,λandρ arevaried.
upper
24
22
20
18
16
14
12
10
8
6
0.8 0.85 0.9 0.95 1
Upper bound on belief
upper
stnemerusaem
fo
rebmun
latoT
24
Active Inference, = 0
Actor-Critic, = 0 22
Active Inference, = 0.3
Actor-Critic, = 0.3 20
Active Inference, = 0.8
Actor-Critic, = 0.8 18
16
14
12
10
8
6
0.8 0.85 0.9 0.95 1
Upper bound on belief
upper
(a) Cost per measurement λ=0.05
stnemerusaem
fo
rebmun
latoT
26
Active Inference, = 0
Actor-Critic, = 0 24
Active Inference, = 0.3
Actor-Critic, = 0.3 22
Active Inference, = 0.8
Actor-Critic, = 0.8 20
18
16
14
12
10
8
0.8 0.85 0.9 0.95 1
Upper bound on belief
upper
(b) Cost per measurement λ=0.1
stnemerusaem
fo
rebmun
latoT
Active Inference, = 0
Actor-Critic, = 0
Active Inference, = 0.3
Actor-Critic, = 0.3
Active Inference, = 0.8
Actor-Critic, = 0.8
(c) Cost per measurement λ=0.2
Fig. 3: Variation of the total number of measurements K |A | of the active inference and the actor-critic algorithmswhen
k=1 k
π ,λ and ρ are varied.
upper
P
• Totalnumberofmeasurements: Fig.3comparesthetotal [1] W.-Y. Chung and S.-J. Oh, “Remote monitoring system with wireless
numberofmeasurements K |A |obtainedbythetwo sensorsmoduleforroomenvironment,”SensorsActuatorsB:Chemical,
k=1 k vol.113,no.1,pp.64–70,Jan.2006.
algorithmsin differentsettings.Clearly,the totalnumber
[2] A. Bujnowski, J. Ruminski, A. Palinski, and J. Wtrorek, “Enhanced
P
of measurementsdecreases with ρ, which is expected as remotecontrol providing medical functionalities,” inProc.Inter.Conf.
mentionedabove.Also,we inferthatthetotalnumberof PervasiveComput.TechHealthc. Workshops,May2013,pp.290–293.
[3] C. Zhong, M. C. Gursoy, and S. Velipasalar, “Deep actor-critic rein-
measurements obtained by both algorithms are similar
forcement learning for anomaly detection,” in Proc. Globecom, Dec.
in all the settings with the active inference algorithm 2019.
collecting slightly fewer measurements compared to the [4] G.Joseph,M.C.Gursoy,andP.K.Varshney,“Anomalydetectionunder
controlled sensing using actor-critic reinforcement learning,” in Proc.
actor-critic algorithm.
IEEEInter.WorkshopSPAWC,May2020.
Thus, we concludethat the two algorithmsachieve compa- [5] H. Chernoff, “Sequential design of experiments,” Ann. Math. Stat.,
vol.30,no.3,pp.755–770, Sep.1959.
rable success rates and incur a similar total cost of sensing,
[6] S. A. Bessler, “Theory and applications of the sequential design of
but the active inference algorithm has better stopping time experiments,k-actionsandinfinitelymanyexperiments:PartI-theory,”
comparedto the actor-criticalgorithm.This indicatesthatour StanfordUnivCAAppliedMathematicsandStatisticsLabs,Tech.Rep.,
1960.
algorithm identifies the anomalies faster than the actor-critic
[7] S.Nitinawarat,G.K.Atia,andV.V.Veeravalli,“Controlledsensingfor
algorithm.Moreover,the stoppingtime of ouralgorithmdoes multihypothesis testing,” IEEETrans. Autom.Control, vol.58, no.10,
not vary much with λ while the stopping time of the actor- pp.2451–2464, May2013.
[8] M. Naghshvar, T. Javidi et al., “Active sequential hypothesis testing,”
critic algorithm increases with λ. This implies that the actor-
Ann.Stat.,vol.41,no.6,pp.2703–2738, 2013.
critic algorithm is more sensitive to the instantaneous cost [9] B. Huang, K. Cohen, and Q. Zhao, “Active anomaly detection in
of sensing λ|A | than the total cost of sensing K λ|A |. heterogeneous processes,” IEEETrans. Inf. Theory, vol. 65, no. 4,pp.
k k=1 k 2284–2301, Aug.2018.
Toelaborate,wenotethatbothalgorithmscontinuetoacquire
[10] D.Kartik,E.Sabir,U.Mitra,andP.Natarajan,“Policydesignforactive
P
measurementsuntilthedesiredlevelconfidencelevelπ upper is sequential hypothesis testing using deep learning,” in Proc. Allerton,
achieved. However, since the actor-critic algorithm optimizes Oct.2018,pp.741–748.
the averagecost ofsensing 1 K λ|A |, as λincreases, it [11] K. Friston, F. Rigoli, D. Ognibene, C. Mathys, T. Fitzgerald, and
K k=1 k G.Pezzulo,“Activeinferenceandepistemicvalue,”J.Cogn.Neurosci.,
picks a fewer number of processes per time instant and this vol.6,no.4,pp.187–214, Oct.2015.
resultsin an increasedstopping P time. On thecontrary,the av- [12] K.Friston,T.FitzGerald, F.Rigoli,P.Schwartenbeck, andG.Pezzulo,
“Active inference: A process theory,” Neural Comput., vol. 29, no. 1,
eragenumberofprocessesselectedbyouralgorithmdoesnot
pp.1–49,Jan.2017.
vary much with λ. Therefore, we achieve better performance [13] K. J. Friston, M. Lin, C. D. Frith, G. Pezzulo, J. A. Hobson, and
by carefully designing the objective function using a novel S.Ondobaka,“Activeinference,curiosityandinsight,”NeuralComput.,
vol.29,no.10,pp.2633–2683,Oct.2017.
entropy based-function and the total cost of sensing whereas
[14] P.Schwartenbeck,J.Passecker,T.U.Hauser,T.H.FitzGerald,M.Kro-
the actor-critic algorithm optimizes the average change in nbichler,andK.J.Friston,“Computationalmechanismsofcuriosityand
entropy and the average cost of sensing. goal-directed exploration,” Elife,vol.8,p.e41703,2019.
[15] B.Millidge, “Deep active inference asvariational policy gradients,” J.
Math.Psychol.,vol.96,p.102348,Jan.2020.
V. CONCLUSION
Inthis paper,we presentedananomalydetectionalgorithm
using an active inference-based approach. We modeled the
problem of anomaly detection as an active inference problem
aiming at the detection accuracy exceeding a desired value
while minimizing the delay and total cost of sensing. We
designedanewobjectivefunctionbasedonentropyandimple-
mented the active inference algorithm using a deep learning-
based approach. Through simulation results, we compared
our algorithm with an algorithm based on the deep actor-
critic method in terms of the success rate, stopping time,
and total cost of sensing. The results demonstrated that our
algorithm can detect the anomalies quicker (as indicated by
thesmallerstoppingtimes)andachievesacompetitivesuccess
ratewithasimilarcostofsensingastheactor-criticalgorithm.
However, we detect all the anomalous processes at a given
time, assuming that the (normal or anomalous) behaviors
of the processes remain unchanged until the agent makes a
decision. Extending our algorithm to track any changes in
the behavior of the processes over a longer time period is an
interesting direction for future work.
REFERENCES