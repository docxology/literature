arXiv:2105.06288v1  [cs.LG]  12 May 2021
Anomaly Detection via Controlled Sensing and
Deep Active Inference
Geethu Joseph, Chen Zhong, M. Cenk Gursoy , Senem V elipasala r, and Pramod K. V arshney
Department of of Electrical Engineering and Computer Scien ce
Syracuse University
New Y ork 13244, USA
Emails:{gjoseph,czhong03,mcgursoy ,svelipas,varshney}@syr.edu.
Abstract—In this paper , we address the anomaly detection
problem where the objective is to ﬁnd the anomalous processe s
among a given set of processes. T o this end, the decision-mak ing
agent probes a subset of processes at every time instant and
obtains a potentially erroneous estimate of the binary vari able
which indicates whether or not the corresponding process is
anomalous. The agent continues to probe the processes until it
obtains a sufﬁcient number of measurements to reliably iden tify
the anomalous processes. In this context, we develop a seque ntial
selection algorithm that decides which processes to be prob ed at
every instant to detect the anomalies with an accuracy excee ding
a desired value while minimizing the delay in making the deci sion
and the total number of measurements taken. Our algorithm
is based on active inference which is a general framework to
make sequential decisions in order to maximize the notion of
free energy. W e deﬁne the free energy using the objectives of the
selection policy and implement the active inference framew ork
using a deep neural network approximation. Using numerical
experiments, we compare our algorithm with the state-of-th e-art
method based on deep actor-critic reinforcement learning a nd
demonstrate the superior performance of our algorithm.
Index T erms—Active hypothesis testing, anomaly detection,
active inference, quickest state estimation, sequential d ecision-
making, sequential sensing.
I. I N T RO D U CT IO N
In many practical applications such as remote health mon-
itoring using sensors, the goal is to identify the anomalies
among a given set of functionalities of a system [1], [2].
Here, the system is equipped with multiple sensors and each
sensor monitors a different, but not necessarily independe nt
functionality (which we henceforth refer to as a process) of
the system. The sensor sends its observations to the decisio n-
making agent over a communication link, and the received
observation may be distorted due to the unreliability in the
sensor hardware and/or the noisy link (e.g., a wireless chan nel)
between the sensor and the agent. Hence, the decision agent
needs to probe each process multiple times before it declare s
one or more of the processes to be anomalous with the desired
conﬁdence. Repeatedly probing all the processes allows the
agent to quickly ﬁnd any potential system malfunction, but
The information, data, or work presented herein was funded i n part by Na-
tional Science Foundation (NSF) under Grant 1618615, Grant 1739748, Grant
1816732 and by the Advanced Research Projects Agency-Energ y (ARP A-E),
U.S. Department of Energy , under A ward Number DE-AR0000940 . The views
and opinions of authors expressed herein do not necessarily state or reﬂect
those of the United States Government or any agency thereof.
this incurs a large cost (e.g., higher energy consumption
that reduces the life span of the sensor network). Therefore ,
the agent uses the controlled sensing technique with which it
probes a small subset of processes at every time instant. In t his
context, we address the question of how the agent sequential ly
chooses a subset of processes so that it accurately detects t he
anomalies with a minimum delay and a minimum number of
sensor measurements.
A classical approach to solve the sequential sensor selecti on
problem is based on the active hypothesis testing frame-
work [3], [4] where the decision-making agent constructs a
hypothesis corresponding to each of the possible states of t he
processes and determine which one of these hypotheses is tru e.
Active hypothesis testing is a well-studied problem and sev eral
solution strategies have been proposed in the literature [5 ]–[9].
However, these approaches provide model-based algorithms
which are designed under simpliﬁed modeling assumptions.
This has motivated the researchers to design data-driven
deep learning algorithms [3], [4], [10]. These algorithms a re
not only more ﬂexible than traditional algorithms, but they
also possess reduced computational complexity. The existi ng
literature along these lines relies on the most fundamental re-
inforcement learning (RL) algorithms such as Q-learning [1 0]
and actor-critic [3], [4]. However, recently a new framewor k
called active inference has been shown to be a promising
complement to the traditional RL approaches for several
sequential decision-making problems [11]–[13]. Therefor e, in
this paper, we develop and implement a novel policy to select
processes to obtain measurements at each step, inspired by
the active inference approach.
The contributions of the paper are as follows: we ﬁrst
deﬁne the notion of free-energy based on the entropy as-
sociated with the estimate of the states of the processes
and the cost of sensing. This allows us to reformulate the
anomaly detection problem as an active inference problem
in which the goal is to minimize the free energy. W e then
implement our algorithm using deep neural networks which
are relatively less explored in the context of active infere nce.
Our algorithm balances the model-based and the data-driven
approaches of active inference. Speciﬁcally, we use the mod el-
based posterior updates to tackle the uncertainties in the
observations, and the data-driven neural network to handle
the underlying statistical dependence between the process es.
The active inference approach has many similarities to the
reinforcement-based algorithms, such as learning probabi listic
models, exploration and exploitation of various actions, a nd
efﬁcient planning. So we compare our algorithm with the
existing RL-based approach presented in [4] using numerica l
simulations. W e observe that the delay in estimation is smal ler
for our method while the corresponding accuracy and cost
of sensing are competitive to the performance of the RL-
based method given in [4]. This advantage makes our active
inference-based approach a better alternative to the exist ing
RL-based method.
II. A N O M A LY DE T E CT IO N PRO BL E M
W e consider N random processes that are potentially sta-
tistically dependent. Each process is in one of the two state s:
normal (denoted by 0) or anomalous (denoted by 1). The
states of these processes are denoted by a random vector
s ∈ { 0, 1}N . The goal of the work is to detect the anomalous
processes out of the N processes, which is equivalent to
estimating the random vector s. The dependence pattern and
the number of anomalous processes are unknown to the
decision-making agent.
T o estimate s, the decision-making agent probes one or
more processes at every time instant and obtains potentiall y
erroneous observations of the corresponding entries of s. Let
the set of processes probed at time k be Ak ∈ P and the
corresponding observation vector be yAk (k) ∈ { 0, 1}|Ak|.
Here, P denotes the power set of {1, 2, . . . , N } without the
null set ( |P| = 2N − 1). The observation corresponding to the
ith process at time k, denoted by yi(k) ∈ { 0, 1}, obeys the
following probabilistic model:
yi(k) =
{
si with probability 1 − p
1 − si with probability p, (1)
where p ∈ [0, 1] denotes the probability that the observation
differs from the actual state of the process. W e assume
that given s, the observations obtained across different time
instants are jointly (conditionally) independent. Also, p robing
each process incurs a cost of sensing of λ ≥ 0, i.e., the cost
of sensing at time k is |Ak| λ.
At each time k, the agent determines which processes to
observe ( Ak) until it declares the estimate of s with the desired
conﬁdence. The selection policy is designed such that the
stopping time K and the total cost of sensing λ ∑ K
k=1 |Ak|
are minimized.
III. A N O M A LY DE T E CT IO N US IN G DE E P ACT IV E
IN F E RE N CE
The active inference framework relies on a normative theory
of brain function based on its perception of the environment .
At a high level, the active inference agent maintains a gen-
erative model that represents its perception. The generati ve
model Q comprises a joint probability distribution on the
state of the environment, the actions, and the correspondin g
observations. The generative model assigns higher probabi li-
ties to the states and actions that are favorable to the agent ,
and therefore, it is biased towards the agent’s preferences .
Given a generative model, the agent inverts the model using
the method of approximate Bayesian inference. T o this end,
it deﬁnes a variational distribution q that the agent controls.
The distribution q is optimized by minimizing the Kullback-
Leibler (KL) divergence between the distributions q and Q.
Therefore, if we choose actions from the distribution q, they
fulﬁll the agent’s preferences. The KL divergence between
the variational distribution and the generative model is ca lled
the variational free energy. In short, the goal of the active
inference agent is to minimize its expected free energy (EFE )
into the future up to the stopping time K. Next, we provide
the details of the active inference framework in the context of
anomaly detection.
A. Environment
The environment of the active inference framework refers
to the set of states, actions, and observations. In the conte xt
of our anomaly detection problem, we deﬁne the state of the
active inference framework at time k as the posterior belief
π(k) on the random vector s ∈ { 0, 1}N . Since there are
m = 2N possible values for s, the posterior belief is an
m−dimensional vector π ∈ [0, 1]m. Further, the actions refer
to the selection of which processes to observe Ak ∈ P , and
yAk denotes the observations.
W e ﬁrst note that at time k, the information available to
the agent is the set of processes observed till time k and the
corresponding observation vectors:
{
Aj , yAj
} k
j=1
. Using this
information, the posterior belief vector π(k) ∈ [0, 1]m can be
computed in closed form as follows [4]:
πi(k) =
πi(k − 1) ∏
a∈A k
[
(1 − p)
/BD
Ea,k,i + p/BD
Ec
a,k,i
]
∑ m
i=1 πi(k − 1) ∏
a∈A k
[
(1 − p)
/BD
Ea,k,i + p/BD
Ec
a,k,i
] ,
(2)
where /BD is the indicator function and the event Ea,k,i ≜
{ya(k) =sa|H = i} denotes the event that the observation
obtained and the corresponding state are the same, when the
index corresponding to the true value of s is H = i. Also, the
event Ec
a,k,i ≜ {ya(k) ̸= sa|H = i} denotes the complement
of Ea,k,i . As a result, given the previous state π(k − 1), the
action Ak and the observation yA, we can exactly compute
the updated posterior belief π(k) using (2). Therefore, the
generative model that learns the environment is a distribut ion
on the actions and the observations: Q(Ak, yAk |π(k − 1)) .
B. Preferences
In this subsection, we consider the preferences of the agent
that deﬁnes the generative model. Recall that our goal is to
estimate the vector s with conﬁdence exceeding a speciﬁc
level while minimizing the stopping time K and the cost of
sensing λ ∑ K
k=1 |Ak|. Clearly, the best estimate of s based
on the posterior belief corresponds to i∗ (k) ≜ arg max
i=1, 2,...,m
πi(k),
and the conﬁdence associated with the estimation is πi∗ (k)(k).
Therefore, the agent terminates the detection algorithm wh en
arg max
i=1, 2,...,m
πi(k) > π upper, (3)
where πupper is the desired level of conﬁdence. In short, the
decision making relies only on the posterior belief π(k). Also,
as k increases, we get more observations and the posterior
belief becomes more accurate. Therefore, the selection pol icy
µ is a function of the latest value of the posterior belief:
µ(π(k − 1)) =Ak.
Further exploring the objective of the policy design, we not e
that minimizing the stopping time is identical to driving th e
largest entry of π(k) to πupper as soon as possible. W e achieve
this by minimizing the entropy H(π(K)) of π(K) because the
entropy is minimized when the largest entry of π(K) is 1 and
the remaining entries are zeros. Here, the entropy is given b y
H(π) =−
m∑
i=1
πi log(πi). (4)
W e note that this approach is different from the Bayesian log
likelihood ratio based-approach in [3], [4], [10]. Therefo re,
we deﬁne the instantaneous objective function that the agen t
aims to minimize at time k as follows:
r(k) =H(π(k)) − H(π(k − 1)) +λ |Ak| . (5)
This deﬁnition ensures that the overall objective function is
given by
K∑
k=1
r(k) =H(π(K)) − H(π(0)) +
K∑
k=1
λ |Ak| , (6)
where minimizing H(π(K)) − H(π(0)) minimizes the en-
tropy in the posterior belief as H(π(0)) is a constant, and
minimizing ∑ K
k=1 λ |Ak| minimizes the total cost of sens-
ing. The instantaneous objective function r(k) represents the
preferences of the agent at time k and it is encoded into the
generative model as the prior probability on the belief vect or:
Q(yAk |Ak, π(k − 1))
= σ (−H(π(k)) +H(π(k − 1)) − λ |Ak|) , (7)
where σ(·) is the softmax function. Also, π(k) is a function
of π(k − 1), Ak and yAk due to (2). W e also note that
Q(yAk , Ak|π(k−1)) =Q(yAk |Ak, π(k−1))Q(Ak|π(k−1)).
(8)
Therefore, the generative model is completely deﬁned if we
specify the distribution Q(Ak|π(k − 1)). This distribution is
deﬁned based on the EFE of the future as we discuss in the
following subsection.
C. T otal expected free energy
The variational free energy F is the KL divergence between
the variational distribution q(A|π(k − 1)) and the generative
model Q(A|π(k − 1)). Thus,
F (k) =
∑
A∈P
q(A|π(k − 1)) log q(A|π(k − 1))
Q(A|π(k − 1)). (9)
The goal of the agent is to minimize the total free-energy of
the expected trajectories into the future:
G(A, π) =
K∑
j=k
E {F (j)|Ak = A, π(k − 1) =π} . (10)
In other words, the agent computes the expected free-energy
of all paths into the future and probabilistically chooses a n
action that minimizes the expected free-energy. Therefore , a
popular choice for the distribution over the actions assign ed
by the generative model is a Boltzmann distribution over the
expected free energies [11], [14], [15]:
Q(A|π(k − 1)) =σ (−G(A, π(k − 1))) , (11)
where σ(·) is again the softmax function, and G is given by
(10).
So far, we have presented the conceptual aspects of our
algorithm. W e next discuss how to compute the expressions
in (9) and (10).
D. Deep-learning based implementation
W e implement our algorithm using deep neural networks.
W e start with the computation of the free energy in (9):
F = −H(q(A|π(k − 1)))
−
∑
A∈P
q(A|π(k − 1)) logQ(A|π(k − 1)), (12)
where the entropy term H(q(A|π(k − 1))) is a function
of the variational distribution q which is controlled by the
agent. W e implement this distribution using a neural networ k
which we refer to as the policy network . The policy neural
network takes the posterior belief π(k − 1) as the input
and outputs stochastic selection policy qθ ∈ [0, 1]m− 1 which
is a probability distribution on P and parameterized by θ.
Therefore, the entropy term is computed using the entropy of
the distribution outputted by the neural network. This neur al
network also gives the policy implemented by the agent, whic h
is sampled from the distribution q learned at time k:
Ak = µ(π(k − 1)) ∼ qθ (π(k − 1)). (13)
Further, the second term in (12) can be determined using
(11) and (10). From (10), the EFE for a single time-step can
be approximated as follows [15]:
G(Ak, π(k − 1)) ≈ − log Q(yAk |Ak, π(k − 1))
+ EA∼ Q(·|π (k) {G(A, π(k))} . (14)
Here, the ﬁrst term is determined using (7). However, the
second term in (14) involves explicit computation into the
future values. Therefore, we learn a bootstrap estimate of
this quantity using a neural network which we refer to as
the bootstrapped EFE-network. Let Gφ (A) denote this neural
network where φ is the parameter of the network. In other
words, the estimate of the neural network is the predicted
value of the free-energy of the system. Thus, (14) reduces to
G(Ak, π(k − 1)) =H(π(k)) − H(π(k − 1))
+ λ |Ak| + EA∼ Q(·|π (k) {Gφ (Ak+1, π(k))} . (15)
Substituting (15) and (11) into (12) completes the derivati on
of the algorithm.
T o summarize, our solution involves two neural networks
qθ and Gφ which represent the policy and the expected free-
energy, respectively. At every time instant, we sample an
action from the output distribution of the policy network
qθ and obtain the corresponding observation yAk . Next, we
compute the bootstrapped EFE estimate and the variational
free energy using the neural networks and (12) and (15).
Finally, the parameter θ of the policy network is modiﬁed
by minimizing the variational free energy F (k). Similarly, the
parameter φ of the bootstrapped EFE-network is optimized by
comparing EFE-network output with the value of the expected
value G(A) calculated at time k. W e use the ℓ2−norm of the
difference between the two estimates:
L = ∥Gφ (A) − G(A)∥2 . (16)
The pseudo-code of the algorithm is summarized in Algo-
rithm 1 below .
Algorithm 1 Active inference for anomaly detection
Initialization: • Policy network qθ (a|π) with parameters θ
• Bootstrapped EFE-network Gφ (π; a) with parameters φ
1: repeat
2: Initialize the prior state π0 ∈ [0, 1]m (can be learned
from the training data)
3: Time index k = 0
4: while k < T and max
i
πi > π upper and k < T max do
5: Choose action Ak ∼ qθ (π(k − 1))
6: Generate observations yAk,k
7: Compute π(k + 1)using (2)
8: Compute the bootstrapped EFE estimate G using
(15)
9: Compute the variational free energy F using (11)
and (12)
10: Update the policy network network by minimizing
the variational free energy F with respect to θ
11: Update the bootstrapped EFE-network by minimiz-
ing the boostrapping loss in (16) with respect to φ
12: Increase time index k = k + 1
13: end while
14: until
15: Declare the estimate corresponding to arg max
i
πi
IV . N U M E RICA L RE S U LT S
In this section, we present numerical results comparing our
algorithm with the actor-critic method in [4]. The simulati on
setup is similar to that in [4]. W e choose the number of
processes as N = 3and thus, m = 2N = 8. The probability of
a process being normal is taken as q = 0.8. Here, the ﬁrst and
second processes are assumed to be statistically dependent ,
and the third process is independent of the other two. The
correlation between the dependent processes is captured by
the parameter ρ ∈ [0, 1]:
P {s1 = 0, s2 = 0} = q2 + ρq(1 − q) (17)
P {s1 = 0, s2 = 1} = q(1 − q)(1 − ρ) (18)
P {s1 = 1, s2 = 0} = q(1 − q)(1 − ρ) (19)
P {s1 = 1, s2 = 1} = (1− q)2 + ρq(1 − q). (20)
Also, we assume that the crossover probability of the obser-
vations is p = 0.8, and the maximum number of time slots
for each episode (trial or run) is Tmax = 300.
For the active inference algorithm, we implement the policy
neural network and the bootstrapped EFE-network with three
layers and the ReLU activation function between consecutiv e
layers. T o update the parameters of the neural networks, we
apply the Adam Optimizer, and we set the learning rates of
the policy network and the bootstrapped EFE-network as 10− 6
and 5 × 10− 6, respectively. The implementation of the actor-
critic method is the same as that in [4] except that we use
the entropy based-reward function as deﬁned in (5). Also, we
choose the learning rates of the actor and critic networks as
5 × 10− 4 and 5 × 10− 3, respectively.
The simulation results are presented in Figs. 1 to 3. Our
observations from the numerical results are as follows:
• Success rate: In Fig. 1, we plot the success rates of
the two algorithms as a function of the upper bound on
the posterior πupper. The success rate is deﬁned as the
ratio between the number of times the algorithm correctly
identiﬁes all the anomalous processes to the total number
of trials. W e observe that the success rates achieved
by both algorithms are comparable in all the settings.
Also, the success rate depends primarily on πupper and
it is almost insensitive to λ and ρ. This is intuitive
because πupper sets the conﬁdence level with which the
algorithms identify the anomalies, and therefore, for the
same conﬁdence level, the success rates achieved by the
algorithms are almost the same.
• Stopping time: In Fig. 2, we show the variation of the
stopping time K with πupper. W e see that the stopping
time increases with πupper in all cases, as a higher
value of πupper requires the algorithms to collect more
observations before they make the decision regarding the
anomalous processes. Also, we observe that the stopping
time decreases with an increase in ρ for all values of λ
and πupper. This decrease is expected due to the fact that
as the correlation increases, an observation correspondin g
to one of the dependent processes gives more information
about the other. Consequently, the algorithms require
fewer observations, and thus, a smaller stopping time,
to achieve the same conﬁdence level. Finally, we notice
that the stopping time for the active inference algorithm
is less than that of the actor-critic algorithm.
0.8 0.85 0.9 0.95 1
Upper bound on belief upper
0.86
0.88
0.9
0.92
0.94
0.96
0.98
1
Success rate
Active Inference,  = 0
Actor-Critic,  = 0
Active Inference,  = 0.3
Actor-Critic,  = 0.3
Active Inference,  = 0.8
Actor-Critic,  = 0.8
(a) Cost per measurement λ = 0. 05
0.8 0.85 0.9 0.95 1
Upper bound on belief upper
0.86
0.88
0.9
0.92
0.94
0.96
0.98
1
Success rate
Active Inference,  = 0
Actor-Critic,  = 0
Active Inference,  = 0.3
Actor-Critic,  = 0.3
Active Inference,  = 0.8
Actor-Critic,  = 0.8
(b) Cost per measurement λ = 0. 1
0.8 0.85 0.9 0.95 1
Upper bound on belief upper
0.85
0.9
0.95
1
Success rate
Active Inference,  = 0
Actor-Critic,  = 0
Active Inference,  = 0.3
Actor-Critic,  = 0.3
Active Inference,  = 0.8
Actor-Critic,  = 0.8
(c) Cost per measurement λ = 0. 2
Fig. 1: V ariation of the success rate of the active inference and the actor-critic algorithms when πupper, λ and ρ are varied.
0.8 0.85 0.9 0.95 1
Upper bound on belief upper
4
6
8
10
12
14
16
18
20
22Stopping time K
Active Inference,  = 0
Actor-Critic,  = 0
Active Inference,  = 0.3
Actor-Critic,  = 0.3
Active Inference,  = 0.8
Actor-Critic,  = 0.8
(a) Cost per measurement λ = 0. 05
0.8 0.85 0.9 0.95 1
Upper bound on belief upper
0
5
10
15
20
25Stopping time K
Active Inference,  = 0
Actor-Critic,  = 0
Active Inference,  = 0.3
Actor-Critic,  = 0.3
Active Inference,  = 0.8
Actor-Critic,  = 0.8
(b) Cost per measurement λ = 0. 1
0.8 0.85 0.9 0.95 1
Upper bound on belief upper
0
5
10
15
20
25Stopping time K
Active Inference,  = 0
Actor-Critic,  = 0
Active Inference,  = 0.3
Actor-Critic,  = 0.3
Active Inference,  = 0.8
Actor-Critic,  = 0.8
(c) Cost per measurement λ = 0. 2
Fig. 2: V ariation of the stopping time K of the active inference and the actor-critic algorithms whe n πupper, λ and ρ are varied.
0.8 0.85 0.9 0.95 1
Upper bound on belief upper
6
8
10
12
14
16
18
20
22
24Total number of measurements
Active Inference,  = 0
Actor-Critic,  = 0
Active Inference,  = 0.3
Actor-Critic,  = 0.3
Active Inference,  = 0.8
Actor-Critic,  = 0.8
(a) Cost per measurement λ = 0. 05
0.8 0.85 0.9 0.95 1
Upper bound on belief upper
6
8
10
12
14
16
18
20
22
24Total number of measurements
Active Inference,  = 0
Actor-Critic,  = 0
Active Inference,  = 0.3
Actor-Critic,  = 0.3
Active Inference,  = 0.8
Actor-Critic,  = 0.8
(b) Cost per measurement λ = 0. 1
0.8 0.85 0.9 0.95 1
Upper bound on belief upper
8
10
12
14
16
18
20
22
24
26Total number of measurements
Active Inference,  = 0
Actor-Critic,  = 0
Active Inference,  = 0.3
Actor-Critic,  = 0.3
Active Inference,  = 0.8
Actor-Critic,  = 0.8
(c) Cost per measurement λ = 0. 2
Fig. 3: V ariation of the total number of measurements ∑ K
k=1 |Ak| of the active inference and the actor-critic algorithms whe n
πupper, λ and ρ are varied.
• T otal number of measurements: Fig. 3 compares the total
number of measurements ∑ K
k=1 |Ak| obtained by the two
algorithms in different settings. Clearly, the total numbe r
of measurements decreases with ρ, which is expected as
mentioned above. Also, we infer that the total number of
measurements obtained by both algorithms are similar
in all the settings with the active inference algorithm
collecting slightly fewer measurements compared to the
actor-critic algorithm.
Thus, we conclude that the two algorithms achieve compa-
rable success rates and incur a similar total cost of sensing ,
but the active inference algorithm has better stopping time
compared to the actor-critic algorithm. This indicates tha t our
algorithm identiﬁes the anomalies faster than the actor-cr itic
algorithm. Moreover, the stopping time of our algorithm doe s
not vary much with λ while the stopping time of the actor-
critic algorithm increases with λ. This implies that the actor-
critic algorithm is more sensitive to the instantaneous cos t
of sensing λ |Ak| than the total cost of sensing ∑ K
k=1 λ |Ak|.
T o elaborate, we note that both algorithms continue to acqui re
measurements until the desired level conﬁdence level πupper is
achieved. However, since the actor-critic algorithm optim izes
the average cost of sensing 1
K
∑ K
k=1 λ |Ak|, as λ increases, it
picks a fewer number of processes per time instant and this
results in an increased stopping time. On the contrary, the a v-
erage number of processes selected by our algorithm does not
vary much with λ. Therefore, we achieve better performance
by carefully designing the objective function using a novel
entropy based-function and the total cost of sensing wherea s
the actor-critic algorithm optimizes the average change in
entropy and the average cost of sensing.
V . C O N CL U S IO N
In this paper, we presented an anomaly detection algorithm
using an active inference-based approach. W e modeled the
problem of anomaly detection as an active inference problem
aiming at the detection accuracy exceeding a desired value
while minimizing the delay and total cost of sensing. W e
designed a new objective function based on entropy and imple -
mented the active inference algorithm using a deep learning -
based approach. Through simulation results, we compared
our algorithm with an algorithm based on the deep actor-
critic method in terms of the success rate, stopping time,
and total cost of sensing. The results demonstrated that our
algorithm can detect the anomalies quicker (as indicated by
the smaller stopping times) and achieves a competitive succ ess
rate with a similar cost of sensing as the actor-critic algor ithm.
However, we detect all the anomalous processes at a given
time, assuming that the (normal or anomalous) behaviors
of the processes remain unchanged until the agent makes a
decision. Extending our algorithm to track any changes in
the behavior of the processes over a longer time period is an
interesting direction for future work.
RE F E RE N CE S
[1] W .-Y . Chung and S.-J. Oh, “Remote monitoring system with wireless
sensors module for room environment, ” Sensors Actuators B: Chemical,
vol. 113, no. 1, pp. 64–70, Jan. 2006.
[2] A. Bujnowski, J. Ruminski, A. Palinski, and J. Wtrorek, “ Enhanced
remote control providing medical functionalities, ” in Proc. Inter . Conf.
P ervasive Comput. T ech Healthc. W orkshops, May 2013, pp. 290–293.
[3] C. Zhong, M. C. Gursoy , and S. V elipasalar, “Deep actor-c ritic rein-
forcement learning for anomaly detection, ” in Proc. Globecom , Dec.
2019.
[4] G. Joseph, M. C. Gursoy , and P . K. V arshney , “ Anomaly dete ction under
controlled sensing using actor-critic reinforcement lear ning, ” in Proc.
IEEE Inter . W orkshop SP A WC, May 2020.
[5] H. Chernoff, “Sequential design of experiments, ” Ann. Math. Stat. ,
vol. 30, no. 3, pp. 755–770, Sep. 1959.
[6] S. A. Bessler, “Theory and applications of the sequentia l design of
experiments, k-actions and inﬁnitely many experiments: Pa rt I - theory , ”
Stanford Univ CA Applied Mathematics and Statistics Labs, T ech. Rep.,
1960.
[7] S. Nitinawarat, G. K. Atia, and V . V . V eeravalli, “Contro lled sensing for
multihypothesis testing, ” IEEE Trans. Autom. Control , vol. 58, no. 10,
pp. 2451–2464, May 2013.
[8] M. Naghshvar, T . Javidi et al. , “ Active sequential hypothesis testing, ”
Ann. Stat. , vol. 41, no. 6, pp. 2703–2738, 2013.
[9] B. Huang, K. Cohen, and Q. Zhao, “ Active anomaly detectio n in
heterogeneous processes, ” IEEE Trans. Inf. Theory , vol. 65, no. 4, pp.
2284–2301, Aug. 2018.
[10] D. Kartik, E. Sabir, U. Mitra, and P . Natarajan, “Policy design for active
sequential hypothesis testing using deep learning, ” in Proc. Allerton ,
Oct. 2018, pp. 741–748.
[11] K. Friston, F . Rigoli, D. Ognibene, C. Mathys, T . Fitzge rald, and
G. Pezzulo, “ Active inference and epistemic value, ” J. Cogn. Neurosci.,
vol. 6, no. 4, pp. 187–214, Oct. 2015.
[12] K. Friston, T . FitzGerald, F . Rigoli, P . Schwartenbeck , and G. Pezzulo,
“ Active inference: A process theory , ” Neural Comput. , vol. 29, no. 1,
pp. 1–49, Jan. 2017.
[13] K. J. Friston, M. Lin, C. D. Frith, G. Pezzulo, J. A. Hobso n, and
S. Ondobaka, “ Active inference, curiosity and insight, ” Neural Comput.,
vol. 29, no. 10, pp. 2633–2683, Oct. 2017.
[14] P . Schwartenbeck, J. Passecker, T . U. Hauser, T . H. Fitz Gerald, M. Kro-
nbichler, and K. J. Friston, “Computational mechanisms of c uriosity and
goal-directed exploration, ” Elife, vol. 8, p. e41703, 2019.
[15] B. Millidge, “Deep active inference as variational pol icy gradients, ” J.
Math. Psychol. , vol. 96, p. 102348, Jan. 2020.