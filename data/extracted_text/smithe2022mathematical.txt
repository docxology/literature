Mathematical Foundations for a
Compositional Account of the
Bayesian Brain
Toby St Clere Smithe
St Edmund Hall
University of Oxford
A thesis submitted for the degree of
Doctor of Philosophy
Trinity 2023
arXiv:2212.12538v3  [q-bio.NC]  19 Dec 2023
Acknowledgements
This thesis would not exist in anything like this form without the marvellous Applied
Category Theory community, a more welcoming and thoughtful group of researchers
one could not wish to find. This community makes a serious and thoroughgoing effort
to be inclusive and outward-looking, and it was in this spirit that they set up theApplied
Category Theory Adjoint School , which I attended in 2019, and to which I recommend
any category-theory-curious thinker to apply. Without that experience, and the group
of friends I made there, none of this would have been possible.
Before I attended the Adjoint School, I was trying to understand too much about
the brain, and seeking a mathematically coherent unifying framework with which
I could organize my thoughts. In Oxford, I was a member of the Department of
Experimental Psychology, but had become aware of the work being done on cognition
and linguistics in the Quantum Group, in the Department of Computer Science, and
so I began attending lectures and hanging around there. It was there that I attended
the Open Games workshop in 2018, at which I realized that predictive coding and
open games had the same abstract structure; a fact that took me longer than it should
have to formalize, but about which I started telling anyone who listened. The first
individuals who took me seriously were Jules Hedges and Brendan Fong, and I thank
them heartily for their encouragement and assistance: it was after discussion with Jules
(and Bruno Gavranović) at the Sixth Symposium on Compositional Structures (SYCO
6, in Leicester) that I proved abstractly that “Bayesian updates compose optically”;
and it was Brendan Fong who let me know about the Adjoint School, at which we
(Brendan, Bruno, David Spivak, David Jaz Myers, and Sophie Libkind, as well as others
occasionally, including Jules, Eliana Lorch, and davidad) discussed autopoiesis from a
categorical perspective.
After these meetings, and through my Quantum Group interactions, I acquired some
funding from the Foundational Questions Institute to concentrate on the category
theory of predictive coding and approximate inference, which was distributed through
the Topos Institute. I thank everyone who made these interactions possible and
delightful, including (in no particular order) the following individuals that I have not
yet named: Samson Abramsky; Bob Coecke; Johannes Kleiner; Tim Hosgood; Owen
Lynch; Valeria de Paiva; Evan Patterson; Sam Staton; Juliet Szatko; Tish Tanski; Sean
Tull; and Vincent Wang-Maścianica.
Outside of Oxford, I have been fortunate to be part of some wonderful interactions
through the Active Inference and Strathclyde MSP (Mathematically Structured
Programming) communities. I first spoke about categorical active inference to Karl
Friston’s group in March 2020, shortly after my first visit to Glasgow at the end of
2019; and I found Glasgow so appealing that I now find myself living there. For these
interactions, besides those named above, I must recognize: Dylan Braithwaite; Matteo
Capucci; Lance da Costa; Neil Ghani; Maxwell Ramstead; Riu Rodríguez Sakamoto;
and Dalton Sakthivadivel.
I would not have had the opportunity to pursue this research at all had I not been
granted a position in the Oxford Experimental Psychology department, where I have
been a member of the Oxford Centre for Theoretical Neuroscience and Artificial
Intelligence (OCTNAI), under the direction of Simon Stringer. I thank Simon for his
patience and latitude, particularly when my plans were not quite as he would have
expected, and I thank my Oxford co-supervisor (and present director of graduate
studies), Mark Buckley, and my previous director of graduate studies, Brian Parkinson,
for their always excellent advice. Thanks also to the other student members of OCTNAI
(particularly Dan, Hannah, Harry, James, Nas, and Niels) for being so welcoming to
an oddball such as myself. And at this point, it would be remiss not to thank also
the administrative staff of the Department, and my college, St Edmund Hall, who are
always helpful and wise; in particular, Rebecca Cardus and Vinca Boorman, who have
guided me through much of Oxford’s strange bureaucracy.
Finally, and most of all, I thank my family and my beloved wife, Linda, who in particular
has suffered through this long journey with me with beyond-infinite patience, love, and
understanding (so much patience, in fact, that she humoured the category-theoretic
content of my wedding speech!). Thank you, to you all. It takes a village!
3
Abstract
This dissertation reports some first steps towards a compositional account of active inference
and the Bayesian brain. Specifically, we use the tools of contemporary applied category theory
to supply functorial semantics for approximate inference. To do so, we define on the ‘syntactic’
side the new notion of Bayesian lens and show that Bayesian updating composes according to the
compositional lens pattern. Using Bayesian lenses, and inspired by compositional game theory,
we define fibrations of statistical games and classify various problems of statistical inference as
corresponding sections: the chain rule of the relative entropy is formalized as a strict section, while
maximum likelihood estimation and the free energy give lax sections. In the process, we introduce
a new notion of ‘copy-composition’.
On the ‘semantic’ side, we present a new formalization of general open dynamical systems
(particularly: deterministic, stochastic, and random; and discrete- and continuous-time) as certain
coalgebras of polynomial functors, which we show collect into monoidal opindexed categories (or,
alternatively, into algebras for multicategories of generalized polynomial functors). We use these
opindexed categories to define monoidal bicategories of cilia: dynamical systems which control
lenses, and which supply the target for our functorial semantics. Accordingly, we construct functors
which explain the bidirectional compositional structure of predictive coding neural circuits under
the free energy principle, thereby giving a formal mathematical underpinning to the bidirectionality
observed in the cortex. Along the way, we explain how to compose rate-coded neural circuits
using an algebra for a multicategory of linear circuit diagrams , showing subsequently that this is
subsumed by lenses and polynomial functors.
Because category theory is unfamiliar to many computational neuroscientists and cognitive
scientists, we have made a particular effort to give clear, detailed, and approachable expositions
of all the category-theoretic structures and results of which we make use. We hope that this
dissertation will prove helpful in establishing a new “well-typed” science of life and mind, and in
facilitating interdisciplinary communication.
Contents
1. Introduction 1
1.1. Overview of the dissertation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.2. Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2. Basic category theory for computational and cognitive (neuro)scientists 11
2.1. Categories, graphs, and networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.1.1. Three examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.1.1.1. Neural circuits: dynamical networks of neurons . . . . . . . . . 12
2.1.1.2. Bayesian networks: belief and dependence . . . . . . . . . . . . 12
2.1.1.3. Computations: sets and functions . . . . . . . . . . . . . . . . . 13
2.1.2. From graphs to categories . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.1.2.1. Diagrams in a category, functorially . . . . . . . . . . . . . . . . 19
2.2. Connecting the connections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.2.1. Enriched categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.2.2. 2-categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.2.3. On functorial semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
2.2.4. Adjunction and equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . 34
2.3. Universal constructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
2.3.1. The universality of common patterns . . . . . . . . . . . . . . . . . . . . . 41
2.3.1.1. Disjunctions, or coproducts . . . . . . . . . . . . . . . . . . . . 41
2.3.1.2. Conjunctions, products, and sections . . . . . . . . . . . . . . . 43
2.3.1.3. Subobjects and equalizers . . . . . . . . . . . . . . . . . . . . . . 45
2.3.1.4. Coequalizers and quotients . . . . . . . . . . . . . . . . . . . . . 47
2.3.2. The pattern of universality . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
2.3.3. Limits and colimits: mapping in to and out of diagrams . . . . . . . . . . . 51
2.3.3.1. Functoriality of taking limits . . . . . . . . . . . . . . . . . . . . 54
2.3.3.2. (Co)limits as adjoints . . . . . . . . . . . . . . . . . . . . . . . . 55
i
2.3.3.3. Hom preserves limits . . . . . . . . . . . . . . . . . . . . . . . . 56
2.3.4. Closed categories and exponential objects . . . . . . . . . . . . . . . . . . 59
2.3.4.1. Dependent products . . . . . . . . . . . . . . . . . . . . . . . . . 62
2.4. The Yoneda Lemma: a human perspective . . . . . . . . . . . . . . . . . . . . . . . 63
2.4.1. Formalizing categorical reasoning via the Yoneda embedding . . . . . . . 64
2.4.2. Knowing a thing by its relationships . . . . . . . . . . . . . . . . . . . . . 65
3. Algebraic connectomics 73
3.1. Categories and calculi for process theories . . . . . . . . . . . . . . . . . . . . . . 74
3.1.1. String diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
3.1.2. Monoidal categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
3.1.3. Closed monoidal categories . . . . . . . . . . . . . . . . . . . . . . . . . . 82
3.1.4. Bicategories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
3.2. Parameterized systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
3.2.1. Internal parameterization . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
3.2.2. External parameterization . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
3.3. Systems from circuits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
3.3.1. Multicategorical algebra for hierarchical systems . . . . . . . . . . . . . . 94
3.3.2. Linear circuit diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
3.3.3. An algebra of rate-coded neural circuits . . . . . . . . . . . . . . . . . . . 98
3.4. From monoids to monads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
3.4.1. Comonoids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
3.5. Polynomial functors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
4. The compositional structure of Bayesian inference 122
4.1. Compositional probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
4.1.1. Discrete probability, algebraically . . . . . . . . . . . . . . . . . . . . . . . 125
4.1.1.1. Stochastic matrices . . . . . . . . . . . . . . . . . . . . . . . . . 129
4.1.1.2. Monoidal structure . . . . . . . . . . . . . . . . . . . . . . . . . 130
4.1.1.3. Copy-discard structure . . . . . . . . . . . . . . . . . . . . . . . 131
4.1.1.4. Bayesian inversion . . . . . . . . . . . . . . . . . . . . . . . . . 132
4.1.2. Abstract Bayesian inversion . . . . . . . . . . . . . . . . . . . . . . . . . . 133
4.1.3. Density functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
ii
4.1.4. S-finite kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
4.1.5. On probability monads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
4.2. Dependent data and bidirectional processes . . . . . . . . . . . . . . . . . . . . . . 144
4.2.1. Indexed categories and the Grothendieck construction . . . . . . . . . . . 144
4.2.1.1. The monoidal Grothendieck construction . . . . . . . . . . . . . 151
4.2.2. Grothendieck lenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
4.2.2.1. Monoidal categories of lenses . . . . . . . . . . . . . . . . . . . 158
4.3. The bidirectional structure of Bayesian updating . . . . . . . . . . . . . . . . . . . 159
4.3.1. State-dependent channels . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
4.3.2. Bayesian lenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
4.3.3. Bayesian updates compose optically . . . . . . . . . . . . . . . . . . . . . 164
4.3.4. Lawfulness of Bayesian lenses . . . . . . . . . . . . . . . . . . . . . . . . . 167
5. Statistical games 171
5.1. Compositional approximate inference, via the chain rule for relative entropy . . . 171
5.2. ‘Copy-composite’ Bayesian lenses . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
5.2.1. Copy-composition by coparameterization . . . . . . . . . . . . . . . . . . 173
5.2.2. Lax functors, pseudofunctors, their transformations, and indexed bicategories 181
5.2.3. Coparameterized Bayesian lenses . . . . . . . . . . . . . . . . . . . . . . . 184
5.2.4. Coparameterized Bayesian updates compose optically . . . . . . . . . . . 187
5.3. Statistical games for local approximate inference . . . . . . . . . . . . . . . . . . . 191
5.3.1. Attaching losses to lenses . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
5.3.2. Inference systems and loss models . . . . . . . . . . . . . . . . . . . . . . 197
5.3.3. Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
5.3.3.1. Relative entropy and Bayesian inference . . . . . . . . . . . . . 199
5.3.3.2. Maximum likelihood estimation . . . . . . . . . . . . . . . . . . 201
5.3.3.3. Autoencoders via the free energy . . . . . . . . . . . . . . . . . 202
5.3.3.4. The Laplace approximation . . . . . . . . . . . . . . . . . . . . . 204
5.4. Monoidal statistical games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
5.4.1. Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
5.4.1.1. Relative entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
5.4.1.2. Maximum likelihood estimation . . . . . . . . . . . . . . . . . . 220
5.4.1.3. Free energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
iii
5.4.1.4. Laplacian free energy . . . . . . . . . . . . . . . . . . . . . . . . 221
5.5. Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
6. Open dynamical systems, coalgebraically 227
6.1. Categorical background on dynamics and coalgebra . . . . . . . . . . . . . . . . . 228
6.1.1. Dynamical systems and Markov chains . . . . . . . . . . . . . . . . . . . . 228
6.1.2. Coalgebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
6.2. Open dynamical systems on polynomial interfaces . . . . . . . . . . . . . . . . . . 232
6.2.1. Deterministic systems in general time . . . . . . . . . . . . . . . . . . . . 232
6.2.2. Polynomials with ‘effectful’ feedback, and open Markov processes . . . . 239
6.2.3. Open random dynamical systems . . . . . . . . . . . . . . . . . . . . . . . 242
6.3. Cilia: monoidal bicategories of cybernetic systems . . . . . . . . . . . . . . . . . . 247
6.3.1. Hierarchical bidirectional dynamical systems . . . . . . . . . . . . . . . . 247
6.3.2. Differential systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
7. Approximate inference doctrines for predictive coding 261
7.1. Channels with Gaussian noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
7.2. Externally parameterized Bayesian lenses and statistical games . . . . . . . . . . . 265
7.3. Approximate inference doctrines . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
7.3.1. Predictive coding circuits and the Laplace doctrine . . . . . . . . . . . . . 274
7.3.2. Synaptic plasticity with the Hebb-Laplace doctrine . . . . . . . . . . . . . 281
8. Future directions 284
8.1. Structured worlds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
8.1.1. Bayesian sensor fusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
8.1.2. Learning structure and structured learning . . . . . . . . . . . . . . . . . . 287
8.1.3. Compositional cognitive cartography . . . . . . . . . . . . . . . . . . . . . 288
8.2. Societies of systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
8.2.1. Active inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
8.2.2. What is the type of a plan? . . . . . . . . . . . . . . . . . . . . . . . . . . 291
8.2.3. Reinforcement learning, open games, and ecosystems . . . . . . . . . . . 292
8.3. The mathematics of life . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
8.3.1. Bayesian mechanics and the free energy principle . . . . . . . . . . . . . . 294
8.3.2. Biosemiotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
iv
8.4. Fundamental theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
8.4.1. Geometric methods for (structured) belief updating . . . . . . . . . . . . . 296
8.4.2. Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
8.4.3. Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
A. Auxiliary material 299
A.1. From monads to multicategories . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
B. Bibliography 304
v
1. Introduction
The work of which this dissertation is a report began as a project to understand the brain’s “cognitive
map”, its internal representation of the structure of the world. Little of that work is reported here,
for it rapidly became clear at the outset that there was no coherent framework in which such a
project should most profitably be undertaken. This is not to say that no progress on understanding
the cognitive map can be made, a claim which would be easily contradicted by the evidence. Rather,
each research group has its own language and its own research questions, and it is not always
evident how to translate concepts from one group, or even one moment in time, faithfully to
another; what translation is done is performed at best highly informally.
If the aim of science1 is to tell just-so stories, or if the aim is only to answer one’s own research
questions in isolation, then this state of affairs may be perfectly satisfactory. But the brain and the
behaviours that brains produce are so marvellous and so complex, and the implications of a finer
understanding so monumental, that one cannot but hope that science could do better. Of course, of
late, science has not been doing better, with disciplines as socially important as psychology [201]
and medicine [23, 135, 188] and machine learning [134, 149] struck by crises of reproducibility.
At the same time, as broadband internet has spread across the globe, the sheer amount of output
produced by scientists and other researchers has ballooned, contributing to the impossibility of
verification and the aforementioned translational difficulties, at least if one desires to do other than
simply following the herd. In some sense, although scientists all now speak English, science still
lacks a lingua franca , or at least a sufficiently precise one.
As luck would have it, while mainstream science has been suffering from this loss of faith,
the first phrases of a potentially adequate precise new language have begun to spread, with the
coalescence of a new community of researchers in applied category theory 2. One part of the present
1Or indeed, “if the aim of scientists”, as science itself may not have volition of its own.
2The first major interdisciplinary meeting of applied category theorists (or at least the first meeting sufficiently confident
to take Applied Category Theory as its name) was held in 2018 in Leiden, although categorical methods have for
some time been used in computer science [210] and physics [16], and especially at their nexus [2, 68, 69]. More
sporadically, category theory had shown up elsewhere, such as in biology [86, 220], network theory [93–95], game
theory [3, 89, 119], cognitive science [37, 85, 183, 209] and linguistics [67, 70, 130], and in 2014 a workshop was held
at Dagstuhl bringing together some of these researchers [4], in what was to be a precursor to the Applied Category
1
difficulty of scientific translation is that each research group has not only its own language, but
also its own perspective; and another part of the difficulty is that these languages and perspectives
are not well connected, with the English language a very lossy medium through which to make
these connections. Fortunately, the language of category theory—being a mathematical rather than
a natural language—resolves both of these difficulties.
Category theory is the mathematics of pattern, composition, connection, and interaction; its
concepts are as crisp and clear as the water of a mountain pool; its simplicity lends it great power.
Categories describe how objects can be constructed from parts, and such compositional descriptions
extend to categories themselves: as a result, the language of category theory is ‘homoiconic’, and
can be used to translate constructions between contexts. One is able to abstract away from irrelevant
details, and show precisely how structures give rise to phenomena; and by choosing the abstractions
carefully, it becomes possible to see that, sometimes, important constructions are ‘universal’, able
to be performed in any relevant context. As a result, category theory resolves both problems of
scientific translation indicated above: concepts expressed categorically are inevitably expressed in
context, and not in isolation; and these contexts are naturally interconnected as if by a categorical
web (with the connections also expressed categorically). Moreover, not being English, categorical
definitions tend to be extremely concise and information-dense; and since the basic concepts of
category theory are themselves simple, concepts so expressed are not biased by geography or
geopolitics.
From the middle of the 20th century, the concepts of category theory began to revolutionize much
of mathematics3, and applied category theorists such as the present author believe that the time is
nigh for this revolution to spread throughout the sciences and alleviate some of their struggles.
Just as the internet constitutes physical infrastructure that fundamentally accelerates human
communications, we expect category theory to constitute conceptual infrastructure of similar
catalytic consequence. This thesis is a contribution to building this infrastructure, in the specific
domain of computational neuroscience and the general domain of (what was once, and will be again,
called) cybernetics4. In particular, we show that a prominent theory of brain function—predictive
Theory meetings; many of those researchers still work in this new interdisciplinary field.
3The basic concepts of category theory were originally written down by Eilenberg and Mac Lane in order to formalize
processes of translation, and so clarify structures in the ways indicated in the main text above, in the field of algebraic
topology. This occurred at the end of the first half of the 20th century, in 1945 [87]. The ideas soon spread beyond
algebraic topology, gathering momentum rapidly from the 1950s, in which Cartan defined the concept of sheaf [56,
57] and Grothendieck reconceived the foundations of algebraic geometry [121]. By the mid-1960s, and especially
through the work of Lawvere on logic [165] and set theory [166], it was clear that category theory would be able to
supply supple but sturdy new foundations for all of mathematics.
4Owing to its affinity for pattern and abstraction, it is hard to do interesting domain-specific work in category theory
without there being at least some more general results to be found, and indeed this is the case here: what began as
2
coding—has a clear compositional structure, that explains the bidirectional circuitry observed in
the brain [21], and that renders precise connections to the structure of statistical and machine
learning systems [187, 221, 278], as well as to the structure of much larger scale adaptive systems
traditionally modelled by economic game theory [119].
Predictive coding models were originally developed in the neuroscience of vision to explain
observations that neural activity might decrease as signals became less surprising [216] (rather
than increase as signals became more ‘preferred’), as well as to explain the robustness of sensory
processing to noise [246] and as a source of metabolic efficiency [32]5. The typical form of these
models involves a neuron or neural ensemble representing the system’s current prediction of (or
expectation about) its input, alongside another neuron or ensemble representing the difference
between this prediction and the actual input (i.e., representing the prediction error). We can think
of the former ensemble as directed from within the brain towards the sensory interface (such as
the retina), and the latter ensemble as carrying information from the world into the brain: this is
the aforementioned bidirectionality.
Another important observation about visual processing in the brain is that its circuitry seems
to be roughly hierarchical [179], with regions of cortex further from the retina being involved in
increasingly abstract representation [212]. Given a model of predictive coding at the level of a single
circuit, accompanied by models of how sensory circuits are coupled (and their representations
transformed), a natural next step is to construct hierarchical predictive coding models, in an attempt
to extend the benefits of the single circuit to a whole system; and indeed such hierarchical circuits
were prominently proposed in the literature [104, 216].
This hierarchical structure is a hint of compositionality, and thus a sign that a categorical
approach may be helpful and enlightening. This impression is strengthened when one considers
a particularly influential class of predictive coding models, obtained in the context of the “free
energy principle” [100, 104, 107], where the underlying equations themselves exhibit a form of
compositionality which is (more or less explicitly) used to obtain the hierarchical models6. Despite
this hint of compositionality, the equations of motion for these hierarchical systems are typically
derived from scratch each time [21, 48, 76, 108, 148, 264, 265], a redundant effort that would not be
a project in theoretical neuroscience swiftly became a study of adaptive and cybernetic systems more broadly, of
which the brain is of course the prime exemplar.
5If the prediction is good, then communicating the difference between prediction and actuality can be done much more
efficiently than transmitting the whole incoming signal, which would contain much redundant information. This is
the principle underlying most data compression algorithms.
6That is to say, the dynamics of each level of hierarchy i are governed by a quantity Fi, and the dynamics of two
adjacent levels i and i `1 are governed by Fi `Fi`1; see Buckley et al. [48, Eq. 72].
3
required had a compositional formalism such as category theory been used from the start. This
thesis supplies such a categorical formalism and exemplifies it with hierarchical predictive coding
under the free energy principle.
The “free energy” framework not only underpins a modern understanding of predictive coding,
but has more broadly been proposed as a unified theory of brain function [100], and latterly of all
adaptive or living systems [38, 102, 159, 204]. In the neuroscientific context, it constitutes a theory
of the Bayesian brain , by which most or all brain function can be understood as implementing
approximate Bayesian inference [160]; in the more broadly biological (or even metaphysical)
contexts, this claim is generalized to state that all life can be understood in this way. However,
despite these claims to universality, these proposals have to date been quite informally specified,
leading to confusion [28, 103] and charges of unfalsifiability [38, 71, 281]. As we will see, category
theory has a rich formal vocabulary for precisely describing universal constructions, and so not
only does a categorical formulation of the free energy framework promise to clarify the current
confusions, but it may be expected also to shed light on its potential universality. In particular, as
we discuss in Chapter 8, we will be able to make precise the questions of whether any dynamical
system of the appropriate type can universally be seen as performing approximate inference (in our
language, “playing a statistical game”), and of whether any cybernetic system (such as an economic
game player) can be expressed as an active inference system.
The notion of active inference is closely related to the free energy framework: an active inference
model of a system describes both the processes by which it updates its internal states on the
basis of incoming signals, and the processes by which it chooses how to act, using approximate
Bayesian inference. In this thesis, we do not get as far as a completely general formulation of active
inference, but we hope that our development ofstatistical games and their “dynamical semantics” in
approximate inference doctrines will provide a useful starting point for such a formulation, and in our
final chapter (8) we sketch how we might expect this formulation to go. Because active inference
models, and the free energy framework more broadly, are descriptions of systems that are ‘open’
to an environment, interacting with it, and therefore situated “in context”, they are particularly
suited to a category-theoretic reformulation. Likewise, Bayesianism and the free energy framework
lend themselves to a subjectivist metaphysics [102, 114, 115], which is itself in alignment with
the unavoidable perspective-taking of categorical models, and which is not dissimilar from the
emerging ‘biosemiotic’ reconceptualization of biological information-processing [20]. As we have
indicated, categorical tools help us to draw connections between concepts, and we see our efforts
4
as a contribution to this endeavour.
It is through these connections that we hope eventually to make contact again with the cognitive
map. As noted above, the state of the art is fragmented, but there exist current models that are
expressed in the language of approximate (variational) inference [279], models expressed in the
language of reinforcement learning [257], and models that attempt to combine the two [185]. We
will see throughout the thesis that reinforcement learning (and its cousin, game theory) is closely
related to approximate inference, and so we expect that the foundations developed here, along
with the extensions proposed in §8.1.3, will help us unify these accounts. The key observation that
we expect to drive such a development is that learning a cognitive map (alternatively, learning
a “world model”) means internalizing a representation of the structure of the environment; and
comparing and translating structures is category theory’s forte.
Of course, even if the theory that we develop is sufficient to unify these computational-
phenomenological models, this is not to say it will satisfy all neuroscientists, many of which
may be expected to desire more biologically detailed models. In the contemporary undergraduate
neuroscience curriculum, one is taught informally to relate models at a high ‘computational’ level
to lower level models concerned with biological ‘implementation’, following Marr’s “three levels
of explanation” [179]. As we discuss in §2.2.3, this story is a shadow of the categorical notion
of functorial semantics , by which structures are translated precisely between contexts formalized
as categories. Although we concentrate on the more abstract computational level in this thesis,
our discussion of functorial semantics foreshadows the introduction of formal algebraic tools for
building biologically plausible neural circuit models (§3.3).
Our treatment of cognitive and neural systems is not the first to adopt categorical methods,
but we do believe that it is the first to do so in a comprehensively integrated and wide-ranging
way, taking functorial semantics seriously. Categorical concepts have been variously proposed in
biology as early as 1958 [220], and in cognitive science (with one eye toward the brain) since at
least 1987 [84, 85]; more recently, category theory has been used to study classic cognitive-science
concepts such as systematicity [209]. While inspirational, these studies do not make the most of
the translational power of categories, using only some concepts or methods in isolation. Moreover,
by working almost purely categorically, these works were invariably rather abstract, and did not
make direct contact with the tools and concepts of mainstream mathematical science. As a result,
they did not have the unifying impact or adoption that we hope the new wave of applied category
theoretical developments to have.
5
Our primary motivation in writing this thesis is to lay the groundwork for well-typed cognitive
science and computational neuroscience. ‘Types’ are what render categorical concepts so precise,
and what allow categorical models to be so cleanly compositional: two systems can only “plug
together” if their interface types match. Because every concept in category theory has a type (i.e.,
every object is an object of some category), categorical thinking is forced to be very clear. As we
will sketch in §2.3.4, the “type theories” (or “internal languages”) of categories can be very richly
structured, but still the requirement to express concepts with types is necessarily burdensome. But
this burden is only the burden of thinking clearly: if one is not able to supply a detailed type, one
can resort to abstraction. And, to avoid the violence of declaring some object to be identified as of
some type7, it is necessary to understand the relationships between types; fortunately, as we will
soon make clear, and as we have attempted to emphasize, category theory is fundamentally the
mathematics of relationship.
Contemporary science is unavoidably computational, and the notion of ‘type’ that we invoke
here is closely related to (though not identical with) the informal notion of type that is used in
computer programming. Just as one of the strategies adopted to overcome the crises of modern
science that we invoked at the opening of this introduction is the making available of the code and
data that underlie scientific studies, we can envisage a near future in which accompanying these is
a formal specification of the types of the concepts that each study is about8. Some work along these
lines has already begun, particularly with the development of the Algebraic Julia ecosystem [122].
The free energy framework, like the structurally adjacent framework of compositional game
theory, has a strong flavour of teleology (that follows directly from its mathematics): systems act in
order to make their predictions come true. We therefore hope that, although we do not quite get as
far as a full compositional theory of active inference, the contributions reported in this dissertation
may in some small way help to make this particular prediction (of a well-typed science) come
true, and thereby help to overcome some of the aforenoted crises of scientific faith—as well as to
shed light not only on the form and function of ‘Bayesian’ brains, but also other complex adaptive
systems, such as the whole scientific community itself.
7A perspective for which we must thank Brendan Fong.
8One might think of this specification as akin to a scientifically elaborated version of the notion of header file in
programming languages such as C or C++: these files specify the types of functions and data structures, typically
without instantiating these types with detailed implementations. We can thus think of category theory as a very rich
metaprogramming language for the mathematical sciences (and this analogy goes quite far, as categorical proofs are
typically ‘constructive’ and hence correspond to computable functions, as we also sketch in §2.3.4).
6
1.1. Overview of the dissertation
Category theory being quite alien to most researchers in computational neuroscience (and the
cognitive sciences more broadly), we begin the work of this dissertation in Chapter 2 with
a comprehensive review of the concepts and results needed to understand our mathematical
contributions. Using three hopefully familiar examples, we introduce categories as contrapuntal
to graphs, which are more familiar to scientists, but which lack important features of categories
such as composition and, somehow, dynamism. We then explain how enriched categories allow us
to “connect the connections” of categories, and attach extra data to them, and we exemplify these
concepts with the 2-category of categories, functors, and natural transformations—as well as a more
formal discussion of functorial ‘translation’ and semantics. The remainder of Chapter 2 is dedicated
to introducing the remaining key concepts of basic category theory: universal constructions, and
the Yoneda Lemma (categories’ fundamental theorem). All of these ideas are very well known to
category theorists.
In Chapter 3, we begin to reapproach neural modelling, and more generally the ‘algebraic’
modelling of the structure of interacting systems. We explain how ‘monoidal’ categories allow us to
consider processes “in parallel” (as well as just sequentially), and how this gives us a formal account
of the concept of ‘parameterized’ system. We then change the perspective a little, and introduce
our first piece of original work: an account of how to connect neural circuits into larger-scale
systems, using ‘multicategorical’ algebra. The remainder of the chapter is dedicated to developing
the theory of such algebra to the point needed later in the thesis, ending with the introduction
of polynomial functors which will supply a rich syntax for the interaction of systems, as well as a
language in which to express their dynamical semantics.
Chapter 4 presents our first main result, that Bayesian updating composes according to the
categorical ‘lens’ pattern. This result is abstractly stated, and so applies to whichever compositional
model of probability one might be interested in—but because we are later interested in concrete
models, we spend much of the chapter recapitulating compositional probability theory using the
tools introduced in Chapters 2 and 3 and instantiating it in discrete and continuous settings. We
also introduce and contextualize the lens pattern, in order to define our new notion ofBayesian lens,
which provides a mathematical formalization of the bidirectionality of predictive coding circuits.
Our main aim in this thesis is to formalize predictive coding through functorial semantics, and
Bayesian lenses will provide an important part of the ‘syntax’ of statistical models that we need. But
the Bayesian lenses that satisfy the main result of Chapter 4 are ‘exact’, while natural systems are
7
inherently approximate. In order to measure the performance of such approximate systems, Chapter
5 introduces our next new notion, the concept of statistical game , which attaches loss functions to
lenses. These statistical games collect into a categorical structure known as a fibration (a kind of
categorified fibre bundle), and we can use the sections of this fibration to classify well-behaved
systems of approximate inference intoloss models. These loss models include well-known quantities
such as the relative entropy, (maximum) likelihood, the free energy, and the Laplace approximation
of the latter. However, in order to make this classification work, we first introduce a new kind
of categorical composition, which we call copy-composition, and which seems to cleave the basic
process of composition in categories of stochastic channels, which typically proceeds first by
copying and then by marginalization (‘discarding’).
Having developed the syntactic side of predictive coding, we turn in Chapter 6 to the semantics,
which is found in a new abstract formalization of the concept of open dynamical system . We
make much use here of the language of polynomial functors: these will represent the interfaces of
interacting systems, and the dynamical systems themselves will be defined as particular classes of
morphisms of polynomials. We extend the traditional notion of polynomial functor to a setting
which allows for non-determinism, and thereby obtain new categories of open Markov process
and random dynamical system, both in discrete and continuous time. We then synthesize these
developments with the algebraic structures of Chapter 3, to define monoidal bicategories of
‘hierarchical’ cybernetic systems that we call cilia, as they control lenses.
Connecting these pieces together, Chapter 7 presents our functorial formalization of predictive
coding, using a new notion of approximate inference doctrine , by which statistical models are
translated into dynamical systems. This formalizes the process by which research in active inference
turns the abstract specification of a “generative model” into a dynamical system that can be simulated
and whose behaviours can then be compared with experimentally observed data. We explain how
this functorial process is decomposed into stages, and then exhibit them in two ways: first, with
the basic ‘Laplacian’ form of predictive coding; and then by introducing ‘Hebbian’ plasticity.
Finally, Chapter 8 reviews the prospects for future work, from the mathematics of the cognitive
map (a programme that we call compositional cognitive cartography ), to the composition of multi-
agent systems and ecosystems and the connections with compositional game theory, categorical
cybernetics, and categorical systems theory. We close with some speculation on a new mathematics
of life, along with associated developments of fundamental theory.
8
1.2. Contributions
The main individual contribution of this thesis is the formalization of models of predictive coding
circuits as functorial semantics, and the associated development and exemplification of fibrations of
statistical games, as well as the introduction of Bayesian lenses and the proof that Bayesian updates
compose optically. We believe our presentation of general open dynamical systems as certain
polynomial coalgebras also to be novel, along with the concept ofcilia and their associated monoidal
bicategories. The categories of statistical games (and of Bayesian lenses) supply the syntax, and
the monoidal bicategories of cilia the semantics, for our functorial treatment of predictive coding,
and hence the basis for our compositional active inference framework. Each of these structures is
to our knowledge new, although of course inspired by much work that has gone before, and by
interactions with the beneficent community of researchers of which this author finds himself a
member.
Each of these strands of work has in some way been exhibited through publication, principally
as refereed presentations at the conference on Applied Category Theory (ACT) in 2020 [251], 2021
[252], and 2022 [254] (each published in the conference proceedings); but also in preliminary form at
the NeurIPS 2019Context and Compositionality workshop [247], through a number of more informal
invited talks (e.g. [249]), as one main theme of a full-day workshop at the 2022 Cognitive Science
Society conference [12], and our ongoing series of preprints on compositional active inference
[250, 253]. Our work on Bayesian lenses, in collaboration with Dylan Braithwaite and Jules Hedges
[42]9, has been accepted for publication at MFCS 2023; and we are presently preparing for journal
publication an account of our compositional framework for predictive coding aimed explicitly at
computational neuroscientists.
Besides these specific novel contributions, we hope that this dissertation contributes to a
renaissance of cognitive and computational (neuro)science through the adoption of categorical
methods; it is for this reason that we have been so diligent in our exposition of the basic theory.
We hope that this exposition proves itself a useful contribution for interested researchers, and that
its cognitive-neuroscientific framing is sufficiently novel to be interesting.
Some work performed during the author’s DPhil studies is not included in this dissertation.
In particular, there has unfortunately not been the scope to include our simulation results on a
fragment of the circuitry underlying the cognitive map—a study on the development of place and
head-direction cells, which was published as [255]—although this did motivate our algebra of
9See Remark 4.3.1 for the scholarly history.
9
rate-coded neural circuits (§3.3), which is to the best of our knowledge novel (though much inspired
by earlier work on wiring-diagram algebras [237, 282]). We have also not exhibited our work on
Bayesian optics (as an alternative to Bayesian lenses) [248], as this would require a digression
through some unnecessarily complicated theory; and we have not presented in detail the examples
of “polynomial life” presented at ACT 2021 [252].
A first draft of this thesis was produced in December 2022, at which point the author intended to
submit it. However, shortly before submission, the author realized that the then-current treatment
of statistical games could be much improved. This led to the present fibrational account, and the
new notion of loss model (which formalizes the chain rule of the relative entropy), but which also
demanded a corresponding revision of the treatment of predictive coding. At the cost of some
higher-categorical machinery, we believe these changes amount to a substantial improvement,
worth the delay in submission. The new account of statistical games has been accepted as a
proceedings paper at ACT 2023.
10
2. Basic category theory for computational
and cognitive (neuro)scientists
This chapter constitutes a comprehensive review of the concepts and results from basic category
theory that scaffold the rest of the thesis, written for the computational neuroscientist or cognitive
scientist who has noticed the ‘network’ structure of complex systems like the brain and who wonders
how this structure relates to the systems’ function. Category theory gives us a mathematical
framework in which precise answers to such questions can be formulated, and reveals the
interconnectedness of scientific ideas. After introducing the notions of category and diagram
(§2.1), we swiftly introduce the notions of enriched category, functor, and adjunction (§2.2), with
which we can translate and compare mathematical concepts. We then explain how category theory
formalizes pattern as well as translation, using the concept of universal construction (§2.3), which
we exemplify with many common and important patterns. Finally, we introduce the fundamental
theorem of category theory, the Yoneda Lemma, which tells us that to understand a thing is to see
it from all perspectives (§2.4).
Category theory is well established in the foundations of mathematics, but not yet explicitly in
the foundations of science. As a result, although the only slightly original part of this chapter is its
presentation, we have given proofs of most results and plentiful examples, in order to familiarize
the reader with thinking categorically.
2.1. Categories, graphs, and networks
We begin by motivating the use of category theory by considering what is missing from a purely
graph-theoretic understanding of complex computational systems. Later in the thesis, we will see
how each of the diagrams depicted below can be formalized categorically, incorporating all the
scientifically salient information into coherent mathematical objects.
11
2.1.1. Three examples
2.1.1.1. Neural circuits: dynamical networks of neurons
In computational and theoretical neuroscience, it is not unusual to encounter diagrams depicting
proposed architectures for neural circuits, such as on the left or right below:
E
I
On the left, we have depicted a standard “excitatory-inhibitory circuit” motif, in which one neuron
or ensemble of neurons E receives input from an external source as well as from a counterposed
inhibitory circuit I which itself is driven solely by E. On the right, we have reproduced a figure
depicting a “predictive coding” circuit from Bogacz [33], and we see that the E-I circuit is indeed
motivic, being recapitulated twice: we could say that the predictive coding circuit is composed
from interconnected E-I motifs, in a sense similarly to the composition of the E-I circuit from the
subnetworks E and I of neurons.
Both circuits have evident graphical structure — the nodes are the white circles, and the edges
the black wires between them — but of course there is more to neural circuits than these graphs:
not only do graphs so defined omit the decorations on the wires (indicating whether a connection
is excitatory or inhibitory), but they miss perhaps the more important detail, that these are circuits
of dynamical systems , which have their own rich structure and behaviours. Moreover, mere graphs
miss the aforementioned compositionality of neural circuits: we can fill in the white circles with
neurons or ensembles or other circuits and we can wire circuits together, and at the end of doing
so we have another ‘composite’ neural circuit.
Working only with graphs means we have to treat the decorations, the dynamics, and the
compositionality informally, or at least in some other data structure, thereby increasing the overhead
of this accounting.
2.1.1.2. Bayesian networks: belief and dependence
In computational statistics, one often begins by constructing a model of the causal dependence
between events, which can then be interrogated for the purposes of inference or belief-updating.
Such models are typically graphical, with representations as shown below; the nodes are again the
12
circles, and the dashed edge implies the repetition of the depicted motif:
On the left, the graph represents a model of an event with two possible antecedents; on the right, a
set of events (or an event, repeated) with a recurrent cause. Although these graphical models —
otherwise known as Bayesian networks — may encode useful information about causal structure,
in themselves they do not encode the information about how events are caused; this is data that
must be accounted for separately. And once again, mere graphs are unlike causality in that they
are non-compositional: the structure does not explain how, given the causal dependence of B on A
and A1and of C on B, one might model the dependence of C on A.
2.1.1.3. Computations: sets and functions
In a similar way, pure computations — in the sense of transformations between sets of data — are
often depicted graphically:
Wf
Wo
Wi
Wc
Uf
Uo
Ui
Ui
`
`
`
`
d
d
`
d
σ
σ
σ
σ
ct´1
xt
ht´1
ct
ht
Here, we have depicted a single ‘cell’ from a long short-term memory network [132]: a function
that ingests three variables ( ct´1, an internal state; xt, an external input; and ht´1, an internal
‘memory’), and emits two (ct, a new internal state; and ht, an updated memory). This function is
itself composed from other functions, depicted above as boxes. (One typically takes the variables
ct, xt, ht as vectors of given dimension for all t, so that the domain and codomain of the function
are products of vector spaces; the boxes Wi and Ui represent matrices which act on these vectors;
the boxes `and ddenote elementwise sum and product; the box σ represents the elementwise
13
application of a logisitic function; and the splitting of wires represents the copying of data.) The
nodes of the graph in this instance are the functions (boxes), and the edges encode the flow of
information. Once more, however, a purely graphical model does not account for the compositional
structure of the computation: we could fill in the boxes with other graphs (representing elaborations
of the computations implied), and we could adjoin another such diagram beside and connect the
wires where the types match. To account for this compositionality — here and in the examples
above — we will need to add something to the structure: we need to move from graphs to categories.
2.1.2. From graphs to categories
A category is a directed graph in which edges can be composed: whenever the target of an edge f
is the source of another edge g, then there must be a composite edge denoted g ˝f whose source is
the source of f and whose target is the target of g, as in the following diagram.
‚
‚ ‚f
g
g˝f
This composition rule incorporates into the structure a way to allow systems with compatible
interfaces to connect to each other, and for the resulting composite system also to be a system
of the same ‘type’; but as we will see, it has some other important consequences. Firstly, every
(‘small’) category has an underlying directed graph: but because of the composition rule, this
underlying graph typically has more edges than the graphs of the examples above, in order to
account for the existence of composites. Secondly, it is the edges, which in a categorical context we
will call morphisms, that compose: the nodes, which we will call objects, represent something like
the ‘interfaces’ at which composition is possible. This means that we cannot just interpret a circuit
diagram “as a category”, whose objects are ensembles of neurons and whose morphisms are their
axons: as we will see in §3.3, we need to do something a bit more sophisticated.
Before we get to that, however, we must first define categories precisely. We will take a graphical
approach, with a view to interpreting the above examples categorically, starting with the diagram
demonstrating the composition of g ˝f: how should we interpret this in a category? To answer
this question, we first need to specify exactly what we mean by ‘graph’.
Definition 2.1.1. A directed graph G is a set G0 of nodes along with a set Gpa, bqof edges from
a to b for each pair a, b: G0 of nodes. We will sometimes write G1 to denote the disjoint union
14
of the sets of edges, G1 :“ř
a,b Gpa, bq. If e : Gpa, bqis an edge from a to b, we will write this as
e : a Ñb and call a its source or domain and b its target or codomain. This assignment of domain
and codomain induces a pair of functions,dom, cod : G1 ÑG0 respectively, such that fore : a Ñb
we have dompeq“ a and codpeq“ b.
A category is a graph whose edges can be ‘associatively’ composed together, and where every
node has a special edge from itself to itself called its ‘identity’.
Definition 2.1.2. A (small) category C is a directed graph whose nodes are each assigned a
corresponding identity edge and whose edges are equipped with a composition operation ˝that is
associative and unital with respect to the identities. In the context of categories, we call the nodes
C0 the objects or 0-cells, and the edges C1 the morphisms or 1-cells.
Identities are assigned by a function id : C0 ÑC1 satisfying dompidaq“ a “codpidaqfor every
object a. The composition operation is a family of functions ˝a,b,c : Cpb, cqˆ Cpa, bqÑ Cpa, cqfor
each triple of objects a, b, c. The notation Cpa, bqindicates the set of all morphisms a Ñb, for each
pair of objects a and b; we call this set the hom set from a to b.
Given morphisms f : a Ñb and g : b Ñc, their composite a
f
Ý Ñb
g
Ý Ñc is written g ˝f, which
we can read as “g after f”.
Associativity means that h ˝pg ˝fq “ ph ˝gq˝ f, and so we can omit the parentheses to
write h ˝g ˝f without ambiguity. Unitality means that, for every morphism f : a Ñb, we have
idb ˝f “f “f ˝ida.
Remark 2.1.3. We say small category to mean that both the collection of objects C0 and the
collection of morphisms C1 is a true set, rather than a proper class. We will say a category is locally
small if, for every pair a, bof objects in C, the hom set Cpa, bqis a set (rather than a proper class);
this allows for the collection of objects still to be a proper class, while letting us avoid “size issues”
such as Russell’s paradox in the course of normal reasoning.
More precisely, we can fix a ‘universe’ of sets, of size assumed to be smaller than a hypothesized
(and typically inaccessible) cardinal ℵi. Then we say that a category is locally small with respect to
ℵi if every hom set is within this universe, or small if both C0 and C1 are. We say that a category is
large if it is not small, but note that the ‘set’ of objects or morphisms of a large category may still
be a ‘set’, just in a larger universe: a universe whose sets are of cardinality at most ℵi`1 ąℵi.
In the remainder of this thesis, we will typically assume categories to be locally small with
respect to a given (but unspecified) universe.
15
Our first example of a category is in some sense the foundation of basic category theory, and
supplies a classic illustration of size issues.
Example 2.1.4. The category Set has sets as objects and functions as morphisms. The identity on
a set A is the identity functionidA : A ÑA : a ÞÑa. Composition of morphisms in Set is function
composition: given f : A ÑB and g : B ÑC, their composite is the function g ˝f : A ÑC
defined for each a : A by pg ˝fqpaq“ gpfpaqq; it is easy to check that function composition is
associative.
Note that Set is a large category: the set Set0 of all sets of at most size ℵi must live in a larger
universe.
Not all categories are large, of course. Some are quite small, as the following examples
demonstrate.
Example 2.1.5. There is a category with only two objects 0 and 1 and four morphisms: the
identities id0 : 0 Ñ0 and id1 : 1 Ñ1, and two non-identity morphisms s, t: 0 Ñ1, as in the
following diagram:
0 1
s
t
When depicting categories graphically, we often omit identity morphisms as they are implied by
the objects.
Example 2.1.6. There is a category, denoted 1, with a single object ˚and a single morphism, its
identity.
Example 2.1.7. The natural numbers N form the morphisms of another category with a single
object ˚: here, composition is addition and the identity morphism id˚ : ˚Ñ˚ is the number 0.
Since addition is associative and unital with respect to 0, this is a well-defined category.
Since a category is a directed graph equipped with a composition operation, we can ‘forget’ the
latter to recover the underlying graph on its own.
Proposition 2.1.8. Given a category C, we can obtain a directed graph pC0, C1qby keeping the
objects C0 and morphisms C1 and forgetting the composition and identity functions.
Proof. Take the objects to be the nodes and the morphisms to be the edges.
16
However, in the absence of other data, obtaining a category from a given graph is a little
more laborious, as we must ensure the existence of well-defined composite edges. The following
proposition tells us how we can do this.
Proposition 2.1.9. Given a directed graph G, we can construct the free category generated by G,
denoted FG, as follows. The objects of FG are the nodes G0 of G. The morphisms FGpa, bqfrom
a to b are the paths in G from a to b: finite lists pe, f, gqof edges in which the domain of the first
edge is a, the codomain of any edge equals the domain of its successor (if any), and the codomain
of the last edge is b. Composition is by concatenation of lists, and the identity morphism for any
node is the empty list pq.
Proof. Let f :“pf1, . . . , flq: a Ñb, g :“pg1, . . . , gmq: b Ñc, and h :“ph1, . . . , hnq: c Ñd be
paths. Then
h ˝pg ˝fq“p h1, . . . , hnq˝p f1, . . . , fl, g1, . . . , gmq
“pf1, . . . , fl, g1, . . . , gm, h1, . . . , hnq
“pg1, . . . , gm, h1, . . . , hnq˝p f1, . . . , flq“p h ˝gq˝ f
so concatenation of lists is associative. Concatenation is trivially unital on both right and left:
pq˝pf1, . . . , flq“p f1, . . . , flq“p f1, . . . , flq˝pq. So the free category as defined is a well-defined
category.
Remark 2.1.10. Observe that the underlying graph ofFG is not in general the same as the original
graph G: because the edges of G have no composition information (even if, given a pair of edges
a Ñ b and b Ñ c, there is an edge a Ñ c), we needed a canonical method to generate such
information, without any extra data. Since there is a notion of path in any graph, and since paths
are naturally composable, this gives us the canonical method we seek.
We begin to see some important differences between categories and graphs, as foreshadowed
above. Categories are somehow more ‘dynamical’ objects, more concerned with movement and
change than graphs; later in Chapter 6, we will even see how a general definition of dynamical
system emerges simply from some of the examples we have already seen.
At this point, to emphasize that categories allow us to study not just individual structures
themselves but also the relationships and transformations between structures, we note that directed
graphs themselves form a category.
17
Example 2.1.11. Directed graphs pG0, G1, domG, codGqare the objects of a category, denoted
Graph. Given directed graphs G :“pG0, G1, domG, codGqand H :“pH0, H1, domH, codHq, a
morphism f : G ÑH is a graph homomorphism from G to H: a pair of functions f0 : G0 ÑG0
and f1 : G1 ÑH1 that preserve the graphical structure in the sense that for every edge e in G,
f0pdomGpeqq“ domHpf1peqqand f0pcodGpeqq“ codHpf1peqq. Since graph homomorphisms are
pairs of functions, they compose as functions, and the identity morphism on a graph G is the pair
pidG0, idG1qof identity functions on its sets of nodes and edges.
In large part, the power of category theory derives from its elevation of relationship and
transformation to mathematical prominence: objects are represented and studied in context, and
one we gain the ability to compare patterns of relationships across contexts. By expressing these
patterns categorically, we are able to abstract away irrelevant detail, and focus on the fundamental
structures that drive phenomena of interest; and since these patterns and abstract structures
are again expressed in the same language, we can continue to apply these techniques, to study
phenomena from diverse perspectives. Indeed, as we will soon see, category theory is ‘homoiconic’,
able to speak in its language about itself.
Accordingly, it is often helpful to apply graphical or diagrammatic methods to reason about
categories: for example, to say that two (or more) morphisms are actually equal. We can illustrate
this using the category Graph: the definition of graph homomorphism requires two equalities to
be satisfied. These equalities say that two (composite) pairs of functions are equal; since functions
are morphisms in Set, this is the same as saying that they are equal as morphisms there. Using the
fact that Set has an underlying graph, we can represent these morphisms graphically, as in the
following two diagrams:
G1 H1
G0 H0
f1
f0
domG domH
G1 H1
G0 H0
f1
f0
codG codH (2.1)
Then to say that f0 ˝domG “domH ˝f1 and f0 ˝codG “codH ˝f1 is to say that these diagrams
commute.
Definition 2.1.12. We say that two paths in a graph are parallel if they have the same start and
end nodes. We say that a diagram in a category C commutes when every pair of parallel paths in
the diagram corresponds to a pair of morphisms in C that are equal.
18
To clarify this definition, we can use category theory to formalize the concept of diagram, which
will have the useful side-effect of simultaneously rendering it more general and more precise.
2.1.2.1. Diagrams in a category, functorially
The richness of categorical structure is reflected in the variety of diagrammatic practice, and in this
thesis we will encounter a number of formal diagram types. Nonetheless, there is one type that is
perhaps more basic than the rest, which we have already begun to calldiagrams in a category : these
are the categorical analogue of equations in algebra. Often in category theory, we will be interested
in the relationships between more than two morphisms at once, and expressing such relationships
by equations quickly becomes cumbersome; instead, one typically starts with a directed graph and
interprets its nodes as objects and its edges as morphisms in one’s category of interest.
Formally, this interpretation is performed by taking the category generated by the graph and
mapping it ‘functorially’ into the category of interest. However, in order to account for relationships
such as equality between the morphisms represented in the graph, the domain of this mapping
cannot be as ‘free’ as in Proposition 2.1.9, as it needs to encode these relationships. To do this, we
can quotient the free category by the given relationships, as we now show.
Proposition 2.1.13 (Mac Lane [175, Prop. II.8.1]). Let G be a directed graph, and suppose we are
given a relation „a,b on each set FGpa, bqof paths a Ñb; write „for the whole family of relations,
and call it a relation on the category C. Then there is a category FG{„, the quotient of the free
category FG by „, which we call the category generated by G with relations „or simply generated
by pG, „q.
The objects of FG{„are again the nodes G0. The morphisms are equivalence classes of paths
according to „, extended to a congruence: suppose p „a,b p1; then they both belong to the same
equivalence class rps, and correspond to the same morphism rps: a Ñb in FG{„.
Before we can make sense of and prove this proposition, and thus establish that composition in
FG{„does what we hope, we need to define congruence.
Definition 2.1.14. Suppose „is a relation on the category C. We call „a congruence when its
constituent relations „a,b are equivalence relations compatible with the compositional structure of
C. This means that
1. if f „a,b f1 : a Ñb and g „b,c g1 : b Ñc, then g ˝f „a,c g1˝f1; and
2. for each pair of objects a, b: C, „a,b is a symmetric, reflexive, transitive relation.
19
The notion of congruence is what allows us to extend the family of relations „to composites of
morphisms and thus ensure that it is compatible with the categorical structure; constructing the
most parsimonious congruence from „is the key to the following proof.
Proof sketch for Proposition 2.1.13. First of all, we extend„to a congruence; we choose the smallest
congruence containing „, and denote it by –. Explicitly, we can construct –in two steps. First,
define an intermediate relation »as the symmetric, reflexive, transitive closure of „. This means
that if f »f1, then either f „f1, or f1 „f (symmetry), or f “f1 (reflexivity), or there exists
some ϕ : a Ñc such that f „ϕ and ϕ „f1(transitivity). Next, define –as the closure of »under
composition. This means that if φ –φ1 : a Ñc, then either φ »φ1, or there exist composable
pairs f, f1 : a Ñb and g, g1 : b Ñc such that f »f1 and g »g1, and such that φ “g ˝f and
φ1 “g1˝f1. To see that –is the least congruence on FG, observe that every congruence must
contain it by definition.
Having constructed the congruence –, we can form the quotient of FG by it, which we denote
by FG{„in reference to the generating relation „. As in the statement of the proposition, the
objects of FG{„are the nodes of G and the morphisms are equivalence classes of paths, according
to –; since –is by definition an equivalence relation, these equivalence classes are well-defined.
Moreover, the composite of two equivalence classes of morphisms rfs: a Ñb and rgs: b Ñc
coincides with the equivalence class rg ˝fs.
Example 2.1.15. To exemplify the notion of category generated with relations, let J denote the
following directed graph
G1 H1
G0 H0
φ1
φ0
δG δH
and let „be the relation φ0 ˝δG „δH ˝φ1. Then the category FJ {„generated by pJ , „qhas
four objects (G1, G0, H1, H0) and nine morphisms: an identity for each of the four objects; the
morphisms φ0 : G0 ÑH0, φ1 : G1 ÑH1, δG : G1 ÑG0, and δH : H1 ÑH0; and a single
morphism G1 ÑH0, the equivalence class consisting of φ0 ˝δG and δH ˝φ1.
The category FJ {„generated in this example expresses the commutativity of one of the
diagrams defining graph homomorphisms, but as things stand, it is simply a category standing
alone: to say that any particular pair of functions pf0, f1qsatisfies the property requires us to
interpret the morphisms φ0 and φ1 accordingly as those functions . That is, to interpret the diagram,
20
we need to translate it, by mapping FJ {„into Set. Such a mapping of categories is known as a
functor.
Definition 2.1.16. A functor F : C Ñ D from the category C to the category D is a pair of
functions F0 : C0 ÑD0 and F1 : C1 ÑD1 between the sets of objects and morphisms that preserve
domains, codomains, identities and composition, meaning that F0pdomCpfqq “domDpF1pfqq
and F0pcodCpfqq “codDpF1pfqqfor all morphisms f, F1pidaq “idFpaq for all objects a, and
F1pg ˝fq“ F1pgq˝ F1pfqfor all composites g ˝f in C.
Remark 2.1.17. Note that we could equivalently say that a functor C ÑD is a homomorphism
from the underlying graph of C to that of D that is additionally functorial, meaning that it preserves
identities and composites.
Notation 2.1.18. Although a functor F consists of a pair of functions pF0, F1q, we will typically
write just F whether it is applied to an object or a morphism, since the distinction will usually be
clear from the context. Since function composition (and hence application) is associative, we will
also often omit brackets, writing F afor Fpaq, except where it is helpful to leave them in.
For each object c in a category C, there are two very important functors, thehom functors, which
exhibit C in Set “from the perspective” of c by returning the hom sets out of and into c.
Definition 2.1.19. Given an object c : C, its covariant hom functor Cpc, ´q: C ÑSet is defined
on objects x by returning the hom sets Cpc, xqand on morphisms g : x Ñy by returning the
postcomposition function Cpc, gq: Cpc, xqÑ Cpc, yqdefined by mapping morphisms f : c Ñx
in the set Cpc, xqto the composites g ˝f : c Ñy in Cpc, yq. To emphasize the action of Cpc, gq
by postcomposition, we will sometimes write it simply as g ˝p´q. (That Cpc, ´qis a well-defined
functor follows immediately from the unitality and associativity of composition in C.)
The covariant hom functor Cpc, ´q“looks forward” along morphisms emanating out of c, in
the direction that these morphisms point, and therefore in the direction of composition in C: it is
for this reason that we say it is covariant. Dually, it is of course possible to “look backward” at
morphisms pointing into c. Since this means looking contrary to the direction of composition in C,
we say that the resulting backwards-looking hom functor is contravariant. To define it as a functor
in the sense of Definition 2.1.16, we perform the trick of swapping the direction of composition in
C around and then defining a covariant functor accordingly.
21
Definition 2.1.20. For any category C there is a corresponding opposite category C op with the
same objects as C and where the hom set C oppa, bqis defined to be the ‘opposite’ hom set in C,
namely Cpb, aq. Identity morphisms are the same in C op as in C, but composition is also reversed. If
we write ˝for composition in C and ˝op for composition in C op, then, given morphisms g : c Ñb
and f : b Ña in C op corresponding to morphisms g : b Ñc and f : a Ñb in C, their composite
f ˝op g : c Ñ a in C op is the morphism g ˝f : a Ñ c in C. (Observe that this makes C op a
well-defined category whenever C is.)
Remark 2.1.21. Because we can always form opposite categories in this way, categorical
constructions often come in two forms: one in C, and a ‘dual’ one in C op. Typically, we use
the prefix co- to indicate such a dual construction: so if we have a construction in C, then its dual
in C op would be called a coconstruction.
The dual of the covariant hom functor Cpc, ´q: C ÑSet is the contravariant hom functor.
Definition 2.1.22. Given an object c : C, its contravariant hom functor Cp´, cq: C op ÑSet is
defined on objects x by returning the hom sets Cpx, cq. Given a morphism f : x Ñy in C, we
define the precomposition function Cpf, cq: Cpy, cqÑ Cpx, cqby mapping morphisms g : y Ñc
in the set Cpy, cqto the composites g ˝f : x Ñc in Cpx, cq. To emphasize the action of Cpf, cq
by precomposition, we will sometimes write it simply as p´q˝ f. (That Cp´, cqis a well-defined
functor again follows from the unitality and associativity of composition in C and hence in C op.)
Remark 2.1.23. A contravariant functor on C is a (covariant) functor on C op.
Notation 2.1.24. In line with other mathematical literature, we will also occasionally write the
precomposition function p´q˝ f as f˚; dually, we can write the postcomposition function g ˝p´q
as g˚. In these forms, the former action f˚ is also known as pullback along f, as it “pulls back”
morphisms along f, and the latter action g˚is also known as pushforward along g, as it “pushes
forward” morphisms along g. There is a close relationship between the pulling-back described here
and the universal construction also known as pullback (Example 2.3.43): f˚p´qdefines a functor
which acts by the universal construction on objects and by precomposition on morphisms, which
we spell out in Definition 4.2.28.
Functors are the homomorphisms of categories, and just as graphs and their homomorphisms
form a category, so do categories and functors.
22
Example 2.1.25. The category Cat has categories for objects and functors for morphisms. The
identity functor idC on a category C is the pair pidC0, idC1qof identity functions on the sets of
objects and morphisms. Since functors are pairs of functions, functor composition is by function
composition, which is immediately associative and unital with respect to the identity functors so
defined. Note that, without a restriction on size, Cat is a large category, like Set.
As an example, we observe that the construction of the category FG{„generated by pG, „q
from the free category FG is functorial.
Example 2.1.26. There is a ‘projection’ functor r¨s : FG Ñ FG{„. It maps every object to
itself, and every morphism to the corresponding equivalence class. The proof of Proposition 2.1.13
demonstrated the functoriality: identities are preserved by definition, and we haverg˝fs“r gs˝rfs
by construction.
With the notion of functor to hand, we can formalize the concept of diagram simply as follows.
Definition 2.1.27. A J-shaped diagram in a category C is a functor D : J ÑC. Typically, J is a
small category generated from a graph with some given relations, and the functor D interprets J
in C.
Example 2.1.28. The diagrams expressing the commutativity conditions for a graph homomor-
phism (2.1) are therefore witnessed by a pair of functors FJ {„Ñ Set from the category FJ {„
generated in Example 2.1.15 into Set: each functor interprets φ0 and φ1 as f0 and f1 respectively,
while one functor interprets δG as domG and δH as domH and the other interprets δG as codG and
δH as codH. The fact that there is only a single morphism G1 ÑH0 in FJ {„(even though there
are two in FJ ) encodes the requirements that f0 ˝domG “domH ˝f1 and f0 ˝codG “codH ˝f1.
Throughout this thesis, we will see the utility of diagrams as in Definition 2.1.27: not only will
they be useful in reasoning explicitly about categorical constructions, but in §2.3.3 they will also be
used to formalize ‘universal constructions’, another concept which exhibits the power of category
theory.
Despite this, ‘mere’ categories and their diagrams are in some ways not expressive enough:
often we will want to encode looser relationships than strict equality, or to compose diagrams
together by ‘pasting’ them along common edges; we may even want to consider morphisms between
morphisms! For this we will need to ‘enrich’ our notion of category accordingly.
23
2.2. Connecting the connections
As we have indicated, basic category theory is not sufficient if we want to encode information
about the relationships between morphisms into the formal structure. In this section, we will see
how to enrich the notion of category by letting the morphisms collect into more than just sets,
and how this leads naturally to higher category theory, where we have morphisms between the
morphisms, and from there to the notion of adjunction, with which we can translate concepts
faithfully back and forth between contexts. Amidst the development, we discuss the concept of
“functorial semantics” from a scientific perspective, considering how categorical tools let us supply
rich semantics for structured models of complex systems such as the brain.
2.2.1. Enriched categories
We can think of the condition that a diagram commutes — or equivalently the specification of an
equivalence relation on its paths — as a ‘filling-in’ of the diagram with some extra data. For example,
we can ‘fill’ the diagram depicting the graph homomorphism condition f0 ˝domG “domH ˝f1
with some annotation or data witnessing this relation, as follows:
G1 H1
G0 H0
f1
f0
domG domH
If we have a composite graph homomorphism g ˝f : G Ñ I, we should be able to paste the
commuting diagrams of the factors together and fill them in accordingly:
G1 H1 I1
G0 H0 I0
domG domH domI
f1 g1
f0 g0
and we should be able to ‘compose’ the filler equalities to obtain the diagram for the composite:
G1 H1 I1
G0 H0 I0
domG domI
f1 g1
f0 g0
.
24
The extra data with which we have filled these diagrams sits ‘between’ the morphisms, and so
if we wish to incorporate it into the categorical structure, we must move beyond mere sets, for
sets are just collections of elements, with nothing “in between”. What we will do is allow the hom
sets of a category to be no longer sets, but objects of another ‘enriching’ category. Now, observe
that, in pasting the two diagrams above together, we had to place them side by side: this means
that any suitable enriching category must come equipped with an operation that allows us to place
its objects side by side; in the basic case, where our categories just have hom sets, the enriching
category is Set, and this side-by-side operation is the product of sets.
Definition 2.2.1. Given sets A and B, their product is the set A ˆB whose elements are pairs
pa, bqof an element a : A with an element b : B.
We have already made use of the product of sets above, when we defined the composition
operation for (small) categories in Definition 2.1.2. In general, however, we don’t need precisely a
product; only something weaker, which we call tensor. In order to define it, we need the notion of
isomorphism.
Definition 2.2.2. A morphism l : c Ñd in a 1-category is an isomorphism if there is a morphism
r : d Ñc such that l ˝r “idd and idc “r ˝l. We say that l and r are mutually inverse.
Definition 2.2.3. We will say that a category C has a tensor product if it is equipped with a functor
b: C ˆC ÑC along with an object I : C called the tensor unit and three families of isomorphisms:
1. associator isomorphisms αa,b,c : pa bbqb c „Ý Ña bpb bcqfor each triple of objects a, b, c;
2. left unitor isomorphisms λa : I ba „Ý Ña for each object a; and
3. right unitor isomorphisms ρa : a bI „Ý Ña for each object a.
Remark 2.2.4. The notion of tensor product forms part of the definition of monoidal category ,
which we will come to in §3.1.2. Beyond having a tensor product, a monoidal category must have
structure isomorphisms that are coherent with respect to the ambient categorical structure, which
itself satisfies properties of associativity and unitality; this is an echo of the microcosm principle
which we discuss in Remark 3.4.7. However, to give the full definition the notion of monoidal
category requires us to introduce the notion of natural transformation , which we otherwise do not
need until Definition 2.2.17; moreover, questions of coherence of tensor products will not yet arise.
Unsurprisingly, the product of sets gives us our first example of a tensor product structure.
25
Example 2.2.5. The product of sets gives us a tensor product ˆ : Set ˆSet Ñ Set. To see
that it is functorial, observe that, given a product of sets A ˆB and a function f : A ÑA1, we
naturally obtain a function f ˆB : A ˆB ÑA ˆA1by applying f only to the A-components of
the elements of the product A ˆB; likewise given a function g : B ÑB1. The unit of the tensor
product structure is the set 1 with a single element ˚. The associator and unitors are almost trivial:
for associativity, map ppa, bq, cqto pa, pb, cqq.
Using the tensor product to put morphisms side by side, we can define the notion of enriched
category.
Definition 2.2.6. Suppose pE, b, I, α, λ, ρqis a category equipped with a tensor product. An
E-category C, or category C enriched in E, constitutes
1. a set C0 of objects;
2. for each pair pa, bqof C-objects, an E-object Cpa, bqof morphisms from a to b;
3. for each object a in C, an E-morphism ida : I ÑCpa, aqwitnessing identity; and
4. for each triple pa, b, cqof C-objects, an E-morphism ˝a,b,c : Cpb, cqb Cpa, bq ÑCpa, cq
witnessing composition;
such that composition is unital, i.e. for all a, b: C
Cpa, bqb I Cpa, bqb Cpa, aq
Cpa, bq
ρCpa,bq
Cpa,bqbida
˝a,a,b and
Cpa, bqb Cpa, aq I bCpa, bq
Cpa, bq
λCpa,bq
idbbCpa,bq
˝a,b,b ,
and associative, i.e. for all a, b, c, d: C
`
Cpc, dqb Cpb, cq
˘
bCpa, bq Cpc, dqb
`
Cpb, cqb Cpa, bq
˘
Cpb, dqb Cpa, bq Cpc, dqb Cpa, cq
Cpa, dq
αa,b,c,d
˝b,c,dbCpa,bq Cpc,dqb˝a,b,c
˝a,b,d ˝a,c,d
.
Our first example of enriched categories validates the definition.
26
Example 2.2.7. A locally small category is a category enriched in pSet, ˆ, 1q.
Remark 2.2.8. In Set, morphisms 1 ÑA out of the unit set 1 correspond to elements of A: each
such morphism is a function mapping the unique element ˚: 1 to its corresponding element of A.
This is why identities in enriched category theory are given by morphisms I ÑCpa, aq, and it is
also why we will call morphisms out of a tensor unit generalized elements . (Even more generally,
we might say that morphisms X ÑA are generalized elements of shape X, reflecting our use of
the word ‘shape’ to describe the domain of a diagram.)
To incorporate nontrivial fillers into our diagrams, we move instead to enrichment in prosets.
Example 2.2.9. A preordered set or proset is a category where there is at most one morphism
between any two objects. The objects of such a ‘thin’ category are the points of the proset, and the
morphisms encode the (partial) ordering of the points; as a result, they are often written a ďa1.
Functors between prosets are functions that preserve the ordering, and the restriction of Cat to
prosets produces a category that we denote by Pro. The product of sets extends to prosets as
follows: if A and B are prosets, then their product is the proset A ˆB whose points are the points
of the product set A ˆB and a morphism pa, bqďp a1, b1qwhenever there are morphisms a ďa1
and b ďb1in A and B respectively.
A category enriched in Pro is therefore a category whose hom sets are (pre)ordered and whose
composition operation preserves this ordering, which we can illustrate as follows:
A B C
f g
f1 g1
ď
ď ˝ÞÝ Ñ A C
g˝f
g1˝f1
ď
We can see how enrichment in Pro generalizes the situation with which we introduced this
section, where we considered filling diagrams with data witnessing the equality of morphisms:
here we have inequality data, and it is not hard to see how enriched composition encompasses the
pasting-and-composing discussed there (just replace the cells here by the squares above).
In order to make these filled diagrams precise, we need to extend the notion of functor to the
enriched setting; and so we make the following definition.
Definition 2.2.10. Suppose C and D are E-categories. Then an E-functor F constitutes
1. a function F0 : C0 ÑD0 between the sets of objects; and
2. for each pair pa, bq: C0 ˆC0 of objects in C, an E-morphism Fa,b : Cpa, bqÑ DpF0a, F0bq
27
which preserve identities
I
Cpa, aq DpF0a, F0aq
ida idF0a
Fa,a
and composition
Cpb, cqb Cpa, bq Cpa, cq
DpF0b, F0cqb DpF0a, F0bq DpF0a, F0cq
Fb,cbFa,b
˝a,b,c
Fa,c
˝F0a,F0b,F0c
.
A diagram in an E-enriched category C is therefore a choice of E-enriched category J (the
diagram’s shape) and an E-functor J ÑC. J encodes the objects, morphisms and relationships
of the diagram, and the functor interprets it in C. In this enriched setting, we need not quotient
parallel paths in the shape of a diagram (which destroys their individuality); instead, we have extra
data (the fillers) encoding their relationships.
2.2.2. 2-categories
We have seen that filling the cells of a diagram with inequalities pushes us to consider enrichment
in Pro. Since Pro is the category of categories with at most one morphism (i.e., the inequality)
between each pair of objects, a natural generalization is to allow a broader choice of filler: that is,
to allow there to be morphisms between morphisms. This means moving from enrichment in Pro
to enrichment in Cat, and hence to the notion of 2-category. We therefore make the following
definition.
Definition 2.2.11. A strict 2-category is a category enriched in the 1-category Cat. This means
that, instead of hom sets, a 2-category has hom categories: the objects of these hom categories are
the 1-cells of the 2-category, and the morphisms of the hom categories are the 2-cells; the 0-cells
of the 2-category are its objects. To distinguish between the composition defined by the enriched
category structure from the composition within the hom categories, we will sometimes call the
former horizontal and the latter vertical composition.
Remark 2.2.12. We say 1-category above to refer to the ‘1-dimensional’ notion of category defined
in Definition 2.1.2.
28
Remark 2.2.13. We say strict to mean that the associativity and unitality of composition hold up
to equality; later, it will be helpful to weaken this so that associativity and unitality only hold up to
“coherent isomorphism”, meaning that instead of asking the diagrams in Definition 2.2.6 simply to
commute (and thus be filled by equalities), we ask for them to be filled with ‘coherently’ defined
isomorphism. Weakening 2-categorical composition in this way leads to the notion of bicategory
(§3.1.4).
In order to give a well-defined notion of enrichment in Cat, we need to equip it with a suitable
tensor product structure; for this, we can extend the product of sets to categories, as follows.
Proposition 2.2.14. Given categories C and D, we can form the product category C ˆD. Its set
of objects pC ˆDq0 is the product set C0 ˆD0. Similarly, a morphism pc, dqÑp c1, d1qis a pair
pf, gqof a morphism f : c Ñc1in C with a morphism g : d Ñd1in D; hence pC ˆDq1 “C1 ˆD1.
Composition is given by composing pairwise in C and D: pf1, g1q˝p f, gq:“pf1˝f, g1˝gq.
Proof. That composition is associative and unital inCˆD follows immediately from those properties
in the underlying categories C and D.
Remark 2.2.15. Using the product of categories, we can gather the co- and contravariant families
of hom functors Cpc, ´qand Cp´, cqinto a single hom functor Cp´, “q: C op ˆC ÑSet, mapping
px, yq: C op ˆC to Cpx, yq.
Proposition 2.2.16. The product of categories extends to a functorˆ: CatˆCat ÑCat. Given
functors F : C ÑC1and G : D ÑD1, we obtain a functor F ˆG by applying F to the left factor
of the product C ˆD and G to the right.
Proof. Sufficiently obvious that we omit it.
The archetypal 2-category is Cat itself, as we will now see: morphisms between functors are
called natural transformation , and they will play an important rôle throughout this thesis.
Definition 2.2.17. Suppose F and G are functors C ÑD. A natural transformation α : F ñG
is a family of morphisms αc : FpcqÑ Gpcqin D and indexed by objects c of C, such that for any
morphism f : c Ñc1in C, the following diagram — called a naturality square for α — commutes:
F c Gc
F c1 Gc1
αc
αc1
F f Gf .
29
When the component 1-cells of a natural transformation α are all isomorphisms, then we call α a
natural isomorphism .
Example 2.2.18. Every morphism f : a Ñ b in a category C induces a (contravariant)
natural transformation Cpf, ´q : Cpb, ´q ñCpa, ´qbetween covariant hom functors, acting
by precomposition. Dually, every morphism h : c Ñd induces a (covariant) natural transformation
Cp´, hq: Cp´, cqñ Cp´, dqbetween contravariant hom functors, acting by postcomposition. To
see that these two families are natural, observe that the square below left must commute for all
objects a, b, c: C and morphisms f : a Ñb and h : c Ñd, by the associativity of composition in C
(as illustrated on the right)
Cpb, cq Cpa, cq
Cpb, dq Cpa, dq
Cpf,cq
Cpb,hq
Cpf,dq
Cpa,hq
g g ˝f
h ˝g h ˝g ˝f
and that it therefore constitutes a naturality square for bothCpf, ´qand Cp´, hq. Note also that we
can take either path through this square as a definition of the function Cpf, hq: Cpb, cqÑ Cpa, dq
which thus acts by mapping g : b Ñc to h ˝g ˝f : a Ñd.
Remark 2.2.19. We will see in §3.1.2 that the families of structure morphisms for a tensor product
(and hence used in the definition of enriched category) are more properly required to be natural
transformations.
The existence of morphisms between functors implies that the collection of functors between
any pair of categories itself forms a category, which we now define.
Proposition 2.2.20. The functors between two categories C and D constitute the objects of a
category, called the functor category and denoted by CatpC, Dqor DC, whose morphisms are the
natural transformations between those functors. The identity natural transformation on a functor
is the natural transformation whose components are all identity morphisms.
Proof. First, observe that the identity natural transformation is well-defined, as the following
diagram commutes for any morphism f : c Ñc1:
F c F c
F c1 F c1
idFc
idFc1
F f F f
30
(Note that in general, we will depict an identity morphism in a diagram as an elongated equality
symbol, as above.) Given two natural transformations α : F ñG and β : G ñH, their composite
is the natural transformation defined by composing the component functions: pβ ˝αqc :“βc ˝αc.
We can see that this gives a well-defined natural transformation by pasting the component naturality
squares:
F c Gc Hc
F c1 Gc1 Hc1
αc
αc1
F f Gf
βc
βc1
Hf
Since the two inner squares commute, so must the outer square. And since the composition
of natural transformations reduces to the composition of functions, and the identity natural
transformation has identity function components, the composition of natural transformations
inherits strict associativity and unitality from composition in Set.
This gives us our a first nontrivial example of a 2-category.
Example 2.2.21. Functor categories constitute the hom categories of the strict 2-category Cat,
and henceforth we will write Cat1 to denote the 1-category of categories and functors; we can
therefore say that Cat is enriched in Cat1. The 0-cells of Cat are categories, the 1-cells are
functors, and the 2-cells are natural transformations. If α is a natural transformation F ñG, with
F and G functors C ÑD, then we can depict it as filling the cell between the functors:
C D
F
G
α
(More generally, we will depict 2-cells in this way, interpreting such depictions as diagrams of
enriched categories in the sense discussed above.)
Since Cat is a 2-category, it has both vertical composition (composition within hom-categories)
and horizontal (composition between them). In Proposition 2.2.20, we introduced the vertical
composition, so let us now consider the horizontal, which we will denote by ˛to avoid ambiguity.
The horizontal composition of 1-cells is the composition of functors (as morphisms in Cat1),
but by the definition of enriched category, it must also extend to the 2-cells (here, the natural
transformations). Suppose then that we have natural transformations φ and γ as in the following
31
diagram:
B C D
F G
F1 G1
φ γ
The horizontal composite γ ˛φ is the natural transformation GF ñG1F1with components
GF b
Gφb
ÝÝÑGF1b
γF1b
ÝÝÑG1F1b .
Notation 2.2.22 (Whiskering). It is often useful to consider the horizontal composite of a natural
transformation α : F ñ G with (the identity natural transformation on) a functor, as in the
following diagrams, with precomposition on the left and postcomposition on the right:
D C C1
L F
GL
αidL C C1 D1
F
G
R
R
α idR
We will often write the left composite α ˛L : F Lñ GL as αL, since its components are
αLd : F LdÑGLd for all d : D; and we will often write the right composite R ˛α : RF ñRG
as Rα, since its components are Rαc : RF cÑRGc for all c : C. This use of notation is called
whiskering.
2.2.3. On functorial semantics
At this point, we pause to consider category theory from the general perspective of our motivating
examples, to reflect on how category theory might surprise us: as we indicated in §2.1.2, categories
are more ‘dynamical’ than graphs, more preoccupied with change, and so behave differently; in fact,
they have a much richer variety of behaviours, and just as categories can often be very well-behaved,
they can also be quite unruly. Through its homoiconicity—its ability to describe itself—the use
of category theory impels us to consider not only how individual systems are constructed, nor
only how systems of a given type can be compared, but also how to compare different classes of
system. In this way, category theory rapidly concerns itself with notions not only of connection
and composition, but also of pattern and translation.
Scientifically, this is very useful: in the computational, cognitive, or otherwise cybernetic sciences,
we are often concerned with questions about when and how natural systems ‘compute’. Such
questions amount to questions of translation, between the abstract realm of computation to the
more concrete realms inhabited by the systems of interest and the data that they generate; one often
asks how natural structures might correspond to ‘algorithmic’ details, or whether the behaviours of
32
systems correspond to computational processes. It is for this reason that we chose our motivating
examples, which exhibited (abstract) natural structure as well as two kinds of informational
or computational structure: a central question in contemporary neuroscience is the extent to
which neural circuits can be understood as performing computation (particularly of the form now
established in machine learning). This question is in some way at the heart of this thesis, which
aims to establish a compositional framework in which the theories of predictive coding and active
inference may be studied.
The dynamism of categories is a hint that it is possible to describe both the structure of systems
and their function categorically, with a ‘syntax’ for systems on the one hand and ‘semantics’ on the
other. This is the notion of functorial semantics [165], by which we translate syntactic structures
in one category to another category which supplies semantics: the use of functors means that
this translation preserves basic compositional structure, and we often ask for these functors to
preserve other structures, too; a typical choice, that we will adopt in Chapter 3 is to uselax monoidal
functors, which preserve composition in two dimensions, allowing us to place systems “side by
side” as well as “end to end”.
Of course, the particular choices of syntactic and semantic category will depend upon the subject
at hand—in this thesis we will be particularly interested in supplying dynamical semantics for
approximate inference problems—but typically the syntactic category will have some ‘nice’ algebraic
structure that is then preserved and interpreted by the functorial semantics. This is, for instance,
how functorial semantics lets us understand processes that “happen on graphs”, and as a simple
example, we can consider diagrams in Set: the shape of the diagram tells us how to compose the
parts of a system together, while the diagram functor gives us, for each abstract part, a set of possible
components that have a compatible interface, as well as functions realizing their interconnection.
In categorical ‘process’ theory, and the more general categorical theory of systems, one therefore
often considers the objects of the ‘syntactic’ category as representing the shapes or interfaces of
systems and the morphisms as representing how the different shapes can plug together. This is
an algebraic approach to systems design: mathematically, the syntactic structure is encoded as a
monad, and the functorial semantics corresponds to a monad algebra, as we explain in Chapter 3;
and the desire for composition richer than merely end-to-end is another motivation for venturing
into higher category theory. In Chapter 6, we will ‘unfold’ a combination of these ideas, to construct
bicategories whose objects represent interfaces, whose 1-cells are processes ‘between’ the interfaces
that can be composed both sequentially and in parallel, and whose 2-cells are homomorphisms of
33
processes. This bicategory will then in Chapter 7 supply the semantics for models of predictive
coding.
In science, there is rarely only one way to study a phenomenon, and our collective understanding
of phenomena is therefore a patchwork of perspectives. At the end of this chapter, we will discuss
the Yoneda Lemma, which formalizes this observation that to understand a thing is to see it from all
perspectives, and it is for this reason that we expect category theory to supply alingua franca for the
mathematical sciences. In computational neuroscience specifically, an influential methodological
theory is David Marr’s “three levels of explanation” [179], in which complex cognitive systems are
profitably studied at the levels of ‘computation’, ‘algorithm’, and ‘implementation’. These levels
are only very informally defined, and the relationships between them not at all clear. We hope
that functorial semantics and other categorical approaches can replace such methodologies so that
instead of a simplistic hierarchical understanding of systems, we can progressively and clearly
expose the web of relationships between models.
2.2.4. Adjunction and equivalence
We discussed above the use of functors to translate between mathematical contexts. Often, we
are interested not only in translation in one direction, but also in translating back again. When
we have a pair of functors—or 1-cells more generally—in opposite directions and when the two
translations are somehow reversible, we often find ourselves with an adjunction; for example, the
functorial mappings of graphs to categories and back are adjoint (Example 2.2.25 below), and we
conjecture in Chapter 8 that the mapping of “statistical games” to dynamical systems forms part
of an adjunction, too. Adjunctions are particularly well-behaved ‘dual’ translations, and they will
therefore be of much use throughout this thesis. For its conceptual elegance, we begin with an
abstract definition, which exhibits the fundamental essence.
Definition 2.2.23. Suppose L : C ÑD and R : D ÑC are 1-cells of a 2-category. We say that
they are adjoint or form an adjunction, denoted L % R, if there are 2-cells η : idC ñ RL and
ϵ : LR ñidD, called respectively the unit and counit of the adjunction, which satisfy the triangle
equalities ϵL ˝Lη “idL and Rϵ ˝ηR “idR, so called owing to their diagrammatic depictions:
L LRL
L
Lη
ϵL and
R RLR
R
ηR
Rϵ
34
The unit and counit of the adjunction measure ‘how far’ the round-trip composite functors
LR : C ÑC and RL : D ÑD leave us from our starting place, as indicated in the following
diagrams:
D
C C
L R
idC
η and
C
D D
R L
idD
ϵ
The triangle identities then ensure that the round-trips have an isomorphic ‘core’, so that it is
possible to translate morphisms on one side to the other losslessly (which we will exemplify in
Proposition 2.2.26), and that the adjunction has a natural ‘algebraic’ interpretation (which we will
encounter in Proposition 3.4.13).
In the specific case of the 2-categoryCat, we can make the following alternative characterization
of adjunctions. Here we see that the “isomorphic core” of the adjunction can be characterized by
saying that morphisms into objects in C that come from D via R are in bijection with morphisms
out of objects in D that come from C via L.
Definition 2.2.24. Suppose L : C ÑD and R : D ÑC are functors between categories C and
D. We say that they are adjoint functors when there is an isomorphism between the hom-sets
DpLc, dq– Cpc, Rdqthat is natural in c : C and d : D.
Given a morphism f : Lc Ñd in D, we denote its (right) adjunct in C by f7 : c ÑRd. Inversely,
given a morphism g : c ÑRd in C, we denote its (left) adjunct in D by g5 : Lc Ñd. The existence
of the isomorphism means that f75 “f and g “g57
.
Example 2.2.25. The functor F : Graph ÑCat mapping a graph to the corresponding free
category (Proposition 2.1.9) is left adjoint to the forgetful functor U : Cat ÑGraph mapping
a category to its underlying graph (Proposition 2.1.8). To see this, we need to find a natural
isomorphism CatpFG, Cq –GraphpG, UCq. A graph homomorphism G ÑUC is a mapping
of the nodes of G to the objects of C and of the edges of G to the morphisms of C that preserves
sources (domains) and targets (codomains). A functor FG ÑC is a mapping of the nodes of G to
the objects of C along with a mapping of paths in G to morphisms in C that preserves domains,
codomains, identities and composites. A path in G is a list of ‘composable’ edges, with the identity
path being the empty list, so such a mapping of paths is entirely determined by a mapping of
edges to morphisms that preserves domains and codomains. That is to say, a functor FG ÑC
is determined by, and determines, a graph homomorphism G Ñ UC, and so the two sets are
isomorphic: in some sense, functors between free categoriesare graph homomorphisms. To see that
35
the isomorphism is natural, observe that it doesn’t matter if we precompose a graph homomorphism
G1 ÑG (treated as a functor between free categories) or postcompose a functor C ÑC1(treated as
a graph homomorphism): because graph homomorphisms compose preserving the graph structure,
we would still have an isomorphism CatpFG1, C1q– GraphpG1, UC1q.
Before we can properly say that adjoint functors form an adjunction, we need to prove it. As the
following proof shows, the mappings p´q7and p´q5define and are defined by the unit and counit
of the adjunction.
Proposition 2.2.26. Functors that form an adjunction in Cat are exactly adjoint functors.
Proof. We need to show that functors that form an adjunction are adjoint, and that adjoint functors
form an adjunction; that is, we need to show that any pair of functors L : C ÑD and R : D ÑC
satisfying the definition of adjunction in Definition 2.2.23 necessarily constitute adjoint functors
according to Definition 2.2.24, and that if L and R are adjoint according to Definition 2.2.24 then
they form an adjunction according to Definition 2.2.23: i.e., the two definitions are equivalent.
We begin by showing that if L %R, then L and R are adjoint functors. This means we need
to exhibit a natural isomorphism DpLc, dq– Cpc, Rdq. We define a function p´q7 : DpLc, dqÑ
Cpc, Rdqby setting
f7 :“c
ηc
Ý ÑRLc
Rf
Ý Ý ÑRd
and a function p´q5 : Cpc, RdqÑ DpLc, dqby setting
g5 :“Lc
Lg
ÝÑLRd
ϵd
Ý Ñd .
We then use naturality and the triangle equalities to show that f75 “f and g57
“g:
f75
“Lc
Lf7
ÝÝÑLRd
ϵd
Ý Ñd
“Lc
Lηc
ÝÝÑLRLc
LRf
Ý ÝÝ ÑLRd
ϵd
Ý Ñd
“Lc
Lηc
ÝÝÑLRLc ϵLc
Ý Ý ÑLc
f
Ý Ñd
“Lc
f
Ý Ñd
g57
“c
ηc
Ý ÑRLc
Rg5
ÝÝÑRd
“c
ηc
Ý ÑRLc RLcÝÝÑRLRd
Rϵd
ÝÝÑRd
“c
g
Ý ÑRd
ηRd
ÝÝÑRLRd
Rϵd
ÝÝÑRd
“c
g
Ý ÑRd
In each case the first two lines follow by definition, the third by naturality, and the fourth by the
triangle equality; hence we have an isomorphism DpLc, dq –Cpc, Rdq. The naturality of this
isomorphism follows from the naturality of η and ϵ. We first check that the isomorphisms p´q7are
36
natural in c, which means that the following squares commute for all ϕ : c1 Ñc in C:
DpLc, dq Cpc, Rdq
DpLc1, dq Cpc1, Rdq
DpLϕ,dq
p´q7
c1,d
Cpϕ,Rdq
p´q7
c,d
This requires in turn that pf ˝Lϕq7 “f7˝ϕ, which we can check as follows:
pf ˝Lϕq7 “c1 ηc1
Ý Ý ÑRLc1 RLϕ
Ý ÝÝ ÑRLc
Rf
Ý Ý ÑRd
“c1 ϕ
Ý Ñc
ηc
Ý ÑRLc
Rf
Ý Ý ÑRd
“c1 ϕ
Ý Ñc
f7
Ý ÑRd
where the second equality holds by the naturality of η. The naturality of p´q7in d requires that
pϕ1˝fq7 “Rϕ1˝f7for all ϕ1 : d Ñd1, which can be checked almost immediately:
pϕ1˝fq7 “c
ηc
Ý ÑRLc
Rf
Ý Ý ÑRd
Rϕ1
ÝÝÑRd1
“c
f7
Ý ÑRd
Rϕ1
ÝÝÑRd1
Dually, the naturality of p´q5 : Cpc, RdqÑ DpLc, dqin d requires that pRϕ1˝gq5 “ϕ1˝g5for all
ϕ1 : d Ñd1, which obtains by the naturality of ϵ:
pRϕ1˝gq5 “Lc
Lg
ÝÑLRd
LRϕ1
ÝÝÝÑLRd1 ϵd1
ÝÑd1
“Lc
Lg
ÝÑLRd
ϵd
Ý Ñd
ϕ1
Ý Ñd1
“Lc
g5
Ý Ñd
ϕ1
Ý Ñd1
The naturality of p´q5in c, which requires that pg ˝ϕq5 “g5˝Lϕ, obtains similarly immediately:
pg ˝ϕq5 “Lc1 Lϕ
Ý Ý ÑLc
Lg
ÝÑLRd
ϵd
Ý Ñd
“Lc1 Lϕ
Ý Ý ÑLc
g5
Ý Ñd
Thus p´q7and p´q5are both natural in c and d, and hence L and R are adjoint functors.
To show the converse, that if L : C ÑD and R : D ÑC are adjoint functors then L %R,
we need to establish natural transformations η : idC ñRL and ϵ : LR ñidD from the natural
isomorphisms p´q7and p´q5, such that the triangle equalitiesϵL ˝Lη “idL and Rϵ ˝ηR “idR are
satisfied. We first define η componentwise, by observing that ηc must have the type c ÑRLc, and
37
that the image of idLc : Lc ÑLc under p´q7is of this type, and therefore defining ηc :“pidLcq7.
Dually, we define ϵ by observing that ϵd must have the type LRd Ñd, and that the image of idRd
under p´q5has this type. We therefore defineϵd :“pidRdq5. To see that these definitions constitute
natural transformations, observe that they are themselves composed from natural transformations.
Explicitly, the naturality of η means that for any f : c Ñc1, we must have RLf ˝ηc “ηc1 ˝f, and
the naturality of ϵ means that for any g : d Ñd1, we must have g ˝ϵd “ϵd1 ˝LRg. These obtain
as follows:
RLf ˝ηc “c
pidLcq7
Ý ÝÝÝ ÑRLc
RLf
Ý ÝÝ ÑRLc1
“c
pLf˝idLcq7
ÝÝÝÝÝÝÑRLc1
“c
pidLc1˝Lfq7
ÝÝÝÝÝÝÝÑRLc1
“c
f
Ý Ñc1 pidLc1q7
ÝÝÝÝÑRLc1
“ηc1 ˝f
g ˝ϵd “LRd
pidRdq5
ÝÝÝÝÑd
g
Ý Ñd1
“LRd
pRg˝idRdq5
Ý ÝÝÝÝÝÝ Ñd1
“LRd
pidRd1˝Rgq5
ÝÝÝÝÝÝÝÑd1
“LRd
LRg
Ý ÝÝ ÑLRd1 pidRd1q5
Ý ÝÝÝÝ Ñd1
“ϵd1 ˝LRg
In each case, the first equality holds by definition, the second by naturality of p´q7and p´q5(left
and right, respectively) in d, the third by naturality of id, the fourth by naturality in c, and the last
by definition. It remains to check that η and ϵ so defined satisfy the triangle equalities. Expressed
componentwise, we demonstrate that ϵLc ˝Lηc “idLc and that Rϵd ˝ηRd “idRd as follows:
ϵLc ˝Lηc “Lc
LpidLcq7
Ý ÝÝÝÝ ÑLRLc
pidRLcq5
Ý ÝÝÝÝ ÑLc
“Lc
pidRLc ˝pidLcq7q5
Ý ÝÝÝÝÝÝÝÝÝ ÑLc
“Lc
pidLcq75
Ý ÝÝÝÝ ÑLc
“Lc idLc
ÝÝÑLc
Rϵd ˝ηRd “Rd
pidLRdq7
Ý ÝÝÝÝ ÑRLRd
RpidRdq5
ÝÝÝÝÝÑRd
“Rd
ppidRdq5˝idLRdq7
ÝÝÝÝÝÝÝÝÝÝÑRd
“Rd
pidRdq57
Ý ÝÝÝÝ ÑRd
“Rd
idRd
ÝÝÑRd
The first equality (on each side) holds by definition, the second (on the left) by naturality of p´q5in
c and (on the right) by naturality of p´q7in d, the third by unitality of composition, and the fourth
by the 7/5isomorphism. This establishes that L %R, and hence the result.
Sometimes, the ‘distances’ measured by the unit and counit are so small that the categories C
and D are actually ‘equivalent’: this happens when the unit and counit are natural isomorphisms,
meaning that the isomorphic core of the adjunction extends to the whole of C and D. This gives us
the following definition.
Definition 2.2.27. Suppose L %R in a 2-category. When the unit and counit of the adjunction
are additionally isomorphisms, we say that L and R form an adjoint equivalence .
38
Remark 2.2.28. More generally, an equivalence of categories is a pair of functors connected by
natural isomorphisms of the form of the unit and counit of an adjunction, but which may not
necessarily satisfy the triangle identities; however, given any such equivalence, it is possible to
modify the unit or counit so as to upgrade it to an adjoint equivalence. Henceforth, we will have
no need to distinguish equivalences from adjoint equivalences, so we will say simply ‘equivalence’
for both. If there is an equivalence between a pair of categories, then we will say that the two
categories are equivalent.
Note that the notion of equivalence of categories can be generalized toequivalence in a 2-category ,
by replacing the categories by 0-cells, the functors by 1-cells, and the natural isomorphisms by
invertible 2-cells.
The structure of an equivalence of categories can alternatively be specified as properties of the
functors concerned, which in some situations can be easier to verify.
Definition 2.2.29. We say that a functor F : C ÑD is
1. full when it is surjective on hom sets, in the sense that the functions Fa,b : Cpa, bq Ñ
DpF a, F bqare surjections;
2. faithful when it is injective on hom sets, in the sense that the functions Fa,b are injections;
3. fully faithful when it is both full and faithful (i.e., isomorphic on hom sets); and
4. essentially surjective when it is surjective on objects up to isomorphism, in the sense that for
every object d : D there is an object c : C such that F c–d.
Proposition 2.2.30. Categories C and D are equivalent if and only if there is a functor F : C ÑD
that is fully faithful and essentially surjective.
Proof [213, Lemma 9.4.5]. First, we show that if F %G : D ÑC is an equivalence of categories,
then F : C ÑD is fully faithful and essentially surjective. For the latter, observe that G gives
us, for any d : D, an object Gd : C and ϵd is by definition an isomorphism F Gd„Ý Ñd; hence
F is essentially surjective. To show that F is fully faithful means showing that each function
Fa,b : Cpa, bqÑ DpF a, F bqis an isomorphism; we can define the inverse F´1
a,b as the following
composite:
DpF a, F bq
GFa,Fb
ÝÝÝÝÑCpGF a, GFbq
Cpηa,η´1
b q
Ý ÝÝÝÝÝ Ñ Cpa, bq
g ÞÑ Gg ÞÑ
`
a
ηa
ÝÑGF a
Gg
Ý Ý ÑGF b
η´1
b
ÝÝÑb
˘
39
Here, the function Cpηa, η´1
b qis the function f ÞÑη´1
b ˝f ˝ηa obtained from the hom functor
(Remark 2.2.15). Hence F´1
a,b pgq:“η´1
b ˝Gg ˝ηa. To see that this is indeed the desired inverse,
consider applying the functor F to the morphism F´1
a,b pgq; we have the following equalities:
F a
F ηa
ÝÝÑF GF a
F Gg
Ý ÝÝ ÑF GF b
F η´1
b
ÝÝÝÑF b
“F a
g
Ý ÑF b
F ηb
ÝÝÑF GF b
F η´1
b
ÝÝÝÑF b
“F a
g
Ý ÑF b
where the first equality holds by the naturality of η and the second equality holds since ηb is an
isomorphism. Since F is therefore isomorphic on hom sets, it is fully faithful.
Next, we show that if F : C Ñ D is fully faithful and essentially surjective, then there is a
functor G : D ÑC and natural isomorphisms η : idC ñGF and ϵ : F GñidD. On objects d : D,
we can define Gd : C as any choice of object such that F Gd„Ý Ñd: such an object must exist since
F is essentially surjective. We then define ϵd to be the associated isomorphism F GdÑd; it is
easy to verify that ϵ so defined is natural. On morphisms, let the functions Gd,e be defined as the
composite functions
Dpd, eq
Dpϵd,ϵ´1
e q
Ý ÝÝÝÝÝ Ñ DpF Gd, FGeq
F´1
Gd,Ge
ÝÝÝÝÑ CpGd, Geq
g ÞÑ
`
F Gd
ϵd
Ý Ñd
g
Ý Ñe ϵ´1
e
Ý Ý ÑF Ge
˘
ÞÑ F´1
Gd,Ge
`
ϵ´1
e ˝g ˝ϵd
˘ .
Since F is a fully faithful functor and ϵ is a natural isomorphism, this makes G a well-defined
functor. Finally, we define η as having the components ηc :“F´1
c,GF c
`
ϵ´1
F c
˘
; since ϵ is a natural
isomorphism, so is ϵ´1, which is thus preserved as such by the inverse action of F in defining η.
This establishes all the data of the equivalence.
(Note that we can actually prove a little more: it is not hard to verify additionally that the two
constructions are inverse, so that equivalences are themselves equivalent to fully faithful essentially
surjective functors.)
Remark 2.2.31. In the above proof, we assumed the axiom of choice, defining Gd as a choice
of object such that F Gd „Ý Ñd. It is possible to avoid making this assumption, by asking for
the surjection on objects F0 : C0 ÑD0 to be ‘split’ in the sense that it comes with a function
s : D0 ÑC0 such that F0pspdqq– d in D for every object d : D; then we just set Gd :“spdq.
40
2.3. Universal constructions
In the preceding sections, we have used diagrams to represent some patterns in a categorical
context, and we have discussed how functors allow us to translate patterns and structures between
contexts; indeed, we used functors to formalize diagrams themselves. But an important facet of
the notion of pattern is replication across contexts, and in many important situations, we will
encounter patterns that apply to all objects in a category. We call such patternsuniversal, and much
of science is a search for such universal patterns: for example, much of physics, and by extension
much of the theory of the Bayesian brain, is a study of the universal principle of stationary action.
In this section, we introduce the formal characterization of universality and exemplify it with some
examples that will be particularly important later on — as well as some examples that we have
encountered already.
2.3.1. The universality of common patterns
We begin with some basic examples of universal patterns.
2.3.1.1. Disjunctions, or coproducts
Our first example of a universal pattern is thecoproduct, which captures the essence of the following
examples — situations like disjunction, where there is an element of choice between alternatives.
Example 2.3.1. Given two propositions, such as P1 :“ “ ´ is flat” and P2 :“ “ ´ is sharp”, we
can form their disjunction P1 _P2 (meaning “ ´ is flat or sharp”). Similarly, given two subsets
P1, P2 ĎX, we can form their join or union, P1 YP2: an element x is an element of P1 YP2 if
(and only if) it is an element of P1 or an element of P2.
Example 2.3.2. Given two numbers, we can form their sum; for instance, 1 `2 “ 3. More
generally, given two sets A and B, we can form their disjoint union , denoted A `B. The elements
of A `B are pairs pi, xqwhere x is an element of A or of B and i indicates which set x is drawn
from (this ensures that if an element x of A is the same as an element of B, it is added twice to the
disjoint union). Therefore, if A has 1 element and B has 2, then A `B has 3 elements.
Remark 2.3.3. The preceding example illustrates how we can think of numbers equivalently as
sets of the indicated cardinality. Many operations on sets are generalizations of familiar operations
on numbers in this way.
41
Example 2.3.4. Given two graphs, G and G1, we can form the sum graph G `G1, whose set of
nodes is G0 `G1
0 and whose set of edges is G1 `G1
1.
Example 2.3.5. Given two vector spaces V and V 1, we can form their direct sum V ‘V 1, whose
vectors are linear combinations of vectors either in V or in V 1.
Each of these is an example of a coproduct, which we now define.
Definition 2.3.6. Given objects A and B in a category C, their coproduct (if it exists) is an object,
canonically denoted A`B, equipped with two morphismsinjA : A ÑA`B and injB : B ÑA`B
such that, for any object Q equipped with morphisms f : A ÑQ and g : B ÑQ, there is a unique
morphism u : A `B ÑQ such that f “u ˝injA and g “u ˝injB. The morphisms injA and injB
are called injections, and the morphism u is called the copairing and often denoted by rf, gs.
Example 2.3.7. Morphisms of subsets are inclusions, so given subsets P1, P2 Ď X, there are
evident inclusions P1 ĎP1 YP2 and P2 ĎP1 YP2. Moreover, given a subset Q such that P1 ĎQ
and P2 ĎQ, it is clearly the case that P1 ĎP1 `P2 ĎQ and P2 ĎP1 `P2 ĎQ.
Similarly, morphisms of propositions are implications, so givenP1 and P2 such that P1 ÑQ and
P2 ÑQ, then it is necessarily the case that P1 ÑP1 _P2 ÑQ and P2 ÑP1 _P2 ÑQ: clearly,
both P1 and P2 imply P1 _P2 by definition, and if both P1 and P2 imply Q, then so does P1 _P2.
Example 2.3.8. Given sets A and B, the injections injA : A ÑA `B and injB : B ÑA `B
are the corresponding inclusions: injA maps a to p1, aqand injB maps b to p2, bq, where 1 tags
an element as coming from A, and 2 tags an element as coming from B. Given f : A ÑQ and
g : B ÑQ the copairing rf, gs: A `B ÑQ is the function that takes an element pi, xqand
returns fpxqif i “1 or gpxqif i “2; it is from this that the ‘choice’ interpretation arises for the
coproduct.
Example 2.3.9. Morphisms of vectors spaces are linear maps, and if the spaces are finite-
dimensional, then we can represent these maps as matrices: if V is n-dimensional and W is
m-dimensional, then a morphism V ÑW is a matrix of shape pm, nq; writing the elements of V
and W as column vectors, such a matrix has m rows and n columns. Moreover, in this case, the
direct sum V ‘W is pn `mq-dimensional.
Therefore suppose that V , V 1and W have respective dimensions n, n1and m, and suppose we
have linear maps f : V ÑW and g : V 1 ÑW. The injection V ÑV ‘V 1 is the block matrixˆ1n
0n1
˙
where 1n is the n-by-n identity matrix and 0n1 is the n1-by-n1 zero matrix; similarly, the
42
injection V 1 ÑV ‘V 1is the block matrix
ˆ0n
1n1
˙
. And the copairing rf, gs: V ‘V 1 ÑW is the
block matrix
`
f g
˘
.
Remark 2.3.10. The coproducts we have considered so far have all been binary, being coproducts
of only two objects. More generally, we can often consider coproducts of more objects, by repeating
the binary coproduct operation; typically, there is an isomorphism pA `Bq` C –A `pB `Cq.
We can extend this further to finite (and, often, infinite) collections of objects. Suppose then that
tAiuis a collection of objects indexed by i : I, where I is a set, and form the iterated coproduct
ř
i:I Ai; we will call this object a dependent sum , because the summands Ai depend on i : I. In the
case where the objects Ai are all sets, the dependent sum ř
i Ai is again a set, whose elements are
pairs pi, xqwhere i is an element of I and x is an element of Ai. In other categories C, typically the
name dependent sum is reserved for the case when all of the objects Ai and the object I are objects
of C. But when I remains a set, we may still be able to form the I-indexed coproduct ř
i Ai in C.
2.3.1.2. Conjunctions, products, and sections
Our next example of a universal pattern is the product, which captures situations like conjunction,
in which things come along in separable pairs of individuals.
Example 2.3.11. Given two propositions, such asP1 :“ “´is small” and P2 :“ “´is connected”,
we can form their conjunction P1 ^P2 (meaning “ ´ is small and connected”). Similarly, given
two subsets P1, P2 Ď X, we can form their meet or intersection, P1 XP2: an element x is an
element of P1 XP2 if (and only if) it is an element of P1 and an element of P2.
Example 2.3.12. Given two numbers, we can form their product; for instance, 2 ˆ3 “6. More
generally, as we saw in Definition 2.2.1, we can form the product of any two sets A and B, denoted
A ˆB. The elements of A ˆB are pairs pa, bqwhere a is an element of A and b is an element of
B. Therefore, if A has 2 elements and B has 3, then A ˆB has 6 elements.
Remark 2.3.13. When all the summands of a dependent sum are the same set or objectA regardless
of their associated index i : I, then the object ř
i:I A is isomorphic to the product I ˆA: this is
simply a categorification of the fact that “multiplication is repeated addition”.
Example 2.3.14. Given vector spaces V and V 1(of respective dimensions n and n1), their product
is again the direct sum V ‘V 1. Since the direct sum of vector spaces is both a product and a
coproduct, it is also said to be a biproduct.
43
Categorically, the product is the dual of the coproduct.
Definition 2.3.15. Given objects A and B in a category C, their product (if it exists) is an object,
canonically denoted A ˆB, equipped with two morphisms projA : A ˆB Ñ A and projB :
A ˆB ÑB such that, for any object Q equipped with morphisms f : Q ÑA and g : Q ÑB,
there is a unique morphism u : Q ÑA ˆB such that f “projA ˝u and g “projB ˝u. The
morphisms projA and projB are called projections, and the morphism u is called the pairing and
often denoted by pf, gq.
Example 2.3.16. Given subjects P1, P2 Ď X, there are evident inclusions P1 XP2 Ď P1 and
P1 XP2 ĎP2. Moreover, given a subset Q such that Q ĎP1 and Q ĎP2, it is clearly then the
case that Q ĎP1 XP2 ĎP1 and Q ĎP1 XP2 ĎP2.
Similarly, given propositions P1 and P2 such that Q ÑP1 and Q ÑP2, it is (by the definition
of “and”) the case that Q ÑP1 ^P2 ÑP1 and Q ÑP1 ^P2 ÑP2.
Example 2.3.17. Given sets A and B, the projections projA : AˆB ÑA and projB : AˆB ÑB
are the functions pa, bqÞÑ a and pa, bqÞÑ b respectively. Given f : Q ÑA and g : Q ÑB, the
pairing pf, gq: Q ÑA ˆB is the function x ÞÑ
`
fpxq, gpxq
˘
; note that this involves ‘copying’ x,
which will be relevant when we come to consider copy-discard categories in §3.1.
Remark 2.3.18. Above, we observed that a coproduct with constant summands A is equivalently
a product I ˆA of the indexing object I with A; we therefore have a projection projI : I ˆA ÑI.
More generally, for any dependent sum ř
i:I Ai, there is a projection ř
i:I Ai ÑI; in the case of
dependent sums in Set, this is unsurprisingly the function pi, xqÞÑ i.
Example 2.3.19. Suppose we have vector spaces V , V 1and W of respective dimensions n, n1and
m. The projection V ‘V 1 ÑV is the block matrix
`
1n 0n1
˘
, and the projection V ‘V 1 ÑV 1
is the block matrix
`
0n 1n1
˘
. Given linear maps f : W Ñ V and g : W Ñ V 1, the pairing
pf, gq: W ÑV ‘V 1is the block matrix
ˆf
g
˙
. Note that, in a sign of the duality between products
and coproducts, the projections and the pairing are respectively the injections and the copairing
transposed.
Remark 2.3.20. Just as in the case of coproducts, we can also consider products of more than two
objects, by repeating the product operation; there is again typically an isomorphismpAˆBqˆC –
A ˆpB ˆCq. If tAiuis a collection of objects indexed by i : I (with I again a set), we can form
44
the dependent product 1 ś
i:I Ai. In the case where I is finite and the objects Ai are all sets, the
dependent product ś
i:I Ai is again a set, whose elements can equivalently be seen as listspai, . . .q
indexed by i with each element ai drawn from the corresponding set Ai or as functions s with
domain I and codomain the dependent sumř
i:I Ai such that eachspiqis tagged byi. This situation
is summarized by the commutativity of the diagram
I ř
i:I Ai
I
s
π
where π is the projection and which therefore requires thatπ ˝s “idI. Such a function s is known
as a section of p, and we can think of sections therefore as dependent functions , since the types of
their output values (i.e., Ai) may depend on the input values i.
The notion of section is important enough to warrant a general definition.
Definition 2.3.21. Suppose p : E ÑB is a morphism. A section of p is a morphism s : B ÑE
such that p ˝s “idB.
2.3.1.3. Subobjects and equalizers
Our next examples of universal patterns do not involve pairing or grouping objects together to make
new ones. For instance, in many situations, it is of interest to restrict our attention to ‘subobjects’
(of a single given object) that satisfy a certain property, which may not extend to the whole object
at hand.
Example 2.3.22. In examples above, we saw that subsets and propositions behave similarly with
respect to disjunctions and conjunctions. More broadly, there is a correspondence between subsets
and propositions, if we think of propositions on a set X as functions X Ñ 2, where 2 is the
2-element set tK, Juof truth values (where we interpret Kas ‘false’ and Jas ‘true’). Every subset
A ĎX has an associated injection, A ãÑX, and there is a correspondence between such injections
and propositions PA : X Ñ2, where PApxqis true whenever x is an element of A. This situation
1This set-indexed product is also known as an indexed product , to emphasize that the factors are indexed by the set I;
since I has elements, we can properly think of these as indices, which may not be true for other kinds of object. We
will see in Definition 2.3.63 how to use categorical structure to abstract away the requirement that I be a set.
45
can be summarized by the commutativity of the diagram
A 1
X 2
!
J
PA
where 1 is the singleton set t˚u, ! is the unique function sending every element of A to ˚, and Jis
the function ˚ÞÑJ picking the truth value J. If, in a category C, there is an object such that, for
any subobject A ãÑX, there is a unique morphism X Ñ2 such that the above diagram commutes
(and moreover defines a pullback square in the sense of Example 2.3.43 below), then we say that
the object 2 is a subobject classifier in C; in this case, we interpret 1 as the ‘terminal’ object in C
(introduced below, in Example 2.3.41).
A pattern that will be particularly common is that in which we care about a subset of elements
of a set that make two functions equal. This can be generalized to arbitrary categories using the
following notion.
Definition 2.3.23. Suppose f and g are both morphisms X ÑY . Their equalizer is an object E
equipped with a function e : E ÑX such that f ˝e “g ˝e (so that e is said to equalize f and g)
as in the commuting diagram
E X Y
f
g
e
and such that, for any d : D ÑX equalizing f and g, there is a unique morphism u : D ÑE such
that d “e ˝u, as in the diagram
D E X Y
f
g
eu
d
.
Example 2.3.24. Via the correspondence between subsets and propositions, we can express the
conjunction of propositions as an equalizer. Suppose have have two propositions PA : X Ñ2
and PB : X Ñ2, corresponding to subsets A ãÑX and B ãÑX, whose inclusions we denote by
ιA and ιB respectively. The equalizer of A ˆB X
ιA˝projA
ιB˝projB
is the subset of A ˆB whose
elements are pairs pa, bqin which a “b in X. This subset is isomorphic to the meet A XB, which
corresponds as a proposition to the conjunction PA ^PB.
46
2.3.1.4. Coequalizers and quotients
We can also make objects ‘smaller’ by dividing them into equivalence classes, as we did when
quotienting free categories by given relations (cf. Proposition 2.1.13). In general, this pattern is
captured by the notion of coequalizer, which is dual to the notion of equalizer in the same way that
coproducts are dual to products.
Definition 2.3.25. Suppose f and g are both morphisms X ÑY . Their coequalizer is an object P
equipped with a function p : Y ÑP such that p ˝f “p ˝g (with p said to coequalize f and g) as
in the commuting diagram
X Y P
f
g
p
and such that, for any q : Y ÑQ coequalizing f and g, there is a unique morphism u : P ÑQ
such that q “u ˝p, as in the diagram
X Y P Q
f
g
p u
q
.
Example 2.3.26. A relation „on a set X is a proposition on X ˆX, and thus equivalently a
subset R ãÑX ˆX; let ι denote the inclusion. The coequalizer of R X
proj1˝ι
proj2˝ι
is the set
of equivalence classes of X according to „, which is precisely the quotient X{„.
2.3.2. The pattern of universality
There is a common pattern to the common patterns above: in each case, we described an object
U equipped with some morphisms, such that, given any object X with morphisms of a similar
shape, there was a unique morphismu relating X and U. The existence of such a unique morphism
for any comparable X makes the object U a universal representative of the situation at hand and
has a number of powerful consequences: in particular, it entirely characterizes the object U up to
isomorphism. Much of the power of category theory comes from the use of universal properties to
classify, compare, and reason about situations of interest — for the general notion of universality
itself can be characterized categorically.
Definition 2.3.27. Suppose F : C ÑD is a functor and X : D an object. We define two dual
universal constructions. A universal morphism from X to F is a morphism u : X ÑF Ufor a
corresponding universal object U : C such that for any f : X ÑF Vin D there exists a unique
e : U ÑV such that f “X uÝ ÑF UF eÝ Ý ÑF V.
47
Dually, a universal morphism from F to X is a morphism u : F UÑX for a given U : C such
that for any f : F VÑX in D there exists a unique e : V ÑU such that f “F V F eÝ Ý ÑF UuÝ ÑX.
We can now formalize the universal properties of the examples we met above, beginning with
the coproduct.
Example 2.3.28. Let ∆ : C ÑC ˆC denote the functor X ÞÑpX, Xq. A coproduct of X and
Y in C is a universal morphism from the object pX, Yqin C ˆC to ∆: that is, an object X `Y
in C and a morphism pinjX, injY q : pX, Yq Ñ pX `Y, X`Y qin C ˆC such that, for any
pf, gq : pX, Yq Ñ pQ, Qqin C ˆC, the copairing rf, gs : X `Y Ñ Q uniquely satisfies the
equation pf, gq“p X, Yq
pinjX,injY q
ÝÝÝÝÝÝÑpX `Y, X`Y q
prf,gs,rf,gsq
Ý ÝÝÝÝÝÝ ÑpQ, Qq.
Example 2.3.29. Again let ∆ : C ÑC ˆC denote the functor X ÞÑpX, Xq. A product of X and
Y in C is a universal morphism from the object pX, Yq: C ˆC to ∆: that is, an object X ˆY
in C and a morphism pprojX, projY q : pX ˆY, XˆY q Ñ pX, Yqin C ˆC such that, for any
pf, gq: pQ, QqÑp X, Yqin C ˆC, the pairing pf, gq: Q ÑX ˆY uniquely satisfies the equation
pf, gq“p Q, Qq
ppf,gq,pf,gqq
ÝÝÝÝÝÝÝÑpX ˆY, XˆY q
pprojX,projY q
Ý ÝÝÝÝÝÝÝ ÑpX, Yq.
Remark 2.3.30. If we let 2 denote the two-object discrete category t‚ ‚u, then there is an
equivalence C ˆC –C2 and so a pair of morphisms in C is equivalently a natural transformation
in C2. (This is a categorification of the familiar fact that “exponentiation is repeated multiplication”,
which we will explore in §2.3.4.)
Consequently, the functor ∆ from the preceding examples can equivalently be defined as a
functor C ÑC2. Letting the exponent take a more general shape, we obtain a family of constant
functors.
Proposition 2.3.31. Suppose C and D are categories, and d : D is an object. Then there is a
constant functor on d, denoted ∆d : C Ñ D, which takes each object c : C to d : D and each
morphism f : c Ñc1 to idd; note that F c“d “F c1. The assignment d ÞÑ∆d is itself trivially
functorial, giving a functor ∆ : D ÑDC which we call the constant functor functor .
Example 2.3.32. Let J be the category with two objects,1 and 2, and two non-identity morphisms
α, β: 1 Ñ2, as in the diagram 1 2
α
β
, and let ∆ be the constant functor functor C ÑCJ .
Now suppose f and g are two morphisms X ÑY in C. To construct their equalizer as a universal
morphism, let D be the diagram J Ñ C mapping α ÞÑ f and β ÞÑ g. Then an equalizer of
f and g is a universal morphism from ∆ to D (with D being an object of the functor category
48
CJ ): that is, an object E : C equipped with a natural transformation ϵ : ∆E ñD satisfying the
universal property that, for any φ : ∆F ñD there exists a unique morphism u : F ÑE such
that φ “ϵ ˝∆u.
Unraveling this definition, we find that such a natural transformation ϵ consists of a pair of
morphisms ϵ1 : E ÑX and ϵ2 : E ÑY making the following naturality squares commute:
E X
E Y
ϵ1
ϵ2
f
E X
E Y
ϵ1
ϵ2
g
We can therefore set ϵ1 “e, where e is the equalizing morphism E ÑX. The commutativity of
the naturality squares enforces that f ˝e “ϵ2 “g ˝e and hence that f ˝e “g ˝e, which is the
first condition defining the equalizer. Unwinding the universal property as expressed here shows
that the morphisms φ1 and u correspond exactly to the morphisms d and u of Definition 2.3.23.
Example 2.3.33. The case of a coequalizer is precisely dual to that of an equalizer. Therefore, let
J , ∆, and D be defined as above. A coequalizer of f, g: X ÑY is then a universal morphism
from D to ∆.
Example 2.3.34. In Proposition 2.1.13, we constructed a category generated with relations FG{„
as a quotient of a free category on a graph FG. Since this category FG{„is a quotient and
quotients are coequalizers (by Example 2.3.26), the projection functor FG Ñ FG{„(Example
2.1.26) constitutes the associated universal morphism, in the sense dual to the morphism ϵ1 of
Example 2.3.32.
Example 2.3.35. The free category construction itself (Proposition 2.1.9) exhibits a universal
property, as a consequence of the free-forgetful adjunction (Example 2.2.25): given a graph G and a
category C, any functor FG ÑC is uniquely determined by a graph homomorphismG ÑUC from
G to the underlying graph ofC. More precisely, there is a universal morphism from the singleton set
1 to the functor CatpF´, Cqfor every category C. This means that, for any graph G, every functor
f : FG ÑC factors as FG F hÝ Ý ÑF UC uÝ ÑC where u is the universal morphism and h is the unique
graph homomorphism. This universal property follows abstractly from facts that we will soon
encounter: that adjoint functors are ‘representable’ (Proposition 2.4.16); and that representable
functors are universal (Proposition 2.4.23). We hinted at this property in Example 2.2.25, where we
observed that functors between free categories ‘are’ graph homomorphisms: the homomorphism h
here is the graph homomorphism corresponding to the functor f, and u renders it as a functor into
C.
49
When an object satisfies a universal property, then this property characterizes the object uniquely:
as a result, universal properties are powerful constructions, telling us that for certain questions,
there can be only one possible answer. Characterizing an object by a universal property abstracts
away from contextually irrelevant details (for example, the particular elements making up a set),
and crystallizes its essence.
The uniqueness of universal morphisms is formalized by the following proposition.
Proposition 2.3.36 (Universal constructions are unique up to unique isomorphism) . Suppose
u : X ÑF Uand u1 : X ÑF U1are both universal morphisms from X : C to F : C ÑD. Then
there is a unique isomorphism i : U ÑU1.
To prove this, we need to know that functors preserve isomorphisms.
Proposition 2.3.37. If F : C Ñ D is a functor and f : x Ñ y is an isomorphism in C, then
F f: F xÑF yis an isomorphism in D.
Proof. For f to be an isomorphism, there must be a morphismf´1 : y Ñx such that f´1 ˝f “idx
and f ˝f´1 “idy. We have idF x“Fpidxq“ Fpf´1 ˝fq“ F f´1 ˝F f, where the first and third
equations hold by the functoriality of F and the second equation holds ex hypothesi . Similarly,
idF y“Fpidyq“ Fpf ˝f´1q“ F f˝F f´1. Therefore F f´1 is both a right and left inverse for
F f, and so F fis an isomorphism.
Proof of Proposition 2.3.36. Since u1is a morphism from X to F, the universal property of u says
that there exists a unique morphism i : U ÑU1 such that u1 “F i˝u. Similarly, the universal
property of u1stipulates that there exists a unique morphism i1 : U1 ÑU such that u “F i1˝u1.
We can substitute the latter into the former and the former into the latter:
u1 “X u1
Ý ÑF U1 i1
Ý ÑF UF iÝÑF U1
“X u1
Ý ÑF U1 Fpi˝i1q
ÝÝÝÝÑF U1
“X u1
Ý ÑF U1 F idU1
Ý ÝÝÝ ÑF U1
u “X uÝ ÑF UF iÝÑF U1 F i1
Ý Ý ÑF U
“X uÝ ÑF U
Fpi1˝iq
ÝÝÝÝÑF U
“X uÝ ÑF UF idU
ÝÝÝÑF U
and since functors preserve isomorphism, we have i ˝i1 “idU1 and i1˝i “idU . Therefore, i is an
isomorphism which is unique by definition.
50
2.3.3. Limits and colimits: mapping in to and out of diagrams
Many of the universal constructions above2 fall into their own general pattern, in which a diagram
of objects and morphisms is specified, and a universal morphism is produced which encodes the
data of mapping into or out of that diagram, in a sufficiently parsimonious way that any other
way of mapping into or out of the diagram factors through it. In the case of the (co)product, the
diagram is simple: simply a pair of objects, with no morphisms between them. In the case of the
(co)equalizer, the diagram is a little more complex, being a ‘fork’ of the form 1 2
α
β
. We
can generalize these examples further, to consider the most parsimonious ways of mapping into
or out of diagrams of arbitrary shape: these universal constructions are called colimits and limits
respectively, and to formalize them, we need to define what it means to map into or out of a diagram.
For this purpose, we use the following notion of cone over a diagram.
Definition 2.3.38. A cone over the J-shaped diagram D in C is a natural transformation ∆c ñD
for a given object c : C which we call its apex. Dually, a cocone under D with apex c is a natural
transformation D ñ∆c. We say that J is the shape of the cone.
With this definition to hand, the notions of limit and colimit are easy to define.
Definition 2.3.39. A limit is a universal cone, and a colimit is a universal cocone. More explicitly,
if D is a J-shaped diagram in C, then the limit of D is a universal morphism from the constant
diagram functor functor ∆ : C ÑCJ to D (considered as an object of the functor category), and
the colimit of D is a universal morphism from D to ∆; alternatively, a colimit in C is a limit in C op.
In both cases, the apex of the cone is the universal object of the construction, which in the case of
the limit of D we denote by lim D, and in the case of the colimit, colim D.
Note that we will often say ‘(co)limit’ to refer to the apex of the universal (co)cone, even though
the (co)limit is properly the whole universal construction. We are entitled to say “ the (co)limit”
thanks to the uniqueness of universal constructions.
We will often denote a universal cone by proj and call its component morphisms projections;
dually, we will often denote a universal cocone by inj and call its morphisms injections.
Example 2.3.40. We can now exemplify the pattern of the limiting examples above. We will draw
diagrams to depict the shape categories, with each symbol ‚indicating a distinct object and each
arrow Ñindicating a distinct non-identity morphism.
2Products, coproducts, equalizers, and coequalizers.
51
1. A coproduct is a colimit of shape
␣
‚ ‚
(
;
2. a product is a limit of shape
␣
‚ ‚
(
;
3. an equalizer is a limit of shape
␣
‚ ‚
(
; and
4. a coequalizer is a colimit of shape
␣
‚ ‚
(
.
Of course, these are not the only possible shapes of limits and colimits. Some others will be
particularly important, too.
Example 2.3.41. Let 0 denote the category with no objects or morphisms. A limit of shape 0 is
known as a terminal object . This is an object 1 such that, for every object X, there is a unique
morphism ! : X Ñ1. The terminal object in Set is a singleton set t˚u.
Dually, a colimit of shape 0 is known as an initial object : an object 0 such that, for every object
X, there is a unique morphism !: 0 ÑX. The initial object in Set is the empty set.
Remark 2.3.42. In Remark 2.2.8, we noted that morphisms 1 ÑA in Set correspond to elements
of A. In general categories C with a terminal object, one sometimes calls morphisms out of the
terminal object global elements . The word ‘global’ emphasizes the special position of the terminal
object in a category, which has a unique view of every object.
Example 2.3.43. A pullback is a limit of shape
␣
‚ ‚ ‚
(
. That is, given morphisms
f : A ÑX and g : B ÑX, their pullback is an object P and morphisms projA : P ÑA and
projB : P ÑB making the following diagram commute
P B
A X
g
f
projB
projA
{
in the universal sense that, for any object Q and morphisms πA : Q ÑA and πB : Q ÑB such
that f ˝πA “g ˝πB, then there is a unique morphism u : Q ÑP such that πA “projA ˝u and
πB “projB ˝u. We indicate a pullback square using the symbol { as in the diagram above, and will
variously denote the limiting object P by A ˆX B, f˚B, or g˚A, depending on the context.
The interpretation of the pullback is something like a generalized equation: in the category Set,
the pullback A ˆX B is the subset of the product A ˆB consisting of elements pa, bqfor which
fpaq“ gpbq. Alternatively, it can be understood as a kind of generalized intersection: given two
objects A and B and “ways of assigning them properties in X” f and g, the pullback A ˆX B is
52
the generalized intersection of A and B according to these X-properties. In fact, we already saw
this latter interpretation in Example 2.3.24, where we exhibited an intersection as an equalizer;
now we can see that that equalizer was ‘secretly’ a pullback.
Remark 2.3.44. Dually, a colimit of shape
␣
‚ ‚ ‚
(
is known as apushout. Whereas
a pullback has an interpretation as a subobject of a product, a pushout has an interpration as a
quotient of a coproduct. In this work, we will make far more use of pullbacks than pushouts.
The observation that pullbacks can be interpreted as subobjects of products (and dually that
pushouts can be interpreted as quotients of coproducts) is a consequence of the more general result
that all limits can be expressed using products and equalizers (and hence dually that colimits can
be expressed using coproducts and coequalizers).
Proposition 2.3.45. Let D : J ÑC be a diagram in C, and suppose the products ś
j:J0 Dpjqand
ś
f:J1 Dpcod fqexist. Then, if it exists, the equalizer of
ś
j:J0 Dpjq ś
f:J1 Dpcod fq
ś
f:J1pDf ˝projdom f q
ś
f:J1 projcod f
is the limit of D.
Proof sketch. Observe that the equalizer of the diagram above is an object L such that, for every
morphism f : j Ñj1in J, the diagram
L
Dj Dj1
projj projj1
Df
commutes, and such that any cone overD factors through it. This is precisely the universal property
of the limit of D, and so by Proposition 2.3.36, pL, projqis the limit.
Remark 2.3.46. As we indicated above, a dual result holds expressing colimits using coequalizers
and coproducts. Because results obtained for limits in C will hold for colimits in C op, we will
henceforth not always give explicit dualizations.
53
2.3.3.1. Functoriality of taking limits
In the statement of Proposition 2.3.45, we used the fact that taking products extends to morphisms,
too: a fact that was exemplified concretely in Example 2.2.5, and which follows from the fact that a
pair of morphisms in C is equivalently a morphism in C ˆC. We then saw in Remark 2.3.30 that
C ˆC –C2. By letting the exponent again vary, the functoriality of taking products generalizes to
the functoriality of taking limits, as long as C has all limits of the relevant shape.
Proposition 2.3.47 (Taking limits is functorial). Suppose C has all limits of shape J (i.e., for any
diagram D : J ÑC, the limit lim D exists in C). Then lim defines a functor CatpJ, CqÑ C.
Proof. We only need to check the assignment is well-defined on morphisms and functorial. Suppose
D and D1 are two diagrams J Ñ C with corresponding limiting cones u : ∆ lim D ñ D and
u1 : ∆ lim D1 ñ D1, and suppose δ : D ñ D1 is a natural transformation. Observe that the
composite natural transformation ∆lim D
u
ù ñD
δ
ù ñD1is a cone on D1, and that cones on D1are in
bijection with morphisms in C into the apex object lim D1. Therefore, by the universal property of
the limit, there is a unique morphism d : lim D Ñlim D1such that δ ˝u “u1˝∆d. This situation
is summarized by the commutativity of the following diagram, where the dashed arrow indicates
the uniqueness of ∆d:
∆lim D ∆lim D1
D D1
∆d
δ
u u1
We define the action of the functor lim : CatpJ, CqÑ C on the natural transformation δ by this
unique morphism d, setting lim δ :“d : lim D Ñlim D1.
It therefore only remains to check that this assignment is functorial (i.e., that it preserves identities
and composites). To see that lim preserves identities, just take δ “idD in the situation above;
clearly, by the uniqueness of d, we must have lim idD “idlim D. Now suppose δ1 : D1 ÑD2 is
another natural transformation. To see that limpδ1˝δq“ lim δ1˝lim δ, consider the pasting of the
associated diagrams:
∆lim D ∆lim D1 ∆lim D2
D D1 D2
∆d
δ
u u1
∆d1
u2
δ1
∆d1d
54
We have limpδ1˝δq“ d1d, which is unique by definition. Therefore we must have d1d “d1˝d “
limpδ1q˝ limpδq, and hence limpδ1˝δq“ limpδ1q˝ limpδqas required.
2.3.3.2. (Co)limits as adjoints
Since taking limits is functorial, it makes sense to ask if the functor lim has an adjoint, and indeed
it does, in a familiar form.
Proposition 2.3.48. The functor lim : CJ ÑC is right adjoint to the constant diagram functor
functor ∆ : C ÑCJ , i.e. ∆ %lim.
Proof. We need to show that CJ p∆c, Dq –Cpc, lim Dqnaturally in c : C and D : J ÑC. It is
sufficient to demonstrate naturality in each argument separately, by the universal property of
the product in Cat. We have already established naturality in c : C in Lemma 2.3.51 and shown
that taking limits is functorial (Proposition 2.3.47). So it only remains to show that this extends
to naturality in D : J ÑC, which requires the commutativity of the following diagram for any
δ : D ÑD1, where we write αD for the isomorphism Cpc, lim Dq „Ý ÑCJ p∆c, Dq:
Cpc, lim Dq CJ p∆c, Dq
Cpc, lim D1q CJ p∆c, D1q
αD
αD1
CJp∆c,δqCpc,lim δq
Chasing a morphism β : c Ñlim D around this diagram, we find that its commutativity amounts
to the commutativity of the following diagram of cones for all φ : i Ñj in J, where by definition
αDpβqi “πi ˝β and αD1plim δ ˝βqi “π1
i ˝lim δ ˝β:
Di D1i
c lim D lim D1
Dj D1j
Dφ
δi
D1φ
δj
πi
πj
π1
i
π1
j
lim δβ
πi˝β
πj˝β
This diagram commutes by definition, so the isomorphism is natural in D, which therefore
establishes the desired adjunction.
Remark 2.3.49. Dually, if all colimits of shape J exist in C, then colim is left adjoint to ∆.
55
Later, we will see that every adjoint functor exhibits a universal property (Propositions 2.4.16
and 2.4.23, results that we’ve already seen exemplified in Example 2.3.35), and this therefore gives
us another perspective on the universality of limits.
2.3.3.3. Hom preserves limits
We end this section with a useful result on the interaction between the covariant hom functors
Cpc, ´q: C ÑSet and taking limits.
Proposition 2.3.50 (Hom functor preserves limits) . Suppose D : J Ñ C is a diagram in the
category C. There is an isomorphism Cpc, lim Dq– lim Cpc, Dp´qqwhich is natural in c : C.
To prove this proposition, it helps to have the following lemma, which establishes a natural
isomorphism between the set of morphisms into a limit and the set of cones on the corresponding
diagram.
Lemma 2.3.51. Cpc, lim Dq– CJ p∆c, Dq, naturally in c : C.
Proof. For a given c : C, the isomorphism Cpc, lim Dq –CJ p∆c, Dqfollows directly from the
universal property of the limit: morphisms from c into the limiting object lim D are in bijection
with cones ∆c ñD. So it only remains to show that this isomorphism is natural in c : C. Writing
α : Cp´, lim Dqñ CJ p∆p´q, Dqfor the natural transformation that takes each morphism into the
limit to the corresponding cone on D, naturality amounts to the commutativity of the following
square for each f : c1 Ñc in C:
Cpc, lim Dq CJ p∆c, Dq
Cpc1, lim Dq CJ p∆c1, Dq
Cpf,lim Dq CJp∆f ,Dq
αc
αc1
Commutativity of this naturality square witnesses the fact that, given a morphism g : c Ñlim D,
you can either take the corresponding cone αcpgqand pull it back along ∆f (at its apex) to obtain
the cone αcpgq˝ ∆f , or you can form the composite morphism g ˝f and take its cone αc1pg ˝fq,
and you’ll have the same cone: αcpgq˝ ∆f “αc1pg ˝fq. This is illustrated by the commutativity
of the following diagram, which shows fragments of the limiting cone denoted π, the cone αcpgq,
56
and the cone αc1pg ˝fq, for a morphism φ : i Ñj in J:
Di
c1 c lim D
Dj
Dφ
πi
πj
gf
αcpgqi
αc1pg˝fqi
αcpgqj
αc1pg˝fqj
By the universal property of the limit, we must haveαc1pg ˝fqi “αcpgqi ˝f “πi ˝g ˝f naturally
in i, and hence αc1pg ˝fq“ αcpgq˝ ∆f .
Proof of Proposition 2.3.50. By Lemma 2.3.51, we have a natural isomorphism CJ p∆c, Dq –
lim Cpc, Dp´qq, so it suffices to establish a natural isomorphism CJ p∆c, Dq –lim Cpc, Dp´qq.
This says that cones on D with apex c are isomorphic to the limit of Cpc, Dp´qq : J Ñ Set,
naturally in c. First, note that this limiting cone in Set is constituted by a family of functions
tpi : lim Cpc, Dp´qqÑ Cpc, Diqui:J , as in the following commuting diagram:
Cpc, Diq
lim Cpc, Dp´qq
Cpc, Djq
Cpc,Dφq
pi
pj
Next, note there is a bijection between cones ∆c ñD on D in C with apex c, as in the commuting
diagram below-left, and cones ∆1 ñCpc, Dp´qqin Set, as in the commuting diagram below-right.
Di
c
Dj
βi
Dφ
βj
Cpc, Diq
1
Cpc, Djq
Cpc,Dφq
βi
βj
By the univeral property of the limit, any cone tβiuas on the right factors uniquely through
lim Cpc, Dp´qq, as in the following commuting diagram. Similarly, any elementβ of lim Cpc, Dp´qq
induces a corresponding cone tpipβqu, by composition with the limiting cone p. To see that this
correspondence is an isomorphism, observe that the element of the set lim Cpc, Dp´qqassigned to
the cone tpipβqumust be exactly β, since the universal property of lim Cpc, Dp´qqensures that β
57
is uniquely determined.
Cpc, Diq
˚ lim Cpc, Dp´qq
Cpc, Djq
Cpc,Dφq
pi
pj
β
βi
βj
It only remains to check that this correspondence is natural in c, so suppose f is any morphism
c1 Ñc in C. If we write p´ : lim Cpc, Dp´qqÑ CJ p∆c, Dqto denote the function β ÞÑtpipβqu,
and p1
´ to denote the corresponding function for c1, naturality requires the following square to
commute:
lim Cpc, Dp´qq CJ p∆c, Dq
lim Cpc1, Dp´qq CJ p∆c1, Dq
p´
p1
´
CJp∆f ,Dqlim Cpf,Dp´qq
The commutativity of this square in turn corresponds to the commutativity of the following diagram
in Set, for any cone β:
Cpc, Diq Cpc1, Diq
1 lim Cpc, Dp´qq lim Cpc1, Dp´qq
Cpc, Djq Cpc1, Djq
Cpf,Diq
Cpf,Djq
Cpc1,Dφq
p1
i
p1
j
Cpc,Dφq
lim Cpf,Dp´qq
pi
pj
β
By the correspondence between cones ∆c ñD in C and cones ∆1 ñCpc, Dp´qqin Set, this
diagram commutes if and only if the following diagram commutes:
Di
c1 c
Dj
βi
Dφ
βj
f
βi˝f
βj˝f
This diagram commutes by the definition of β and of the composites tβi ˝fu, thereby establishing
the naturality of the isomorphism lim Cpc, Dp´qq –CJ p∆c, Dq. Since we also have a natural
isomorphism CJ p∆c, Dq– Cpc, lim Dq, we have established the result.
58
The preceding proof established more than just the hom functor’s preservation of limits: it gave
us another useful natural isomorphism, this time betwen the set of cones ∆c ñD in C and the set
of cones ∆1 ñCpc, Dqon the diagram Cpc, Dq: J ÑSet with apex the terminal set 1.
Corollary 2.3.52. There is an isomorphism CJ p∆c, Dq– SetJ p∆1, Cpc, Dqq, natural in c : C.
Remark 2.3.53. Since limits in C op are colimits in C, Proposition 2.3.50 implies that the
contravariant hom functors Cp´, cqturn limits into colimits; i.e. Cplim D, cq– colim CpDp´q, cq.
2.3.4. Closed categories and exponential objects
A distinguishing feature of adaptive systems such as the brain is that they contain processes
which themselves control other processes, and so it is useful to be able to formalize this situation
compositionally. When a category contains objects which themselves represent the morphisms of
the category, we say that the category is closed: in such categories, we may have processes whose
outputs are again processes, and we may think of the latter as controlled by the former.
A basic instance of this mathematical situation is found amidst the natural numbers, where
repeated multiplication coincides with exponentiation, as in2 ˆ2 ˆ2 “23. If we think of numbers
as sets of the corresponding size, and let 23 denote the set of functions 3 Ñ2, then it is not hard
to see that there are 8 such distinct functions. If we generalize this situation from numbers to
arbitrary objects, and from functions to morphisms, we obtain the following general definition of
exponentiation.
Definition 2.3.54. Let ˆdenote the product in a category C. When there is an object e : C
such that Cpx, eq– Cpx ˆy, zqnaturally in x, we say that e is an exponential object and denote
it by zy. The image of idzy under the isomorphism is called the evaluation map and is written
evy,z : zy ˆy Ñz.
Example 2.3.55. In Set, given sets A and B, the exponential object BA is the set of functions
A ÑB. Given a function f : A ÑB, the evaluation map evB,A acts by applying f to elements of
A: i.e., evB,Apf, aq“ fpaq.
Typically, we are most interested in situations where every pair of objects is naturally
exponentiable, which induces the following adjunction, formalizing the idea that exponentiation is
repeated multiplication.
59
Proposition 2.3.56. When the isomorphism Cpxˆy, zq– Cpx, zyqis additionally natural in z, we
obtain an adjunction p´qˆ y %p´qy called the product-exponential adjunction , and this uniquely
determines a functor C op ˆC ÑC : py, zqÞÑ zy that we call the internal hom for C.
Proof. That the natural isomorphism induces an adjunction is immediate from Proposition 2.2.26;
the counit of this adjunction is the family of evaluation maps ev : p´qy ˆy ñidC. The uniqueness
of the internal hom follows from the uniqueness of adjoint functors (which we will establish in
Corollary 2.4.18).
Definition 2.3.57. A category C in which every pair of objects has a product is called Cartesian. A
Cartesian category C with a corresponding internal hom is called Cartesian closed .
Example 2.3.58. We’ve already seen that Set is Cartesian closed. So is Cat: the internal hom CB
is the category of functors B ÑC.
Example 2.3.59 (A non-example). The category Meas of measurable spaces with measurable
functions between them is Cartesian but not Cartesian closed: the evaluation function is not always
measurable [15]. In this context, we will introduce quasi-Borel spaces (originally due to Heunen
et al. [131]) in §4.1.5, in order to construct stochastic processes which emit functions.
It is not hard to prove the following result, which says that Cartesian closed categories can
“reason about themselves”.
Proposition 2.3.60. A Cartesian closed category is enriched in itself.
This ‘internalization’ is witnessed by the hom functors, which in the case of a Cartesian closed
enriching category E become E-functors.
Proposition 2.3.61. Suppose C is an E-category where E is Cartesian closed. Then the hom
functor Cp´, “qis an E-functor C op ˆC ÑE. On objects pc, dq, the hom functor returns the object
Cpc, dqin E of morphisms c Ñd. Then, for each quadruple pb, c, a, dqof objects in C, we define an
E-morphism C oppb, aqˆ Cpc, dqÑ E
`
Cpb, cq, Cpa, dq
˘
as the image of the composite
`
Cpa, bqˆ Cpc, dq
˘
ˆCpb, cq αÝ ÑCpa, bqˆ
`
Cpc, dqˆ Cpb, cq
˘
¨¨¨
¨¨¨
Cpa,bqˆ˝b,c,d
Ý ÝÝÝÝÝÝÝ ÑCpa, bqˆ Cpb, dq σÝ ÑCpb, dqˆ Cpa, bq
˝a,b,d
ÝÝÝÑCpa, dq
under the product-exponential isomorphism
E
`
Cpa, bqˆ Cpc, dq, Cpa, dqCpb,cq˘
–E
´`
Cpa, bqˆ Cpc, dq
˘
ˆCpb, cq, Cpa, dq
¯
60
where α is the associativty of the product and σ is its symmetry X ˆY –Y ˆX, and where we
have used that C oppb, aq“ Cpa, bq.
Remark 2.3.62. The rôle of the symmetry here is testament to the fact that we can read a composite
morphism g ˝f as either “g after f” or “f before g”.
Proof sketch. To give an E-functor (Definition 2.2.10) is to give a function on objects and a family
of E-morphisms (corresponding to the hom objects of C) such that identities and composites are
preserved. We have given such a function and such a family in the statement of the proposition,
and so it remains to check the axioms: these follow by the unitality and associativity of composition
in an E-category (Definition 2.2.6).
When E is Cartesian closed, then as a corollary its hom functor Ep´, “qis an E-functor.
When a diagram commutes, every parallel path is equal when interpreted as a morphism. If a
diagram commutes up to some 2-cell or 2-cells, then parallel paths can be transformed into each
other using the 2-cell(s). Much categorical reasoning therefore consists in using morphisms in the
base of enrichment to translate between different hom objects; the simplest such of course being
pre- and post-composition. In the next section, we will see many explicit examples of this kind of
reasoning when we prove the Yoneda Lemma—which says that the hom objects contain all the data
of the category—but we have already seen examples of it above, when we considered adjunctions:
after all, adjunctions are families of isomorphisms between hom objects.
When a category is Cartesian closed, it is its own base of enrichment, and so one does not have
to move to an external perspective to reason categorically about it: one can do so using its ‘internal
language’. We have already seen a correspondence between the language of logic and that of sets,
in which we can think of elements of sets as witnesses to the proof of propositions represented
by those sets, and where logical operations such as conjunction and disjunction correspond to
operations on sets. This correspondence extends to Cartesian closed categories generally: universal
constructions such as those we have introduced above can be interpreted as encoding the logic of
the internal language.
More precisely, Cartesian closed categories are said to provide the semantics for dependent type
theory: a higher-order logic in which propositions are generalized by ‘types’3. One can construct
a ‘syntactic’ category representing the logic of the type theory, and then interpret it functorially
3A type is something like a proposition in which we’re ‘allowed’ to distinguish between its witnesses, which we call
terms of the given type.
61
in a Cartesian closed category. This correspondence is known as the Curry-Howard-Lambek
correspondence, which says that logical proofs correspond to morphisms in a Cartesian closed
category, and that such morphisms can equally be seen as representing the functions computed
by deterministic computer programs. (In general, the correspondence is an adjoint one: dually,
one can construct from a given category a ‘syntactic’ category encoding the logic of its internal
language.)
When a category moreover has (internal) dependent sums and products, then it can be interpreted
as a model of dependent type theory , in which types themselves may depend on values; for instance,
one might expect that the type of a weather forecast should depend on whether one is on land or at
sea. We will not say much more about dependent type theory, although we will make implicit use
of some of its ideas later in the thesis. Therefore, before moving on to the Yoneda Lemma, we will
say just enough to define the notion of dependent product ‘universally’, without reference to sets.
2.3.4.1. Dependent products
In Remark 2.3.20, we discussed products where the factors were indexed by an arbitrary set and
explained how they correspond to sets of generalized ‘dependent’ functions, where the codomain
type may vary with the input. In that case, we were restricted to considering products indexed by
sets, but with the machinery of limits at hand, we can ‘internalize’ the definition to other Cartesian
closed categories.
Definition 2.3.63. Suppose C is Cartesian closed and has all limits, and suppose p : E ÑB is a
morphism in C. The dependent product of p along B is the pullback object ś
B p as in the diagram
ś
B p EB
1 BB
idB
pB{
where 1 is the terminal object, idB is the element picking the identity morphism B ÑB, and pB is
the postcomposition morphism induced by the functoriality of exponentiation.
Remark 2.3.64. When p is the projection ř
b:B Pb ÑB out of a dependent sum, we will write its
dependent product as ś
b:B Pb. Since a product B ˆC is isomorphic to the dependent sum ř
b:B C,
note that this means we can alternatively write the exponential object CB as ś
b:B C.
To understand how Definition 2.3.63 generalizes Remark 2.3.20, we can interpret the former in
Set and see that the two constructions coincide. The set EB is the set of functions s : B ÑE,
62
and pB acts by s ÞÑp ˝s. The indicated pullback therefore selects the subset of EB such that
p ˝s “idB. This is precisely the set of sections of p, which is in turn the dependent product of p in
Set.
Remark 2.3.65. Definition 2.3.63 is entirely internal to C: it depends only on structure that is
available within C itself, and not on ‘external’ structures (such as indexing sets) or knowledge
(such as knowledge of the make-up of the objects of C). It is epistemically parismonious: a purely
categorical definition, stated entirely in terms of universal constructions.
Remark 2.3.66. Under the Curry-Howard-Lambek correspondence, exponential objects represent
the propositions that one proposition implies another; in type theory, they represent the type
of functions from one type to another. As dependent exponential objects, dependent products
could therefore be seen as representing ‘dependent’ implications; as we have already seen, they do
represent the type of dependent functions. However, dependent products and sums have another
kind of logical interpretation: as quantifiers. That is, the logical proposition represented byś
b:B Pb
is @b : B.P pbq: an element of ś
b:B Pb is a proof that, for all b : B, the proposition Ppbqis satisfied.
Dually, the proposition represented by ř
b:B Pb is Db : B.P pbq: an element of ř
b:B Pb is a pair
pb, xqof a witness to B and a witness x of the satisfaction of Ppbq.
2.4. The Yoneda Lemma: a human perspective
We end this chapter by introducing the fundamental theorem of category theory, the Yoneda
Lemma, which expresses mathematically the idea that to know how a thing is related to other
things is to know the identity of the thing itself. The notion of relational identity is recognized
throughout human endeavour. In linguistics, it underlies the observation of Firth [92] that “you
shall know a word by the company it keeps!”, which in turn is the foundation of distributional
semantics and thus much of contemporary natural language processing in machine learning. In
culture, it is illustrated by the ancient parable of the blind men and the elephant, in which the
identity of the creature is only known by stitching together evidence from many perspectives.
In society, it is reflected in the South African philosophy of ubuntu (meaning “I am because we
are”) and the M¯aori notion of whanaungatanga (in which personal identity is developed through
kinship), and the observation that “actions speak louder than words”. Finally, the Yoneda Lemma is
manifest in science, where our understanding of phenomena derives from the accumulation across
contexts of results and their interpretation and translation: no single individual understands the
totality of any subject, and no subject or phenomenon is understood in isolation.
63
2.4.1. Formalizing categorical reasoning via the Yoneda embedding
In §2.3.4, we saw how Cartesian closed categories allow us to internalize categorical reasoning. The
category Set is the archetypal Cartesian closed category, and constitutes the base of enrichment
for all locally small categories. The Yoneda embedding allows us to move from reasoning about
the objects in any given category C to reasoning about the morphisms between its hom sets: the
natural transformations between hom functors. In this context, the hom functors constitute special
examples of functors into the base of enrichment, which we call ‘presheaves’ (contravariantly) and
‘copresheaves’ (covariantly), and which can be thought of as C-shaped diagrams in Set.
Definition 2.4.1. Let C be a category. A presheaf on C is a functor C op ÑSet. Dually, a copresheaf
is a functor C ÑSet. The corresponding functor categories are the categories of (co)presheaves on
C.
Remark 2.4.2. In the enriched setting, when C is enriched in E, an E-presheaf is an E-functor
C op ÑE and an E-copresheaf is an E-functor C ÑE.
As a first example of a presheaf, we have an alternative definition of the notion of directed graph.
Example 2.4.3. Let G denote the category of Example 2.1.5 containing two objects 0 and 1 and
two morphisms s, t: 0 Ñ1. Then a directed graph is a presheaf on G.
This definition is justified by the following proposition.
Proposition 2.4.4. There is an equivalence of categories Graph –SetGop
, where Graph is the
category of directed graphs introduced in Example 2.1.11.
Proof. To each graph G we can associate a presheaf G : Gop Ñ Set by defining Gp0q :“ G0,
Gp1q:“G1, Gpsq:“domG and Gptq:“codG; and to each presheaf we can likewise associate a
graph, so that we have defined a bijection on objects. It therefore only remains to show that there
is a bijection between graph homomorphisms and natural transformations accordingly: but this is
easy to see once we have observed that the graph homomorphism axioms are precisely the law of
naturality, as illustrated diagrammatically in (2.1).
Taking again a general perspective, the Yoneda embedding is the embedding of a categoryC into
its presheaf category, obtained by mapping c : C to the presheaf Cp´, cq; and there is of course a
dual ‘coYoneda’ embedding.
64
Remark 2.4.5. We say ‘embedding’ to mean a functor that is injective on objects and faithful
(injective on hom sets). The Yoneda embedding will turn out to be fully faithful (bijective on hom
sets), as a consequence of the Yoneda lemma.
Owing to its importance, we make a formal definition of the Yoneda embedding.
Definition 2.4.6. Let C be a category. By applying the product-exponential adjunction in Cat to
the hom functor Cp´, “q: C op ˆC ÑSet, we obtain a functor よ: C ÑSetC op
: c ÞÑCp´, cq
of C into its presheaf category, and dually a functor よ: C op Ñ SetC : c ÞÑ Cpc, “qinto the
copresheaf category. We call the former functor the Yoneda embedding and the latter the coYoneda
embedding. When C is an E-category and E is Cartesian closed, then the Yoneda embedding is
instead an E-functor C ÑEC op
(and likewise for the coYoneda embedding).
Remark 2.4.7. This abstract definition does not make explicit howよ acts on morphisms. However,
we have already seen this action, when we first exemplified natural transformations in Example
2.2.18.
As we discussed in §2.3.4, much categorical reasoning corresponds to following morphisms
between hom objects, and often the reasoning is agnostic either to where one starts, or to where
one ends up. The Yoneda embedding witnesses such proofs as morphisms in the (co)presheaf
categories. As an example, consider the proof of Proposition 2.4.20 below: each step corresponds to
the application of a natural transformation.
Remark 2.4.8. It also so happens that every (co)presheaf category is very richly structured,
inheriting its structure from the base of enrichment. For example, this means that the presheaf
category SetC op
has all limits, is Cartesian closed, has a subobject classifier, and dependent sums
and products, even whenC has none of these. (Interestingly, this means that the category of directed
graphs is accordingly richly structured, being a presheaf category by Proposition 2.4.4.) As a result,
(co)presheaf categories are very powerful places to do categorical reasoning.
2.4.2. Knowing a thing by its relationships
The Yoneda lemma says that every (co)presheaf on C is determined by “how it looks from C”. Since
under the (co)Yoneda embedding every object gives rise to a (co)presheaf, a corollary of the Yoneda
lemma is that every object can be identified by its relationships.
65
Remark 2.4.9. If the base of enrichment of a category is Cartesian closed, then one can prove an
analogous enriched version of the Yoneda lemma. We will only prove the standardSet-enriched
case here.
We will also only prove the Yoneda lemma for presheaves; there is of course a dual coYoneda
lemma for copresheaves, which follows simply by swapping C for C op.
Theorem 2.4.10 (Yoneda lemma). Let F : C op ÑSet be a presheaf on C. Then for each c : C,
there is an isomorphism F c–SetC op
pCp´, cq, Fq. Moreover, this isomorphism is natural in both
F : C op ÑSet and c : C.
Proof. We first define a mapping γ : F cÑ SetC op
pCp´, cq, Fqas follows. Given h : F c, we
define the natural transformation γphq : Cp´, cq ñF to have components γphqb : Cpb, cq Ñ
F b: f ÞÑ F fphq; note that since h : F cand f : b Ñ c, we have F f: F cÑ F band hence
F fphq : F b. To check that this definition makes γphqinto a natural transformation, suppose
g : a Ñb. We need to check F g˝γphqb “γphqa ˝Cpg, cq. Since Cpg, cqpfq“ f ˝g, this means
verifying F g˝F fphq“ Fpf ˝gqphq. But F is a contravariant functor, so Fpf ˝gq“ F g˝F f,
thereby establishing naturality.
Conversely, we define a mapping γ1 : SetC op
pCp´, cq, Fq ÑF cas follows. Suppose α is a
natural transformation Cp´, cqñ F, so that its component at c is the function αc : Cpc, cqÑ F c.
We define γ1pαq:“αcpidcq.
Next, we need to establish that γ and γ1are mutually inverse. First, we check that γ1˝γ “idF c.
Given h : F c, we have
γ1pγphqq“ γphqcpidcq“ Fpidcqphq“ idF cphq“ h
as required. We now check that γ ˝γ1 “ idSetC op
pCp´,cq,Fq. Given α : Cp´, cq ñF, we have
γ1pαq “αcpidcqby definition. Hence γpγ1pαqq : Cp´, cq ñF has components γpγ1pαqqb :
Cpb, cq ÑF bwhich act by f ÞÑF fpαcpidcqq. So we need to show that F fpαcpidcqq “αbpfq.
This follows directly from the naturality of α. The commutativity of the naturality square on the
left in particular holds at idc : Cpc, cqas on the right:
Cpc, cq F c
Cpb, cq F b
αc
F fCpf,cq
αb
idc αcpidcq
f αbpfq“ F fpαcpidcqq
66
Note that Cpf, cqpidcq“ idc ˝f “f. This establishes that γ ˝γ1 “idSetC op
pCp´,cq,Fq, and since
γ1˝γ “idF c, we have F c–SetC op
pCp´, cq, Fq.
It remains to verify that this isomorphism is natural inF and c. Suppose φ : F ñF1is a natural
transformation, and write γ1
F cfor the function γ1defined above, and γ1
F1c for the corresponding
function for F1. Naturality in F means that the diagram on the left below commutes, which we
can see by chasing the natural transformation α as on the right:
SetC op
pCp´, cq, Fq F c
SetC op
pCp´, cq, F1q F1c
SetC op
pCp´,cq,φq
γ1
Fc
γ1
F1c
φc
α γ1
F cpαq
φ ˝α γ1
F1cpφ ˝αq“ φc ˝γ1
F cpαq
Since γ1
F cpαqc :“αcpidcqand γ1
F1cpφ˝αqc :“φc ˝αcpidcq, the equation γ1
F1cpφ˝αq“ φc ˝γ1
F cpαq
holds by definition, thereby establishing naturality in F. Finally, suppose f : b Ñc in C, and write
γF cfor the function γ defined above and γF bfor the corresponding function for b : C. Naturality
in c means the commutativity of the following diagram:
F c SetC op
pCp´, cq, Fq
F b SetC op
pCp´, bq, Fq
SetC op
pCp´,fq,Fq
γFc
γFb
F f
Suppose h : F c. The component of γF cphqat a : C is the function γF cphqa : Cpa, cq ÑF a
defined by g ÞÑ F gphq. The component of SetC op
pCp´, fq, Fq˝ γF cphqat a : C is thus the
function γcphqa ˝Cpa, fq: Cpa, bqÑ F ataking g : a Ñb to Fpf ˝gqphq. On the other hand,
the component of γF bpF fphqqat a : C is the function γF bpF fphqqa : Cpa, bq ÑF ataking g
to F gpF fphqq. Since F is a contravariant functor, we have Fpf ˝gqphq “F gpF fphqq. This
establishes the commutativity of the naturality square, and thus naturality in c as well as F.
The identification of an object with its collection of hom sets is formalized by the following
corollary.
Corollary 2.4.11 (Representables are unique up to isomorphism). Suppose there is an isomorphism
of presheaves Cp´, aq– Cp´, bq. Then a –b in C.
This corollary follows from the next one, which expresses that the image of the Yoneda embedding
is isomorphic with C itself.
67
Corollary 2.4.12. The Yoneda embedding is fully faithful.
Proof. The Yoneda embedding defines a family of functions on the hom sets of C:
よb,c : Cpb, cqÑ SetC op
pCp´, bq, Cp´, cqq
f ÞÑCp´, fq
By the Yoneda lemma, we immediately have SetC op
pCp´, bq, Cp´, cqq –Cpb, cq, which is the
required isomorphism of hom sets.
Next, we have the following fact, that fully faithful functors transport isomorphisms in their
codomain to their domain (they ‘reflect’ them).
Proposition 2.4.13 (Fully faithful functors reflect isomorphisms). Suppose F : C ÑD is a fully
faithful functor. If f : a Ñb is a morphism in C such that F fis an isomorphism in D, then f is an
isomorphism in C.
Proof. F f: F aÑF bbeing an isomorphism means that there is a morphism g1 : F bÑF ain D
such that g1˝F f“idF aand F f˝g1 “idF b. By the functoriality of F, we have idF a“F ida and
idF b“F idb. Hence g1˝F f“F ida and F f˝g1 “F idb. Since F is isomorphic on hom sets, there
is a unique g : b Ña such that g1 “F g. Hence F g˝F f“F ida and F f˝F g“F idb. By the
functoriality of F, we have F g˝F f“Fpg ˝fqand F f˝F g“Fpf ˝gq. Hence Fpg ˝fq“ F ida
and Fpf ˝gq“ F idb. Finally, since F is isomorphic on hom sets, we must have g ˝f “ida and
f ˝g “idb, and hence f is an isomorphism in C.
And this gives us the proof we seek:
Proof of Corollary 2.4.11. Since the Yoneda embedding is fully faithful (Corollary 2.4.12), it reflects
isomorphisms by Proposition 2.4.13.
Presheaves in the image of the Yoneda embedding consequently play a special rôle in category
theory: to show that an arbitrary presheafF is isomorphic to Cp´, cqis to identify it with the object
c itself, and in this case, we can say that F is represented by c. We therefore make the following
definition.
Definition 2.4.14. Suppose F is a presheaf on C. We say that it is representable if there is a natural
isomorphism F –Cp´, cqfor some object c : C which we call its representing object ; we call the
natural isomorphism Cp´, cqñ F its representation. Dually, if F is instead a copresheaf, we call
68
it corepresentable if there is a natural isomorphism F –Cpc, “q, with c being the corepresenting
object; we call the natural isomorphism Cpc, “qñ F its corepresentation.
Remark 2.4.15. Corepresentable copresheaves will play an important rôle later in this thesis: their
coproducts are called polynomial functors (§3.5), and these will be used to formalize the interfaces
of interacting adaptive systems.
Via the uniqueness of representables, the Yoneda lemma underlies universal constructions, since
knowing the morphisms into or out of an object is enough to identify that object. The definition of
a limit, notably, is the statement that morphisms into it correspond to morphisms into a diagram;
and this in turn is equivalently the statement that lim is right adjoint to ∆. Indeed, adjointness
is itself a certain kind of representability: the definition of adjoint functor (2.2.24) is precisely a
natural characterization of morphisms into and out of objects, as related by the adjunction!
Proposition 2.4.16 (Adjoints are representable). Suppose R : D ÑC is right adjoint to L. Then
for every d : D, the presheaf DpL´, dq: C op ÑSet is represented by the object Rd : C. Dually,
the copresheaf Cpc, R´q: D ÑSet is corepresented by the object Lc : D.
Proof. Since L $R, we have an isomorphism DpLc, dq– Cpc, Rdqnatural in c and d. Therefore
in particular we have a natural isomorphism of presheaves Cp´, Rdqñ DpL´, dqand a natural
isomorphism of copresheaves DpLc, ´qñ Cpc, R´q; the former is a representation and the latter
a corepresentation.
From this, we can formalize the representability of limits and colimits.
Corollary 2.4.17 (Limits are representations). Suppose D : J ÑC is a diagram in C. A limit
of D is a representation of CJ p∆p´q, Dq : C op Ñ Set, or equivalently of SetJ p∆1, Cp´, Dqq.
Dually, a colimit of D is a corepresentation of CJ pD, ∆p´qq : C Ñ Set, or equivalently of
SetJ p∆1, CpD, ´qq.
Proof. If C has all limits of shape J, then this follows directly from the facts thatlim is right adjoint
to ∆ (Proposition 2.3.48) and that adjoints are representable (Proposition 2.4.16); the dual result
follows similarly from the fact that colim is left adjoint to ∆.
Otherwise, the limit case follows immediately from Lemma 2.3.51 (or equivalently Corollary
2.3.52) and the definition of representation (2.4.14); the colimit case is formally dual.
Accordingly, we recover the uniqueness of universal constructions.
69
Corollary 2.4.18. Adjoint functors are unique up to unique isomorphism.
Corollary 2.4.19. Limits and colimits are unique up to unique isomorphism.
Using these ideas, we obtain the following useful result relating limits and adjoint functors.
Proposition 2.4.20 (Right adjoints preserve limits). Suppose D : J ÑD is a diagram in D and
L $R : D ÑC is an adjunction. Then R lim D –lim RD in C.
Proof. We have the following chain of natural isomorphisms:
Cpc, Rlim Dq– DpLc, lim Dq since R is right adjoint to L
–lim DpLc, Dq since hom preserves limits
–lim Cpc, RDq since R is right adjoint to L
–Cpc, lim RDq since hom preserves limits
Since representables are unique up to isomorphism and we have established an isomorphism of
presheaves Cp´, Rlim Dq– Cp´, lim RDq, we must have R lim D –lim RD in C.
Remark 2.4.21. There is of course a dual result that left adjoints preserve colimits.
Remark 2.4.22. One might speculate about the converse: is it the case that the preservation of
limits by a functor is enough to guarantee the existence of its left adjoint? The answer to this
question is, “under certain conditions” on the size and structure of the categories and functors
involved, and a positive answer is called an adjoint functor theorem . The “certain conditions” hold
quite generally, and so it is often sufficient just to check whether a functor preserves limits (or
colimits) to see that it is a right (or left) adjoint.
We end this chapter by closing the loop between universality and representability.
Proposition 2.4.23 (Universality of representability). Representable presheaves F : C op ÑSet
correspond bijectively to universal morphisms from 1 : Set to F.
Proof. A representation of F is a choice of object c : C and a natural isomorphism υ : Cp´, cqñ F.
We construct a bijection between the set of representations ofF and the set of universal morphisms
from 1 to F. Therefore suppose given a representationυ : Cp´, cqñ F of F; its component atc : C
is the isomorphism υc : Cpc, cqÑ F c. The Yoneda lemma assigns to υ an element γ1pυq: 1 ÑF c
satisfying γ1pυq“ υcpidcq. We now show that this element υcpidcqsatisfies the universal property
70
that for all f : 1 ÑF bthere exists a unique morphism h : b Ñc in C such that f “F h˝υcpidcq.
Therefore let f be any such element 1 ÑF b. Since υ is a natural isomorphism, it has an inverse
component at b : C, denoted υ1
b : F bÑ Cpb, cq, and so we obtain by composition an element
h :“ 1
f
Ý ÑF b
υ1
b
Ý ÑCpb, cqof Cpb, cq. Such an element is precisely a morphism h : b Ñ c in C.
Consider now the following diagram:
1 Cpc, cq Cpb, cq
F c F b
idc Cph,cq
υbυc
F h
υcpidcq
The triangle on the left commutes by definition and the square on the right commutes by the
naturality of υ, so that the whole diagram commutes. The composite morphism Cph, cq˝ idc along
the top of the diagram picks out the element idc ˝h of Cpb, cq. By the unitality of composition, this
element is equal to h itself, so we can rewrite the diagram as follows:
1 Cpb, cq
F c F b
υb
F h
υcpidcq
h
Next, we can substitute the definition h :“υ1
b ˝f, and observe that υb ˝υ1
b “idF b(since υb is an
isomorphism with υ1
b its inverse):
1 F b Cpb, cq
F c F b
υb
F h
υcpidcq
f υ1
b
The commutativity of this diagram means thatf “F h˝υcpidcq. Moreover, since h “υ1
b ˝f and υ1
b
is an isomorphism, h is unique for a given f. Therefore υcpidcq: 1 ÑF cis a universal morphism
from 1 to F.
Next, suppose given a universal morphism u : 1 ÑF c. The Yoneda lemmea associates to this
element a natural transformation γpuqwhose component at b is the function γpuqb : Cpb, cqÑ F b
which acts by f ÞÑF fpuq. We need to show that this function is an isomorphism for everyb : C, so
that γpuq: Cp´, cqñ F is a natural isomorphism and hence F is represented by c. We therefore
need to define an inverse function φb : F bÑCpb, cq, which we do using the universal property
71
of u: for each element f : 1 ÑF b, we have a unique morphism h : b Ñc such that f “F hpuq.
This unique h is an element of Cpb, cq, and so we can simply define φbpfq:“h. The uniqueness
of h ensures that φb is an inverse of γpuqb: observe that γpuqb ˝φb acts by f ÞÑh ÞÑF hpuqand
f “F hpuqby definition; in the opposite direction, we necessarily have f ÞÑF fpuqÞÑ f.
We have constructed mappings between the set of representations ofF and universal morphisms
from 1 to F, so it remains to show that these mappings are mutually inverse. This again follows
directly from the Yoneda lemma: the mapping of representations to universal morphisms takes a
representation υ to the element γ1pυqinduced by the Yoneda lemma; and the mapping of universal
morphisms to representations takes a universal morphism u to the natural transformation γpuq
induced by the Yoneda lemma. Since the functions γ and γ1are mutually inverse, so must these
mappings be: γ ˝γ1pυq“ υ and γ1˝γpuq“ u.
Using the universality of representability, and the uniqueness of universal morphisms, and the
representability of limits and adjoints, we therefore obtain alternative proofs of the uniqueness of
those universal constructions.
72
3. Algebraic connectomics
In Chapter 2, we motivated applied category theory in the context of complex systems like brains
by its abilities to relate structure and function, to translate between models and frameworks, and
to distil phenomena to their essences. However, the focus in that chapter was on ‘one-dimensional’
morphisms, which can be understood as connecting one interface to another, with the composition
of 1-cells representing something like the ‘end-to-end’ composition of processes; although we
considered some higher-dimensional category theory, this was largely restricted to weakening
equalities and thus comparing morphisms.
Because systems can be placed ‘side-by-side’ as well as end-to-end, and because two systems
placed side by side may be nontrivially wired together, in this chapter we extend the higher-
dimensional categorical language accordingly, with a particular focus once more on the graphical
and diagrammatic representation of systems and processes. In line with the distinction made in
§2.2.3 between syntax and semantics, our treatment here of the syntax of wiring—ofconnectomics—is
largely ‘algebraic’. Later, in Chapter 6, we will see how our semantic focus will be ‘coalgebraic’.
We will begin therefore by introducing the graphical calculus of monoidal categories, which
allow us to depict and reason about sequential and parallel composition simultaneously. We follow
this with the formal underpinnings of the structure—to use the term from Chapter 2, a monoidal
structure is a ‘well-behaved’ tensor product—before explaining how monoidal categories relate
to the higher category theory of Chapter 2 using the notion of bicategory. We then make use of
the extra freedom afforded by bicategories to consider parameterized systems, with which we can
model systems that not only act but also learn.
By this point, we will find ourselves ready to apply our new toolkit, and so in §3.3, we use
functorial semantics to define a graphical algebra for neural circuits, revisiting our first example
from Chapter 2. This involves a change of perspective from the graphical calculus with which we
begin the chapter: instead of using the composition of morphisms to encode the plugging-together
of systems at the same ‘scale’ or “level of hierarchy”, we use composition to encode the wiring of
circuits at one level into systems at a higher level. Although formally closely related to monoidal
73
categories, this ‘hierarchical’ perspective is strictly speakingmulticategorical and allows morphisms’
domains to take very general shapes.
After this extended example, we return to algebra, explaining what makes monoidal categories
monoidal, and using the related concept of monad to explain how we think of them as algebraic;
monads will later prove to be of importance in categorical approaches to probability theory. Finally,
we end the chapter by introducing the richly structured category of polynomial functorsSet ÑSet,
which we will use in Chapter 6 both to formalize a wide variety of open dynamical systems as well
as to specify the shapes of those systems’ interfaces.
Excepting the extended example of §3.3, the content of this chapter is well known to category-
theoreticians. However, since it is not well known to mathematical scientists, we have again
endeavoured to supply detailed motivations for the concepts and results that we introduce.
3.1. Categories and calculi for process theories
In this section, we introduce an alternative way of depicting morphisms and their composites in
categories equipped with notions of both sequential and parallel composition. Such categories
are useful for representing processes in which information flows: we formalize the processes
as morphisms, and consider the flow as from domain to codomain, even when the categories
themselves are quite abstract and lack a notion of time with which to make sense of ‘flow’. In
such contexts, the categories are often not only monoidal, but also copy-discard categories, since a
distinctive feature of classical information is that it can be copied and deleted. Monoidal categories
will therefore be important not only in depicting composite computations (as indicated in §2.1.1.3),
but also in depicting and manipulating the factorization of probabilistic models (as indicated in
§2.1.1.2).
3.1.1. String diagrams
Rather than beginning with the formal definition of “monoidal category”, we start with the
associated graphical calculus of string diagrams and its intuition.
Sequential and parallel composition Diagrams in the graphical calculus depict morphisms
as boxes on strings: the strings are labelled with objects, and a string without a box on it can
be interpreted as an identity morphism. Sequential composition is represented by connecting
strings together, and parallel composition by placing diagrams adjacent to one another; sequential
composition distributes over parallel, and so we can of course compose parallel boxes in sequence.
74
Because monoidal structures are “well-behaved tensor products”, we will typically denote them
using the same symbols that we adopted in Chapter 2, with sequential composition denoted by˝and
parallel composition (tensor) denoted by b. Diagrams will be read in the direction of information
flow, which will be either bottom-to-top or left-to-right; we will adopt the former convention in
this section.
In this way, c : X ÑY , idX : X ÑX, d ˝c : X cÝ ÑY dÝ ÑZ, and f bg : X bY ÑA bB are
depicted respectively as:
c
X
Y
X
X
d
Z
c
X
f
X
A
g
Y
B
A monoidal structure comes with a monoidal unit , which we will also continue to call a tensor unit ,
and which will be not be depicted in diagrams, but rather left implicit. (Alternatively, it is depicted as
the “empty diagram”.) This is justified, as we will see, by the requirement thatI bX –X –X bI
naturally in X.
States and costates In Remark 2.2.8, we called a morphism I ÑX out of the tensor unit a
generalized element , but owing to the many rôles they play, such morphisms go by many names.
When we think of X as representing a system, we will also call such morphismsstates of X. Dually,
morphisms X ÑI can be called costates, or sometimes effects. When the unit object is the terminal
object (such as when the monoidal structure is given by the categorical product), then these costates
are trivial. In other categories, costates may be more effectful, and so carry more information: for
example, in a category of vector spaces, states are vectors, costates are linear functionals, and so
the composite of a state with a costate is an inner product.
Graphically, states η : I ÑX and costates ϵ : X ÑI will be represented respectively as follows:
η
X
ϵ
X
Discarding, marginalization, and causality In a category with only trivial effects, we can
think of these as witnessing the ‘discarding’ of information: in electronics terms, they “send the
75
signal to ground”. For this reason, we will denote such trivial effects by the symbol , writing
X : X ÑI for each object X.
We can use discarding to depictmarginalization. Given a ‘joint’ state (a state of a tensor product)
ω : I ÑX bY , we can discard either Y or X to obtain ‘marginal’ states ω1 of X and ω2 of Y
respectively, as in the following depiction:
ω“ω1
X X
and ω “ ω2
YY
.
We will see in Chapter 4 how this corresponds to the marginalization familiar from probability
theory.
To make the notion of discarding more mathematically precise, we can use it to encode a causality
condition: physically realistic processes should not be able to affect the past.
Definition 3.1.1. Whenever a morphism c satisfies the equation
“c
we will say that c is causal: the equation says that, if you do c and throw away the result, the effect
is of not having done c at all—and so c could not have had an anti-causal effect on its input.
Remark 3.1.2. If in a category every morphism is causal, then this is equivalently a statement of
the naturality of family of discarding morphisms X : X ÑI, which implies that there is only one
such morphism X ÑI for every object X, and which therefore means that I must be a terminal
object.
Some categories of interest will have nontrivial costates, yet we will still need notions of
discarding and marginalization. In these categories, it suffices to ask for each object X to be
equipped with a ‘comonoid’ structure (to be elaborated in §3.4.1), of which one part is a ‘counit’
morphism X ÑI which can play a discarding rôle, and which we will therefore also denote by
X.
Copying The other part of a comonoid structure on X is a ‘copying’ map X : X ÑX bX,
which has an intuitive graphical representation. As we will see in §3.4.1, the comonoid laws say
76
that copying must interact nicely with the discarding maps:
““ and “
These equations say that making a copy and throwing it away is the same as not making a copy (left,
counitality; and that in copying a copy, it doesn’t matter which copy you copy (right,coassociativity).
Definition 3.1.3. A category with a comonoid structure p X, Xqfor every object X is called a
copy-discard category [60].
Symmetry In all our applications, the tensor product structure will be symmetric, meaning that
X bY can reversibly be turned into Y bX simply by swapping terms around. In the graphical
calculus, we depict this by the swapping of wires, which we ask to satisfy the following equations:
“ and “
The equations say that swapping is self-inverse (on the left), and that copying is invariant under the
symmetry (on the right). (Strictly speaking, the right equation is an axiom called cocommutativity
that we additionally ask the comonoid structure to satisfy in the presence of a symmetric tensor.)
3.1.2. Monoidal categories
It being important to use tools appropriate for the jobs at hand, we will not always work just with
the graphical calculus: we will need to translate between string diagrams and the symbolic algebra
of Chapter 2. In the first instance, this means making mathematical sense of the graphical calculus
itself, for which the key definition is that of the monoidal category.
Definition 3.1.4. We will call a categoryC monoidal if it is equipped with a functorb: C ˆC ÑC
called the tensor or monoidal product along with an object I : C called the monoidal unit and three
natural isomorphisms
1. an associator α : pp´qbp´qqbp´qñp´qbpp´qbp´qq ;
2. a left unitor λ : I bp´qñp´q ; and
77
3. a right unitor ρ : p´qb I ñp´q
such that the unitors are compatible with the associator, i.e. for all a, b: C the diagram
pa bIqb b a bpI bbq
a bb
ρabidb
αa,I,b
ida bλb
commutes, and such that the associativity is ‘order-independent’,i.e. for all a, b, c, d: C the diagram
pa bpb bcqqb d a bppb bcqb dq
ppa bbqb cqb d a bpb bpc bdqq
pa bbqbp c bdq
αabb,c,d αa,b,cbd
αa,b,cbidd
αa,bbc,d
ida bαb,c,d
commutes.
We call C strict monoidal if the associator and unitors are equalities rather than isomorphisms;
in this case, the diagrams above commute by definition.
Example 3.1.5. Any category equipped with a tensor product in the sense of Definition 2.2.3
where the structure isomorphisms are additionally natural and satisfy the axioms of compatibility
and order-independence is a monoidal category.
Example 3.1.6. If pC, b, Iqis a monoidal category, then so is pC op, bop, Iq, where bop is the
induced opposite functor C op ˆC op ÑC op.
The associativity of the tensor is what allows us to depict string diagrams “without brackets”
indicating the order of tensoring, and the unitality is what allows us to omit the monoidal unit
from the diagrams. Note that the functoriality of the tensor means that bdistributes over ˝as in
pf1˝fqbp g1˝gq“p f1bg1q˝p f bgq, both of which expressions are therefore depicted as
f1
f
g1
g
.
The symmetry of a monoidal structure is formalized as follows.
78
Definition 3.1.7. A symmetric monoidal category is a monoidal category pC, b, I, α, λ, ρqthat
is additionally equipped with a natural isomorphism σ : p´qbp“q ñ p“qbp´q , called the
symmetry, such that σb,a ˝σa,b “idabb for all a, b: C, and whose compatibility with the associator
is witnessed by the commutativity of the following diagram:
pa bbqb c a bpb bcq pb bcqb a
pb baqb c b bpa bcq b bpc baq
αa,b,c σa,bbc
αb,c,aσa,bbidc
αb,a,c idb bσa,c
Here is a familiar family of examples of symmetric, but not strict, monoidal categories.
Example 3.1.8. Any category within which every pair of objects has a product is said to have
finite products , and any category with finite products and a terminal object is a monoidal category.
This includes the Cartesian products of sets (Definition 2.2.1 and Example 2.2.5) and of categories
(Propositions 2.2.14 and 2.2.16).
To see that the Cartesian product of sets is not strictly associative, observe that the elements
of A ˆpB ˆCqare tuples pa, pb, cqqwhereas the elements of pA ˆBqˆ C are tuples ppa, bq, cq;
evidently, these two sets are isomorphic, but not equal, and the same holds for the product of
categories.
At the same time, it is easy to see that a Cartesian product is symmetric: we haveAˆB –B ˆA
by the mapping pa, bqØp b, aq.
And here is a family of examples of strict, but not symmetric, monoidal categories.
Example 3.1.9. If C is any category, then the categoryCC of endofunctors C ÑC is a strict monoidal
category, where the monoidal product is given by composition ˝of endofunctors and the monoidal
unit is the identity functor idC on C. That the monoidal structure here is strict follows from the fact
that composition in a category is strictly associative and unital.
In practice, we will tend to encounter strict monoidal categories only when the monoidal structure
derives from the composition operator of a category, as in the preceding example. However, when
we work with the graphical calculus, we are often implicitly working with strict monoidal structure,
as a result of the following important theorem.
Theorem 3.1.10 (Mac Lane [175, Theorem XI.3.1]). Every monoidal category is strong monoidally
equivalent to a strict monoidal one.
79
As a consequence of this coherence theorem , any two string diagrams where one can be
transformed into the other by a purely topological transformation are equal, as in the following
example (read from left to right):
“
This follows because the coherence theorem renders parallel morphisms entirely constructed from
identities, associators and unitors (and the symmetry, as long as it is strictly self-inverse) equal “on
the nose”1.
To make sense of the notion of strong monoidal equivalence , we need a notion of functor that
preserves monoidal structure; we define the ‘weak’ case first.
Definition 3.1.11. Suppose pC, bC, ICqand pD, bD, IDqare monoidal categories. A lax monoidal
functor pC, bC, ICqÑp D, bD, IDqis a triple of
1. a functor F : C ÑD;
2. a state ϵ : ID ÑFpICqcalled the unit; and
3. a natural transformation, the laxator, µ : Fp´qbD Fp“qñ Fpp´qbC p“qq
satisfying the axioms of
(a) associativity, in that the following diagram commutes
pFpaqbD FpbqqbD Fpcq FpaqbD pFpbqbD Fpcqq
Fpa bC bqbD Fpcq FpaqbD Fpb bC cq
Fppa bC bqbC cq Fpa bC pb bC cqq
αD
Fpaq,Fpbq,Fpcq
FpaqbDµb,cµa,bbDFpcq
µabCb,c
FpαC
a,b,cq
µFpaq,bbCc
where αC and αD are the associators of the respective monoidal structures on C and D; and
1This process of turning natural isomorphisms into equalities is called strictification.
80
(b) (left and right) unitality, in that the following diagrams commute
ID bD Fpaq FpICqbD Fpaq
Fpaq FpIC bC aq
λD
Fpaq µIC,a
ϵbDFpaq
FpλC
aq
and
FpaqbD ID FpaqbD FpICq
Fpaq Fpa bC ICq
ρD
Fpaq µa,IC
FpaqbDϵ
FpρC
aq
where λC and λD are the left, and ρC and ρD the right, unitors of the respective monoidal
structures on C and D.
A strong monoidal functor is a lax monoidal functor for which the unit and laxator are isomorphisms.
A strong monoidal equivalence is therefore an equivalence of categories in which the two functors
are strong monoidal.
Remark 3.1.12. Laxness can be read as a sign of an “emergent property”: if F is lax monoidal,
then this means there are systems of type FpX bY qthat do not arise simply by placing a system
of type FpXqbeside a system of type FpY qusing b; whereas if F is strong monoidal, then there
are no such ‘emergent’ systems. More generally, we can think of emergence as an indication of
higher-dimensional structure that is hidden when one restricts oneself to lower dimensions (and
hence can appear mysterious). In this example, the higher-dimensional structure is the 2-cell of the
laxator.
There is of course a notion of monoidal natural transformation, making monoidal categories, lax
monoidal functors, and monoidal natural transformations into the constituents of a 2-category.
Definition 3.1.13. If pF, µ, ϵqand pF1, µ1, ϵ1qare lax monoidal functorspC, bC, ICqÑp D, bD, IDq,
then a monoidal natural transformation α : pF, µ, ϵq ñ pF1, µ1, ϵ1qis a natural transformation
α : F ñF1that is compatible with the unitors
ID
FpICq F1pICq
ϵ ϵ1
αIC
and the laxators
F abD F b F1a bC F1b
Fpa bC bq F1pa bC bq
αabDαb
αabCb
µa,b µ1
a,b
81
for all a, b: C.
Proposition 3.1.14. Monoidal categories, lax monoidal functors, and monoidal natural transfor-
mations form the 0-cells, 1-cells, and 2-cells of a 2-category, denoted MonCat.
Proof. Given composable lax monoidal functors pF, ϵ, µq : pC, bC, ICq Ñ pD, bD, IDq and
pF1, ϵ1, µ1q: pD, bD, IDqÑp E, bE, IEq, form their horizontal composite as follows. The functors
compose as functors, G ˝F. The composite state is given by IE
ϵ1
Ý ÑF1pIDq F1ϵÝ Ý ÑF1FpICq. The
laxator is given by
F1Fp´qbE F1Fp“q
µ1
Fp´q,Fp“q
ù ùùùùùù ñF1pFp´qbD Fp“qq
F1µa,b
ù ùùù ñF1Fpp´qbC p“qq.
The identity lax monoidal functor onC is given bypidC, idIC, idp´qbCp“qq. Unitality and associativity
of composition of lax monoidal functors follow straightforwardly from unitality and associativity
of composition of morphisms, functors, and natural transformations. Monoidal natural transforma-
tions compose vertically as natural transformations, and it is easy to see that the composites satisfy
the compatibility conditions by pasting the relevant diagrams.
3.1.3. Closed monoidal categories
Since one source of monoidal structures is the generalization of the categorical product, it is no
surprise that there is a corresponding generalization of exponentials: a ‘tensor-hom’ adjunction
that induces a concept of closed monoidal category. Such categories will be important later in the
thesis when we consider learning and adaptive systems: our compositional model of predictive
coding, for example, will be built on a certain generalized exponential (see Remark 6.3.2).
Definition 3.1.15. Let pC, b, Iqbe a monoidal category. When there is an object e : C such
that Cpx, eq –Cpx by, zqnaturally in x, we say that e is an internal hom object and denote it
by ry, zs. The image of idry,zsunder the isomorphism is called the evaluation map and is written
evy,z : ry, zsb y Ñz.
Proposition 3.1.16. When the isomorphism Cpx by, zq– Cpx, ry, zsqis additionally natural in
z, we obtain an adjunction p´qb y %ry, ´scalled the tensor-hom adjunction , which uniquely
determines a functor C op ˆC ÑC : py, zqÞÑr y, zsthat we call the internal hom for C.
Proof. A direct generalization of the Cartesian case (Proposition 2.3.56).
Definition 3.1.17. A monoidal category C with a corresponding internal hom is called monoidal
closed.
82
Example 3.1.18. The category of finite-dimensional real vector spaces and linear maps between
them is monoidal closed with respect to the tensor product of vector spaces, as each space of linear
maps is again a vector space and the tensor is necessarily bilinear.
As in the Cartesian case, monoidal closed categories can reason about themselves.
Proposition 3.1.19. A monoidal closed category is enriched in itself.
And when a category is enriched in a symmetric monoidal category, then its hom functor is
likewise enriched.
Proposition 3.1.20. Suppose C is an E-category where E is symmetric monoidal closed. Then the
hom functor Cp´, “qis an E-functor.
Proof. A direct generalization of Proposition 2.3.61.
Remark 3.1.21. Since Cartesian closed categories have a rich internal logic, via the Curry-Howard-
Lambek correspondence, one might wonder if there is an analogous situation for monoidal closed
categories. To a certain intricate extent there is: the internal logic of monoidal closed categories
is generally known as linear logic , and its corresponding language linear type theory . These are
‘refinements’ of intuitionistic logic and type theory which of course coincide in the Cartesian case,
but which more generally clarify certain logical interactions; we shall say no more in this thesis,
except that such logics find application in quantum mechanics, owing to the monoidal closed
structure of vector spaces, where the linear structure constrains the use of resources (in relation,
for example, to the famous quantum ‘no-cloning’ and ‘no-deleting’ theorems).
With respect to dependent types, the situation is a little more vexed, as the existence of well-
behaved dependent sums and products classically depends on the existence of pullbacks and their
coherence with products (and, for example, the tensor product of vector spaces is not a categorical
product); this means that classical dependent data is somehow not resource-sensitive. Nonetheless,
various proposals have been made to unify linear logic with dependent type theory[14, 111, 174,
182, 269]: the simplest of these proceed by requiring dependence to be somehow Cartesian, which is
the approach we will take in Chapter 6 when we face a similar quandary in the context of defining
a category of polynomial functors with non-deterministic feedback. (We will see in Chapter 4 that
the property of Cartesianness is equally closely related to determinism.)
83
3.1.4. Bicategories
Monoidal categories are not the first two-dimensional categorical structures we have so far
encountered, the other primary example being 2-categories. These two classes of examples are
closely related: a strict monoidal category is a 2-category with one object; and so just as a monoidal
category is a correspondingly weakened version, a bicategory is a ‘weak 2-category’.
Definition 3.1.22. A bicategory B is constituted by
1. a set B0 of objects or 0-cells;
2. for each pair pA, Bqof B-objects, a category BpA, Bqcalled the hom category , the objects of
which are the morphisms or 1-cells from A to B, and the morphisms of which are the 2-cells
between those 1-cells;
3. for each 0-cell A, a 1-cell ida : BpA, Aqwitnessing identity; and
4. for each triple pA, B, Cqof 0-cells, a functor ˛A,B,C : BpB, Cqˆ BpA, Bq ÑBpA, Cq
witnessing horizontal composition (with vertical composition referring to composition within
each hom category);
5. for each pair pA, Bqof 0-cells, natural isomorphisms ρA,B (the right unitor ) and λA,B (the
left unitor ) witnessing the unitality of horizontal composition, as in the diagrams
BpA, Bqˆ 1 BpA, Bqˆ BpA, Aq
BpA, Bq
PBpA,Bq
BpA,BqˆidA
˛A,A,B
ρA,B
and
BpB, Bqˆ BpA, Bq 1 ˆBpA, Bq
BpA, Bq
ΛBpA,Bq
idBˆBpA,Bq
˛A,B,B
λA,B
where Λ : 1 ˆp´qñp´q and P : p´qˆ 1 ñp´q are the (almost trivial) left and right
unitors of the product ˆon Cat; and
6. for each quadruple pA, B, C, Dqof 0-cells, a natural isomorphism αA,B,C,D witnessing the
84
associativity of horizontal composition, as in the diagram
`
BpC, Dqˆ BpB, Cq
˘
ˆBpA, Bq BpC, Dqˆ
`
BpB, Cqˆ BpA, Bq
˘
BpB, Dqˆ BpA, Bq BpC, Dqˆ BpA, Cq
BpA, Dq
ABpC,Dq,BpB,Cq,BpA,Bq
˛B,C,DˆBpA,Bq BpC,Dqˆ˛A,B,C
˛A,B,D ˛A,C,D
αA,B,C,D
where A : pp´qˆp´qqˆp´qñp´qˆpp´qˆp´qq is the (almost trivial) associator of the
product ˆon Cat;
such that the unitors are compatible with the associator, i.e. for all 1-cells a : BpA, Bqand
b : BpB, Cqthe diagram
pb ˛idBq˛ a b ˛pidB ˛aq
b ˛a
ρb˛ida
αb,idB,a
idb ˛λa
commutes (where we have omitted the subscripts indexing the 0-cells onα, ρ, and λ); and such that
the associativity is ‘order-independent’, i.e. for all 1-cells a : BpA, Bq, b : BpB, Cq, c : BpC, Dq,
and d : BpD, Eqthe diagram
pa ˛pb ˛cqq˛ d a ˛ppb ˛cq˛ dq
ppa ˛bq˛ cq˛ d a ˛pb ˛pc ˛dqq
pa ˛bq˛p c ˛dq
αa˛b,c,d αa,b,c˛d
αa,b,c˛idd
αa,b˛c,d
ida ˛αb,c,d
commutes (where we have again omitted the subscripts indexing the 0-cells on α).
Remark 3.1.23. Just as a 2-category is a category enriched in Cat, a bicategory is a category
weakly enriched in Cat. This is easy to see by comparing Definition 3.1.22 with Definition 2.2.6: the
former is obtained from the latter by taking E to be Cat and filling the unitality and associativity
85
diagrams with nontrivial fillers which are required to satisfy coherence laws generalizing those of
the monoidal category structure (Definition 3.1.4). Conceptually, we can see this weakening in the
context of our brief discussion of emergence above (Remark 3.1.12): we recognize the property of
axiom-satisfaction as a shadow of a higher-dimensional structure (the fillers), which we categorify
accordingly.
Bicategories will appear later in this thesis when we construct categories of dynamical hierarchical
inference systems: the construction proceeds by using polynomial functors to “wire together”
categories of dynamical systems, and the composition of polynomials distributes weakly but
naturally over the categories of systems, thereby producing a category weakly enriched in Cat.
Before then, we will encounter bicategories in the abstract context of general parameterized
morphisms, where the 2-cells witness changes of parameter.
For now, our first examples of bicategories are induced by monoidal categories, which are
equivalently single-object bicategories.
Proposition 3.1.24. Suppose pC, b, Iqis a monoidal category. Then there is a bicategory BC with
a single 0-cell, ˚, and whose category of 1-cellsBCp˚, ˚qis C. The identity 1-cell is I, and horizontal
composition is given by the monoidal product C; vertical composition is just the composition
of morphisms in C. The unitors and associator of the bicategory structure are the unitors and
associator of the monoidal structure. We call BC the delooping of C.
Proof. The bicategory axioms are satisfied immediately, because the structure morphisms satisfy
the (in this case identical) monoidal category axioms.
In the opposite direction, the equivalence is witnessed by the following proposition.
Proposition 3.1.25. Suppose B is a bicategory with a single 0-cell, ˚, and whose horizontal
composition is denoted ˛. Then
`
Bp˚, ˚q, ˛, id˚
˘
is a monoidal category.
Remark 3.1.26. It is possible to define a notion of monoidal bicategory, as something like a
monoidal category weakly enriched in Cat, or as a one-object ‘tricategory’, and in many cases the
bicategories considered below are likely to have such structure. We will say a little more about this
in Remark 3.4.7 below, but will not define or make formal use of this higher structure in this thesis.
More generally, there are analogues of the other structures and results of basic category theory
introduced both in this chapter and in Chapter 2 that are applicable to higher-dimensional categories
such as bicategories, but they too will not play an important rôle in this thesis.
86
3.2. Parameterized systems
A category does not have to be monoidal closed for us to be able to talk about “controlled processes”
in it: its being monoidal is sufficient, for we can consider morphisms of the form P bX ÑY and
treat the object P as an object of adjustable parameters. Parameterized morphisms of this form
can easily be made to compose: given another morphism Q bY ÑZ, we can straightforwardly
obtain a composite parameterized morphism pQ bPqb X ÑZ, as we elaborate in §3.2.1 below.
Categories of such parameterized morphisms play a central rôle in the compositional modelling
of cybernetic systems[54, 251], where we typically see the parameter as controlling the choice of
process, and understand learning as a ‘higher-order’ process by which the choice of parameter is
adjusted. More concretely, consider the synaptic strengths or weights of a neural network, which
change as the system learns about the world, affecting the predictions it makes and actions it takes;
or consider the process of Bayesian inference, where the posterior is dependent on a parameter
that is typically called the ‘prior’.
In this section, we introduce two related formal notions of parameterization: ‘internal’, where the
parameter object constitutes a part of the domain of morphisms in a category; and ‘external’, where
the parameters remain outside of the category being parameterized and the choice of morphism
is implemented as a morphism in the base of enrichment. We will make use of both kinds of
parameterization in this thesis.
Remark 3.2.1. Parameterization can be understood as introducing a new dimension into a category
of processes. Consequently, the parameterization (either internal or external) of a category will
produce a bicategory. When representing processes graphically, such as when using the string
diagram calculus, this extra dimension becomes particularly explicit, and although we won’t make
use of graphical representations of parameterized processes in this thesis, they are typical in the
applied-categorical literature, particularly in the literature on categorical cybernetics; for example,
see Capucci et al. [54, Fig. 1], Cruttwell et al. [74, pp.1–2], and Capucci [51, Fig. 1].
3.2.1. Internal parameterization
Internal parameterization generalizes the case with which we opened this section, of morphisms
P bX ÑY , to a situation in which the parameterization may have different structure to the
processes at hand, so that the parameterizing objects live in a different category. For this reason, we
describe the ‘actegorical’ situation in which a category of parameters M acts on on a category of
processes C to generate a category of parameterized processes. Nonetheless, even in this case, the
87
parameter ends up constituting part of the domain of the morphism representing the parameterized
process.
The first concept we need is that of an ‘actegory’, which categorifies the better known
mathematical notion of monoid action2.
Definition 3.2.2 (M-actegory). Suppose M is a monoidal category with tensor b and unit object
I. We say thatC is a left M-actegory when there is a functord: MˆC ÑC called the action along
with natural unitor and associator isomorphismsλd
X : I dX „Ý ÑX and ad
M,N,X : pM bNqdX „Ý Ñ
M dpN dXqcompatible with the monoidal structure of pM, b, Iq, in a sense analogous to the
coherence data of a monoidal category (Definition 3.1.4). This means that the following triangle
and pentagon diagrams must commute, where ρ and α are the right unitor and the associator of
the monoidal structure on M.
pM b Iqd C M dpI dCq
M dC
ad
M,I,C
ρM bidC idM dλd
C
pK b pM b Nqqd C K dppM b Nqd Cq
ppK b Mqb Nqd C K dpM dpN dCqq
pK b Mqdp N dCq
ad
K,MbN,C
αK,M,N bidC idK bad
M,N,C
ad
KbM,N,C ad
K,M,NdC
Given an actegory, we can define a category of correspondingly parameterized morphisms.
Proposition 3.2.3 (Capucci et al. [54]). Let pC, d, λd, adqbe an pM, b, Iq-actegory. Then there is
a bicategory of M-parameterized morphisms in C, denoted Parapdq. Its objects are those of C. For
each pair of objects X, Y, the set of 1-cells is defined as ParapdqpX, Yq:“ř
M:M CpM dX, Yq;
we denote an elementpM, fqof this set byf : X MÝÑY . Given 1-cells f : X MÝÑY and g : Y NÝ ÑZ,
their composite g ˝f : X NbMÝ ÝÝÝ ÑZ is the following morphism in C:
pN b Mqd X
ad
N,M,X
Ý ÝÝÝÝ ÑN dpM dXq
idN df
ÝÝÝÝÑN dY
g
Ý ÑZ
2For a comprehensive reference on actegory theory, see Capucci and Gavranović [52].
88
Given 1-cells f : X MÝÑY and f1 : X M1
Ý Ý ÑY , a 2-cell α : f ñf1is a morphism α : M ÑM1in
M such that f “f1˝pα didXqin C; identities and composition of 2-cells are as in C.
And when the action is ‘strong’ and the monoidal structure onC is symmetric, these parameterized
categories inherit a monoidal structure.
Definition 3.2.4. Suppose C is a monoidal category and F : C ÑC is an endofunctor. A right
strength for F is a natural transformation strr
X,Y : F XbY ÑFpX bY qmaking the following
diagrams commute:
F XbpY bZq FpX bpY bZqq
pF XbY qb Z FpX bY qb Z FppX bY qb Zq
αFX,Y,Z
strr
X,Y bidZ strr
XbY,Z
FpαX,Y,Z q
strr
X,Y bZ
F XbI FpX bIq
F X
strr
X,I
F ρXρFX
An action d: M ˆC ÑC induces a family of functors M dp´q : C ÑC, natural in M : M.
If each of these is equipped with a right strength, also natural in M : M, then we call the
resulting transformation strr
M,X,Y a right strength for d. Dually, there are notions of left strength,
strl
X,Y : X bF YÑFpX bY qand costrength, with the latter obtained in the usual way as a
strength in C op (reverse all the defining arrows).
Note that, if C is symmetric monoidal, then a left strength induces a right strength (by swapping)
and likewise a right strength induces a left strength.
Proposition 3.2.5 (Capucci et al. [54, §2.1]). When C is equipped with both a symmetric monoidal
structure pb, Iqand an pM, b, Iq-actegory structure d, and these are compatible in that the action
dhas a strength isomorphism, the symmetric monoidal structure pb, Iqlifts to Parapdq.
The tensor of objects in Parapdqis then defined as the tensor of objects in C, and the tensor of
morphisms (1-cells) f : X MÝÑY and g : A NÝ ÑB is given by the composite
f bg : X bA MbNÝ ÝÝÝ ÑY bB :“ pM bNqdpX bAq
ιM,N,X,A
Ý ÝÝÝÝÝ ÑpM dAqbpN dAq
fbg
ÝÝÑY bB
89
where the interchanger ιM,N,X,A : pM bNqdpX bAq „Ý ÑpM dXqbpN dAqis obtained using
the associator of the actegory structure and the costrengths:
ιM,N,X,A :“pM b Nqdp X bAq
ad
M,N,pXbAq
ÝÝÝÝÝÝÝÑM dpN dpX bAqq ¨¨¨
¨¨¨
Mdcostrl
N,X,A
Ý ÝÝÝÝÝÝÝÝ ÑM dpX bpN dAqq
costrr
M,X,NdA
Ý ÝÝÝÝÝÝÝÝ ÑpM dXqbp N dAq.
(Note that the costrengths are obtained as the inverses of the strengths.)
We can see a monoidal product b: C ˆC ÑC as an action of C on itself, and this induces the
self-parameterization of C.
Proposition 3.2.6 (Self-parameterization). If pC, b, Iqis a monoidal category, then it induces a
parameterization Parapbqon itself. For each M, X, Y: C, the morphisms X MÝÑY of Parapbq
are the morphisms M bX ÑY in C.
Notation 3.2.7. When considering the self-paramterization induced by a monoidal category
pC, b, Iq, we will often write ParapCqinstead of Parapbq.
It will frequently be the case that we do not in fact need the whole bicategory structure. The
following proposition tells us that we can also just work 1-categorically, as long as we work with
equivalence classes of isomorphically-parameterized maps, in order that composition is suffiently
strictly associative.
Proposition 3.2.8. Each bicategory Parapdqinduces a 1-category Parapdq1 by forgetting the
bicategorical structure. The hom sets Parapdq1pX, Yqare given by UParapdqpX, Yq{„ where
U is the forgetful functor U : Cat ÑSet and f „g if and only if there is some 2-cell α : f ñg
that is an isomorphism. We callParapdq1 the 1-categorical truncation of Parapdq. When Parapdq
is monoidal, so is Parapdq1.
Remark 3.2.9. We can understand the 1-categorical truncation ofParapdqas grouping the objects
of each hom-category into their isomorphism-connected components.
3.2.2. External parameterization
In a monoidal closed category, morphisms P bX Ñ Y correspond bijectively to morphisms
P Ñ rX, Ys. The fact that monoidal closed categories are enriched in themselves presents an
opportunity for generalization in a different direction to the actegorical approach taken above:
90
that is, given a category of processes C enriched in E, we can think of an externally parameterized
process from X to Y as a morphism P ÑCpX, Yqin E.
This notion of external parameterization can be operationally more faithful to the structure of
systems of interest, even though in the case of monoidal closed categories it is equivalent. For
example, the improvement of the performance of a system of inference due to learning is often
treated ‘externally’ to the inference process itself: the learning process might proceed by observing
(but not interfering with) the inference process, and updating the parameters accordingly; and, if
treated dynamically, the two processes might be assumed to exhibit a separation of timescales such
that the parameters are stationary on the timescale of inference. We will make such assumptions
when we formalize learning in Chapter 7, and so we will make use of external parameterization.
The definition of external parameterization is simplified by using the ‘slice’ construction.
Definition 3.2.10. Suppose X is an object of a categoryE. We define theslice of E over X, denoted
E{X, as the category of ‘bundles’ over X in E: its objects are morphisms p : A ÑX into X for
any A : E, which we call bundles over X and write as pA, pq. The morphisms f : pA, pqÑp B, qq
in E{X are morphisms f : A ÑB in E such that q ˝f “p, as in the diagram
A B
X
f
p q
.
We therefore define external parameterization using slices over hom objects.
Definition 3.2.11. Given a categoryC enriched in pE, ˆ, 1q, we define theexternal parameterization
PC of C in E as the following bicategory. 0-cells are the objects of C, and each hom-category
PCpA, Bqis given by the slice category E{CpA, Bq. The composition of 1-cells is by composing
in C after taking the product of parameters: given f : Θ ÑCpA, Bqand g : Ω ÑCpB, Cq, their
composite g ˝f is
g ˝f :“Ω ˆΘ
gˆf
ÝÝÑCpB, Cqˆ CpA, Bq ‚Ý ÑCpA, Cq
where ‚is the composition map for C in E. The identity 1-cells are the points on the identity
morphisms in C. For instance, the identity 1-cell onA is the corresponding pointidA : 1 ÑCpA, Aq.
We will denote 1-cells using our earlier notation for parameterized morphisms: for instance,
f : A ΘÝ ÑB and idA : A 1Ý ÑA. The horizontal composition of 2-cells is given by taking their
product.
91
Remark 3.2.12. External parameterization is alternatively obtained as the change-of-enrichment
induced by the covariant self-indexing , the functor E{p´q : E Ñ E-Cat, given on objects by
X ÞÑE{X and on morphisms by the functor induced by post-composition3. A base of enrichment
must a fortiori be a monoidal category, and in this case E{p´qis a lax monoidal functor. A lax
monoidal functor out of the base of enrichment induces a corresponding change-of-enrichment
pseudofunctor4, and P is obtained precisely as the change-of-enrichment induced by E{p´q.
One important consequence of this is that P defines a pseudofunctor P : E-Cat Ñ
pE-Catq-Cat. Note that we take enrichment here to meanweak enrichment, in the sense indicated
by Remark 2.2.13. In the case of locally small categories, where E “Set, this means that P has the
type Cat ÑBicat, as suggested above. (We will discuss the definition of Bicat in §5.2.2, where
we also define pseudofunctors between bicategories.)
Remark 3.2.13. In prior work, this external parameterization has been called ‘proxying’ [53].
We prefer the more explicit name ‘external parameterization’, reserving ‘proxying’ for a slightly
different double-categorical construction to appear in future work by the present author.
Remark 3.2.14. Both internal and external parameterization are jointly generalized by the notion
of locally graded category [172], which can be understood to mean “presheaf-enriched category”.
If M acts on C by d, then the hom category ParapdqpA, Bqis the category of elements of the
presheaf Cp´d A, Bq: Mop ÑSet. Similarly, the hom category PCpA, Bqis the category of
elements of the presheaf E
`
´, CpA, Bq
˘
: E op Ñ Set. We will see in §4.2.1 that the category
of elements construction yields an equivalence between presheaves and categories-of-elements,
and so we may as well consider Parapdqto be enriched in the presheaf category rMop, Sets
and PC to be enriched in rE op, Sets. The phrase “locally graded” indicates that the ‘hom sets’ of
Parapdqand PC are ‘graded’ by the objects of M and E respectively. We learnt about locally
graded categories from Dylan Braithwaite.
3.3. Systems from circuits
The dominant motivation for the use of monoidal categories so far has been in modelling the
compositional structure of processes, on the basis of the observation that processes may generally
3Later, in Definition 4.2.28, we will encounter the contravariant self-indexing, which has the same action on objects but
is given on morphisms by pullback. Whereas the covariant self-indexing is always well-defined, the contravariant
self-indexing is therefore only well-defined in the more restricted situation where E has all pullbacks.
4A pseudofunctor is a kind of ‘weakened’ functor, for which functoriality only needs to hold up to isomorphism; see
Definition 4.2.10.
92
be composed both sequentially and in parallel, and so 1-dimensional category theory alone is
insufficient. The processes for which this kind of structure is most suited are those that exhibit a
flow of information. For example, if we take the morphisms of the category Set as computable
functions, then we see that the corresponding “process theory” is adequate for interpreting diagrams
of the form of §2.1.1.3; and we will encounter in Chapter 4 a process-theoretic framework formalizing
probabilistic graphical models of the kind discussed in §2.1.1.2.
In these monoidal categories, processes are represented by morphisms, with composition used
to connect processes together: the composite of two processes is again a process. However, some
morphisms are purely ‘structural’, implementing the plumbing of information flow—such as copying,
discarding, and swapping—and so these categories somewhat blur the boundary between syntax
and semantics. At the same time, it is strange to think of something like a neural circuit as a
‘process’: although it might reify some process in its behaviour, it is rather a system.
To sharpen the syntax-semantics boundary, one can show that every monoidal category arises
as an algebra for a certain monad. We will make these notions precise in §3.4 below, and here
it will suffice to provide some intuition: the monad defines the syntax, and the algebra supplies
a compatible semantics. Algebra in this sense is a vast generalization of the abstract algebra of
familiar mathematics, and typically involves defining symbolic operations and rules by which they
can be combined, substituted, compared, and reduced.
In this section, although we do not explicitly make use of the technology of monads, we exemplify
this approach with an example of compositional connectomics: on the syntactic side, we will
introduce a ‘multicategory’ of linear circuit diagrams which govern patterns of neural connectivity;
while on the semantic side, we will equip this multicategory with a functorial algebra of rate-coded
neural circuits5. We will find that this more explicitly algebraic approach resolves the dilemma
observed above between the compositional structure of processes and that of systems: algebraic
syntax is in some sense about substitution, and so circuit diagrams will have ‘holes’ into which can
be substituted other circuit diagrams. That is to say, a circuit diagram is a morphism which takes a
given pattern of holes and connects them together into a single circuit, as in the following diagram,
5In the Appendix (§A.1), we sketch the connection between this multicategorical story and the monadic one.
93
which brings us back to our first motivating example from §2.1.1.1 and which we formalize below.
E
I
ÞÑ EI
We will use a similar approach when we supply dynamical semantics for approximate inference,
although there, for our general syntax of systems, we will use categories of polynomial functors,
which we introduce in §3.5 at the end of this chapter. In any case, it will turn out that linear
circuit diagrams embed naturally into polynomials, and so the circuits below can be understood as
providing a sample of what is to come.
3.3.1. Multicategorical algebra for hierarchical systems
A multicategory is like a category, but where morphisms may have a ‘complex’ domain, such as a
list of objects [168]. A morphism whose domain is an n-length list is called ‘n-ary’, and we can
abstractly think of such morphisms as ‘n-ary operations’: for example, we will use them to model
connecting n circuits together into a single system. Because these morphisms effect a kind of
‘zooming-out’, we can use them to construct hierarchical or ‘nested’ systems-of-systems.
Definition 3.3.1. A multicategory O consists of
1. a set O0 of objects;
2. a set O1 of morphisms, equipped with
a) a codomain function cod : O1 ÑO0, and
b) a domain function dom : O1 ÑListpO0q, where ListpO0qis the set of finite lists of
objects po1, . . . , onq,
so that each n-ary morphism f has a list of n objects as its domain and a single object as its
codomain, written f : po1, . . . , onqÑ p;
3. an identity function id : O0 ÑO1 such that codpidoq“ o and dompidoq“p oq, so that the
identity on o is written ido : o Ño;
4. a family of composition functions
˝p,poiq,poj
i q : Opo1, . . . , on; pqˆ Opo1
1, . . . , ok1
1 ; o1qˆ¨¨¨ˆ Opo1
n, . . . , okn
n ; onq
ÑOpo1
1, . . . , ok1
1 , . . . , o1
n, . . . , okn
n ; pq
94
written as
pf, f1, . . . , fnqÞÑ f ˝pf1, . . . , fnq
for each object p, n-ary list objects po1, . . . , onq, and n ki-ary lists of objects po1
i , . . . , oki
i q;
satisfying the equations of associativity
f ˝
`
f1 ˝pf1
1 , . . . , fk1
1 q, . . . , fn ˝pf1
n, . . . , fkn
n q
˘
“
`
f ˝pf1, . . . , fnq
˘
˝pf1
1 , . . . , fk1
1 , . . . , f1
n, . . . , fkn
n q
whenever such composites make sense, and unitality
f ˝pido1, . . . ,idonq“ f “idp ˝f
for every f : po1, . . . , onqÑ p.
For our purposes, the order of objects in the lists will not matter, which we formalize with the
notion of symmetric multicategory—analogous to the symmetric monoidal categories of §3.1.2.
Definition 3.3.2. Let Sn be the symmetric group on n elements. A symmetric multicategory O is a
multicategory O which is additionally equipped, for each n : N, with an action σn of Sn on the set
On
1 of n-ary morphisms
σn : Sn ˆOn
1 ÑOn
1
such that composition ˝preserves this action.
Remark 3.3.3. In other applied-category-theoretical contexts, multicategories of this kind are
sometimes called operads (cf. e.g. [18, 97, 98, 171, 206, 222, 231, 237, 243, 244, 268, 282]). Traditionally,
an operad is the same as a multicategory with one object[168]; sometimes therefore, multicategories
are called coloured or typed operads[17, 59, 98, 168]. In order to avoid confusion, we will stick with
‘multicategory’.
Although the multicategorical intuition—of hierarchically constructing complex systems—is
valuable, the following fact means that there is a close connection between multicategories and
monoidal categories, for in a monoidal category, we can interpret an n-ary tensor x1 b¨¨¨b xn as
an n-ary list of objects.
Proposition 3.3.4. Any monoidal category pC, b, Iqinduces a corresponding multicategory OC.
The objects OC0 are the objects C0 of C. The n-ary morphisms pc1, . . . , cnqÑ d are the morphisms
c1 b¨¨¨b cn Ñ d; i.e., OCpc1, . . . , cn; dq :“ Cpc1 b¨¨¨b cn, dq. Identities are as in C, and
composition is defined by pf, f1, . . . , fnqÞÑ f ˝pf1 b¨¨¨b fnq. When C is symmetric, so is OC.
95
Example 3.3.5. An example that will soon become important is the operad Sets of sets and n-ary
functions, which is obtained from the symmetric monoidal category Set by Sets :“O Set.
As we discussed above, we will consider multicategories as supplying a syntax for the composition
of systems, and so actually to compose systems requires the extra data of what those systems are
and how they can be composed according to the syntax. This extra semantic data is called an
‘algebra’ for the multicategory.
Definition 3.3.6. An algebra for a multicategory M is a multifunctor M ÑSets.
Multifunctors are the multicategorical analogues of functors; but fortunately (even though the
definition is not a hard one), we will not need to define them, owing to the following result, which
relates multifunctors and lax monoidal functors.
Proposition 3.3.7 (Leinster [168, Example 4.3.3, Definition 2.1.12]). If the multicategory M arises
from a monoidal category pC, b, Iqas M “OC, then an algebra for M is determined by a lax
monoidal functor pC, b, IqÑp Set, ˆ, 1q.
Remark 3.3.8. In §3.4, we will encounter the concept of “algebra for a monad”, which is perhaps
the more familiar concept in mathematics and computer science. One might therefore wonder
what the relationship between the two notions of ‘algebra’ is: why do they both have this name?
The answer is provided by Leinster [168]: every ‘shape’ of multicategory corresponds to a certain
monad; and every multicategory algebra corresponds to an algebra for a monad derived (in the
context of the particular multicategory at hand) from this multicategory-shape monad. For the
interested reader, we review these results in the Appendix (§A.1). In §3.3.2, we will exemplify the
notion of monad algebra with the more basic result that every small category corresponds to an
algebra for a certain monad. Monad algebras will also prove useful later in the thesis in the context
of compositional probability theory.
3.3.2. Linear circuit diagrams
Let us now exhibit the multicategory formalizing circuit diagrams of the type with which we
opened this section. Although our motivation is multicategorical, for simplicity we will proceed by
defining a symmetric monoidal category. Its objects will represent the ‘output-input’ dimensions of
a circuit, written as pairs of numbers pno, niq, and its morphisms pno, niqÑp mo, miqencode how
to wire a circuit with no outputs and ni inputs together to produce a circuit of mo outputs and mi
inputs: this may involve connecting some of the no outputs to the mo outputs; or connecting some
96
of the mi inputs, or (to allow recurrence) the no outputs, to the ni inputs. The definition may seem
somewhat mysterious at first, but its form is owed to a more abstract structure (lenses) that we will
define later, in §4.2.
Example 3.3.9. We define a symmetric monoidal category
`
LinCirc, `, p0, 0q
˘
of linear circuit
diagrams and consider the induced multicategory OLinCirc. The objects of LinCirc are pairs
pno, niqof natural numbers. A morphism pno, niqÑp mo, miqis a pair of real-valued matrices
pA, Bqwith A of shape pmo, noqand semi-orthogonal (i.e., such that AAT “1mo) and B of shape
pni, no `miq; equivalently, A is a semi-orthogonal linear map Rno ÑRmo and B is a linear map
Rno`mi Ñ Rni. The identity morphism idpno,niq on pno, niqis the pair of matrices p1no, 01noq
where 01no is the block matrix
`
0no 1no
˘
. Given morphisms pA, Bq: pno, niqÑp mo, miqand
pA1, B1q: pmo, miqÑp ko, kiq, their composite is the pair pA1A, BB1
Aqwhere A1A is the usual
matrix product and BB1
A is defined as the following block matrix multiplication:
BB1
A :“B
ˆ1no 0
0 B1
˙¨
˝
1no 0
A 0
0 1 ki
˛
‚
Unitality and associativity of composition follow from those properties of matrix multiplication,
and AA1is easily seen to be semi-orthogonal (by AA1pAA1qT “AA1A1T AT “AAT “1mo), so
LinCirc is a well-defined category.
We now turn to the monoidal structure. The monoidal unit is the pair p0, 0q; note that Ro –1.
The monoidal product `is defined on objects as the pointwise sum: pno, niq`p mo, miq:“pno `
mo, ni `miq; note that Rno`mo –Rno ˆRmo. Given morphisms pA, Bq: pno, niqÑp mo, miq
and pA1, B1q: pn1
o, n1
iqÑp m1
o, m1
iq, their monoidal product pA, Bq`p A1, B1qis defined as the
pair pA ‘A1, B‘B1q: pno `n1
o, ni `n1
iqÑp mo `m1
o, mi `m1
iqwith
A ‘A1 :“
ˆA 0
0 A1
˙
and B ‘B1 :“
ˆB 0
0 B1
˙
¨
˚˚˝
1no 0 0 0
0 0 1 mi 0
0 1 n1o 0 0
0 0 0 1 m1
i
˛
‹‹‚.
For each pair of objects pno, niq and pmo, miq, the symmetry σpno,niq,pmo,miq : pno, niq `
pmo, miqÑp mo, miq`p no, niqis defined as the pair of matrices pσo
n,m, σi
n,mq,
σo
n,m :“
ˆ 0 1 mo
1no 0
˙
and σi
n,m :“
ˆ0 0 0 1 ni
0 0 1 mi 0
˙
.
That this definition produces a well-defined symmetric monoidal structure follows from more
abstract considerations that we explain in Remark 4.2.33 and Corollary 4.2.36: LinCirc is a
subcategory of Cartesian lenses, with the monoidal structure inherited accordingly.
97
The category of linear circuit diagrams is a syntactic category: on its own, it does not do anything.
We need to equip it with semantics.
3.3.3. An algebra of rate-coded neural circuits
We begin by defining a notion of ‘rate-coded’ neural circuit.
Definition 3.3.10. An no-dimensional rate-coded neural circuit with ni-dimensional input is an
ordinary differential equation
9x “´λ dx `h
`
Wpx ‘iq; α, β, γ
˘
where x, λ, α, β, γare real vectors of dimension no, i a real vector of dimension ni, W a real
matrix of shape pno, no `niq, delementwise multiplication, ‘the direct sum (so that x ‘i is the
concatenation
ˆx
i
˙
), and h the logistic function
hpx; α, β, γq“ γ
1 `exp
`
´βpx ´αq
˘
applied elementwise. We summarize the data of such a circuit as the tuple pλ, α, β, γ, Wq.
Remark 3.3.11. Rate-coded neural circuits are a coarse phenomenological model of neural
dynamics. The state variable x represents the firing rates of an ensemble of neurons, either
averaged over time or over subpopulations. Neural activity is of course not so simple: neurons
communicate by the transmission of discrete ‘action potentials’ along their axons. The emission of
an action potential is governed by the electrical potential of its cellular membrane: if this potential
crosses a threshold, then the neuron ‘fires’ an action potential down its axon. The axon crosses the
dendrites of other neurons at junctions called synapses, which modulate and transmit the activity
accordingly: it is these afferent signals which in large part determine the neurons’ membrane
potentials.
There are of course detailed physiological models of this process (cf. e.g. [79, 133, 181, 226]), as
well as many models which aim to capture its statistics and phenomenology in a more explicitly
computational setting (cf. e.g. [81, 82, 117, 136, 146, 202, 227, 273]), but in some situations, one
can simply model neural firing as an inhomogeneous Poisson process: in this case the variable
x encodes the rate parameters of the processes. We expect there to be functorial connections
between the different classes of models: in particular, we expect adjoint functors between certain
spike-emission models and firing rate models of the class defined above; and in the specific case of
98
‘efficient balanced’ networks[32, 82], the relationships are expected to be quite simple. Nonetheless,
we leave the exploration of such functors to future work.
The parameters of a rate-coded neural circuit—the terms λ, α, β, γ, W— have a neurological
interpretation, even though this dynamical model is not physiologically faithful. The term λ
represents the ‘leak’ of voltage from the neuron’s membrane, which has the effect of determining
the timescale of its memory or signal-sensitivity (effectively, the voltage leak entails a process of
filtering). The term α represents an abstraction of the neuron’s firing threshold, and the term β its
sensitivity (i.e., how much its firing rate increases with incoming signals); the term γ determines
the maximum firing rate of the neuron (and is typically normalized to 1). Finally, the matrix W
records the strengths of the synaptic connections within the circuit: positive coefficients represent
excitatory connections, while negative coefficients represent inhibitory connections.
Rate-coded neural circuits can be organized into complex ‘hierarchical’ systems using linear
circuit diagrams: the linear connectivity of the diagrams is used to define the synaptic connection
matrix of the complex, algebraically. The proof that the following construction does actually
constitute an algebra ensures that composing systems from circuits using diagrams is predictably
well-behaved, as we will subsequently exemplify.
Proposition 3.3.12 (Algebra of rate-coded neural circuits). There is a LinCirc-algebra pR, µ, ϵq:
`
LinCirc, `, p0, 0q
˘
Ñ pSet, ˆ, 1qof rate-coded neural circuits. On objects pno, niq, define
Rpno, niqto be the set of no-dimensional rate-coded neural circuits with ni-dimensional input.
Then, given a linear circuit diagram pA, Bq: pno, niqÑp mo, miq, define a function
RpA, Bq: Rpno, niqÑ Rpmo, miq
pλ, α, β, γ, WqÞÑp Aλ, Aα, Aβ, Aγ, WABq
where WAB is the following block matrix product:
WAB :“AW
ˆ1no 0
0 B
˙¨
˝
1no 0
1no 0
0 1 mi
˛
‚
ˆAT 0
0 1 mi
˙
.
The laxator µ is defined componentwise as the family of functions
µpno,niq,pmo,miq : Rpno, niqˆ Rpmo, miqÑ R
`
pno, niq`p mo, miq
˘
taking a pair of circuits pλ, α, β, γ, Wq : Rpno, niqand pλ1, α1, β1, γ1, W1q : Rpmo, miqto the
circuit pλ ‘λ1, α‘α1, β‘β1, γ‘γ1, W W1qwhere x ‘y is again the direct sum
ˆx
y
˙
and where
99
the matrix W W1is defined as
W W1 :“
ˆW 0
0 W1
˙
¨
˚˚˝
1no 0 0 0
0 0 1 ni 0
0 1 mo 0 0
0 0 0 1 mi
˛
‹‹‚.
The unitor ϵ is the isomorphism ϵ : 1 „Ý ÑRp0, 0q.
Proof. We need to check that R is a lax monoidal functor, and begin by verifying functoriality.
So suppose pA1, B1qis a linear circuit diagram pmo, miqÑp ko, kiq. On the terms λ, α, β, γ, the
functoriality of R is immediate from matrix multiplication, so we concentrate on the action of
R on W. We need to show that R
`
pA1, B1q˝p A, Bq
˘
pWq “RpA1, B1q˝ RpA, BqpWq, where
RpA, BqpWq“ WAB as defined above. Note that we can alternatively writeWAB as the following
composite linear map
mo `mi
AT `mi
Ý ÝÝÝÝ Ñno `mi
`mi
ÝÝÝÝÑno `no `mi
no`BÝ ÝÝÝ Ñno `ni
WÝÑno
AÝ Ñmo .
We can therefore write RpA1, B1qpWABqas
ko `ki
A1T `ki
Ý ÝÝÝÝ Ñmo `ki
AT `ki
ÝÝÝÝÑno `ki
`ki
Ý ÝÝÝ Ñno `no `ki ¨¨¨
¨¨¨
no` `ki
ÝÝÝÝÝÝÑno `no `no `ki
no`no`A`ki
ÝÝÝÝÝÝÝÝÑno `no `mo `ki ¨¨¨
¨¨¨ no`no`B1
Ý ÝÝÝÝÝÝ Ñno `no `mi
no`BÝ ÝÝÝ Ñno `ni
WÝÑno
AÝ Ñmo
A1
ÝÑko
and R
`
pA1, B1q˝p A, Bq
˘
pWqas
ko `ki
A1T `ki
Ý ÝÝÝÝ Ñmo `ki
`ki
Ý ÝÝÝ Ñmo `mo `ki
mo`B1
ÝÝÝÝÑmo `mi
AT `mi
Ý ÝÝÝÝ Ñ ¨¨¨
¨¨¨ no `mi
`mi
ÝÝÝÝÑno `no `mi
no`BÝ ÝÝÝ Ñno `ni
WÝÑno
AÝ Ñmo
A1
ÝÑko
so it suffices to check that
mo `ki
AT `ki
ÝÝÝÝÑno `ki
`ki
Ý ÝÝÝ Ñno `no `ki
no` `ki
ÝÝÝÝÝÝÑno `no `no `ki ¨¨¨
¨¨¨ no`no`A`ki
ÝÝÝÝÝÝÝÝÑno `no `mo `ki
no`no`B1
Ý ÝÝÝÝÝÝ Ñno `no `mi
“
mo `ki
`ki
Ý ÝÝÝ Ñmo `mo `ki
mo`B1
ÝÝÝÝÑmo `mi
AT `mi
Ý ÝÝÝÝ Ñno `mi
`mi
ÝÝÝÝÑno `no `mi
100
which we can do using the graphical calculus:
AT
A B1
mo
ki
no
no
mi
(1)
“
AT
A B1
AT
mo
ki
no
no
mi
¨¨¨
¨¨¨
(2)
“
AT
A B1
ATmo
ki
no
no
miAT
(3)
“
AT
B1
ATmo
ki
no
no
mi
¨¨¨
¨¨¨
(4)
“
AT
B1
ATmo
ki
no
no
mi
(5)
“
B1
AT
mo
ki
no
no
mi
where the equality (1) holds because AT is a comonoid morphism (Definition 3.4.23)6, (2) likewise,
(3) because A is semi-orthogonal, (4) by the coassociativity of copying, and (5) again because AT is
a comonoid morphism. Finally, we observe that the last string diagram depicts the linear map
mo `ki
`ki
Ý ÝÝÝ Ñmo `mo `ki
AT `B1
ÝÝÝÝÑno `mi
`mi
ÝÝÝÝÑno `no `mi
which equals the required map
mo `ki
`ki
Ý ÝÝÝ Ñmo `mo `ki
mo`B1
ÝÝÝÝÑmo `mi
AT `mi
Ý ÝÝÝÝ Ñno `mi
`mi
ÝÝÝÝÑno `no `mi
by the unitality of composition. This establishes that R preserves composites; it remains to check
that it preserves identities. Once again, this follows immediately on the terms λ, α, β, γ, so we
concentrate on the action on W. We have
Rp1no, 01noqpWq“ 1noW
ˆ1no 0 0
0 0 1 no
˙¨
˝
1no 0
1no 0
0 1 mi
˛
‚
ˆ1no 0
0 1 mi
˙
which is easily seen to be equal to W itself. Therefore R defines a functor.
We now need to verify that the unitor and laxator satisfy the unitality and associativity axioms of a
lax monoidal functor. We begin by checking associativity, so suppose that we are given three circuits:
6This in turn because ‘is the Cartesian product, and so every morphism is a ‘-comonoid morphism.
101
pλ, α, β, γ, Wq : Rpno, niq, and pλ1, α1, β1, γ1, W1q : Rpmo, miq, and pλ2, α2, β2, γ2, W2q :
Rpko, kiq. Associativity on all the terms but W, W1, W2 follows from the associativity of the
direct sum ‘, and so we just need to check that µpW, µpW1, W2qq“ µpµpW, W1q, W2qwhere
µpW, W1q“ W W1and µpW1, W2q“ W1W2, according to the definition above. Once more, we
use the graphical calculus. Observe that we can depict W W1and W1W2as
W1
W
no
mo
mi
ni
mo
no
and
W2
W1 mo
ko
ki
mi
ko
mo
respectively. Hence µpW, µpW1, W2qqsatisfies the equality
µpW, µpW1, W2qq “
W
no
W1W2
mo
ko
ki
mi
ko
mo
no
ni “
W
no
ki
mi
ko
mo
no
ni
W2
W1 mo
ko
and likewise µpµpW, W1q, W2qsatisfies
µpµpW, W1q, W2q “
W W1
no
W2
mo
ko
ki
mi
ko
mo
no
ni “
W
no
ki
mi
ko
mo
no
ni
W2
W1 mo
ko
.
The two diagrams on the right hand side are equal up to a topological deformation, and so the
depicted morphisms are equal by the coherence theorem for monoidal categories. This establishes
the associativity of the laxator. It remains to establish unitality: but this follows immediately,
because Rp0, 0q– R0 and the 0-dimensional space is the unit for the direct sum‘. Hence pR, µ, ϵq
is a lax monoidal functor, and hence an algebra for
`
LinCirc, `, p0, 0q
˘
.
Remark 3.3.13. At points in the preceding proof, we used the fact that a linear map is a comonoid
morphism, which implies that it commutes with copying. We will define the notion of comonoid
102
morphism in §3.4.1 below; meanwhile, the fact that AT is one follows from the fact that ‘is the
categorical product of vector spaces, and so every linear map is a ‘-comonoid morphism.
Remark 3.3.14. Let us return briefly to the distinction made at the beginning of this section
between processes and systems, and their corresponding categorical incarnations. One might be
tempted to try constructing a symmetric monoidal category of neural circuits using this algebra
whose objects would be natural numbers and whose morphisms i Ñ o would be circuits in
Rpo, iq, treated thus as ‘processes’. But this won’t work, because there is no neural circuit that
will function as an identity morphism! Later in the thesis (§6.3), we will see one way around
this problem, building monoidal categories of hierarchical dynamical systems that are in some
sense analogous to these circuits (while being more general): there, we will use the rich structure
of polynomial functors to define both the syntax of composition as well as the hom categories
(for our construction will be bicategorical) of dynamical systems, and the extra generality will
mean we will have identity systems (that compose appropriately unitally). Until then, we note
that the moral of this observation might be that it affirms that the composition of neural circuits is
multicategory-algebraic (formalizing a notion of hierarchy), rather than merely categorical.
The weight matrices resulting from the linear circuit algebra encode the pattern of connectivity
specified by the diagram, as we now exemplify.
Example 3.3.15. Let us consider the circuit example from the beginning of this section, the wiring
of an inhibitory circuit to an excitatory circuit, as in the diagram
E
I
ÞÑ EI
which depicts a linear circuit diagram E `I ÑEI. In such a diagram, the input dimension of
an object (such as E) must have dimension equalling the sum of the dimensions of the incoming
wires. Dually, the dimension along a wire emanating from an object must have dimension equal to
the output dimension of that object. To distinguish the source and target of a wire, we decorate the
target ends: a filled circle denotes an inhibitory connection, interpreted in the linear circuit as the
negative identity matrix ´1 of the appropriate dimension; and an inverted arrowhead denotes an
excitatory connection, interpreted as the positive identity 1 of the appropriate dimension. We will
write the dimensions of the objectE as poE, iEq, of I as poI, iIq, and of EI as poEI , iEI q. Therefore,
103
in this example, the following equalities must hold: iE “oI `iEI ; iI “oE; and oEI “oE `oI.
The last equation holds because the circuit EI is formed from the sum of the circuits E and I.
To give a circuit diagram pA, Bq: poE, iEq`p oI, iIqÑp oEI , iEI qis to give a semi-orthogonal
real matrix A of shape poEI , oE `oIqand a real matrix B of shape piE `iI, oE `oI `iEI q. Using
the preceding equalities, these are equivalently shaped as poE `oI, oE `oIqand poI `iEI `
oE, oE `oI `iEI q, and we just choose the identity matrix 1oE`oI for A. To define B, we read it
off from the diagram as
B :“
¨
˝
0 ´1oI 0
0 0 1 iEI
1oE 0 0
˛
‚.
Now suppose pλE, α,E , βE, γE, WEqand pλI, α,I , βI, γI, WIqare two rate-coded neural circuits,
the former of type RpoE, iEqand the latter of type RpoI, iIq. How does RpA, Bqact upon them to
give our composite circuit?
On all the parameters but the weight matrices, RpA, Bqacts trivially (since A is just the identity
matrix), and so we will concentrate on the action onWE, WI. Firstly, we need to form the monoidal
product of the weight matrices, µpWE, WIq, which is defined by
µpWE, WIq“
ˆWE 0
0 WI
˙
¨
˚˚˝
1oE 0 0 0
0 0 1 iE 0
0 1 oI 0 0
0 0 0 1 iI
˛
‹‹‚
“
ˆWE 0
0 WI
˙
¨
˚˚˚˚˝
1oE 0 0 0 0
0 0 1 oI 0 0
0 0 0 1 iEI 0
0 1 oI 0 0 0
0 0 0 0 1 oE
˛
‹‹‹‹‚
where the second equality holds by applying the equalities between the dimensions defined above.
The weight matrix RpA, BqpµpWE, WIqqis then defined as
AµpWE, WIq
ˆ1oE`oI 0
0 B
˙¨
˝
1oE`oI 0
1oE`oI 0
0 1 iEI
˛
‚
ˆAT 0
0 1 iEI
˙
.
Since A “1oE`oI , and by substituting the definition of µpWE, WIq, this reduces to
ˆWE 0
0 WI
˙
¨
˚˚˚˚˝
1oE 0 0 0 0
0 0 1 oI 0 0
0 0 0 1 iEI 0
0 1 oI 0 0 0
0 0 0 0 1 oE
˛
‹‹‹‹‚
ˆ1oE`oI 0
0 B
˙¨
˝
1oE`oI 0
1oE`oI 0
0 1 iEI
˛
‚.
104
Then, by substitution and matrix multiplication, we have the following equalities:
¨
˚˚˚˚˝
1oE 0 0 0 0
0 0 1 oI 0 0
0 0 0 1 iEI 0
0 1 oI 0 0 0
0 0 0 0 1 oE
˛
‹‹‹‹‚
ˆ1oE`oI 0
0 B
˙¨
˝
1oE`oI 0
1oE`oI 0
0 1 iEI
˛
‚
“
¨
˚˚˚˚˝
1oE 0 0 0 0
0 0 1 oI 0 0
0 0 0 1 iEI 0
0 1 oI 0 0 0
0 0 0 0 1 oE
˛
‹‹‹‹‚
¨
˚˚˚˚˝
1oE 0 0 0 0
0 1 oI 0 0 0
0 0 0 ´1oI 0
0 0 0 0 1 iEI
0 0 1 oE 0 0
˛
‹‹‹‹‚
¨
˚˚˚˚˝
1oE 0 0
0 1 oI 0
1oE 0 0
0 1 oI 0
0 0 1 iEI
˛
‹‹‹‹‚
“
¨
˚˚˚˚˝
1oE 0 0 0 0
0 0 1 oI 0 0
0 0 0 1 iEI 0
0 1 oI 0 0 0
0 0 0 0 1 oE
˛
‹‹‹‹‚
¨
˚˚˚˚˝
1oI 0 0
0 1 oI 0
0 ´1oI 0
0 0 1 iEI
1oE 0 0
˛
‹‹‹‹‚
“
¨
˚˚˚˚˝
1oE 0 0
0 ´1oI 0
0 0 1 iEI
0 1 oI 0
1oE 0 0
˛
‹‹‹‹‚
so that the resulting weight matrix RpA, BqpµpWE, WIqqis
ˆWE 0
0 WI
˙
¨
˚˚˚˚˝
1oE 0 0
0 ´1oI 0
0 0 1 iEI
0 1 oI 0
1oE 0 0
˛
‹‹‹‹‚
.
Reading off this weight matrix, we see that the E neurons receive external input plus recurrent
excitatory input from themselves as well as inhibitory input from I, and that the I neurons receive
only recurrent excitatory input plus excitatory input from E. This is exactly as it should be,
given the diagram: by formalizing these computations, we render them mechanical (and hence
computer-implementable). In particular, we can treat the resulting EI circuit as a “black box” and
substitute it into other diagrams to construct still larger-scale systems.
Since linear circuit diagrams allow for any linear pattern of connectivity, we can of course
generalize the picture above to allow for more subtle interconnections.
Example 3.3.16. Suppose that instead of incorporating only excitatory or inhibitory connections,
105
we sought something a little more complex, as in the following circuit diagram:
E
I
DC ÞÑ EI
Now, we have decorated the wires with fleches, to indicate the flow of activity; and besides the
circular boxes (representing circuits), we have incorporated square boxes (representing linear
patterns of interconnection). Using the same notation for the dimensions of the circuits E,I and
EI as in Example 3.3.15, this means that the boxes square boxes represent matrices C of shape
piE `iI, n`iEI qand D of shape pn, oE `oIq, where n is the dimension of the D-C wire. If we
again write pA, Bqfor the implied circuit diagram, and we can again setA to be the identity matrix,
and read B from the diagram as the composite matrix
B :“oE `oI `iEI
D‘1iEI
Ý ÝÝÝÝ Ñn `iEI
CÝ ÑiE `iI .
The rest of the calculation follows mechanically, just as before.
One feature missing from the construction in this section is synaptic plasticity: although we have
shown how to compose circuits into systems, it is only the neural firing rates that are dynamical;
the connection matrices remain fixed. In the preceding section, we motivated the introduction of
parameterized categories by their application to learning problems, and indeed one could factorize
the linear circuit algebra above by extracting the connection matrices into parameters; if one
wanted to retain a choice of initial weight matrix, this could also be incorporated into a ‘pointed’
version of the structure.
This parameterized construction would be bicategorical, and so a faithful semantics for it
would no longer land in Set, but rather in Cat: we would have categories of circuits related by
reparameterizations of the weight matrices, and with the dynamics also incorporating plasticity7.
With a sufficiently sophisticated algebra, it would even be possible to allow the circuit diagrams
themselves to be dynamical and subject to learning. We will not pursue this line of enquiry further
here, but we will return to it when we introduce plasticity into approximate inference doctrines:
there, our structures will be sufficiently supple to incorporate all of the concepts sketched here.
7An even more faithful dynamical semantics would land in “bundle dynamical systems”, of the form that we introduce
in Chapter 6: two two levels of the bundle would witness the dynamics of the firing activity and the plasticity, and
the bundles themselves would witness the timescale separation.
106
3.4. From monoids to monads
In order to reach the level of suppleness required by plastic dynamical approximate inference, it will
help to understand the structures underlying the definitions and constructions introduced so far in
this chapter—in particular, we will need a firm grasp of the concepts of monad and comonoid—and
so at this point we return to an expository mode.
The fundamental concept underlying many of the structures we have seen so far is the monoid:
an object equipped with two operations, one binary and one ‘nullary’, with the latter acting as a
‘unit’ for the former, and although the major operation is only binary, it can be chained in order to
form n-ary operations. For this reason, monoids are fundamental to abstract algebra: categories
themselves are “monoids with many objects” (in the same way that a multicategory is an operad
with many objects). Both monads and comonoids can be defined using monoids.
Even though monoids are fundamental and intimately familiar to mathematicians and computer
scientists, they remain underappreciated in computational and cognitive neuroscience. For this
reason, we once again take a fairly pedagogical approach in this section.
Definition 3.4.1. Suppose pC, b, Iqis a monoidal category. A monoid object in C is an object m
equipped with a multiplication morphism µ : m bm Ñ m and a unit morphism η : I Ñ m,
satisfying the axioms of (left and right) unitality
““
ηη
µµ
m
and associativity
µ
µ
µ
µ
“ .
If C is symmetric monoidal then we say that the monoid pm, µ, ηqis commutative if µ commutes
107
with the symmetry as in
“µ µ
.
Since we are doing category theory, it is important to understand morphisms of monoids.
Definition 3.4.2. Suppose pm, µ, ηqand pm1, µ1, η1qare monoids in pC, b, Iq. A monoid morphism
pm, µ, ηqÑp m1, µ1, η1qis a morphism f : m Ñm1 in C that is compatible with the monoidal
structures, i.e. by satisfying the axioms
η
f
η1
“ and
µ
fµ1
f f
“ .
Monoids and their morphisms in C constitute a category MonpCq; composition and identities are
as in C, and it is easy to check that the composite of two monoid morphisms is again a monoid
morphism.
If C is symmetric monoidal, then there is a subcategory CMonpCqãÑMonpCqof commutative
monoids and their morphisms.
In the names MonpCqand CMonpCq, we leave the monoidal structure implicit; should it be
necessary to be explicit, we write MonbpCqand CMonbpCq.
Let us consider some first examples of monoids in monoidal categories.
Example 3.4.3. The natural numbers N equipped with addition ` : N ˆN Ñ N and zero 0
constitute a monoid in Set. (In fact, pN, `, 0qis the free monoid generated by a single element.)
Example 3.4.4. If A is a set, then there is a monoid
`
ListpAq, ˝, pq
˘
of lists of elements of A: the
elements of the setListpAqare finite listspa, b, . . .qof elements ofA; the multiplication˝: ListpAqˆ
ListpAqÑ ListpAqis given by concatenation of lists pb1, . . .q˝p a1, . . .q“p a1, . . . , b1, . . .q; and
the unit 1 ÑListpAqis given by then empty list pq. We saw in the proof of Proposition 2.1.9 that
list concatenation is associative and unital.
108
Example 3.4.5. A monoid pm, ˝, ˚qin Set is a category with a single object, denoted ˚. We
already saw an example of this, in Example 2.1.7: the monoid pN, `, 0q, treated as a category. More
generally, a monoid in a monoidal category pC, b, Iqis a C-enriched category with a single object.
Example 3.4.6. A monoid pC, b, Iqin the monoidal category pCat, ˆ, 1qof categories and
functors is a strict monoidal category: the tensor is the monoid multiplication, and its unit is the
monoid unit. In fact, this explains the name “monoidal category”: a (strict) monoidal category is a
monoid object in Cat.
Remark 3.4.7. Non-strict monoidal categories are ‘weak’ in the same sense that bicategories
are weak 2-categories; after all, a monoidal category is a one-object bicategory. In this way, we
can also weaken the notion of monoid object in a bicategory, so that the axioms of unitality and
associativity only hold up to ‘coherent isomorphism’: that is, up to isomorphisms that cohere with
the weak unitality and associativity of the ambient bicategory. Such weak monoid objects are
called pseudomonoids8, and when interpreted in the monoidal 2-category pCat, ˆ, 1qtheir formal
definition[77, §3] yields exactly the non-strict monoidal categories.
But note that to make sense in general of the notion of pseudomonoid, we first need to have a
notion of monoidal bicategory. Abstractly, such a thing should be a one-object tricategory, but this
often doesn’t help: in those cases, we need something more concrete. Informally, then, a monoidal
bicategory is a bicategory equipped with a monoidal structure that is coherent with the 2-cells, but
as we have begun to see here, to specify all this coherence data quickly becomes quite verbose,
and to prove their satisfaction by any given structure quite arduous, so we will only make use
informally in this thesis of the notions of monoidal bicategory and pseudomonoid — and when we
do, it will be by reference to the familiar structures on and inCat: its Cartesian monoidal structure;
and (non-strict) monoidal categories.
Finally, we note that the general phenomenon, of which we observe an instance here, wherein
algebraic structures (such as monoids) may be defined internally to categories equipped with
higher-dimensional analogues of that same structure is known as the microcosm principle [17].
In Example 3.1.9, we saw that categories of endofunctors are strict monoidal categories. Following
Example 3.4.6, this means that endofunctor categories are equivalently monoid objects. In fact,
since categories are monoids with many objects 9, this means we can consider any object of
endomorphisms as an appropriately typed monoid object.
8One often uses the prefix ‘pseudo-’ in category theory to denote a weak structure.
9This pattern—of extending structures to “many objects”—is sometimes called horizontal categorification , to distinguish
it from the ‘vertical’ categorification of adding an extra dimension of morphism.
109
Example 3.4.8. If c : C is any object in any category C, then the hom-set Cpc, cqis a monoid
`
Cpc, cq, ˝, idc
˘
in Set. More generally, if C is enriched in E, then
`
Cpc, cq, ˝, idc
˘
is a monoid in E.
In each case, we call the monoid the endomorphism monoid on c.
In the case when the endomorphism objects are categories, as in the case of Example 3.1.9, the
monoidal structure makes them into monoidal categories, and so we can consider monoids objects
defined internally to them. More generally, we can do this inside any bicategory, and the resulting
monoids will play an important rôle subsequently.
Remark 3.4.9. Just as a monoidal category is a bicategory with a single object, the hom-category
Bpb, bqfor any 0-cellb in a bicategoryB is a monoidal category: the objects are the 1-cellsb Ñb, the
morphisms are the 2-cells between them, composed vertically; the tensor is horizontal composition
of 1-cells, and its unit is the identity 1-cell idb. We can therefore define a monoid in a bicategory B
to be a monoid in Bpb, bqfor some 0-cell b : B, using this induced monoidal structure.
Since Cat is morally a 2-category (and a fortiori a bicategory), and thus to avoid confusion with
monoid objects in pCat, ˆ, 1q(i.e. strict monoidal categories), we will introduce a new term for
monoids in the bicategory Cat.
Definition 3.4.10. A monad on the category C is a monoid object in the strict monoidal category
pCC, ˝, idCq.
Monads are often defined in a more explicit way, by expressing the monoid structures and axioms
directly and diagrammatically.
Proposition 3.4.11. A monad on C is equivalently a triple pT, µ, ηqof
1. a functor T : C ÑC;
2. a natural transformation µ : T TñT called the multiplication; and
3. a natural transformation η : idC ñT called the unit;
such that, for all c : C, the following diagrams commute:
T T T c T T c
T T c T c
µTc
T µc
µc
µc
and
T c T T c T c
T c
µc
ηTc T ηc
110
A monad is like a monoidal structure for composition: instead of taking two objects and
constructing a single object representing their conjunction (like the tensor of a monoidal category),
a monad takes two levels of nesting and composes them into a single level; this is the source of the
connection between multicategory algebras and monad algebras.
Example 3.4.12. Recall the list monoid from Example 3.4.4. The mapping A ÞÑListpAqdefines
the functor part of a monad List : Set ÑSet; given a function f : A ÑB, Listpfq: ListpAqÑ
ListpBqis defined by applying f to each element of the lists: pa1, a2, . . .qÞÑp fpa1q, fpa2q, . . .q.
The monad multiplication µ : List2 ñList is given by “removing inner brackets” from lists of lists:
µA
`
pa1
1, a2
1, . . .q, pa1
2, . . .q, . . .
˘
“pa1
1, a2
1, . . . , a1
2, . . . , . . .q; equivalently, form the perspective of
Example 3.4.4, this is the concatenation of the ‘inner’ lists into a single list. The monad unit
η : idSet ñList is defined by returning ‘singleton’ lists: ηA : A ÑListpAq: a ÞÑpaq.
There is a close connection between monads and adjunctions: every adjunction induces a monad.
Proposition 3.4.13. Suppose L %R : D ÑC is an adjunction, with unit η : idC ñRL and
counit ϵ : LR ñidD. Then pRL, RϵL, ηqis a monad.
Proof. To see that the associativity axiom is satisfied, observe that RϵLRL “RLϵRL “RLRϵL
by naturality. Right unitality follows by the triangle identity ϵL ˝Lη “idL, which entails the
required equation RϵL ˝RLη “idRL; and left unitality follows from right unitality by naturality,
as ηRL “RLη.
It is also true that every monad arises from an adjunction: in fact, there are typically multiple
adjunctions inducing the same monad, and we will exhibit one extremal case in §4.1.
Remark 3.4.14. This dual correspondence is itself an example of an adjunction—in the quite
general bicategorical sense, following the definition of monad as a monoid in a bicategory—though
we leave the demonstration of this to the reader.
Before we show in generality how every monad arises from an adjunction, we can exhibit the
list monad as a classic special case.
Example 3.4.15 (Lists are free monoids). There is a forgetful functor U : MonpSetq ÑSet,
taking each monoid pM, ˝, ˚q(or monoid morphism f) and forgetting the monoid structure to
return just the set M (or the morphism f). This functor has a left adjoint F : Set ÑMonpSetq,
which takes each set A to the free monoid on A; this free monoid FpAqis precisely the monoid
111
`
ListpAq, ˝, pq
˘
of lists in A, equipped with concatenation as multiplication and the empty list as
unit, as described in Example 3.4.4. The induced monad pList, µ, ηq, described in Example 3.4.12, is
then precisely the monad induced by this adjunction, with List “UF .
At this point, with an example of a monad to hand, we can start to explore their connection to
algebra.
Definition 3.4.16. Suppose pT, µ, ηqis a monad on C. A T-algebra is a choice of object A : C and
a morphism a : T AÑA such that the following diagrams commute:
A T A
A
ηA
a and
T T A T A
T A Aa
aµA
T a
Once again, this being category theory, we are interested less in individual T-algebras than in
their category.
Definition 3.4.17. A morphism of T-algebras pA, aqÑp B, bqis a morphism f : A ÑB that
preserves the T-algebra structures, in the sense that the following diagram commutes:
T A T B
A B
f
ba
T f
T-algebras and their morphisms constitute a category, denoted AlgpTqand called the category of
T-algebras or the Eilenberg-Moore category for T. (Algebra morphisms compose by the composition
of morphisms; a composite morphism of T-algebras is again a morphism of T-algebras by pasting.
Identities are the usual identity morphisms in C.)
We now demonstrate the ‘algebra’ of monad algebras using two familiar examples.
Example 3.4.18. The category of monoids in pSet, ˆ, 1qis equivalent to the category of List-
algebras. A List-algebra is a pair of a set A and a function a : ListpAqÑ A satisfying the algebra
axioms, which mean that a must map singleton lists to their corresponding elements, and that a
must respect the ordering of elements in the list (so that it doesn’t matter whether you apply a to
the lists in a lists of lists, or to the collapsed list resulting from the monad multiplication). To obtain
a monoid, we can simply take the set A. The monoid multiplication is given by the action of a on
112
2-element lists; and the monoid unit is given by the action of a on the empty list. Since a satisfies
the monad algebra laws, the resulting multiplication and unit satisfy the monoid axioms: the monad
laws are a categorification of the monoid axioms, and the algebra laws ensure compatibility with
them.
Dually, given a monoid pA, m, eq, we can construct a List algebra a by induction: on empty lists,
return e; on singleton lists, return their elements; on 2-element lists, apply m; on lists of length
n, apply m to the first two elements to obtain a list of length n ´1 repeatedly until reaching the
2-element case. The monoid laws then ensure that the monad axioms are satisfied.
Example 3.4.19. Recall from Proposition 2.1.8 that one can obtain from any category a directed
graph by forgetting the compositional structure and retaining only the objects and morphisms as
nodes and edges. Recall also from Proposition 2.1.9 that one can obtain from any directed graph G
a category FG, the free category on G whose objects are the nodes of G and whose morphisms are
paths in G. These two constructions form a free-forgetful adjunction, F %U : Cat ÑGraph,
and the induced monad UF : Graph ÑGraph is called the path monad : on objects, it takes a
graph G and returns a graph with the same nodes but whose edges are the paths inG. The category
AlgpUF qof algebras of UF is equivalent to the category Cat of (small) categories.
To see this, note that a UF -algebra is a graph homomorphism UF G ÑG, for some graph G:
a mapping of nodes in UF G to nodes in G, and a mapping of edges in UF G to edges in G that
preserves domains and codomains. Since UF G and G have the same nodes, the simplest choice is
to map each node to itself: we will consider the nodes as the objects of the resulting category. The
mapping of paths to edges induces a composition operation on the edges ofG, which we henceforth
think of as morphisms. The reasoning proceeds inductively, much like the List-algebra case: we
take paths of length 0 to be identity morphisms; paths of length 1 are taken to their constituent
morphisms; paths of length 2 are taken to their composites; and one obtains the composites of
longer paths by induction. Associativity and unitality then follow easily from the monad algebra
laws.
Remark 3.4.20. Both the preceding examples suggest a connection between monad algebras
and inductive reasoning, and indeed one can formalize inductive reasoning (as inductive types )
in terms of algebras. Dually, there is a close connection between ‘coalgebras’ and ‘coinduction’,
which can be used to formalize the behaviours of systems that can be iterated, such as dynamical
systems. As an informal example, thecoinductive type corresponding to List is the type of “streams”:
113
possibly infinite lists of the states or outputs of transition systems. In Chapter 6, we use coalgebra to
formalize the compositional structure of ‘open’ (i.e., interacting) dynamical systems quite generally.
In the Appendix (§A.1), we pursue the monad algebra story a little further, to demonstrate the
connection with multicategory algebra. However, since that connection is not strictly germane to
the rest of the thesis, and with the suggested notion of coalgebra to whet our appetite, we now
turn to monoids’ duals, comonoids.
3.4.1. Comonoids
We introduced comonoids graphically at the beginning of §3.1.1, as a structural manifestation of
copying and discarding, but in the fullest of generality, comonoids are simply monoids in opposite
categories.
Definition 3.4.21. A comonoid in pC, b, Iqis a monoid in C op, when C op is equipped with the
opposite monoidal structure induced by pb, Iq. Explicitly, this means an object c : C equipped with
a comultiplication δ : c Ñc bc and counit ϵ : c ÑI, satisfying counitality and coassociativity laws
formally dual to the corresponding unitality and associativity laws of monoids: read the diagrams
of Definition 3.4.1 top-to-bottom, rather than bottom-to-top. Likewise, if C is symmetric monoidal,
we say that a comonoid in C is cocommutative if its comultiplication commutes with the symmetry.
Example 3.4.22. Every object in a category with finite products ˆand a terminal object 1 is a
comonoid with respect to the monoidal structure pˆ, 1q. The comultiplications δX : X ÑX ˆX
are defined by the pairing pidX, idXq(recall Definition 2.3.15) and the counits ϵX : X Ñ1 are
(necessarily) the unique morphisms into the terminal object.
Coassociativity follows because pidX, pidX, idXqq“ αX,X,X ˝ppidX, idXq, idXq, where α is the
associator of the product. Counitality follows by the naturality of pairing, pidX ˆ!q˝p idX, idXq“
pidX, !q, and because projX ˝pidX, !q“ idX by the universal property of the product; note that
projX is the X component of the right unitor of the monoidal structure, and pidX, !qis its inverse.
Instantiating this example in Set, we see that the comultiplication is given by copying, i.e.,
x ÞÑ px, xq; and the counit is the unique map x ÞÑ ˚into the singleton set. This justifies our
writing of the comonoid structure in copy-discard style as p X, Xq.
In general, when a comonoid structure is to be interpreted as a copy-discard structure, we will
therefore write the struture morphisms as p , qand depict them accordingly in the graphical
calculus, rather than using the boxed forms of Definition 3.4.1. However, copy-discard structures
114
are not the only important comonoids that we will encounter. In the next section, we introduce
the category of polynomial functors Set ÑSet, and since these are endofunctors, their category
inherits a monoidal structure given by functor composition. Comonoids for this monoidal structure
in Poly give us another definition for a now multifariously familiar concept: they are again small
categories, although their morphisms are not functors but rather cofunctors.
Of course, a morphism of comonoids is much like a morphism of monoids.
Definition 3.4.23. A comonoid morphism f : pc, δ, ϵq Ñ pc1, δ1, ϵ1qin pC, b, Iqis a morphism
f : c Ñc1that is compatible with the comonoid structures, in the sense of satisfying axioms dual
to those of Definition 3.4.2. There is thus a category ComonpCqof comonoids in C and their
morphisms, as well as a subcategory CComonpCqãÑComonpCqof commutatitve comonoids.
In the more familiar copy-discard setting, comonoid morphisms also play an important rôle. In
the next chapter, we will see concretely that, in the context of stochastic maps, comonoid morphisms
(with respect to the tensor product) correspond to the deterministic functions. This result is closely
related to the following fact.
Proposition 3.4.24. If every morphism in the monoidal categorypC, b, Iqis a comonoid morphism,
then a bb satisfies the universal property of the product for every a, b: C, and hence bis the
categorical product and I the terminal object in C (up to isomorphism).
Proof. If every morphism is a comonoid morphism, then every object a : C carries a comonoid
structure; assume a choice of comonoid structure p a : a Ña ba, a : a ÑIqfor every a : C.
The universal property of the product says that every morphism f : x Ña bb factors as
f
a
b
x “
a
b
x fa
fb
where fa : x Ña and fb : x Ñb are uniquely defined as
fa :“ f
a
x and fb :“ f b
x .
Since f is ex hypothesi a comonoid morphism, we have
f
a
b
x “ fx
a
b
“
f
a
f b
x “
a
b
x fa
fb
115
where the first equality holds by counitality, the second since f commutes with x ex hypothesi,
and the third by definition. This establishes that a bb satisfies the universal property, and hence
that bis the categorical product.
To see thatI is the terminal object up to isomorphism, suppose that1 is the terminal object. Since
bis the categorical product, there is an isomorphism a „Ý Ña b1 for any a : C, by the universal
property. In particular, there is an isomorphism I „Ý ÑI b1. But since I is the monoidal unit for b,
the component of the left unitor at 1 is an isomorphism I b1 „Ý Ñ1. Hence we have a composite
isomorphism I „Ý ÑI b1 „Ý Ñ1, and so I –1.
Remark 3.4.25. The preceding proposition gives us another way to look at comonoids: we can
think of them as “products without the universal property”. The reason for this is that, since products
are characterized by their (universal) projections, we can use the counits to define projections for
the monoidal product of comonoids: that is, if a and b are comonoids in C, then we can define
(non-universal) projections a
proja
Ð ÝÝ Ýa bb
projb
Ý ÝÝ Ñb by
a
ρa
ÐÝa bI
ida b b
ÐÝÝÝÝÝa bb
abidb
Ý ÝÝÝÝ ÑI bb
λb
Ý Ñb
where ρ and λ denote the right and left unitors of the monoidal structure respectively. The failure
of universality means that the family of projections tprojaua:C in C does not constitute a natural
transformation.
Remark 3.4.26. Abstractly, we can use naturality as a way to characterize deterministic morphisms:
the naturality law for requires that
a
f
Ý Ñb
b
Ý Ý Ñb bb “ a
a
ÝÝÑa ba
fbf
Ý ÝÝ Ñb bb
and this says that first doing f and the copying its output is the same as copying the input and
feeding each copy into f. If f were non-deterministic, then there would be a correlation between
the copies in the former case but not in the latter, and so this equation would not hold. Therefore,
we can think of those morphisms f for which copying is natural as the deterministic morphisms in
C. We will return to this perspective in Remark 4.1.19.
Finally, there is also a notion of comonad, dual to monad: a comonad is quite generally a comonoid
in a bicategory, in the sense of Remark 3.4.9, or, less generally, a comonoid with respect to the
composition product in a category of endofunctors. This means that the polynomial comonoids we
discussed above are by definition comonads.
116
In Remark 3.4.20, we introduced the notion of ‘coalgebra’, and indeed there is a notion of comonad
coalgebra that is dual to the notion of monad algebra; and indeed we will use coalgebras later
to formalize dynamical systems. But although these coalgebras will be morphisms of the form
F XÑX, for F an endofunctor and X an object, the endofunctor F will not necessarily have a
comonad structure, and so the coalgebras will be more general than the algebras we considered
above: there will be no comonad compatibility axioms to satisfy.
In many cases, the endofunctor F will be a polynomial functor, so let us now introduce these.
3.5. Polynomial functors
In order to be considered adaptive, a system must have something to adapt to. This ‘something’ is
often what we call the system’s environment, and we say that the system is open to its environment.
The interface or boundary separating the system from its environment can be thought of as
‘inhabited’ by the system: the system is embodied by its interface of interaction; the interface
is animated by the system. In this way, the system can affect the environment, by changing the
shape or configuration of its interface10; through the coupling, these changes are propagated to the
environment. In turn, the environment may impinge on the interface: its own changes, mediated
by the coupling, arrive at the interface as immanent signals; and the type of signals to which the
system is alive may depend on the system’s configuration (as when an eye can only perceive if its
lid is open). Thus, information flows across the interface.
The mathematical language capturing this kind of inhabited interaction is that of polynomial
functors, which we adopt following Spivak and Niu [235]. We will see that this language—or rather,
its category—is sufficiently richly structured to provide both a satisfactory syntax for the patterns
of interaction of adaptive systems, generalizing the circuit diagrams of §3.3.2, as well as a home for
the dynamical semantics that we will seek.
Polynomial functors are so named because they are a categorification of polynomial functions:
functions built from sums, products, and exponentials, of the form y ÞÑř
i:I bi yai. To categorify
a function of this kind, we can simply interpret the coefficients and exponents and the variable
y as standing for sets rather than mere numbers. In this way, we reinterpret the term yai as
the representable copresheaf Setpai, ´q, so that we can substitute in any set X and obtain the
exponential Xai (just as in the classical case). To categorify the sums and products, we can simply
use the universal constructions available in the copresheaf categorySetSet: these are still available
10Such changes can be very general: consider for instance the changes involved in producing sound (e.g., rapid vibration
of tissue) or light (e.g., connecting a luminescent circuit, or the molecular interactions involved therein).
117
in the subcategory Poly, since Poly is by definition the subcategory of the copresheaf category
on sums of representables (and as we have seen, products are equivalently iterated coproducts).
Remark 3.5.1. Limits and colimits in (co)presheaf categories are computed ‘pointwise’. Therefore,
if F and G are two copresheaves C ÑSet, then their sum F `G is the copresheaf defined by
x ÞÑFpxq` Gpxqand their product is the copresheaf defined by x ÞÑFpxqˆ Gpxq.
We will adopt the standard notation for polynomial functors of Spivak and Niu [235], so that
if p is a polynomial, we will expand it as ř
i:pp1qypris. When treating p as encoding the type of a
system’s interface, we will interpret pp1qas encoding the set of possible configurations (or ‘shapes’)
that the system may adopt, and for each configuration i : pp1q, the set prisis the set of possible
immanent signals (‘inputs’) that may arrive on the interface in configuration i.
Definition 3.5.2. First, if A be any set, we will denote by yA its representable copresheaf yA :“
SetpA, ´q: Set ÑSet. A polynomial functor p : Set ÑSet is then an indexed coproduct of
such representable copresheaves, written p :“ř
i:pp1qypi, where pp1qdenotes the indexing set and
pristhe representing set for each i. The category of polynomial functors is the full subcategory
Poly ãÑSetSet of the copresheaf category spanned by coproducts of representables. A morphism
of polynomials is thus a natural transformation.
Remark 3.5.3. Note that, given a polynomial functor p : Set ÑSet, the indexing set pp1qis
indeed obtained by applying p to the terminal set 1.
We will make much use of the following ‘bundle’ representation of polynomial functors and
their morphisms.
Proposition 3.5.4. Every polynomial functor ř
i:pp1qypi corresponds to a bundle (a function)
p : ř
i:pp1qpi Ñpp1q, where the set ř
i:pp1qpi is the pp1q-indexed coproduct of the representing
objects pi and p is the projection out of the coproduct onto the indexing set pp1q.
Every morphism of polynomials f : p Ñ q corresponds to a pair pf1, f7qof a function f1 :
pp1q Ñqp1qand a pp1q-indexed family of functions f7
i : qrf1piqs Ñprismaking the diagram
below commute. We adopt the notation pris:“pi, and write f7to denote the coproduct ř
i f7
i .
ř
i:pp1qpris ř
i:pp1qqrf1piqs ř
j:qp1qqrjs
pp1q pp1q qp1q
f7
qp
f1
{
118
Given f : p Ñq and g : q Ñr, their composite g ˝f : p Ñr is as marked in the diagram
ř
i:pp1qpris ř
i:pp1qrrg1 ˝f1piqs ř
k:rp1qrrks
pp1q pp1q rp1q
pgfq7
rp
g1˝f1
{
where pgfq7is the coproduct of the pp1q-indexed family of composite maps
rrg1pf1piqqs
f˚g7
Ý ÝÝ Ñqrf1piqs
f7
Ý Ñpris.
The identity morphism on a polynomial p is pidpp1q, idq.
Proof. We just need to show that every natural transformation between polynomial functors
corresponds to a pair of maps pf1, f7q as defined above. The set of natural transforma-
tions ř
i:pp1qypris ñ ř
j:qp1qyqrjs is the hom-set SetSet`ř
i:pp1qypris, ř
j:qp1qyqrjs˘
. Since the
contravariant hom functor takes colimits to limits (Remark 2.3.53), this hom-set is isomor-
phic to ś
i:pp1qSetSetpypris, ř
j:qp1qyqrjsq. By the Yoneda lemma, this is in turn isomorphic
to ś
i:pp1q
ř
j:qp1qprisqrjs. And since products distribute over sums, we can rewrite this as
ř
f1:pp1qÑqp1q
ś
i:pp1qprisqrf1piqs. The elements of this set are precisely pairs of a function f1 :
pp1qÑ qp1qalong with a family of functions qrf1piqsÑ prisindexed by i : pp1q, such that the
diagram above commutes.
We now recall a handful of useful facts about polynomials and their morphisms, each of which
is explained in Spivak and Niu [235] and summarized in Spivak [241].
We will consider the unit polynomial y to represent a ‘closed’ system, since it has no nontrivial
configurations and no possibility of external input. For this reason, morphismsp Ñy will represent
ways to make an open system closed, and in this context the following fact explains why: such
morphisms correspond to a choice of possible input for each p-configuration; that is, they encode
“how the environment might respond to p”.
Proposition 3.5.5. Polynomial morphisms p Ñy correspond to sections pp1qÑ ř
i prisof the
corresponding function p : ř
i prisÑ pp1q.
The following embedding ofSet into Poly will be useful in constructing ‘hierarchical’ dynamical
systems.
Proposition 3.5.6. There is an embedding of Set into Poly given by taking sets X to the linear
polynomials Xy : Poly and functions f : X ÑY to morphisms pf, idXq: Xy ÑY y.
119
There are many monoidal structures on Poly, but two will be particularly important for us. The
first represents the parallel composition of systems.
Proposition 3.5.7. There is a symmetric monoidal structure pb, yqon Poly that we call tensor,
and which is given on objects bypbq :“ř
i:pp1q
ř
j:qp1qyprisˆqrjsand on morphismsf :“pf1, f7q:
p Ñp1and g :“pg1, g7q: q Ñq1by f bg :“
`
f1 ˆg1, Σpf7, g7q
˘
, where Σpf7, g7qis the family
of functions
Σpf7, g7qi,j :“p1rf1piqsˆ q1rg1pjqs
f7
i ˆg7
j
Ý ÝÝÝ Ñprisˆ qrjs
indexed by pi, jq: pp1qˆqp1q. This is to say that the ‘forwards’ component of f bg is the product
of the forwards components of f and g, while the ‘backwards’ component is the pointwise product
of the respective backwards components.
Proposition 3.5.8. pPoly, b, yqis symmetric monoidal closed, with internal hom denoted r´, “s.
Explicitly, we have rp, qs“ ř
f:pÑq y
ř
i:pp1qqrf1piqs. Given an set A, we have rAy, ys– yA.
The second important monoidal structure is that inherited from the composition of endofunctors.
To avoid confusion with other composition operators, we will in this context denote the operation
by ◁.
Proposition 3.5.9. The composition of polynomial functorsq˝p : E ÑE ÑE induces a monoidal
structure on PolyE, which we denote ◁, and call ‘composition’ or ‘substitution’. Its unit is again y.
Comonoids with respect to ◁ play a particularly important rôle in the theory of polynomial
functors, and we will make accordingly much use of them.
Proposition 3.5.10 (Ahman and Uustalu [7, §3.2]) . Comonoids in pPoly, ◁, yqcorrespond to
small categories. If pc, δ, ϵqis a comonoid, then the shapes cp1qare the objects of the corresponding
category C. For each object x : cp1q, crisis the set ř
y:cp1qCpx, yqof morphisms out of x. The
counit morphism ϵ : c Ñ y is, following Proposition 3.5.5, a section of c, and assigns to each
x : cp1qits identity morphism idx : x Ñx. The comultiplication δ : c Ñc ◁c encodes morphisms’
codomains (its forward action) and their composition (its backward action). Finally, the comonoid
laws ensure that the category is well defined.
Remark 3.5.11. ◁-comonoid homomorphisms are not, as one might expect, functors; rather, they
are ‘cofunctors’: they act backwards on morphisms. We will not explore the theory of cofunctors
any further in this thesis, although we will make frequent use of them later in the context of
dynamical systems.
120
The following ◁-comonoids will play a prominent rôle in our dynamical developments.
Proposition 3.5.12. If T is a monoid in pSet, ˆ, 1q, then the comonoid structure on yT witnesses
it as the category BT.
Proposition 3.5.13. Monomials of the form SyS can be equipped with a canonical comonoid
structure witnessing the codiscrete groupoid on S: the category with an object for every element s
of S and a morphism s Ñt for every pair of elements ps, tq.
121
4. The compositional structure of Bayesian
inference
This chapter introduces the fundamental concepts and structures needed for the development
of statistical games in Chapter 5, and proves the crucial result that Bayesian updating composes
according to the ‘lens’ pattern. To make sense of this statement, we first introduce compositional
probability (§4.1), motivating it as a resolution of some imprecision that arises when one works
informally with probability and statistics, particularly in the context of ‘hierarchical’ models. We
exhibit categorical probability theory both abstractly (§4.1.2 and §4.1.3) and concretely (using
discrete probability in §4.1.1 and ‘continuous’ probability in §4.1.4). We then move on to construct
categories of bidirectional processes in §4.2, by first categorifying our earlier discussion of dependent
data using the Grothendieck construction (§4.2.1) and then using this to introduce the lens pattern
(§4.2.2).
In §4.3, we present our novel results. First, we introduce the indexed category of “state-dependent
channels” in §4.3.1. These formalize the type of Bayesian inversions, and so in §4.3.2 we define the
associated notion of Bayesian lens, and show in §4.3.3 that Bayesian updating composes according
to the lens pattern. We end with a brief discussion of the ‘lawfulness’ of Bayesian lenses.
Remark 4.0.1. To gain some intuition about the hierarchical compositional structure of Bayesian
inference, consider sitting close to the screen at a cinema. Your neural activity encodes a belief
about where the characters are on the screen and what they are doing, but your visual field can
only capture a part of the image at any one time. These incoming visual signals contain “low-level”
information, about the light intensity over the patch of screen you can see, and the first job of the
visual system is to infer what this means for what’s going on in this patch. Of course, having been
following the film so far, your brain encodes a high-level belief about what is going on across the
whole screen, and it uses this to predict what to expect in the patch. This intermediate-level belief
is then updated using the received visual signals, through a process of (approximate) Bayesian
inference. The resulting intermediate-level posterior then supplies the input for a second inference
process, updating the prior high-level belief accordingly. Notice that this means that the process of
122
prediction in such a hierarchical inference system points from inside an agent “towards the world”;
and the belief-updating process points the other way, from the world into the agent.
4.1. Compositional probability
In informal literature, Bayes’ rule is often written in the following form:
PpA|Bq“ PpB|Aq¨ PpAq
PpBq
where PpAqis the probability of the event A, and PpA|Bqis the probability of the event A given
that the event B occurred; and vice versa swapping A and B. Unfortunately, this notation obscures
that there is in general no unique assignment of probabilities to events: different observers can hold
different beliefs. Moreover, we are usually less interested in the probability of particular events
than in the process of assigning probabilities to arbitrarily chosen beliefs; and what should be done
if PpBq“ 0 for some B? The aim in this section is to exhibit a general, precise, and compositional,
form of Bayes’ rule; we begin, as before, by introducing the intuition.
In the categorical setting, the assignment of probabilities or beliefs to events will formally be the
task of a state (in the sense of §3.1.1) on the space from which the events are drawn; we should
think of states as generalizing distributions or measures. With this notion to hand, we can write
PπpAqto denote the probability of A according to the state π.
The formalization of conditional probability will be achieved by morphisms that we will call
channels, meaning that we can write PcpB|Aqto denote the probability of B given A according to
the channel c. We can think of the channel c as taking events A as inputs and emitting states cpAq
as outputs. This means that we can alternatively write PcpB|Aq“ PcpAqpBq.
If the input events are drawn from the space X and the output states encode beliefs about Y ,
then the channel c will be a morphism XÑ‚Y . Given a channel c : XÑ‚Y and a channel d : Y Ñ‚Z,
we will understand their composite d ‚c : XÑ‚Z as marginalizing (averaging) over the possible
outcomes in Y . We will see precisely how this works in various settings below.
Notation 4.1.1. In a stochastic context, we will denote channels by the arrow Ñ‚, and write
their composition operator as ‚. We do this to distinguish stochastic channels from deterministic
functions, which we will continue to write as Ñwith composition ˝; in a number of situations, it
will be desirable to work with both kinds of morphism and composition.
123
Given two spaces X and Y of events, we can form beliefs about them jointly, represented by
states on the product space denoted X bY . The numerator in Bayes’ rule represents such a joint
state, by the law of conditional probability or ‘product rule’:
PωpA, Bq“ PcpB|Aq¨ PπpAq (4.1)
where ¨is multiplication of probabilities, π is a state on X, and ω denotes the joint state on X bY .
By composing c and π to form a state c ‚π on Y , we can write
Pω1pB, Aq“ Pc:
π
pA|Bq¨ Pc‚πpBq
where c:
π will denote the Bayesian inversion of c with respect to π.
Joint states in classical probability theory are symmetric—and so the tensor bis symmetric—
meaning that there is a family of isomorphisms swap : X bY „Ý Ñ‚Y bX, as in §3.1.1, and which
will satisfy the symmetric monoidal category axioms (Definition 3.1.4). Consequently, we have
PωpA, Bq“ Pω1pB, Aqwhere ω1 “swap ‚ω, and thus
PcpB|Aq¨ PπpAq“ Pc:
π
pA|Bq¨ Pc‚πpBq (4.2)
where both left- and right-hand sides are called disintegrations of the joint state ω [60]. From this
equality, we can write down the usual form of Bayes’ theorem, now with the sources of belief
indicated:
Pc:
π
pA|Bq“ PcpB|Aq¨ PπpAq
Pc‚πpBq . (4.3)
As long as Pc‚πpBq‰ 0, this equality defines the inverse channel c:
π. If the division is undefined,
or if we cannot guarantee Pc‚πpBq‰ 0, then c:
π can be any channel satisfying (4.2).
There is therefore generally no unique Bayesian inversion c: : Y Ñ‚X for a given channel
c : XÑ‚Y : rather, we have an inverse c:
π : Y Ñ‚X for each prior state π on X. Moreover, c:
π is
not a “posterior distribution” (as written in some literature), but a process which emits a posterior
distribution, given an observation in Y . If we denote our category of stochastic channels byC, then,
by allowing π to vary, we obtain a map of the form c:
p¨q : PX ÑCpY, Xq, where PX denotes a
space of states on X. Note that here we are not assuming the object PX to be an object of C itself
(though it often will be), but rather an object in its base of enrichment, so that here we can think of
c:
p¨qas a kind of externally parameterized channel (in the sense of §3.2.2). Making the type of this
‘state-dependent’ channel c:
p¨qprecise is the task of §4.2.1.
124
Remark 4.1.2. There are two easily confused pieces of terminology here. We will call the channel
c:
π the Bayesian inversion of the channel c with respect to π. Then, given some y PY , the state
c:
πpyqis a new ‘posterior’ distribution on X. We will call c:
πpyqthe Bayesian update of π along c
given y.
In the remainder of this section, we instantiate the ideas above in categories of stochastic channels
of various levels of generality, beginning with the familiar case of discrete (i.e., finitely supported,
or ‘categorical’) probability.
4.1.1. Discrete probability, algebraically
Interpreting the informal Bayes’ rule (4.3) is simplest in the case of discrete or finitely-supported
probability. Here, every event is a set, generated as the disjoint union of so many atomic (singleton)
events, which one can therefore take as the elements of the set. A finitely-suported probability
distribution is then simply an assignment of nonzero probabilities to finitely many elements, such
that the sum of all the assignments is 1. This condition is a convexity condition, and so in this
subsection we will introduce discrete compositional probability theory from a geometric perspective,
using the algebraic tools of the previous chapter.
Definition 4.1.3. Suppose X is a set. A function c : X Ñ r0, 1ssuch that cpxq ą0 for only
finitely many x : X and ř
x:X cpxq“ 1 will be called a discrete or finitely-supported distribution on
X. We write DX to denote the set of discrete distributions on X. A (real-valued) convex set is a set
X equipped with a function ς : DX ÑX called its evaluation.
Convex sets X are sets in which we can form convex combinations of elements. Algebraically,
we can model these convex combinations as distributions on X, and the evaluations realize the
convex combinations (distributions) as elements again of X: geometrically, the evaluation returns
the barycentre of the distribution.
In light of Chapter 3, this situation may seem familiar. Indeed, the assignment X ÞÑDX is
the functor part of a monad on Set, whose algebras are convex sets. This monad arises from a
free-forgetful adjunction between the category of convex sets (the category of algebras of the
monad) and the category Set. Later, we will find that the category of finitely-supported conditional
probability distributions—the category of discrete stochastic channels—is equivalent to the category
of free convex sets and their morphisms: a free convex set onX is equivalently a distribution on X.
Let us first formalize the functor D.
125
Proposition 4.1.4. The mapping of sets X ÞÑDX is functorial. Given a function f : X ÑY , we
obtain a function Df : DX ÑDY mapping c : DX to the distribution Dfpcq: DY ,
Dfpcq: Y Ñr0, 1s
y ÞÑ
ÿ
x:fpxq“y
cpxq.
Proof. Given f : X ÑY and g : Y ÑZ, we have
DgpDfpcqq: Z Ñr0, 1s
z ÞÑ
ÿ
y:gpyq“z
ÿ
x:fpxq“y
cpxq
“
ÿ
x:g˝fpxq“z
cpxq
hence Dg ˝Df “Dpg ˝fq. We also have
DpidXqpcq: X Ñr0, 1s
x ÞÑ
ÿ
x1:idXpx1q“x
cpx1q
“cpxq
and hence Dpidq“ idD.
To obtain the monad structure of D, we will exhibit the free-forgetful adjunction. We start by
defining the category of convex sets, and the specific case of a free convex set.
Definition 4.1.5. The category of (real-valued) convex sets Conv has convex setspX, ςXqas objects.
Its morphisms pX, ςXqÑp Y, ςY qare functions f : X ÑY that preserve the convex structure, in
the sense that the following square commutes:
DX DY
X Y
Df
f
ςX ςY
Definition 4.1.6. If X is any set, then the free convex set on X is the set DX equipped with the
evaluation µX : DDX ÑDX which maps α : DDX to the distribution µXpαq: DX,
µXpαq: X Ñr0, 1s
x ÞÑ
ÿ
c:DX
αpcq¨ cpxq.
126
Notation 4.1.7. To emphasize the algebraic nature of finitely-supported distributions π : DX,
instead of writing them as functions x ÞÑπpxq, we can write them as formal sums or formal convex
combinations ř
x:X πpxq|xy, with each element x : X corresponding to a formal “basis vector” |xy
with the coefficient πpxq. If X is a convex set, then the evaluation realizes this formal sum as an
actual element (‘vector’) in X.
We are now in a position to exhibit the adjunction: the reasoning behind the following proposition
follows the lines of Example 3.4.19 and Proposition 2.2.25 (on the free-forgetful adjunction between
graphs and categories).
Proposition 4.1.8. The mapping of X : Set to the free convex set pDX, µXqdefines a functor
F : Set ÑConv which takes functions f : X ÑY to morphisms F f: pDX, µXqÑp DY, µY q
defined by Df : DX ÑDY . This functor F is left adjoint to the forgetful functor U : Conv Ñ
Set which acts by pX, ςXqÞÑ X.
Using Proposition 3.4.13, the adjunction gives us a monad.
Corollary 4.1.9. The functor D : Set Ñ Set is equivalently the functor part of the monad
induced by the free-forgetful adjunction on convex sets. It therefore acquires a monad structure
pµ, ηqwhere the components of the multiplication µ are the free evaluations µX : DDX ÑDX,
and the unit η has components ηX : X ÑDX which return the ‘Dirac’ distributions, as in
ηXpxq: X Ñr0, 1s
x1 ÞÑ
#
1 iff x “x1
0 otherwise.
And Conv is the category of algebras for this monad.
Corollary 4.1.10. Conv –AlgpDq.
Using Corollary 4.1.10, we can actually exhibit the relationship between the monad D and
its defining adjunction tautologously: every monad T on a category C induces an free-forgetful
adjunction between its category of algebras AlgpTqand C itself, such that the monad generated
by this adjunction is again T. This is precisely the situation here.
Proposition 4.1.11. Suppose pT, µ, ηqis a monad on the category C. There is a forgetful functor
U : AlgpTqÑ C which has a left adjoint F : C ÑAlgpTqtaking each object X : C to the free
T-algebra pT X, µXqon X, where µX : T T XÑT Xis the component of the monad multiplication
µ at X. The unit of the adjunction is the monadic unit η, the counit ϵ is defined by ϵpX,ςXq :“ςX,
and the monad induced by the adjunction is pT, µ, ηq.
127
Proof sketch. The proof that F is left adjoint to U is standard (see Borceux [39, Prop. 4.1.4]), and
that the adjunction generates the monad follows almost immediately from Proposition 3.4.13.
Remark 4.1.12. It must be emphasized that, although every monad arises from such a free-forgetful
adjunction, not every adjunction does! (Consider for example the adjunction∆ %lim of Proposition
2.3.48: ∆ does not assign to each c : C the “free J-shaped diagram on c”, and lim does not simply
forget diagrammatic structure.) Those adjunctions which do arise from monads in this way are
called monadic.
There is a special name for subcategories of free algebras.
Definition 4.1.13. Suppose pT, µ, ηqis a monad on C. The subcategory of AlgpTqon the free
T-algebras pT X, µXqis called the Kleisli category for T, and denoted KℓpTq.
The following proposition gives us an alternative presentation of KℓpTqwhich, when applied to
the monad D, will yield a computationally meaningful category of finitely-supported stochastic
channels.
Proposition 4.1.14. The objects of KℓpTqare the objects of C. The morphisms XÑ‚Y of KℓpTq
are the morphisms X ÑT Yof C. Identity morphisms idX : XÑ‚X are given by the monadic unit
ηX : X ÑT X. Composition is defined by Kleisli extension: given g : Y Ñ‚Z, we form its Kleisli
extension g▷ : T YÑ‚Z as the composite T Y
T g
Ý Ý ÑT T Z
µZ
Ý Ý ÑT Zin C. Then, given f : XÑ‚Y , we
form the composite g ‚f : XÑ‚Z as g▷ ˝f: X
f
Ý ÑT Y
T g
Ý Ý ÑT T Z
µZ
Ý Ý ÑT Z.
Proof. Observe that there is a bijection between the objectsX of C and the freeT-algebras pT X, µXq.
We therefore only need to establish a bijection between the hom-setsAlgpTq
`
pT X, µXq, pT Y, µY q
˘
and KℓpTqpX, Yq, with the latter defined as in the statement of the proposition.
First, we demonstrate that Kleisli extension defines a surjection
KℓpTqpX, YqÑ AlgpTq
`
pT X, µXq, pT Y, µY q
˘
.
Suppose ϕ is any algebra morphism pT X, µXqÑp T Y, µY q; we show that it is equal to the Kleisli
extension of the Kleisli morphism X
ηX
Ý Ý ÑT X
ϕ
Ý ÑT Y:
T X
pϕ˝ηXq▷
ÝÝÝÝÝÑT Y“T X
T ηX
Ý ÝÝ ÑT T X
T ϕ
Ý Ý ÑT T Y
µTY
Ý ÝÝ ÑT Y
“T X
T ηX
Ý ÝÝ ÑT T X
µTX
Ý ÝÝ ÑT X
ϕ
Ý ÑT Y
“T X
idTX
ù ùùù ùT X
ϕ
Ý ÑT Y
“T X
ϕ
Ý ÑT Y
128
where the first equality holds by definition, the second line by naturality of µ, and the third by
the unitality of the monad pT, µ, ηq. Hence every free algebra morphism is in the image of Kleisli
extension, and so Kleisli extension defines a surjection.
Next, we show that this surjection is additionally injective. Supposef, gare two Kleisli morphisms
X ÑT Ysuch that their Kleisli extensions are equal
T X
T f
Ý Ý ÑT T Y
µTY
Ý ÝÝ ÑT Y“T X
T g
Ý Ý ÑT T Y
µTY
Ý ÝÝ ÑT Y
and recall that the identity in KℓpTqis η. We therefore have the following equalities:
X
ηX
Ý Ý ÑT X
T f
Ý Ý ÑT T Y
µTY
Ý ÝÝ ÑT Y“X
ηX
Ý Ý ÑT X
T g
Ý Ý ÑT T Y
µTY
Ý ÝÝ ÑT Y
“X
f
Ý ÑT Y
T ηy
ÝÝÑT T Y
µTY
Ý ÝÝ ÑT Y“X
g
Ý ÑT Y
T ηy
ÝÝÑT T Y
µTY
Ý ÝÝ ÑT Y
“X
f
Ý ÑY “X
g
Ý ÑY .
where the equality in the first line holds ex hypothesi , the second by naturality, and the
third by monadic unitality. Since f “ g when their Kleisli extensions are equal, Kleisli
extension is injective. Since it is also surjective, we have an isomorphism between KℓpTqpX, Yq
and AlgpTq
`
pT X, µXq, pT Y, µY q
˘
. Hence KℓpTq is the subcategory of AlgpTq on the free
algebras.
If T is a monad on C, there is a canonical embedding of C into KℓpTq. In the case of KℓpDq, this
will yield the subcategory of deterministic channels: those which do not add any uncertainty.
Proposition 4.1.15. Suppose T is a monad on C. Then there is an identity-on-objects embedding
C ãÑKℓpTqgiven on morphisms by mapping f : X ÑY in C to the Kleisli morphism X
ηX
Ý Ý Ñ
T X
T f
Ý Ý ÑT Y.
Proof sketch. Functoriality follows from the unitality of η in the monad structure, since Kleisli
composition involves post-composing the monad multiplication, and µT ˝T η“id.
4.1.1.1. Stochastic matrices
At this point, let us exhibit KℓpDqa little more concretely, by instantiating Proposition 4.1.14. Since
a distribution π on the set X is a function X Ñr0, 1s, and following the “formal sum” intuition, we
can alternatively think of π as a vector, whose coefficients are indexed by elements of X (the basis
vectors |xy). Morphisms XÑ‚Y in KℓpDqare functions X ÑDY , and so we can similarly think of
these as stochastic matrices, by the Cartesian closure of Set: a function X ÑDY is equivalently
129
a function X Ñr0, 1sY , which in turn corresponds to a function X ˆY Ñr0, 1s, which we can
read as a (left) stochastic matrix, with only finitely many nonzero coefficients and each of whose
columns must sum to 1. We will adopt ‘conditional probability’ notation for the coefficients of
these matrices: given p : XÑ‚Y , x PX and y PY , we write ppy|xq:“ppxqpyqPr 0, 1sfor “the
probabilty of y given x, according to p”.
Composition in KℓpDqis then matrix multiplication: given p : X ÑDY and q : Y ÑDZ, we
compute their composite q ‚p : X ÑDZ by ‘averaging over’ or ‘marginalizing out’ Y via the
Chapman-Kolmogorov equation:
q ‚p : X ÑDZ :“x ÞÑ
ÿ
z:Z
ÿ
y:Y
qpz|yq¨ ppy|xq |zy.
Here we have again used the formal sum notation, drawing a box to indicate the coefficients (i.e.,
the probabilities returned by the conditional distribution q ‚ppxqfor each atomic event z in Z).
Via the monadic unit, identity morphisms idX : XÑ‚X in KℓpDqtake points to ‘Dirac delta’
distributions: idX :“x ÞÑ1 |xy. The embedding Set ãÑKℓpDqmakes any function f : Y ÑX
into a (deterministic) channel f “ηX ˝f : Y ÑDX by post-composing with ηX.
4.1.1.2. Monoidal structure
We will want to equip KℓpDqwith a copy-discard category structure, in order to represent joint
states (joint distributions) and their marginalization, as well as the copying of information. The first
ingredient making a copy-discard category, after the category itself, is a monoidal structure. Once
again, in the case of KℓpDq, this can be obtained abstractly from a more fundamental structure—the
categorical product pˆ, 1qon Set—as a consequence of D being a ‘monoidal’ monad. We will write
the induced tensor product on KℓpDqas b; its monoidal unit remains the object 1.
Definition 4.1.16. A monoidal monad is a monad in MonCat. This means that it is a monad
pT, µ, ηqin Cat whose functor T : C ÑC is additionally equipped with a lax monoidal structure
pα, ϵqsuch that the monad multiplication µ and unit η are monoidal natural transformations
accordingly.
With this extra structure, it is not hard to verify that the following proposition makes KℓpTq
into a well-defined monoidal category.
Proposition 4.1.17. The Kleisli category KℓpTqof a monoidal monad pT, α, ϵ, µ, ηqis a monoidal
category. The monoidal product is given on objects by the monoidal productbof the base category
130
C. On Kleisli morphisms f : XÑ‚Y and f1 : X1Ñ‚Y 1, their tensor f bg is given by the following
composite in C:
X bX1 fbf1
Ý ÝÝ ÑT XbT X1 αX,X1
Ý ÝÝÝ ÑTpX bX1q
The monoidal unit is the monoidal unit I in C. The associator and unitor of the monoidal category
structure are inherited from C under the embedding C ãÑKℓpTq. When pC, b, Iqis symmetric
monoidal, then so is pKℓpTq, b, Iq.
In the specific case of KℓpDq, the tensor product bis given on objects by the product of sets and
on stochastic channels f : X ÑDA and g : Y ÑDB as
X ˆY
fˆg
ÝÝÑDA ˆDB
αA,B
ÝÝÝÑDpA ˆBq.
Note that because not all joint states have independent marginals, the monoidal product bis not
Cartesian: that is, given an arbitrary ω : DpX bY q, we do not necessarily have ω “pρ, σqfor
some ρ : DX and σ : DY . The laxator takes a pair of distributions pρ, σqin DX ˆDY to the joint
distribution on X ˆY given by px, yqÞÑ ρpxq¨σpyq; ρ and σ are then the (independent) marginals
of this joint distribution. (Of course, the joint distribution pρ, σqis not the only joint distribution
with those marginals: other joint states may have these marginals but also correlations between
them, and this is what it means for not all joint states to have independent marginals.)
Since pSet, ˆ, 1qis symmetric monoidal, KℓpDqis too, with swap isomorphisms swapX,Y :
X bY „Ý Ñ‚Y bX similarly inherited form those of ˆ.
4.1.1.3. Copy-discard structure
The copy-discard structure in KℓpDqis inherited from Set through its embedding: since every
object in KℓpDqis an object in Set, and every object in Set is a comonoid (Example 3.4.22), and
since functors preserve equalities, these comonoid structures are preserved under the embedding.
More explicitly, the discarding channels X are given by x ÞÑ 1 |˚y, and the copiers X by
x ÞÑ1 |x, xy. Note that the copiers are not natural in KℓpDq: in general, ‚f ‰f bf ‚ , as a
result of the possibility of correlations.
Since the projections projX : X ˆY Ñ X in Set satisfy projX “ ρX ˝pidX ˆ Y qwhere
ρX : X ˆ1 ÑX is component of the right unitor, we can see how discarding and projection
give us marginalization, thereby explaining the string diagrams of §3.1.1. Given some joint state
ω : 1Ñ‚X bY , its X-marginal ωX : 1Ñ‚X is given by projX ‚ω, which in KℓpDqis given by the
formal sum formula ř
x:X
ř
y:Y ωpx, yq |xy, where we have again drawn a box to distinguish the
131
probability assigned to |xy, which we note coincides with the classical rule for marginal discrete
probability. (The Y -marginal is of course symmetric.)
Remark 4.1.18. A semicartesian category is a monoidal category in which the monoidal unit is
terminal. In a semicartesian monoidal category, every tensor product X bY is equipped with a
natural family of projections projX : X bY ÑX and projY : X bY ÑY given by ‘discarding’
one of the factors and using the unitor; the existence of such projections is not otherwise implied
by a monoidal structure (though of course it does follow when the tensor is the product).
A related notion is that of an affine functor, which is one that preserves the terminal object, and
of which D is an example. As a result, and following the discussion above, we can see that KℓpDq
is an example of a semicartesian category.
Semicartesian copy-discard categories are also known as Markov categories, following Fritz [109].
Remark 4.1.19. Since 1 is therefore terminal in KℓpDq, Proposition 3.4.24 tells us that those
channels f that do commute with copying (i.e., for which is natural; Remark 3.4.26), and which are
therefore comonoid morphisms, are precisely the deterministic channels: those in the image of the
embedding of Set (and which therefore emit Dirac delta distributions). As a result, we can think of
ComonpKℓpDqqas the subcategory of deterministic channels, and write ComonpKℓpDqq– Set.
(Intuitively, this follows almost by definition: a deterministic process is one that has no informational
side-effects; that is to say, whether we copy a state before performing the process on each copy, or
perform the process and then copy the resulting state, or whether we perform the process and then
marginalize, or just marginalize, makes no difference to the resulting state.)
4.1.1.4. Bayesian inversion
We can now instantiate Bayesian inversion in KℓpDq, formalizing Equation (4.3). Given a channel
p : X ÑDY satisfying the condition in Remark 4.1.20 below, its Bayesian inversion is given by
the function
p: : DX ˆY ÑDX :“pπ, yqÞÑ
ÿ
x:X
ppy|xq¨ πpxqř
x1:X ppy|x1q¨ πpx1q |xy“
ÿ
x:X
ppy|xq¨ πpxq
pp ‚πqpyq |xy
(4.4)
so that the Bayesian update of p along π is the conditional distribution defined by
p:
πpx|yq“ ppy|xq¨ πpxq
pp ‚πqpyq .
Note that here we have used the Cartesian closure ofSet, writing the type ofp:as DX ˆY ÑDX
rather than DX ÑKℓpDqpY, Xq, where KℓpDqpY, Xq“p DXqY .
132
Remark 4.1.20. In the form given above, p:is only well-defined when the support of p ‚π is the
whole of Y , so that, for all y, pp ‚πqpyqą 0; otherwise, the division is ill-defined. Henceforth,
in the context of Bayesian inversion, we will therefore assume that p ‚π has full support (see
Definition 4.1.21).
To avoid this (rather ugly) condition, one can replace it by the assumption that the notion of
‘support’ is well-defined, and modify the type of p: accordingly: this is the refinement made by
Braithwaite and Hedges [41], and were it not for the presently-uncertain nature of support objects
in general, it would now be this author’s preferred approach. This leads to writing the type of
the inversion p:as ř
π:DX supppp ‚πqÑ DX, where supppp ‚πqis the subobject of Y on which
p ‚π is supported: with this type, p:
π is always a well-defined channel. One can then proceed with
the definition of ‘dependent’ Bayesian lenses accordingly; for the details, we refer the reader to
Braithwaite and Hedges [41]. In this thesis, for simplicity of exposition and faithfulness to this
author’s earlier work, we will proceed under the full-support assumption.
4.1.2. Abstract Bayesian inversion
Beyond the concerns of Remark 4.1.20, in a more general setting it is not always possible to define
Bayesian inversion using an equation like Equation (4.4) or Equation (4.3): the expression ppy|xq
might not be well-defined, or there might not be a well-defined notion of division. Instead being
guided by Equation (4.3) in defining Bayesian inversion, we can use Equation (4.2). Therefore,
supposing a channel c : XÑ‚Y and a state π : IÑ‚X in an ambient copy-discard category C, we
can ask for the Bayesian inversion c:
π to be any channel satisfying the graphical equality [60, eq. 5]:
c
π
X Y
“
c:
π
π
c
X Y
(4.5)
This diagram can be interpreted as follows. Given a prior π : IÑ‚X and a channel c : XÑ‚Y , we
form the joint distribution ω :“pidX bcq‚ X ‚π : IÑ‚X bY shown on the left hand side: this
formalizes the product rule,PωpA, Bq“ PcpB|Aq¨PπpAq, and π is the corresponding X-marginal.
133
As in the concrete case of KℓpDq, we seek an inverse channel Y Ñ‚X witnessing the ‘dual’ form of
the rule, PωpA, Bq“ PpA|Bq¨ PpBq; this is depicted on the right hand side. By discarding X, we
see that c ‚π : IÑ‚Y is the Y -marginal witnessing PpBq. So any channel c:
π : Y Ñ‚X witnessing
PpA|Bqand satisfying the equality above is a Bayesian inverse of c with respect to π.
In light of Remark 4.1.20, we therefore make the following definition.
Definition 4.1.21. We say that a channel c : XÑ‚Y admits Bayesian inversion with respect to
π : IÑ‚X if there exists a channel c:
π : Y Ñ‚X satisfying equation (4.5). We say that c admits
Bayesian inversion tout court if c admits Bayesian inversion with respect to all states π : IÑ‚X.
Remark 4.1.22. We need to be careful about the existence of inversions as a consequence of the
fact that c ‚π may not always be fully supported on Y (recall Remark 4.1.20). In this thesis we will
henceforth assume that c ‚π is always fully supported, in order to keep the exposition clear. This
is justified in two ways: first, because we can always restrict to a wide subcategory all of whose
channels do admit inversion; and second, because we may equally work with dependent Bayesian
lenses (as described by Braithwaite and Hedges [41] and noted in Remark 4.1.20).
4.1.3. Density functions
Abstract Bayesian inversion (4.5) generalizes the product rule form of Bayes’ theorem (4.2) but
in most applications, we are interested in a specific channel witnessing PpA|Bq “PpB|Aq¨
PpAq{PpBq. In the typical measure-theoretic setting, this is often written informally as
ppx|yq“ ppy|xq¨ ppxq
ppyq “ ppy|xq¨ ppxqş
x1:X ppy|x1q¨ ppx1qdx1 (4.6)
but the formal semantics of such an expression are not trivial: for instance, what is the object
ppy|xq, and how does it relate to a channel c : XÑ‚Y ?
Following Cho and Jacobs [60], we can interpret ppy|xqas a density function for a channel,
abstractly witnessed by an effect X bY Ñ‚I in our ambient category C. Consequently, C cannot be
semicartesian—as this would trivialize all density functions—though it must still supply comonoids.
We can think of this as expanding the collection of channels in the category to include acausal or
‘partial’ maps and unnormalized distributions or states.
Example 4.1.23. An example of such a category is KℓpDď1q, whose objects are sets and whose
morphisms XÑ‚Y are functions X ÑDpY `1q. Then a stochastic map is partial if it sends any
probability to the added element ˚, and the subcategory of total (equivalently, causal) maps is
KℓpDq(see [61] for more details).
134
A morphism XÑ‚1 in KℓpDď1qis therefore a function X ÑDp1 `1q. Now, a distribution π on
1 `1 is the same as a number ¯π in r0, 1s: note that 1 `1 has two points, and so π assigns ¯π to one
of them and 1 ´¯π to the other. Therefore an effect XÑ‚1 is equivalently a function X Ñr0, 1s,
which is precisely the type we expect for a density function.
We therefore adopt the following abstract definition.
Definition 4.1.24 (Density functions [60, Def. 8.1]). A channel c : XÑ‚Y is said to be represented
by an effect p : X bY Ñ‚I with respect to µ : IÑ‚Y if
c
X
Y
µ
p
X
Y
“ .
In this case, we call p a density function for c.
We will also need the concepts of almost-equality and almost-invertibility.
Definition 4.1.25 (Almost-equality, almost-invertibility [60, Def. 8.2]). Given a state π : IÑ‚X,
we say that two channels c : XÑ‚Y and d : XÑ‚Y are π-almost-equal, denoted c π„d, if
c
π
X Y
“
d
π
X Y
and we say that an effect p : XÑ‚I is π-almost-invertible with π-almost-inverse q : XÑ‚I if
π„
qp
X X
.
135
The following basic results about almost-equality will prove helpful.
Proposition 4.1.26 (Composition preserves almost-equality). If c π„d, then f ‚c π„f ‚d.
Proof. Immediate from the definition of almost-equality.
Proposition 4.1.27 (Almost-inverses are almost-equal). Suppose q : XÑ‚I and r : XÑ‚I are both
π-almost-inverses for the effect p : XÑ‚I. Then q π„r.
Proof. By assumption, we have
π„
qp
π„
rp
.
Then, by the definition of almost-equality (Definition 4.1.25):
qp
π π π
rp
π
“““
. (4.7)
We seek to show that
“
q
π
r
π
. (4.8)
136
Substituting the right-hand-side of (4.7) for π in the left-hand-side of (4.8), we have that
qrp
π
q
π
“
rqp
π
r
π
““
which establishes the result. The second equality follows by the coassociativity of and the third
by its counitality.
With these notions, we can characterise Bayesian inversion via density functions. The result is
due to Cho and Jacobs [60], but we include the graphical proof for expository completeness, as an
example of string-diagrammatic reasoning.
Proposition 4.1.28 (Bayesian inversion via density functions [60, Thm. 8.3]). Suppose c : XÑ‚Y
is represented by the effect p with respect to µ. The Bayesian inverse c:
π : Y Ñ‚X of c with respect
to π : IÑ‚X is given by
p
π
p´1
X
Y
137
where p´1 : Y Ñ‚I is a µ-almost-inverse for the effect
p
π
Y
Proof. We seek to establish the relation (4.5) characterizing Bayesian inversion. By substituting the
density function representations for c and c:
π into the right-hand-side of (4.5), we have
c:
π
π
c
“
µ
p
p
π
p´1
π
“
µ
p p
π
p´1
π
138
“
c
π
“
µ
p
π
as required. The second equality holds by the coassociativity of , the third since p´1 is an
almost-inverse ex hypothesi, and the fourth by the counitality of p , qand the density function
representation of c.
The following proposition is an immediate consequence of the definition of almost-equality and
of the abstract characterisation of Bayesian inversion (4.5). We omit the proof.
Proposition 4.1.29 (Bayesian inverses are almost-equal). Suppose α : Y Ñ‚X and β : Y Ñ‚X are
both Bayesian inversions of the channel c : XÑ‚Y with respect to π : IÑ‚X. Then α c‚π„ β.
4.1.4. S-finite kernels
To represent channels by concrete density functions, we can work in the category sfKrn of
measurable spaces and s-finite kernels. We will only sketch the structure of this category, and refer
the reader to Cho and Jacobs [60] and Staton [258] for elaboration.
Objects in sfKrn are measurable spaces pX, ΣXq; often we will just write X, and leave the
σ-algebra ΣX implicit. Morphisms pX, ΣXqÑ‚pY, ΣY qare s-finite kernels. A kernel k from X to
Y is a function k : X ˆΣY Ñr0, 8ssatisfying the following conditions:
• for all x PX, kpx, ´q: ΣY Ñr0, 8sis a measure; and
• for all B PΣY , kp´, Bq: X Ñr0, 8sis measurable.
A kernel k : X ˆΣY Ñr0, 8sis finite if there exists some r Pr0, 8qsuch that, for all x PX,
kpx, Yq ďr. And k is s-finite if it is the sum of at most countably many finite kernels kn,
k “ř
n:N kn.
Identity morphisms idX : XÑ‚X are Dirac kernels δX : X ˆΣX Ñr0, 8s:“x ˆA ÞÑ1 iff
x PA and 0 otherwise. Composition is given by a Chapman-Kolmogorov equation, analogously to
139
composition in KℓpDq. Suppose c : XÑ‚Y and d : Y Ñ‚Z. Then
d ‚c : X ˆΣZ Ñr0, 8s:“x ˆC ÞÑ
ż
y:Y
dpC|yqcpdy|xq
where we have again used the ‘conditional probability’ notation dpC|yq:“d ˝py ˆCq. Reading
dpC|yqfrom left to right, we can think of this notation as akin to reading the string diagrams from
top to bottom, i.e. from output(s) to input(s).
Monoidal structure on sfKrn There is a monoidal structure on sfKrn analogous to that on
KℓpDq. On objects, X bY is the Cartesian product X ˆY of measurable spaces. On morphisms,
f bg : X bY Ñ‚A bB is given by
f bg : pX ˆY qˆ ΣAˆB :“px ˆyqˆ E ÞÑ
ż
a:A
ż
b:B
δAbBpE|x, yqfpda|xqgpdb|yq
where, as above, δAbBpE|a, bq“ 1 iff pa, bqP E and 0 otherwise. Note that pf bgqpE|x, yq“
pg bfqpE|y, xqfor all s-finite kernels (and allE, x and y), by the Fubini-Tonelli theorem for s-finite
measures [60, 258], and so bis symmetric on sfKrn.
The monoidal unit in sfKrn is again I “1, the singleton set. Unlike in KℓpDq, however, we do
have nontrivial effects p : XÑ‚I, given by kernels p : pX ˆΣ1q– X Ñr0, 8s, with which we
will represent density functions.
Comonoids in sfKrn Every object in sfKrn is a comonoid, analogously to KℓpDq. Discarding
is given by the family of effects X : X Ñr0, 8s:“x ÞÑ1, and copying is again Dirac-like:
X : X ˆΣXˆX :“ x ˆE ÞÑ 1 iff px, xq PE and 0 otherwise. Because we have nontrivial
effects, discarding is only natural for causal or ‘total’ channels: if c satisfies ‚c “ , then cp´|xq
is a probability measure for all x in the domain1. And, once again, copying is natural (that is,
‚c “pc bcq‚ ) if and only if the channel is deterministic.
Channels represented by effects We can interpret the string diagrams of §4.1.3 in sfKrn, and
we will do so by following the intuition of the conditional probability notation and reading the string
diagrams from outputs to inputs. Hence, if c : XÑ‚Y is represented by the effect p : X bY Ñ‚I
with respect to the measure µ : IÑ‚Y , then
c : X ˆΣY Ñr0, 8s:“x ˆB ÞÑ
ż
y:B
µpdyqppy|xq.
1This means that the subcategory of total maps in sfKrn is equivalent to the Kleisli category KℓpGqof the Giry monad
G taking each measurable space X to the space GX of measures over X; see Example 4.1.30 for more details.
140
Note that we also use conditional probability notation for density functions, and so ppy|xq :“
p ˝px ˆyq.
Suppose that c : XÑ‚Y is indeed represented by p with respect to µ, and that d : Y Ñ‚Z is
represented by q : Y bZÑ‚I with respect to ν : IÑ‚Z. Then in sfKrn, d ‚c : XÑ‚Z is given by
d ‚c : X ˆΣZ :“x ˆC ÞÑ
ż
z:C
νpdzq
ż
y:Y
qpz|yqµpdyqppy|xq
Alternatively, by defining the effect ppµqq: X bZÑ‚I as
ppµqq: X ˆZ Ñr0, 8s:“x ˆz ÞÑ
ż
y:Y
qpz|yqµpdyqppy|xq,
we can write d ‚c as
d ‚c : X ˆΣZ :“x ˆC ÞÑ
ż
z:C
νpdzqppµqqpz|xq.
Bayesian inversion via density functions Once again writing π : IÑ‚X for a prior on X, and
interpreting the string diagram of Proposition 4.1.28 for c:
π : Y Ñ‚X in sfKrn, we have
c:
π : Y ˆΣX Ñr0, 8s:“y ˆA ÞÑ
ˆż
x:A
πpdxqppy|xq
˙
p´1pyq
“p´1pyq
ż
x:A
ppy|xqπpdxq,
(4.9)
where p´1 : Y Ñ‚I is a µ-almost-inverse for effect p‚pπbidY q, and is given up toµ-almost-equality
by
p´1 : Y Ñr0, 8s:“y ÞÑ
ˆż
x:X
ppy|xqπpdxq
˙´1
.
Note that from this we recover the informal form of Bayes’ rule for measurable spaces(4.6). Suppose
π is itself represented by a density function pπ with respect to the Lebesgue measure dx. Then
c:
πpA|yq“
ż
x:A
ppy|xqpπpxqş
x1:X ppy|x1qpπpx1qdx1 dx.
4.1.5. On probability monads
Later, it will at times be helpful to work in a category of stochastic channels that is the Kleisli category
for a monad, without fixing that monad in advance; in this case we will speak of aprobability monad.
Unfortunately, an abstract characterization of probability monads is not presently known to the
author, and so we use this term informally. However, when we do so, we have in mind a monoidal
monad that maps spaces to spaces of measures or valuations on them, and that maps morphisms to
the corresponding pushforwards. In the setting of finitary probability, we have already seen one
example, the monad D explored in §4.1.1. Here we note the existence of others.
141
Example 4.1.30 (Giry monad [120]). Let Meas denote the category of measurable spaces, whose
objects are sets equipped withσ-algebras and whose morphisms are measurable functions. TheGiry
monad G : Meas ÑMeas maps each measurable space pX, ΣXqto the space GX of probability
measures α : ΣX Ñr0, 1sover it, equipped with the smallest σ-algebra making the evaluation
functions
evU : GX Ñr0, 1s
α ÞÑαpUq
measurable for all U PΣX. Given a measurable function f : X ÑY , the function Gf : GX ÑGY
is defined by pushforwards: that is, for each α : GX, we define
Gfpαq: ΣY Ñr0, 1s
V ÞÑα
`
f´1pV q
˘
.
(We may also writef˚α to denote Gfpαq.) The unit of the monadη has components ηX : X ÑGX
mapping each point x to the corresponding Dirac measure δx, which is defined by δxpUq“ 1 iff
x PU and δxpUq“ 0 otherwise. Finally, the multiplication µ has components µX : GGX ÑGX
defined by integration, analogous to the ‘evaluation’ of D (Def. 4.1.6): for each ν : GGX, define
µXpνq: ΣX Ñr0, 1s
U ÞÑ
ż
α:GX
αpUqdν .
Note that the subcategory of total morphisms in sfKrn is equivalent to KℓpGq.
The category Meas has all finite limits (it has products and equalizers), and this will mean
that we will be able in Chapter 6 to define “effectful polynomials” in KℓpGq, and hence obtain
categories of continuous-time continuous-space open Markov processes. However, Meas does
not have exponentials and is therefore not Cartesian closed, because the evaluation function
evR,R : MeaspR, Rqˆ R Ñ R : pf, xq ÞÑfpxqis not measurable, for any choice of σ-algeba
on the function space MeaspR, Rq[15]. This means that KℓpGqcannot be enriched in Meas,
and so we cannot define Bayesian lenses internally to Meas. Circumnavigating this obstruction
would complicate our construction of cilia — dynamical systems that control lenses — which are
central to our formalization of predictive coding. This is because the output maps of stochastic
dynamical systems are deterministic functions: in the case of systems in KℓpGq, this means they
are morphisms in Meas; for a general probability monad P : E ÑE, they are morphisms in E.
142
For a system to be able to emit a lens, therefore, the hom objects ofBayesLens must be objects in
E, and this in turn requires KℓpPqto be enriched in E. Fortunately, as the following example notes,
a suitable probability monad does exist.
Example 4.1.31 (Quasi-Borel spaces [131]). A quasi-Borel space is a set X equipped with a set
MX of ‘random variables’ on X taking samples from the real line, MX ĂXR. The set MX is
taken to satisfy three closure properties: (i) MX contains all constant functions; (ii) MX is closed
under composition with measurable functions, such that if ρ PMX and f : R ÑR is measurable
with respect to the standard Borel structure on R, then ρ ˝f PMX; and (iii) MX is closed under
gluing ‘disjoint Borel domains’, meaning that if R is countably partitioned by R –ř
i:N Si, and
if tαiui:N Ă MX, then the function px P Siq ÞÑαipxqis in MX. A function f : X Ñ Y is a
morphism of quasi-Borel spaces if for all ρ P MX, f ˝ρ P MY . Quasi-Borel spaces and their
morphisms form a category,QBS, and this category is Cartesian closed: ifX and Y are quasi-Borel
spaces, then QBSpX, Yqcan be given a quasi-Borel structure MXY by defining
MXY :“tρ : R ÑQBSpX, Yq|
`
ρ5 : R ˆX ÑY
˘
PQBSpR ˆX, Yqu.
A probability measure on a quasi-Borel space X is defined to be a pair of a (standard) probability
measure ν on R and a random variable ρ PMX. Since two different pairs pν, ρqand pµ, τqmay
produce equal pushforward measures, ρ˚ν “ τ˚µ, it makes sense to consider two such QBS
measures to be equivalent if their pushforwards are equal. The set PX of such equivalence classes
of QBS measures on X can then be equipped with the structure of a quasi-Borel space, and the
assignment P is made functorial by the pushforwards action. Finally, the functorP : QBS ÑQBS
can be equipped with the structure of a (monoidal) monad in a manner analogous to the Giry
monad: the unit yields Dirac measures, and the multiplication acts by integration.
We end this section by noting that the notions of s-finite measure and s-finite kernel can be
reconstructed within QBS, so that we may interpret sfKrn to be enriched accordingly [270, §11].
Moreover, Vákár and Ong [270] show that the set T Xof s-finite measures on X can be given a
quasi-Borel structure, and this assignment actually yields a monad T : QBS ÑQBS (by analogy
with the ‘continuation’ monad). This licenses us to take sfKrn to be instead defined as KℓpTq.
For further examples of probability monads, we refer the reader to Jacobs [139].
143
4.2. Dependent data and bidirectional processes
Two properties of Bayesian inversion are particularly notable. Firstly, given a channelXÑ‚Y , its
inversion yields a channel in the opposite direction, Y Ñ‚X. Secondly, this inverse channel does
not exist in isolation, but rather depends on a supplied ‘prior’ distribution. In Chapter 7 we will
want to assign functorially to stochastic channels dynamical systems that invert them, and to do
this requires understanding how inversions compose. The general pattern for the composition
of dependent bidirectional processes is called the lens pattern, and this section is dedicated to
introducing it. The more fundamental aspect is that of dependence, which we began to explore
in the context of dependent sums and products in Chapter 2: we therefore begin this section by
introducing the Grothendieck construction , a ‘fibrational’ framework for composing dependent
processes.
4.2.1. Indexed categories and the Grothendieck construction
At various point above, we have encountered ‘dependent’ objects and morphisms: indexed and
dependent sums (Remark 2.3.10); indexed products (Remark 2.3.20); dependent products (§2.3.4.1);
hints at dependent type theory (end of §2.3.4); parameterized morphisms (§3.2.2); circuit algebras
(§3.3); and, of course, Bayesian inversions. The Grothendieck construction classifies each of these
as examples of a common pattern, allowing us to translate between ‘indexed’ and ‘fibrational’
perspectives: from the indexed perspective, we consider functors from an indexing object into a
category (think of diagrams); from the fibrational perspective, we consider bundles as projection
maps. The correspondence is then, roughly speaking, between “the object indexed by i” and “the
subobject that projects to i”, which is called the ‘fibre’ of the bundle over i.
For this reason, categories of bundles are an important part of the story, from which much else
is generalized. Recall from Definition 3.2.10 that these categories of bundles are slice categories:
the category of bundles over B in C is the slice C{B, whose objects are pairs pE, pqof an object E
and a morphism p : E ÑB; and whose morphisms pE, pqÑp E1, p1qare morphisms α : E ÑE1
of C such that p “p1˝α. We call this the category of bundles over B as a generalization of the
notion of “fibre bundle”, from which we inherit the notion of ‘fibre’.
Definition 4.2.1. Suppose C is a category with finite limits. Given a bundle p : E ÑB in C, its
fibre over b : B is the subobject Eb of E such that ppeq “b for all e : Eb. The fibre Eb can be
144
characterized as the following pullback object, where 1 is the terminal object in C:
Eb E
1 Bb
p{
In the case where C “ Set, there is an equivalence between the slice Set {B and a certain
presheaf category: the category of B-diagrams in Set, which we can equivalently think of as the
category of B-indexed sets.
Definition 4.2.2. Suppose B is a set. The discrete category on X is the category whose objects are
the elements of B and whose only morphisms are identity morphisms idb : b Ñb for each element
b : B. We will denote the discrete category on B simply by B.
Proposition 4.2.3. For each set B, there is an equivalence Set {B –SetB.
Proof. In the direction Set {B ÑSetB, let p : E ÑB be a bundle over B. We construct a functor
P : B ÑSet by defining Ppbq:“Eb, where Eb is the fibre of p over b; there are no nontrivial
morphisms in B, so we are done. Now suppose f : pE, pqÑp F, qqis a morphism of bundles. A
natural transformation φ : P ñQ in SetB is just a family of functions φb : P bÑQb indexed by
b. Hence, given f, we define φb as the restriction of f to Eb for each b : B.
In the direction SetB ÑSet {B, let P : B ÑSet be a functor. We define E as the coproduct
ř
b:B Ppbq, and the bundle p : E ÑB as the projection pb, xqÞÑ b for every pb, xqin ř
b:B Ppbq.
Now suppose φ : P ñQ is a natural transformation in SetB. We define the functionf : pE, pqÑ
pF, qqby the coproduct of the functions φb, as f :“ř
b:B φb.
These two constructions are easily verified as mutually inverse.
If theB in SetB is not just a set, but rather a category, then there is a correspondingly categorified
notion of the category of bundles.
Definition 4.2.4. Suppose F : C ÑSet is a copresheaf on C. Its category of elements C{F has
for objects pairs pX, xqof an object X : C and an element x : F X. A morphism pX, xqÑp Y, yq
is a morphism f : X ÑY in C such that F fpxq“ y, as in the following diagram, where the top
triangle commutes in Set:
1
F X F Y
X Y
x
F f
y
f
.
145
Identities are given by identity morphisms in C, and composition is composition of the underlying
morphisms in C. There is an evident forgetful functor πF : C{F ÑC, which acts on objects as
pX, xqÞÑ X and on morphisms as f ÞÑf.
To validate that the category of elements construction is a good generalization of the slice
category, we have the following example.
Example 4.2.5. The category of elements of a representable copresheaf CpC, ´qis equivalent to
the slice category C{C, from which we derive the similar notation.
Remark 4.2.6. Another way to look at the morphisms pX, xqÑp Y, yqin C{F is as pairs pf, ιq,
where f is a morphism X Ñ Y in C and ι is an identification F fpxq “y. Then composition
in C{F is not just composition of morphisms in C, but also composition of identifications: given
pf, ιq: pX, xqÑp Y, yqand pg, κq: pY, yqÑp Z, zq, the compositepg, κq˝pf, ιqis pg˝f, κ˝F gpιqq,
where κ ˝F gpιqis the composite identification Fpg ˝fqpxq
F gpιq
ù ùùù ùF gpyq
κ
ùùz. We can think of
these identifications as witnesses to the required equalities. This perspective on C{F is analogous
to the process of categorification we considered in Chapter 2, where we added witnesses (fillers) to
equations and diagrams.
A better way to validate the category of elements construction is to generalize the Grothendieck
correspondence, Proposition 4.2.3, which means we need something to correspond to SetB: a
category of categories of elements. These generalized categories of elements are called “discrete
opfibrations”, and constitute our first examples of categorified bundles.
Definition 4.2.7. A discrete opfibration is a functor F : E ÑB such that, for every object E in
E and morphism g : F EÑB in B, there exists a unique morphism h : E ÑE1 in E such that
F h“g (called the lift of g):
E E1
F E B
h
g
Write DOpfibpBqto denote the full subcategory of Cat{B on those objects which are discrete
opfibrations. The subcategory EB of E all of whose objects are mapped by F to B and all of whose
morphisms are mapped to idB is called the fibre of F over B.
Example 4.2.8. The forgetful functorπF : C{F ÑC out of the category of elements of a copresheaf
F is a discrete opfibration: for any object pX, xqin C{F and morphism g : X ÑY in C, there is a
unique morphism g : pX, xqÑp Y, yq, namely where y “F fpxq.
146
And thus we obtain a Grothendieck correspondence at the next level of categorification.
Proposition 4.2.9. For any category B, there is an equivalence DOpfibpBq– SetB.
Proof sketch. We only sketch the bijection on objects; the correspondence on morphisms subse-
quently follows quite mechanically.
Given a discrete opfibrationp : E ÑB, it is easy to check that each fibreEb is a discrete category
and hence a set. Given a morphism f : b Ñc in B, we define a function φ : Eb ÑEc by mapping
each e : Eb to the codomain of the unique lift h. This defines a functor B ÑSet; functoriality
follows from the uniqueness of lifts.
In the inverse direction, given a copresheafF : B ÑSet, take the forgetful functorπF : B{F Ñ
B out of its category of elements, which is a discrete opfibration by the example above. Given a
natural transformation σ : F ñG, define a functor S : B{F ÑB{G on objects as SpX, xq“
pX, σXpxqqand on morphisms f : pX, xqÑp Y, yqas Sf “pX, σXpxqq
f
Ý ÑpY, σY pyqq; this is
well-defined by the naturality of σ and the definition of f, since Gf ˝σXpxq“ σY ˝F fpxqand
F fpxq“ y.
The verification that these two constructions are mutually inverse is straightforward.
In many cases, the dependent data of interest will have more structure than that of mere sets. For
example, in §3.3 we introduced a rate-coded circuit diagrams as an indexing of sets of rate-coded
circuits by a category of circuit diagrams; later, we will see that dynamical systems have a canonical
notion of morphism, and so our dynamical semantics will take the form of an indexed collection of
categories. This requires us to categorify not only the domain of indexing (as we have done above),
but also the codomain of values (as we do now). As with monoidal categories—and as in the case
of circuit algebras—in this higher-dimensional setting, it becomes necessary to work with weak
composition, and the relevant notion of weak functor is the ‘pseudofunctor’.
Definition 4.2.10. Suppose C is a category and B is a bicategory. A pseudofunctor F : C ÑB is
constituted by
1. a function F0 : C0 ÑB0 on objects;
2. for each pair of objects a, b: C, a function Fa,b : Cpa, bqÑ BpF0a, F0bq0 on morphisms;
3. for each object c : C, a 2-isomorphism Fidc : idF0c ñFc,cpidcqwitnessing weak unity, natural
in c; and
147
4. for each composable pair of morphisms f : a Ñb and g : b Ñc in C, a 2-isomorphism
Fg,f : Fb,cpgq˛ Fa,bpfq ñFa,cpg ˝fqwitnessing weak functoriality , natural in f and g,
where we have written composition in C as ˝and horizontal composition in B as ˛;
satisfying the following conditions:
(a) coherence with left and right unitality of horizontal composition, so that the respective
diagrams of 2-cells commute:
idF0b ˛Fa,bpfq Fa,bpfq
Fb,bpidbq˛ Fa,bpfq Fa,bpidb ˝fq
λFa,bpfq
Fidb˛Fa,bpfq
Fidb,f
Fa,bpfq˛ idF0a Fa,bpfq
Fa,bpfq˛ Fa,apidaq Fa,bpf ˝idaq
ρFa,bpfq
Fa,bpfq˛Fida
Ff,ida
(b) coherence with associativity of horizontal composition, so that the following diagram of
2-cells commutes:
pFc,dphq˛ Fb,cpgqq˛ Fa,bpfq Fc,dphq˛p Fb,cpgq˛ Fa,bpfqq
Fb,dph ˝gq˛ Fa,bpfq Fc,dphq˛ Fa,cpg ˝fq
Fa,dpph ˝gq˝ fq Fa,dph ˝pg ˝fqq
αFc,dphq,Fb,cpgq,Fa,bpfq
Fc,dphq˛Fg,f
Fh,g˝f
Fh,g˛Fa,bpfq
Fh˝g,f
.
Remark 4.2.11. If C is in fact a nontrivial bicategory, then the definition of pseudofunctor is
weakened accordingly: the functions Fa,b are replaced by functors between the corresponding
hom-categories, and the equalities in the functoriality conditions (a) and (b) are replaced by the
relevant unitor or associator isomorphism. We will encounter this more general case in the next
chapter, where we introduce the (yet weaker) concept oflax functor : see Definition 5.2.8, and the
associated footnote 5 for the relationship with the present notion of pseudofunctor.
With pseudofunctors, we gain a notion of indexed category.
Definition 4.2.12. An indexed category is a pseudofunctor F : C op ÑCat, for some indexing
category C. An opindexed category is a pseudofunctor F : C ÑCat. Given an (op)indexed category
F, we call the categories F cits fibres, for each object c : C.
148
Working with indexed categories rather than indexed sets, the relevant notion of (op)fibration
is no longer discrete, as there are now (non-trivial) morphisms to account for. Following the
Grothendieck logic, fibrations p : E Ñ B should be in bijective correspondence with indexed
categories F : B op Ñ Cat. This means that we should be able to turn any indexed category
into a fibration by appropriately gluing together its fibres; and conversely, given a fibration p, the
assignment b ÞÑEb2 should define a pseudofunctor B op ÑCat. These considerations yield the
following definition.
Definition 4.2.13. A fibration is a functor p : E Ñ B such that, for every pair of morphisms
f : E1 Ñ E and ϕ : E2 Ñ E in E, and for every morphism g : ppE2q ÑppE1qsuch that
ppϕq“ ppfq˝ g in B, there exists a unique morphism h : E2 ÑE1in E such that pphq“ g and
ϕ “f ˝h:
E2 E1
E
ppE2q ppE1q
ppEq
h
g
f
ϕ
ppϕq
ppfq
The subcategory EB of all those objects mapped by p to B : B and all those morphisms mapped to
idB is called the fibre of p over B. An opfibration is a functorp : E ÑB for which pop : E op ÑB op
is a fibration.
Remark 4.2.14. Note that a discrete (op)fibration is a(n) (op)fibration each of whose fibres is
a discrete category: this means that in each fibre, there are no non-identity morphisms, so that
morphisms f and ϕ in the definition above are trivialized, thereby recovering the form of Definition
4.2.7.
The Grothendieck construction then tells us how to translate from (op)indexed categories to
(op)fibrations: in some situations, it will be easier to work with the one, and in others the other.
In particular, categories of lenses (and polynomial functors) will be seen to arise as Grothendieck
constructions.
Definition 4.2.15. Suppose F : C op ÑCat is a pseudofunctor. Its (contravariant) Grothendieck
construction is the category
ş
F defined as follows. The objects of
ş
F are pairs pX, xqof an object
2with Eb being the subcategory of E sometimes denoted p´1pbqall of whose objects are mapped by p to b, as in the
proof of Proposition 4.2.9.
149
X : C and an object x : F X. A morphism pX, xq Ñ pY, yqis a pair pf, φqof a morphism
f : X ÑY in C and a morphism φ : x ÑF fpyqin F X, as in the following diagram, where the
upper triangle is interpreted in Cat (note the contravariance of F f):
1
F X F Y
X Y
x
F f
y
f
φ
We can thus write the hom set
ş
F
`
pX, xq, pY, yq
˘
as the dependent sumř
f : CpX,Y qF X
`
x, Ffpyq
˘
.
The identity morphism on pX, xqis pidX, idxq, and composition is defined as follows. Given
pf, φq: pX, xqÑp Y, yqand pg, γq: pY, yqÑp Z, zq, their composite pg, γq˝p f, φqis the pair
pg ˝f, F fpγq˝ φq.
The following well-known result tells us that the Grothendieck construction yields fibrations.
Proposition 4.2.16 (Johnson and Yau [145, Prop. 10.1.10]). Suppose F : C op ÑCat is an indexed
category. Then there is a ‘projection’ functor πF :
ş
F ÑC mapping pX, xqÞÑ X and pf, φqÞÑ f,
and this functor is a fibration.
Remark 4.2.17. Dually, there is a covariant Grothendieck construction , for opindexed categories
F : C Ñ Cat. The objects of
ş
F are again pairs pX : C, x: F Xq, but now the morphisms
pX, xq Ñ pY, yqare pairs pf, φqwith f : X ÑY in C as before and now φ : F fpxq Ñy; all
that we have done is swapped the direction of the arrow F fin the diagram in Definition 4.2.15
(compare the identifications in the category of elements of a copresheaf, in Definition 4.2.4). As a
result, we can write the hom set
ş
F
`
pX, xq, pY, yq
˘
in this case as ř
f : CpX,Y qF X
`
F fpxq, y
˘
.
Remark 4.2.18. The Grothendieck construction induces an analogue of Proposition 4.2.9 between
the bicategory of pseudofunctors B op ÑCat and the bicategory of Grothendieck fibrations on B
[145, Theorem 10.6.16]. Indeed there are analogues of Propositions 4.2.9 and 4.2.3 in any categorical
dimension. Because fibrations are the higher-dimensional analogues of bundles, they have a base
category (the codomain) and a ‘total’ category (the domain), which is a kind of colimit of the
fibres (constructed by the Grothendieck construction): strictly speaking, what we have called
the Grothendieck construction above is total category of the full fibrational construction; the
fibration itself is the corresponding forgetful (projection) functor. For a highly readable exposition
of Grothendieck constructions, we refer the reader to Loregian and Riehl [173].
150
4.2.1.1. The monoidal Grothendieck construction
When C is a monoidal category with which F is appropriately compatible, then we can ‘upgrade’
the notions of indexed category and Grothendieck construction accordingly. In this chapter, we
will restrict ourselves to locally trivial monoidal indexed categories, those for which the domain C
is only a category; Moeller and Vasilakopoulou [189] work out the structure for bicategorical C.
(As noted in Remark 4.2.11, in Chapter 5, we will sketch a notion of monoidal indexed bicategory
which amounts to a categorification of the present notion; but that will also in some sense be locally
trivial.)
Definition 4.2.19 (After Moeller and Vasilakopoulou [189, §3.2]). Suppose pC, b, Iqis a monoidal
category. We say that F is a monoidal indexed category when F is a lax monoidal pseudofunctor
pF, µ, ηq: pC op, bop, IqÑp Cat, ˆ, 1q. This means that the laxator µ is given by a natural family
of functors µA,B : F AˆF BÑ FpA bBqalong with, for any morphisms f : A Ñ A1 and
g : B ÑB1 in C, a natural isomorphism µf,g : µA1,B1 ˝pF fˆF gqñ Fpf bgq˝ µA,B. (This
makes µ into a pseudonatural transformation in the sense of Definition 5.2.9.) The laxator and the
unitor η : 1 ÑF Itogether satisfy axioms of associativity and unitality that constitute indexed
versions of the associators and unitors of a monoidal category (Definition 3.1.4).
Explicitly, this means that there must be three families of natural isomorphisms, indexed by
objects A, B, C: C,
1. an associator family αA,B,C : µAbB,C pµA,Bp´, ´q, ´qñ µA,BbCp´, µB,C p´, ´qq;
2. a left unitor λA : µI,Apη, ´qñ idF A; and
3. a right unitor ρA : µA,Ip´, ηqñ idF A
such that the unitors are compatible with the associator, i.e. for all A, B: C the diagram
µAbI,B pµA,Ip´, ηq, ´q µA,IbBp´, µI,B pη, ´qq
µA,Bp´, ´q
µPA,BpρA,´q
αA,I,B p´,η,´q
µA,ΛB p´,λBq
commutes (where P and Λ are the right and left associators of the monoidal structure pb, Iqon
C), and such that the associativity is ‘order-independent’, i.e. for all A, B, C, D: C, the diagram
151
µAbpBbCq,DpµA,BbCp´, µB,Cp´,´qq,´q µA,pBbCqbDp´, µBbC,DpµB,Cp´,´q,´qq
µpAbBqbC,DpµAbB,CpµA,Bp´,´q,´q,´q µA,BbpCbDqp´, µB,CbDp´, µC,Dp´,´qqq
µAbB,CbDpµA,Bp´,´q, µC,Dp´,´qq
αA,BbC,D
µAA,B,C,DpαA,B,C,´q
αAbB,C,D αA,B,CbD
µA,AB,C,Dp´,αB,C,Dq
commutes (where A is the associator of the monoidal structure on C).
The following proposition exhibits the monoidal structure carried by the Grothendieck
construction when the indexed category is monoidal.
Proposition 4.2.20 (Moeller and Vasilakopoulou [189, §6.1]). Suppose pF, µ, ηq: pC op, bop, IqÑ
pCat, ˆ, 1q is a monoidal indexed category. Then the total category of the Grothendieck
construction
ş
F obtains a monoidal structure pbµ, Iµq. On objects, define
pC, Xqbµ pD, Yq:“
`
C bD, µCD pX, Yq
˘
where µCD : F CˆF DÑFpC bDqis the component of µ at pC, Dq. On morphisms pf, f:q:
pC, Xq ÞÑpC1, X1qand pg, g:q: pD, Yq ÞÑpD1, Y1q, define
pf, f:qbµ pg, g:q:“
`
f bg, µCD pf:, g:q
˘
.
The monoidal unit Iµ is defined to be the object Iµ :“
`
I, ηp˚q
˘
. Writing λ : I bp´q ñ p´q
and ρ : p´qb I ñp´q for the left and right unitors of the monoidal structure on C, the left and
right unitors in
ş
F are given by pλ, idqand pρ, idqrespectively. Writing α for the associator of the
monoidal structure on C, the associator in
ş
F is given by pα, idq.
Remark 4.2.21. Sometimes, rather than (or in addition to) an indexed category F being lax
monoidal as a pseudofunctor (which yields a ‘global’ monoidal structure), it may in fact befibrewise
monoidal, meaning that each fibre F Xis itself a monoidal category (yielding ‘local’ monoidal
structures); in this case, the pseudofunctor F can be written with the type C op Ñ MonCat.
In general, the fibrewise monoidal structures may be independent both of each other and of
the lax monoidal structure on F itself, but when C is in fact Cartesian monoidal, the local and
global monoidal structures coincide. For more reading on this, we refer the reader to Moeller and
Vasilakopoulou [189, §4].
152
4.2.2. Grothendieck lenses
Lenses formalize bidirectional processes in which the ‘backward’ process depends on data in the
domain of the ‘forward’ process. The name originates in database theory [34, 99], where the
forward process gives a zoomed-in ‘view’ onto a database record, and the backward process is used
to update it. Following an observation of Myers and Spivak [238], lenses of this general shape can
be given a concise definition using the Grothendieck construction. In order to obtain the backward
directionality of the dependent part, we use the “pointwise opposite” of a pseudofunctor.
Definition 4.2.22. Suppose F : C op ÑCat is a pseudofunctor. We define its pointwise opposite
Fp : C op ÑCat to be the pseudofunctor c ÞÑF cop returning the opposite of each category F c;
given f : c Ñc1, Fpf : F cop ÑF c1op is defined as pF fqop : F cop ÑF c1op.
Categories of Grothendieck lenses are then obtained via the Grothendieck construction of
pointwise opposites of pseudofunctors.
Definition 4.2.23 (Grothendieck lenses [238]). We define the category LensF of Grothendieck
lenses for a pseudofunctorF : C op ÑCat to be the total category of the Grothendieck construction
for the pointwise opposite Fp of F. Explicitly, its objects pLensF q0 are pairs pC, Xqof objects C
in C and X in FpCq, and its hom-sets LensF
`
pC, Xq, pC1, X1q
˘
are the dependent sums
LensF
`
pC, Xq, pC1, X1q
˘
“
ÿ
f : CpC,C1q
FpCq
`
FpfqpX1q, X
˘
(4.10)
so that a morphism pC, Xq ÞÑ pC1, X1q is a pair pf, f:q of f : CpC, C1q and f: :
FpCq
`
FpfqpX1q, X
˘
. We call such pairs Grothendieck lenses for F or F-lenses. We say that the
morphism f is the forward component of the lens, and the morphism f:the backward component.
The identity Grothendieck lens on pC, Xqis idpC,Xq “pidC, idXq. Sequential composition is
as follows. Given pf, f:q: pC, Xq ÞÑpC1, X1qand pg, g:q: pC1, X1q ÞÑpD, Yq, their composite
pg, g:q pf, f:qis defined to be the lens
`
g ‚f, f:˝Fpfqpg:q
˘
: pC, Xq ÞÑpD, Yq.
Notation 4.2.24. In the context of lenses, we will often write the backward map as f:or f7, with
the former particularly used for Bayesian lenses. We will also use ÞÑto denote a lens, and  for
lens composition. Above, we additionally used ‚for composition in the base category and ˝for
composition in the fibres.
Since lenses are bidirectional processes and English is read horizontally, when it comes to string
diagrams for lenses, we will depict these horizontally, with the forwards direction read from left to
right.
153
Whenever C is a monoidal category, it gives rise to a canonical category of lenses, in which the
forwards morphisms are comonoid morphisms in C and the backwards morphisms are (internally)
parameterized by the domains of the forwards ones. Comonoids and their morphisms are necessary
to copy parameters during composition. The resulting ‘monoidal’ lenses are a natural generalization
of the ‘Cartesian’ lenses used in the database setting, and we will see that Bayesian lenses are
similarly constructed using an indexed category of (externally) parameterized morphisms.
Example 4.2.25. Suppose pC, b, Iqis a monoidal category and let ComonpCqbe its subcategory
of comonoids and comonoid morphisms. A monoidal lens pX, Aq ÞÑpY, Bqis a pair pf, f7qof a
comonoid morphism f : X ÑY in ComonpCqand a morphism f7 : X bB ÑA in C. Such
lenses can be characterized as Grothendieck lenses, following Spivak [238, §3.2].
First, define a pseudofunctor P : ComonpCqop ÑCat as follows. On objects X : ComonpCq,
define PX as the category with the same objects as C and with hom-sets given by PXpA, Bq:“
CpX bA, Bq; denote a morphism f from A to B in PX by f : A XÝ ÑB. The identity idA : A XÝ ÑA
is defined as the projection projA : X bA
XbidA
Ý ÝÝÝÝÝ ÑI bA λA
Ý Ý ÑA. Given f : A XÝ ÑB and
g : B XÝ ÑC, their composite g ˝f : A XÝ ÑC is given by the following string diagram in C:
f
gX
A
B
.
Given h : X Ñ Y in ComonpCq, the functor Ph : PY Ñ PX acts by precomposition on
morphisms, taking f : A YÝ ÑB to the morphism Phpfq: A XÝ ÑB given by
X bA hbidA
Ý ÝÝÝ ÑY bA
f
Ý ÑB .
(An alternative way to obtain PX is as the ‘coKleisli’ category of the comonad X bp´q.)
The category of monoidal lenses is then defined to be the category of Grothendieck P-lenses.
The objects of LensP are pairs pX, Aqof a comonoid X and an object A in C, and the morphisms
are monoidal lenses. Given lenses pf, f7q: pX, AqÑp Y, Bqand pg, g7q: pY, BqÑp Z, Cq, the
composite lens has forward component given by g ˝f : X ÑZ and backward component given
by f7˝Pfpg7q: C XÝ ÑA.
We can depict monoidal lenses string-diagrammatically, with the forward and backward
components oriented in opposite directions. To exemplify this, note that, because the forwards
154
components are comonoid morphisms, the following equality holds for all composite monoidal
lenses pg, g7q˝p f, f7q:
f
f7
X
A B
Y
X
g
g7 C
Z
Y “
f
f7
X
A
g
g7 C
Z
f
Here, we have decorated the strings with fletches to indicate the direction of information-flow
and disambiguate the bidirectionality, and drawn boxes around the pairs that constitute each lens.
Note however that the parameterizing input to the backwards component of the first lens is not
constrained to be a copy of the input to the forwards component; it is only for compositional
convenience that we depict lenses this way.
Definition 4.2.26. When C is Cartesian monoidal, so that its monoidal structure pˆ, 1qis the
categorical product, we will call monoidal lenses in C Cartesian lenses .
Remark 4.2.27. The string-diagrammatic depictions of lenses above were not strictly formal, or at
least we have not explain how they might be; we have not exhibited a coherence theorem such as
3.1.10. In this case, the diagrams above are depictions in the graphical calculus of Boisseau [35].
An alternative graphical language for a generalization of lenses called optics[184, 217] has been
described by Román [219].
Monoidal lenses find uses not only in database theory, but in many other situations, too: the
general pattern is “interacting systems where information flows bidirectionally”. In economics
(specifically, compositional game theory), lenses are used to model the pattern of interaction of
economic games: the forward maps encode how players act in light of observations, and the
backward maps encode how utility is passed “backwards in time” from outcomes, in order to
assign credit[119]. In non-probabilistic machine learning, lenses can be used to formalize reverse
differentiation and hence the backpropagation of error (another kind of credit assignment): the
forwards maps represent differentiable processes (such as neural networks), and the backward maps
are the reverse-derivatives used to pass error back (e.g., between neural network layers)[74, 96].
155
Generalizations of lenses known as optics[184, 217] have also been used both to model economic
games with uncertainty (‘mixed’ strategies)[36] and to model the process of dynamic programming
(Bellman iteration) used in the related field of reinforcement learning[128], as well as to model
client-server interactions in computing[277].
In systems theory, lenses can be used to formalize various kinds of dynamical system: the forward
maps encode their ‘outputs’ or ‘actions’, and the backward maps encode how states and inputs
give rise to transitions[191]. This latter application will be a particular inspiration to us, and is
closely related to Example 4.2.30, which expresses polynomial functors as lenses (thereby explaining
Proposition 3.5.4), and for which we need the following canonical family of indexed categories.
Definition 4.2.28. When a categoryC has pullbacks, its slice categoriesC{C collect into an indexed
category C{p´q: C op ÑCat called the (contravariant3) self-indexing of C, and defined as follows.
On objects C : C, the self-indexing unsurprisingly returns the corresponding slice categories C{C.
Given a morphism f : A ÑB, the functor C{f : C{B ÑC{A is defined by pullback. On objects
pE, pq: C{B, we define pC{fqpE, pq:“pf˚E, f˚pq, where f˚E is the pullback object A ˆB E
and f˚p is the associated projection to A. On morphisms φ : pE, pqÑp E1, p1qin C{B, we define
pC{fqpφqas the morphism f˚φ : pf˚E, f˚pqÑp f˚E1, f˚p1qinduced by the universal property
of the pullback f˚E1, as in the commuting diagram
f˚E E
f˚E1 E1
A B
φf˚φ
{
p1
p
f
f˚p1
{
f˚p .
Remark 4.2.29. The functors C{f : C{B ÑC{A are also known as base-change functors, as they
change the ‘base’ of the slice category.
Example 4.2.30. The category Poly of polynomial functors (§3.5) is equivalent to the category
of Grothendieck lenses for the self-indexing of Set: that is, Poly –LensSet {p´q. To see this,
observe that the objects of LensSet {p´qare bundles p : E ÑB of sets. If we define the set pristo
be the fibre Ei of p for each i : B, we have an isomorphism E –ř
i:B pris. We can then define
3‘Contravariant’ in contradistinction to the covariant self-indexing of Remark 3.2.12 (in the context of external
parameterization as change-of-enrichment).
156
a polynomial functor P :“ř
i:B ypris, and then find that Pp1q“ B, which justifies writing the
original bundle as p : ř
i:pp1qprisÑ pp1q. We saw in Proposition 3.5.4 how to associate to any
polynomial functor P a bundle p, and it is easy to check that applying this construction to the P
defined here returns our original bundle p. This shows that the objects of Poly are in bijection
with the objects of LensSet {p´q. What about the morphisms?
A morphism p Ñ q in LensSet {p´q, for p : X Ñ A and q : Y Ñ B is a pair of functions
f1 : A ÑB and f7 : f˚
1 Y ÑX such that f˚
1 q “p ˝f7, as in the following diagram:
X f˚
1 Y Y
A A B
f7
qf˚
1 qp
f1
{
Replacing the bundles p and q by their polynomial representations p : ř
i:pp1qprisÑ pp1qand
q : ř
j:qp1qqrjsÑ qp1q, we see that the pair pf1, f7qis precisely a morphism of polynomials of the
form established in Proposition 3.5.4, and that every morphism of polynomials corresponds to such
a lens. This establishes an isomorphism of hom-sets, and hence Poly –LensSet {p´q.
Lenses are also closely related to wiring diagrams[239, 282] and our linear circuit diagrams
(§3.3.2).
Example 4.2.31. Let FVect denote the category of finite-dimensional real vector spaces and
linear maps between them; write n for the object Rn. FVecthas a Cartesian monoidal product
p`, 0qgiven by the direct sum of vectors (n `m “Rn ‘Rm “Rn`m), and whose unit object
is 0. The category of monoidal lenses in pFVect, `, 0qis the category of linear circuit diagrams
(Example 3.3.9).
Cartesian lenses pX, Aq ÞÑpY, Bqare in some sense ‘non-dependent’ lenses, because the domain
of the backwards map is a simple product X ˆB, in which the object B does not depend on x : X.
We can see polynomial functors as a dependent generalization of Cartesian lenses in Set.
Proposition 4.2.32. The category of monoidal lenses in pSet, ˆ, 1q is equivalently the full
subcategory of Poly on the monomials XyA.
Proof sketch. A morphism of monomials pf1, f7q: XyA ÑY yB is a pair of functions f1 : X ÑY
and f7 : X ˆB ÑA; this is a Cartesian lens pX, AqÑp Y, Bq. There is clearly a bijection of
objects XyA ØpX, Aq.
157
In particular, this situation encompasses linear circuit diagrams, which embed into Poly
accordingly.
Remark 4.2.33. There is a forgetful functor from vector spaces to sets, U : FVect ÑSet. If we
write LenspCqto denote the category of monoidal lenses inC (with the relevant monoidal structure
left implicit), this forgetful functor induces a ‘change of base’ LenspUq : LenspFVectq Ñ
LenspSetq, since the Grothendieck construction is functorial by Remark 4.2.18, and hence so is
the Lens construction. There is therefore a canonical embedding of linear circuit diagrams into
Poly, LenspFVectq
LenspUq
ÝÝÝÝÝÑLenspSetqãÑPoly.
Our dynamical semantics for approximate inference (Chapter 7) can, if one squints a little, be
therefore seen as a kind of probabilistic generalization of our algebra for rate-coded neural circuits:
it will be an algebra for (a stochastic analogue of) the multicategory OPoly with semantics in
categories of (stochastic) dynamical systems. One can see a morphism of polynomials therefore
as a kind of ‘dependent’ circuit diagram, with the forwards component transporting ‘outgoing’
information from inside a (‘nested’) system to its boundary (its external interface), and the backward
component transporting ‘incoming’ information (“immanent signals”) from the boundary internally,
depending on the configuration of the boundary.
Of course, to give an OPoly-algebra is to give a lax monoidal functor, which means knowing
the relevant monoidal structure. While we saw this in the case of polynomial functors of sets in
Proposition 3.5.7, it will be helpful when it comes to generalizing Poly to see how this structure is
obtained. Moreover, we will want a monoidal structure on Bayesian lenses, in order to define joint
approximate inference systems. For these reasons, we now turn to monoidal categories of lenses.
4.2.2.1. Monoidal categories of lenses
The monoidal structures on categories of Grothendieck lenses—at least those of interest here—are a
direct corollary of the monoidal Grothendieck construction, Proposition 4.2.20.
Corollary 4.2.34. When F : C op ÑCat is equipped with a monoidal indexed category structure
pµ, ηq, its category of lenses LensF becomes a monoidal category pLensF , b1
µ, Iµq. On objects
b1
µ is defined as bµ in Proposition 4.2.20, as is Iµ. On morphisms pf, f:q: pC, Xq ÞÑpC1, X1qand
pg, g:q: pD, Yq ÞÑpD1, Y1q, define
pf, f:qb1
µ pg, g:q:“
`
f bg, µop
CD pf:, g:q
˘
158
where µop
CD : FpCqop ˆFpDqop ÑFpC bDqop is the pointwise opposite of µCD . The associator
and unitors are defined as in Proposition 4.2.20.
As an example, this gives us the tensor product on Poly, which is inherited by the category of
Cartesian lenses in Set.
Example 4.2.35. The tensor product structure pb, yqon Poly is induced by a monoidal indexed
category structure pµ, ηqon the self-indexing of pSet, ˆ, 1q. To define the unitor η, first note that
Set {1 –Set, so that η equivalently has the type 1 ÑSet; we thus make the natural choice for η,
the terminal element ˚ÞÑ 1. The laxator µ is defined for each B, C: Set by the functor
µB,C : Set {B ˆ Set {C Ñ Set {pB ˆCq
`
p :
ÿ
i:B
prisÑ B, q :
ÿ
j:C
qrjsÑ C
˘
ÞÑ
ÿ
pi,jq:BˆC
prisˆ qrjs
the naturality and functoriality of which follow from the functoriality of ˆ. Applying Corollary
4.2.34 to this structure, we obtain precisely the tensor product of polynomials introduced in
Proposition 3.5.7.
Corollary 4.2.36. Since the category of Cartesian lenses in Set is the monomial subcategory
of Poly, to which the tensor structure pb, yqrestricts, the latter induces a symmetric monoidal
structure on the former, the unit of which is the object p1, 1q. Given objects pX, Aqand pX1, A1q,
their tensor pX, Aqbp X1, A1qis pX ˆX1, AˆA1q. Given lenses pf, f7q: pX, AqÑp Y, Bqand
pf1, f17q: pX1, A1qÑp Y 1, B1q, their tensor has forward component f ˆf1 : X ˆX1 ÑY ˆY 1
and backward component
X ˆX1ˆB ˆB1 idX ˆσX1,BˆidB1
ÝÝÝÝÝÝÝÝÝÝÑX ˆB ˆX1ˆB1 f7ˆf17
ÝÝÝÝÑA ˆA1
where σ is the symmetry of the product ˆ.
We will see that the monoidal structure on Bayesian lenses is defined similarly. First of all, we
need to define Bayesian lenses themselves.
4.3. The bidirectional structure of Bayesian updating
In this section, we define a collection of indexed categories, each denotedStat, whose morphisms can
be seen as generalized Bayesian inversions. Following Definition 4.2.23, these induce corresponding
categories of lenses which we call Bayesian lenses . In §4.3.3, we show abstractly that, for the
159
subcategories of exact Bayesian lenses whose backward channels correspond to ‘exact’ Bayesian
inversions, the Bayesian inversion of a composite of forward channels is given (up to almost-
equality) by the lens composite of the corresponding backward channels. This justifies calling these
lenses ‘Bayesian’, and provides the foundation for the study of approximate (non-exact) Bayesian
inversion in Chapter 5.
Remark 4.3.1. Bayesian lenses, and the result that “Bayesian updates compose optically”, were
first introduced by the present author in [248]. Braithwaite and Hedges [41] then elaborated the
structure to define dependent Bayesian lenses, solving the ‘divide-by-zero’ issue already indicated
in Remark 4.1.22. All three authors then joined forces to produce a paper [42], published at MFCS
2023, which we take to be a canonical summary of the definitions and basic results.
4.3.1. State-dependent channels
As we saw in §4.1, a channel c : XÑ‚Y admitting a Bayesian inversion induces a family of inverse
channels c:
π : Y Ñ‚X, indexed by ‘prior’ states π : 1Ñ‚X. Making the state-dependence explicit, in
typical cases where c is a probability kernel we obtain a function c: : GX ˆY ÑGX, under the
assumption that c‚π is fully supported for allπ : GX (see Remark 4.1.20 for our justification of this
simplifying assumption). In more general situations, and in light of the full-support assumption,
we obtain a morphism c: : CpI, XqÑ CpY, Xqin the base of enrichment of the monoidal category
pC, b, Iqof c, which for simplicity we take to be Set (although the construction still succeeds for
an arbitrary Cartesian base of enrichment). We call morphisms of this general type state-dependent
channels, and structure the indexing as an indexed category.
Definition 4.3.2. Let pC, b, Iqbe a monoidal category. Define the C-state-indexed category
Stat : C op ÑCat as follows.
Stat : C op Ñ Cat
X ÞÑStatpXq:“
¨
˚˚˝
StatpXq0 :“ C0
StatpXqpA, Bq :“ Set
`
CpI, Xq, CpA, Bq
˘
idA : StatpXqpA, Aq :“
"idA : CpI, XqÑ CpA, Aq
ρ ÞÑ idA
˛
‹‹‚
(4.11)
f : CpY, XqÞÑ
¨
˚˚˚˚˝
Statpfq : StatpXq Ñ StatpY q
StatpXq0 “ StatpY q0
SetpCpI, Xq, CpA, Bqq Ñ Set
`
CpI, Yq, CpA, Bq
˘
α ÞÑ
`
σ : CpI, Yq
˘
ÞÑ
`
αf‚σ : CpA, Bq
˘
˛
‹‹‹‹‚
160
Composition in each fibre StatpXqis as in C. Explicitly, indicating morphisms CpI, XqÑ CpA, Bq
in StatpXqby A XÝ Ñ‚ B, and given α : A XÝ Ñ‚ B and β : B XÝ Ñ‚ C, their composite β ˝α : A XÝ Ñ‚ is defined
by pβ ˝αqρ :“βρ ‚αρ, where here we indicate composition in C by ‚and composition in the
fibres StatpXqby ˝. Given f : Y Ñ‚X in C, the induced functor Statpfq: StatpXqÑ StatpY qacts
by pre-composition (compare Definition 4.2.28 of the functorial action of the self-indexing); for
example:
Statpfqpαq : CpI, Yq
CpI,f q
Ý ÝÝÝ ÑCpI, Xq αÝ ÑCpA, Bq
σ ÞÑ f ‚σ ÞÑ αf‚σ
.
Remark 4.3.3. If we do not wish to make the full-support assumption, and instead we know that
the category C has a well-defined notion of support object [41, 109, 259], then for a given general
channel c : XÑ‚Y , we can write the type of its Bayesian inversionc:as ś
π:CpI,X qC
`
supppc‚πq, Y
˘
.
As Braithwaite and Hedges [41] show, this corresponds to a morphism in a certain fibration, and
gives rise to a category of dependent Bayesian lenses; see Remark 4.1.20.
Notation 4.3.4. Just as we wrote X MÝÑY for an internally M-parameterized morphism in
CpM dX, Yq(see Proposition 3.2.3) and A ΘÝ ÑB for an externally Θ-parameterized morphism
in E
`
Θ, CpA, Bq
˘
(see Definition 3.2.11), we write A XÝ Ñ‚ B for an X-state-dependent morphism
in Set
`
CpI, Xq, CpA, Bq
˘
. Given a state ρ in CpI, Xqand an X-state-dependent morphism f :
A XÝ Ñ‚ B, we write fρ for the resulting morphism in CpA, Bq.
Remark 4.3.5. The similarities between state-dependent channels and externally parameterized
functions are no coincidence: the indexed category Stat is closely related to an indexed category
underlying external parameterization in Set, which in previous work, reported by Capucci,
Gavranović, and St Clere Smithe [53], we called Prox (for ‘proxies’).
When C is a Kleisli category KℓpTq, it is of course possible to define a variant of Stat on the
other side of the product-exponential adjunction, with state-dependent morphisms A XÝ Ñ‚ B having
the types T XˆA ÑT B. This avoids the technical difficulties sketched in the preceding example
at the cost of requiring a monad T. However, the exponential form makes for better exegesis, and
so we will stick to that.
We will want to place inference systems side-by-side, which means we want a monoidal category
structure for Bayesian lenses. Following Corollary 4.2.34, this means Stat needs to be a monoidal
indexed category.
Proposition 4.3.6. Stat is a monoidal indexed category, in the sense of Definition 4.2.19. The
components µXY : StatpXqˆ StatpY q ÑStatpX bY qof the laxator are defined on objects
161
by µXY pA, A1q:“A bA1 and on morphisms f : A XÝ Ñ‚ B and f1 : A1YÝ Ñ‚B1 as the X bY -state-
dependent morphism denoted f bf1and given by the function
µXY pf, f1q: CpI, XbY qÑ CpA bA1, BbB1q
ω ÞÑfωX bf1
ωY
.
Here, ωX and ωY are the X and Y marginals of ω, given by ωX :“projX ‚ω and ωY :“projY ‚ω.
(Note that this makes µ into a strict transformation in the sense of Definition 5.2.9.) The unit
η : 1 ÑStatpIqof the lax monoidal structure is the functor mapping the unique object ˚: 1 to the
unit object I : StatpIq.
Remark 4.3.7. Note that Stat is also fibrewise monoidal in the sense of Remark 4.2.21, as an almost
trivial consequence of C being monoidal. We will not make use of this structure in this chapter, but
we will return to it in the construction of statistical games in §5.3.1.
At this point, we can turn to Bayesian lenses themselves.
4.3.2. Bayesian lenses
We define the category of Bayesian lenses in C to be the category of Grothendieck Stat-lenses.
Definition 4.3.8. The category BayesLensC of Bayesian lenses in C is the category LensStat of
Grothendieck lenses for the functor Stat. A Bayesian lens is a morphism in BayesLensC. Where
the category C is evident from the context, we will just write BayesLens.
Unpacking this definition, we find that the objects of BayesLensC are pairs pX, Aqof objects
of C. Morphisms (that is, Bayesian lenses) pX, Aq ÞÑpY, Bqare pairs pc, c:qof a channel c : XÑ‚Y
and a generalized Bayesian inversion c: : B XÝ Ñ‚ A; that is, elements of the hom objects
BayesLensC
`
pX, Aq, pY, Bq
˘
: “LensStat
`
pX, Aq, pY, Bq
˘
–CpX, Yqˆ Set
`
CpI, Xq, CpB, Aq
˘
.
The identity Bayesian lens on pX, Aqis pidX, idAq, where by abuse of notation idA : CpI, YqÑ
CpA, Aqis the constant map idA defined in Equation (4.11) that takes any state on Y to the identity
on A.
The sequential composite pd, d:q pc, c:qof pc, c:q: pX, Aq ÞÑpY, Bqand pd, d:q: pY, Bq ÞÑ
pZ, Cqis the Bayesian lens
`
pd ‚cq, pc:˝c˚d:q
˘
: pX, Aq ÞÑpZ, Cqwhere pc:˝c˚d:q: C XÝ Ñ‚ A
takes a state π : IÑ‚X to the channel c:
π ‚d:
c‚π : CÑ‚A.
162
To emphasize the structural similarity between Bayesian and monoidal lenses, and visualize the
channel c:
π ‚d:
c‚π, note that following Example 4.2.25, we can depict Bayesian lens composition
using the graphical calculus of Boisseau [35] as
c
c:
X
A B
Y
X
d
d: C
Z
Y “
c
c:
X
A
d
d: C
Z
c
.
Remark 4.3.9. Strictly speaking, these depictions are diagrams in Boisseau [35]’s calculus of
string diagrams for optics, which means that they are not direct depictions of the Bayesian lenses
themselves; rather they are depictions of the corresponding optics, which we define and elaborate
in [248]. Briefly, these optics are obtained by embedding the categories of forrwards and backwards
channels into their corresponding (co)presheaf categories and coupling them together along the
‘residual’ category C; in the depictions, the string diagrams in the forwards and backwards directions
are thus interpreted in these different categories. This explains why we are allowed to ‘copy’ the
channel c in the depiction above, producing the right-hand side by pushing c through the copier as
if it were a comonoid morphism: it is because the comonoids in question are CpI, Xqand CpI, Yq,
and the function CpI, cqis indeed a comonoid morphism, even though c is in general not!
Remark 4.3.10. Note that the definition of Stat and hence the definition of BayesLensC do
not require C to be a copy-delete category, even though our motivating categories of stochastic
channels are; all that is required for the definition is that C is monoidal. On the other hand, as
we can define Bayesian lenses in any copy-delete category, we can define them in Set, where
Setp1, Xq– X for every set X: in this case, Bayesian lenses coincide with Cartesian lenses.
Of course, since Stat is a monoidal indexed category, BayesLensC is a monoidal category.
Proposition 4.3.11. BayesLensC is a monoidal category, with structure
`
pb, pI, Iq
˘
inherited
from C. On objects, define pA, A1qbp B, B1q :“ pA bA1, BbB1q. On morphisms pf, f:q :
pX, Aq ÞÑpY, Bqand pg, g:q: pX1, A1q ÞÑpY 1, B1q, define pf, f:qbp g, g:q:“pf bg, f:bg:q,
where f: bg: : B bB1XbX1
Ý ÝÝÝ Ñ‚ A bA1 acts on states ω : IÑ‚X bX1 to return the channel
163
f:
ωX bg:
ωX1, following the definition of the laxator µ in Proposition 4.3.6. The monoidal unit in
BayesLensC is the pair pI, Iqduplicating the unit in C. When C is moreover symmetric monoidal,
so is BayesLensC.
Proof sketch. The main result is immediate from Proposition 4.3.6 and Corollary 4.2.34. When bis
symmetric in C, the symmetry lifts to the fibres of Stat and hence to BayesLensC.
But BayesLensC is not in general a copy-discard category.
Remark 4.3.12. Although BayesLensC is a monoidal category, it does not inherit a copy-discard
structure from C, owing to the bidirectionality of its component morphisms. To see this, we can
consider morphisms into the monoidal unit pI, Iq, and find that there is generally no canonical
discarding map. For instance, a morphism pX, Aq ÞÑpI, Iqconsists in a pair of a channel XÑ‚I
(which may indeed be a discarding map) and a state-dependent channel I XÝ Ñ‚ A, for which there is
generally no suitable choice satisfying the comonoid laws. Note, however, that a lens of the type
pX, Iq ÞÑpI, Bqmight indeed act by discarding, since we can choose the constant state-dependent
channel B XÝ Ñ‚ I on the discarding map : BÑ‚I. By contrast, the Grothendieck category
ş
Stat
is a copy-delete category, as the morphisms pX, AqÑp I, Iqin
ş
Stat are pairs XÑ‚I and A XÝ Ñ‚ I,
and so for both components we can choose morphisms witnessing the comonoid structure.
4.3.3. Bayesian updates compose optically
In this section we prove the fundamental result that justifies the development ofstatistical games as
hierarchical inference systems in Chapter 5: that the Bayesian inversion of a composite channel is
given up to almost-equality by the lens composite of the backwards components of the associated
‘exact’ Bayesian lenses.
Definition 4.3.13. Let pc, c:q: pX, Xq ÞÑpY, Yqbe a Bayesian lens. We say that pc, c:qis exact
if c admits Bayesian inversion and, for each π : IÑ‚X such that c ‚π has full support, c and c:
π
together satisfy equation (4.5) (p. 133). Bayesian lenses that are not exact are said to beapproximate.
Theorem 4.3.14. Let pc, c:qand pd, d:qbe sequentially composable exact Bayesian lenses. Then,
for any state π on the domain of c, the contravariant component c:˝c˚d:of the composite lens
pd, d:q pc, c:qis the Bayesian inversion of d ‚c. That is to say, Bayesian updates compose optically :
pd ‚cq:
π
d‚c‚π„ c:
π ‚d:
c‚π.
164
Proof. For any suitably-typedπ, the state-dependent channelc:˝c˚d:returns the channelc:
π ‚d:
c‚π :
ZÑ‚X, so to establish the result it suffices to show that
d:c‚π
π
c
d
c:π
X Z
“
c
π
d
X Z
.
We have
d:c‚π
π
c
d
c:π
X Z
“
d
π
c
c:π
X Z
“
c
π
d
X Z
where the first obtains because d:
c‚π is by assumption a Bayesian inverse of d with respect to c ‚π,
and the second because c:
π is likewise a Bayesian inverse of c with respect to π. Hence, c:
π ‚d:
c‚π
and pd ‚cq:
π are both Bayesian inversions of d ‚c with respect to π. Since Bayesian inversions are
almost-equal (Proposition 4.1.29), we have c:
π ‚d:
c‚π
d‚c‚π„ pd ‚cq:
π, as required.
This theorem has the following important immediate consequence.
Corollary 4.3.15. Suppose C:is a subcategory ofC all of whose channels admit Bayesian inversion,
and consider the restriction to C: of the fibration πLens : BayesLensC ÑC of Bayesian lenses,
denoted π:
Lens. Then there is an almost sure section :: C: ÑBayesLensC of π:
Lens taking each
object X to pX, Xqand each channel c : XÑ‚Y to a lens pc, c:q: pX, Xq ÞÑpY, Yqwhere c:is an
almost-surely unique Bayesian inversion of c. Hence the composite C
:
Ý ÑBayesLensC
π:
Lens
Ý ÝÝ ÑC is
equal to the identity functor idC.
Remark 4.3.16. A morphism σ : B Ñ E is a section of π : E Ñ B when π ˝σ “ idB. In
standard category theory, a section of a fibration π is therefore a functor: but, since Bayesian
165
inversion is only defined up to almost-equality, the functoriality of the preceding corollary is
accordingly weakened. This leads to the notion of almost sure section , which we formalize by
lifting the relation of almost-equality from C to BayesLensC, as follows. Suppose pc, c7qand
pd, d7qare lenses pX, Xq ÞÑ pY, Yq. Then we may say that they are equivalent up to almost
equality, denoted pc, c7q « pd, d7q, if for all states α : IÑ‚X, we have c α„ d and c7
α
c‚α„ d7
α. If
additionally we have c “d, we write pc, c7q»p d, d7qand say that they are strongly equivalent.
Note then that the mapping :of the preceding corollary is functorial up to this strong equivalence:
:pdq :pcq»:p d ‚cq; this is what we mean by almost sure section . We believe this notion (and the
implicit more general one of almost sure functor ) to be new, but do not study it further here.
Remark 4.3.17. In the context of finitely-supported probability (i.e., in KℓpDq), almost-equality
coincides with simple equality over the support, and so Bayesian inversions are then just equal
(over the support). This suggests that, in this context, :may be strengthened to a strict functor: but
the qualification over the support means we must use the machinery of dependent Bayesian lenses
(Remark 4.1.20); then, :does yield a strict functor.
Remark 4.3.18. Note that the functor :is not monoidal, because inverting the tensor of two
channels with respect to a joint distribution is not the same as inverting the two channels
independently with respect to the marginals and tensoring them together (unless the joint is
already the product of two independent states); that is, pc bdq:
ω ‰c:
ω1 bd:
ω2, where ω1 and ω2
are the two marginals of the joint state ω. Technically, this situation obtains because there is no
channel X1 bX2Ñ‚X1 bX2 that performs this marginalization-then-tensoring that could play the
part of the laxator of :. (But note that typically a probability monad P will be ‘bimonoidal’, with
the ‘opmonoidal’ structure PpX1 ˆX2qÑ PX1 ˆPX2 witnessing this joint-marginalization
operation [110, §4]; the technical hurdle is that this structure typically interacts nicely with the
monad structure, since the tensor of two Dirac deltas is again a Dirac delta.)
In §5.4, we will use the machinery of statistical games to measure the error produced by inverting
two channels independently, versus inverting them jointly.
Historically, lenses have often been associated with ‘lens laws’: additional axioms guaranteeing
their well-behavedness. These laws originate in the context of database systems, and we now
investigate how well they are satisfied by Bayesian lenses, where one might see an inference system
as a kind of uncertain database. We will find that Bayesian lenses are not lawful in this traditional
sense, because they ‘mix’ information.
166
4.3.4. Lawfulness of Bayesian lenses
The study of Cartesian lenses substantially originates in the context of bidirectional transformations
of data in the computer science and database community [34, 99], where we can think of the view
(or get) function as returning part of a database record, and theupdate (or put) function as ‘putting’
a part into a record and returning the updated record. In this setting, axioms known as lens laws
can be imposed on lenses to ensure that they are ‘well-behaved’ with respect to database behaviour:
for example, that updating a record with some data is idempotent (the ‘put-put’ law).
We might hope that well-behaved or “very well-behaved” lenses in the database context should
roughly correspond to our notion of exact Bayesian lens: with the view that an inference system,
formalized by a Bayesian lens, is something like a probabilistic database. However, as we will see,
even exact Bayesian lenses are only weakly lawful in the database sense: Bayesian updating mixes
information in the prior state (the ‘record’) with the observation (the ‘data’), rather than replacing
the prior information outright.
We will concentrate on the three lens laws that have attracted recent study [35, 217]: GetPut,
PutGet, and PutPut. A Cartesian lens satisfying the former two is well-behaved while a lens
satisfying all three isvery well-behaved, in the terminology of Foster et al. [99]. Informally,GetPut
says that getting part of a record and putting it straight back returns an unchanged record;PutGet
says that putting a part into a record and then getting it returns the same part that we started with;
and PutPut says that putting one part and then putting a second part has the same effect on a
record as just putting the second part (that is, update completely overwrites the part in the record).
We will express these laws graphically, and consider them each briefly in turn.
Note first that we can lift any channel c in the base category C into any state-dependent fibre
StatpAqusing the constant (identity-on-objects) functor taking c to the constant-valued state-
indexed channel ρ ÞÑc that maps any state ρ to c. We can lift string diagrams in C into the fibres
accordingly.
GetPut
Definition 4.3.19. A lens pc, c:qis said to satisfy the GetPut law if it satisfies the left equality
in (4.12) below. Equivalently, because the copier induced by the Cartesian product is natural (i.e.,
˝f “pf ˆfq˝ ), for any state π, we say that pc, c:qsatisfies GetPut with respect to π if it
167
satisfies the right equality in (4.12) below.
c
c:
“ ùñ
π
c
c:
π
π
“ (4.12)
(Note that here we have written the copying map as , since we are assuming an ambient Cartesian
monoidal structure; hence for a Bayesian lens we interpret the left diagram above in the image of
the Yoneda embedding.)
Proposition 4.3.20. When c is causal, the exact Bayesian lens pc, c:qsatisfies the GetPut law
with respect to any state π for which c admits Bayesian inversion.
Proof. Starting from the right-hand-side of (4.12), we have the following chain of equalities
π
c:
π
π
c
c
π
“ “
π
“
π
c
c:
π
“
where the first holds by the counitality of , the second by the causality of c, the third since c
admits Bayesian inversion (4.5) with respect to π, and the fourth again by counitality.
Note that by Bayes’ law, exact Bayesian lenses only satisfy GetPut with respect to states. This
result means that, if we think of c as generating a prediction c ‚π from a prior belief π, then if our
observation exactly matches the prediction, updating the prior π according to Bayes’ rule results in
no change.
168
PutGet The PutGet law is characterized for a lens pv, uqby the following equality:
u
v
“
In general, PutGet does not hold for exact Bayesian lenses pc, c:q. However, because GetPut
holds with respect to states π, we do have c ‚c:
π ‚c ‚π “c ‚π; that is, PutGet holds for exact
Bayesian lenses pc, c:qfor the prior π and ‘input’ c ‚π.
The reason PutGet fails to hold in general is that Bayesian updating mixes information from
the prior and the observation, according to the strength of belief. Consequently, updating a belief
according to an observed state and then producing a new prediction need not result in the same
state as observed; unless, of course, the prediction already matches the observation.
PutPut Finally, the PutPut law for a lens pv, uqis characterized by the following equality:
u
u
u“
PutPut fails to hold for exact Bayesian lenses for the same reason that PutGet fails to hold in
general: updates mix old and new beliefs, rather than entirely replace the old with the new.
Comment In the original context of computer databases, there is assumed to be no uncertainty,
so a ‘belief’ is either true or false. Consequently, there can be no ‘mixing’ of beliefs; and in database
applications, such mixing may be highly undesirable. Bayesian lenses, on the other hand, live in a
fuzzier world: our present interest in Bayesian lenses originates in their application to describing
cognitive and cybernetic processes such as perception and action, and here the ability to mix beliefs
according to uncertainty is desirable.
Possibly it would be of interest to give analogous information-theoretic lens laws that characterize
exact and approximate Bayesian lenses and their generalizations; and we might then expect the
169
‘Boolean’ lens laws to emerge in the extremal case where there is no uncertainty and only Dirac
states. We leave such an endeavour for future work: Bayes’ law (4.5) is sufficiently concise and
productive for our purposes here.
170
5. Statistical games
In this chapter, we characterize a number of well known systems of approximate inference as loss
models (defined in §5.3.2): lax sections of 2-fibrations of statistical games, themselves constructed
(in §5.3.1) by attaching internally-defined loss functions to Bayesian lenses. Our examples include
the relative entropy (§5.3.3.1), which constitutes a strict section, and whose chain rule is formalized
by the horizontal composition of the 2-fibration. In order to capture this compositional structure,
we first introduce the notion of ‘copy-composition’ (in §5.2.1), alongside corresponding bicategories
through which the composition of copy-discard categories factorizes. These latter bicategories are
obtained as a variant of the Copara construction [54, §2] (dual to the internal parameterization of
§3.2.1), and so we additionally introduce coparameterized Bayesian lenses (§5.2.3), proving that
coparameterized Bayesian updates compose optically (§5.2.4), as in the non-coparameterized case.
Besides the relative entropy, our other examples of loss models are given by maximum likelihood
estimation (§5.3.3.2), the free energy (which gives us in §5.3.3.3 a characterization of autoencoders),
and the ‘Laplace’ approximation to the free energy (§5.3.3.4). It is this latter loss model which will,
in Chapter 7, finally yield the dynamical semantics for predictive coding.
We begin with a discussion of compositional approximate inference from the ‘lens’ perspective,
focusing on the relative entropy.
5.1. Compositional approximate inference, via the chain rule for
relative entropy
In Chapter 4, we observed that the Bayesian inversion of a composite stochastic channel is (almost
surely) equal to the ‘lens composite’ of the inversions of the factors; that is, Bayesian updates
compose optically (‘BUCO’, Theorem 4.3.14). Formalizing this statement for a given category C
yields a fibration of Bayesian lenses as a Grothendieck construction of the indexed category of
state-dependent channels (Definition 4.3.8), and Bayesian inversion almost surely yields a section :
of the corresponding fibration (Corollary 4.3.15). This section :picks out a special class of Bayesian
lenses, which we call exact as they compute ‘exact’ inversions (Definition 4.3.13), but although
171
the category BayesLenspCqhas many other morphisms, its construction is not extravagant: by
comparison, we think of the non-exact lenses as representing approximate inference systems. This
is particularly necessary in computational applications, because computing exact inversions is
usually intractable, but this creates a new problem: choosing an approximation, and measuring
its performance. In this chapter, we formalize this process, by attaching loss functions to Bayesian
lenses, thus creating another fibration, of statistical games . Sections of this latter fibration encode
compositionally well-behaved systems of approximation that we call loss models .
A classic example of a loss model will be supplied by the relative entropy, which in some sense
measures the ‘divergence’ between distributions: the game here is then to minimize the divergence
between the approximate and exact inversions. If π and π1 are two distributions on a space X,
with corresponding density functions pπ and pπ1 (both with respect to a common measure), then
their relative entropy Dpπ, π1qis the real number given by Ex„π rlog pπpxq´ log pπ1pxqs1. Given
a pair of channels α, α1 : AÑ‚B (again commensurately associated with densities), we can extend
D to a map Dα,α1 : A ÑR`in the natural way, writing a ÞÑD
`
αpaq, α1paq
˘
. We can assign such
a map Dα,α1 to any such parallel pair of channels, and so, following the logic of composition in
C2, we might hope for the following equation to hold for all a : A and composable parallel pairs
α, α1 : AÑ‚B and β, β1 : BÑ‚C:
Dβ‚α,β1‚α1paq“ E
b„αpaq
“
Dβ,β1pbq
‰
`Dα,α1paq
The right-hand side is known as the chain rule for relative entropy, but, unfortunately, the
equation does not hold in general, because the composites β ‚α and β1‚α1each involve an extra
expectation:
Dβ‚α,β1‚α1paq“ E
c„β‚αpaq
“
log pβ‚αpaqpcq´ log pβ1‚α1paqpcq
‰
“ E
c„β‚αpaq
„
log E
b„αpaq
rpβpc|bqs´ log E
b„α1paq
“
pβ1pc|bq
‰ȷ
However, we can satisfy an equation of this form by using ‘copy-composition’: if we write B to
denote the canonical ‘copying’ comultiplication on B, and define β ‚2 α :“pidB bβq‚ B ‚α, as
depicted by the string diagram
α βA
B
C
1For details about this ‘expectation’ notation E, see 5.3.19.
2In which, following the Chapman-Kolmogorov rule, a composite channel β ‚α can be expressed as the expectation of
β under α, i.e. a ÞÑEb„αpaqrβpbqs.
172
then Dβ‚2α,β1‚2α1paqdoes equal the chain-rule form on the right-hand side:
Dβ‚2α,β1‚2α1paq“ E
b„αpaq
E
c„βpbq
“
log pβpc|bqpαpb|aq´ log pβ1pc|bqpα1pb|aq
‰
“ E
b„αpaq
„
E
c„βpbq
“
log pβpc|bq´ log pβ1pc|bq
‰
`log pαpb|aq´ log pα1pb|aq
ȷ
“ E
b„αpaq
“
Dβ,β1pbq
‰
`Dα,α1paq
where the second line follows by the linearity of expectation. This result exhibits a general pattern
about copy-discard categories (Definition 3.1.3) such as C: composition ‚can be decomposed into
first copying , and then discarding . If we don’t discard, then we retain the ‘intermediate’
variables, and this results in a functorial assignment of relative entropies to channels.
The rest of this chapter is dedicated to making use of this observation to construct loss models,
including (but not restricted to) the relative entropy. The first complication that we encounter
is that copy-composition is not strictly unital, because composing with an identity retains an
extra variable. To deal with this, in §5.2, we construct a bicategory of copy-composite channels,
extending the Copara construction, and build coparameterized (copy-composite) Bayesian lenses
accordingly; we also prove a corresponding BUCO result. In §5.3, we then construct 2-fibrations
of statistical games, defining loss functions internally to any copy-discard category C that admits
“bilinear effects”. Because we are dealing with approximate systems, the 2-dimensional structure of
the construction is useful: loss models are allowed to be lax sections. We then characterize the
relative entropy, maximum likelihood estimation, the free energy, and the ‘Laplacian’ free energy
as such loss models.
Unsurprisingly, each of these loss functions are moreover (lax) monoidal, and, assuming C is
symmetric monoidal, each of the constructions mentioned here result in monoidal (2-)fibrations.
We explore this monoidal structure in §5.4.
5.2. ‘Copy-composite’ Bayesian lenses
5.2.1. Copy-composition by coparameterization
In a locally small copy-discard category C (Definition 3.1.3), every object A is equipped with a
canonical comonoid structure p A, Aq, and so, by the comonoid laws (Definition 3.4.21), it is
almost a triviality that the composition functions ‚: CpB, Cqˆ CpA, BqÑ CpA, Cqfactorize as
CpB, Cqˆ CpA, Bq
pidB b´qˆC
´
idA, B
¯
Ý ÝÝÝÝÝÝÝÝÝÝÝÝÝ ÑCpB bB, BbCqˆ CpA, BbBq ¨¨¨
¨¨¨ ‚Ý ÑCpA, BbCq
CpidA,projCq
ÝÝÝÝÝÝÝÑCpA, Cq
173
where the first factor copies the B output of the first morphism and tensors the second morphism
with the identity on B, the second factor composes the latter tensor with the copies, and the
third discards the extra copy of B3. This is, however, only almost trivial, since it witnesses the
structure of Chapman-Kolmogorov style composition in categories of stochastic channels such as
KℓpDq, the Kleisli category of the (finitary) distributions monad D : Set ÑSet (§4.1.1.1). There,
given channels c : AÑ‚B and d : BÑ‚C, the composite d ‚c is formed first by constructing the
‘joint’ channel, denoted d ‚2 c and defined by pd ‚2 cqpb, c|aq:“dpc|bqcpb|aq, and then discarding
(marginalizing over) b : B, giving
pd ‚cqpc|aq“
ÿ
b:B
pd ‚2 cqpb, c|aq“
ÿ
b:B
dpc|bqcpb|aq.
Of course, the channel d ‚2 c is not a morphism AÑ‚C, but rather AÑ‚B bC; that is, it is
coparameterized by B, in a sense formally dual to the notion of parameterization of §3.2.1.
Moreover, as noted above, ‚2 is not strictly unital: given the composites idB ‚2f and f‚2 idA,
we need 2-cells that discard the coparameters introduced by the copying; and, inversely, we need
2-cells f ÞÑidB ‚2f and f ÞÑf‚2 idA that introduce them. The former are of course given by the
discarding structure
fA
B
M
B
ÞÑ fA
B
M
“ fA
B
M
(5.1)
fA
B
M
A
ÞÑ fA
B
M
“ fA
B
M
(5.2)
while the latter are given by copying:
fA
B
M
ÞÑ fA
B
M
B
(5.3)
3 We define projC :“B bC
BbidC
Ý ÝÝÝÝÝ ÑI bC
λC
Ý Ý ÑC, using the comonoid counit B and the left unitor λC of C’s
monoidal structure.
174
fA
B
M
ÞÑ fA
B
M
A
(5.4)
These putative 2-cells clearly need access to copies of the domain and codomain of f, and hence
are not available in the standard Copara construction obtained by formally dualizing Para. For
this reason, we construct a bicategory Copara2pCqas a variant of the Copara construction, in
which a 1-cell A ÑB may be any morphism AÑ‚M bB in C, and where horizontal composition
is precisely copy-composition. We will henceforth drop the cumbersome notation ‚2, and write
simply ‚for horizontal composition in Copara2pCq, matching the composition operator of C itself.
(Later, if we need to be explicit about horizontal composition, we will sometimes use the symbol ˛.)
Theorem 5.2.1. Let pC, b, Iqbe a copy-discard category. Then there is a bicategory Copara2pCq
as follows. Its 0-cells are the objects of C. A 1-cell f : A ÝÑ
M
B is a morphism f : A ÑM bB in
C. A 2-cell φ : f ñf1, with f : A ÝÑ
M
B and f1 : A Ý Ý Ñ
M1
B, is a morphism φ : A bM bB ÑM1
of C, satisfying the change of coparameter axiom:
f1A
B
M1
“ fA
B
M1
φ
Given 2-cells φ : f ñf1and φ1 : f1 ñf2, their vertical composite φ1dφ : f ñf2is given by
the following string diagram:
φ φ1
The identity 2-cell idf : f ñ f on f : A ÝÑ
M
B is given by the projection morphism projM :
A bM bB ÑM obtained by discarding A and B, as in footnote 3.
The horizontal composite g ˝f : A Ý ÝÝÝÝÝ Ñ
MbBbN
C of 1-cells f : A ÝÑ
M
B then g : B Ý Ñ
N
C is given
175
by the following string diagram in C:
fA g
C
N
M
B
Strictly speaking, we define the coparameter ofg ˝f to be pM bBqbN. The identity 1-cell idA on
A is given by the inverse of the left unitor of the monoidal structure onC, i.e. idA :“λ´1
A : A Ý Ñ
I
A,
with coparameter thus given by the unit object I.
The horizontal composite γ ˝φ : pg ˝fqñp g1˝f1qof 2-cells φ : f ñf1 and γ : g ñg1 is
given by the string diagram
φ
γ
A
M
B
N
C
N1
B
M1
.
Proof. To show thatCopara2pCqis a bicategory, we need to establish the unitality and associativity
of vertical composition; verify that horizontal composition is well-defined and functorial; establish
the weak unitality and associativity of horizontal composition; and confirm that the corresponding
unitors and associator satisfy the bicategorical coherence laws. Then, to prove that Copara2pCq
is moreover monoidal, we need to demonstrate that the tensor as proposed satisfies the data of a
monoidal bicategory. However, since the monoidal structure is inherited from that ofC, we will
elide much of this latter proof, and demonstrate only that the tensor is functorial; the rest follows
straightforwardly but tediously.
We begin by confirming that vertical composition dis unital and associative. To see that dis
unital, simply substitute the identity 2-cell (given by projection onto the coparameter) into the
string diagram defining dand then apply the comonoid counitality law twice (once on the left,
once on the right). The associativity of drequires that φ2dpφ1dφq“p φ2dφ1qd φ, which
176
corresponds to the following graphical equation:
φ φ1 φ2 “ φ1 φ2φ
To see that this equation is satisfied, simply apply the comonoid coassociativity law twice (once
left, once right).
Next, we check that horizontal composition˝is well-defined, which amounts to checking whether
the horizontal composite of 2-cells satisfies the change of coparameter axiom. Again, we reason
graphically. Given 2-cells φ and γ between composable pairs of 1-cells f, f1and g, g1, our task is to
verify that
f1A g1
C
N1
M1
B
“ f g
γ ˝φ
N1
B
M1
A
C
.
Since φ and γ satisfy change of coparameterex hypothesi, the left hand side is equal to the morphism
fA
M1
φ
g
C
N1
γ
B
.
177
By comonoid coassociativity, this is in turn equal to
f g
φ
γ N1
B
M1
A
C
which, by the definition of ˝, is precisely equal to
f g
γ ˝φ
N1
B
M1
A
C
and so this establishes the result.
We now verify that ˝so defined is functorial on 2-cells, beginning with the preservation of
composition. We need to validate the equation pγ1 ˝φ1qdp γ ˝φq “ pγ1 dγq˝p φ1 dφq(for
appropriately composable 2-cells). This amounts to checking the following equation, which can be
seen to hold by two applications of comonoid coassociativity:
φ
γ
A
M
B
N
C
φ1
γ1
N2
B
M2
“
φ φ1
γ γ1
A
M
B
N
C
B
M2
N2
It is easy to verify that ˝preserves identities, i.e. that idg ˝idf “idg˝f ; just substitute the identity
2-cells into the definition of ˝on 2-cells, and apply comonoid counitality four times.
178
Next, we establish that horizontal composition is weakly associative, which requires us to supply
isomorphisms αf,g,h : ph ˝gq˝ f ñh ˝pg ˝fqnatural in composable triples of 1-cells h, g, f.
Supposing the three morphisms have the types f : A ÝÑ
M
B, g : B Ý Ñ
N
C, and h : C Ý Ñ
O
D, we can
choose af,g,h to be the 2-cell represented by the morphism
A b
`
pM bBqbpp N bCqb Oq
˘
bD
proj
ÝÝÑpM bBqbpp N bCqb Oq ¨¨¨
¨¨¨
αC
pMbBq,pNbCq,O
ÝÝÝÝÝÝÝÝÝÝÑppM bBqbp N bCqqb O ¨¨¨
¨¨¨
αC
pMbBq,N,C bidO
Ý ÝÝÝÝÝÝÝÝÝÝ ÑpppM bBqb Nqb Cqb O
where the first factor is the projection onto the coparameter and αC denotes the associator of
the monoidal structure pb, Iqon C. In the inverse direction, we can choose the component
α´1
f,g,h : h ˝pg ˝fqñp h ˝gq˝ f to be the 2-cell represented by the morphism
A b
`
pppM bBqb Nqb Cqb O
˘
bD
proj
ÝÝÑpppM bBqb Nqb Cqb O ¨¨¨
¨¨¨
αC,´1
pMbBq,N,C bidO
Ý ÝÝÝÝÝÝÝÝÝÝ ÑppM bBqbp N bCqqb O ¨¨¨
¨¨¨
αC,´1
pMbBq,pNbCq,O
ÝÝÝÝÝÝÝÝÝÝÑpM bBqbpp N bCqb Oq
where αC,´1 denotes the inverse of the associator on pC, b, Iq. That the pair of αf,g,h and α´1
f,g,h
constitutes an isomorphism in the hom category follows from the counitality of the comonoid
structures. That this family of isomorphisms is moreover natural follows from the naturality of the
associator on pC, b, Iq.
We come to the matter that motivated the construction of Copara2pCq: the weak unitality of
copy-composition, witnessed here by the weak unitality of horizontal composition. We need to
exhibit two families of natural isomorphisms: the left unitors with components λf : idB ˝f ñf,
and the right unitors with componentsρf : f ˝idA ñf, for each morphismf : A ÝÑ
M
B. Each such
component will be defined by a projection morphism, and weak unitality will then follow from the
counitality of the comonoid structures. More explicitly,λf is witnessed byprojM : AbMbBbB Ñ
M; its inverse λ´1
f is witnessed by projMbB : A bM bB Ñ M bB; ρf is witnessed by
projM : AbAbM bB ÑM; and its inverseρ´1
f is witnessed byprojAbM : AbM bB ÑAbM.
Checking that these definitions give natural isomorphisms is then an exercise in counitality that
we leave to the reader.
All that remains of the proof thatCopara2pCqis indeed a bicategory is to check that the unitors
are compatible with the associator (i.e., pidg ˝λf qdαg,idB,f “ρg ˝idf ) and that associativity is order-
dependent (i.e., the associator α satisfies the pentagon diagram). The latter follows immediately
179
from the corresponding fact about the associator αC on pC, b, Iq. To demonstrate the former, it is
easier to verify that pidg ˝λf qd αg,idB,f dpρ´1
g ˝idf q“ idg˝f . This amounts to checking that the
following string diagram is equally a depiction of the morphism underlying idg˝f :
idf
ρ´1
g
A
M
B
N
C
λf
idg
N
B
M
(Note that here we have elided the associator from the depiction. This is allowed by comonoid
counitality, and because string diagrams are blind to bracketing.) Substituting the relevant
morphisms into the boxes, we see that this diagram is equal to
A
M
B
N
C
N
B
M
and six applications of counitality give us idg˝f . This establishes that Copara2pCqis a bicategory.
Remark 5.2.2. When C is symmetric monoidal, Copara2pCqinherits a monoidal structure,
elaborated in Proposition 5.4.1.
Remark 5.2.3. In order to capture the bidirectionality of Bayesian inversion we will need to
consider left- and right-handed versions of the Copara2 construction. These are formally dual,
and when C is symmetric monoidal (as in most examples) they are isomorphic. Nonetheless, it
makes formalization easier if we explicitly distinguish Coparal
2pCq, in which the coparameter is
placed on the left of the codomain (as above), from Coparar
2pCq, in which it is placed on the right.
Aside from the swapping of this handedness, the rest of the construction is the same.
180
We end this section with three easy (and ambidextrous) propositions relatingC and Copara2pCq.
Proposition 5.2.4. There is an identity-on-objects lax embedding p´q : C ãÑ Copara2pCq,
mapping f : X ÑY to f : X Ý Ñ
I
Y , which (in the left-handed case) has the underlying morphism
X
f
Ý ÑY
λ´1
Y
ÝÝÑI bY (where λ is the left unitor of the monoidal structure on C). The laxator
ιpgq˝ ιpfqÑ ιpg ˝fqdiscards the coparameter obtained from copy-composition.
Remark 5.2.5. We will define the notion of lax functor in Definition 5.2.8 below. A lax embedding
is a lax functor that is an embedding in the sense of Remark 2.4.5: that is, a lax functor that is
faithful on hom categories.
Proposition 5.2.6. There is a ‘discarding’ functor p´q : Copara2pCqÑ C, which takes any
coparameterized morphism and discards the coparameter.
Proposition 5.2.7. p´q is a section of p´q . That is, idC “C
p´q
ãÝ ÝÝ ÑCopara2pCq
p´q
Ý ÝÝ ÑC; more
suggestively, this can be written p´q“p´q .
5.2.2. Lax functors, pseudofunctors, their transformations, and indexed
bicategories
In order to define bicategories of statistical games, coherently with loss functions like the relative
entropy that compose by copy-composition, we first need to define coparameterized (copy-
composite) Bayesian lenses. Analogously to non-coparameterized Bayesian lenses, these will
be obtained by applying a Grothendieck construction to an indexed bicategory [19, Def. 3.5] of
state-dependent channels, Stat2. Constructing copy-composite Bayesian lenses in this way is the
subject of §5.2.3; in this section, we supply the necessary higher-categorical prerequisites.
An indexed category is a homomorphism of bicategories with codomain Cat and locally trivial
domain, and analogously an indexed bicategory will be a homomorphism of tricategories with
codomain Bicat (appropriately defined) and locally ‘2-trivial’ domain. In order to stay as close as
possible to the matter at hand, we do not give here an explicit definition of ‘tricategory’ or indeed
of ‘indexed bicategory’, and instead refer the reader to [19, §3]. The definition of Stat2 will of
course provide an example of an indexed bicategory, but in order to state it we will nonetheless
need to extend the notion of pseudofunctor from Definition 4.2.10 to the case where the domain is
a true bicategory; and we will also need morphisms of pseudofunctors, called pseudonatural (or
strong) transformations.
181
We will begin by defining the notion of lax functor , of which pseudofunctors constitute a
special case. Just as a lax monoidal functor F is equipped with a natural family of morphisms
FpXqb FpY qÑ FpX bY q(the laxator; cf. Definition 3.1.11), a lax functor is a weak functor
equipped with a natural family of 2-cells Fpgq˛ Fpfqñ Fpg ˝fq; this lax functoriality will be
important when we come to study loss models in §5.3.2.
Definition 5.2.8 (Johnson and Yau [145, Def. 4.1.2]). Suppose B and C are both bicategories. A lax
functor F : B ÑC is constituted by
1. a function F0 : B0 ÑC0 on 0-cells;
2. for each pair of 0-cells a, b: B, a functor Fa,b : Bpa, bqÑ CpF0a, F0bq;
3. for each 0-cell b : B, a natural transformation
1 Bpb, bq
CpF0b, F0bq
idF0b
idb
Fb,b
F1
witnessing lax unity , with component 2-cells Fb : idF0b ñFb,bpidbq;
4. for each triple of 0-cells a, b, c: B, a natural transformation
Bpb, cqˆ Bpa, bq Bpa, cq
CpF0b, F0cqˆ CpF0a, F0bq CpF0a, F0cq
˝
Fa,cFb,cˆFa,b
˛
F2
witnessing lax functoriality and called the laxator4, with component 2-cells
Fg,f : Fb,cpgq˛ Fa,bpfqñ Fa,cpg ˝fq
where ˝and ˛denote horizontal composition in B and C respectively;
satisfying the following conditions:
4By analogy with the laxator of a lax monoidal functor (Definition 3.1.11). Since monoidal category is a special case of
bicategory, the notion of lax functor (between bicategories) generalizes the notion of lax monoidal functor (between
monoidal categories).
182
(a) coherence with the left and right unitality of horizontal composition, so that for all 1-cells
f : a Ñb the following diagrams commute:
idF0b ˛Fa,bpfq Fa,bpfq
Fb,bpidbq˛ Fa,bpfq Fa,bpidb ˝fq
λC
Fa,bpfq
Fa,bpλB
f qFb˛Fa,bpfq
Fidb,f
Fa,bpfq˛ idF0a Fa,bpfq
Fa,bpfq˛ Fa,apidaq Fa,bpf ˝idaq
ρC
Fa,bpfq
Fa,bpρB
f qFa,bpfq˛Fa
Ff,ida
where λB, λC and ρB, ρC are the left and right unitors for the horizontal composition in B
and C respectively;
(b) coherence with the associativity of horizontal composition, so that for all 1-cells f : a Ñb,
g : b Ñc, and h : c Ñd, the following diagram commutes:
pFc,dphq˛ Fb,cpgqq˛ Fa,bpfq Fc,dphq˛p Fb,cpgq˛ Fa,bpfqq
Fb,dph ˝gq˛ Fa,bpfq Fc,dphq˛ Fa,cpg ˝fq
Fa,dpph ˝gq˝ fq Fa,dph ˝pg ˝fqq
αC
Fc,dphq,Fb,cpgq,Fa,bpfq
Fc,dphq˛Fg,f
Fh,g˝f
Fh,g˛Fa,bpfq
Fh˝g,f
Fa,dpαB
h,g,f q
where αB and αC are the associators for the horizontal composition in B and C respectively.
A pseudofunctor is a lax functor F for which F1 and F2 are natural isomorphisms5.
The variable laxness of lax functors is recapitulated in the laxness of their morphisms; again, we
begin with the weakest case.
Definition 5.2.9. Suppose F and G are lax functors B ÑC. A lax transformation α : F ÑG
consists of
1. a 1-cell αb : F bÑGb in C for each 0-cell b : B;
2. a natural transformation αb,c : αb˚G ñαc˚F (where αb˚denotes pre-composition by αb,
and αc˚denotes post-composition by αc) for each pair b, cof objects in B, with component
5 Compare Definition 4.2.10, treating C there as a bicategory with discrete hom-categories.
183
2-cells
F b F c
Gb Gc
F f
αb αc
Gf
αf
for each 1-cell f : b Ñc in B;
satisfying conditions of lax unity and lax naturality whose precise general form the reader may
find in Johnson and Yau [145, Def. 4.2.1].
A strong transformation (or pseudonatural transformation ) is a lax transformation for which the
component 2-cells constitute natural isomorphisms.
It is notable that, unlike natural transformations, lax transformations do not compose, not
even laxly; see Johnson and Yau [145, Motivation 4.6.1]. This means that there is no bicategory
of bicategories, lax functors, and lax transformations, analogous to Cat. However, strong
transformations between pseudofunctors do compose, weakly, up to isomorphism. These
isomorphisms collect into 3-cells known as modifications, producing a tricategory Bicat whose
0-cells are bicategories, 1-cells are pseudofunctors, 2-cells strong transformations, and 3-cells
modifications. This tricategory constitutes the codomain of an indexed bicategory.
Remark 5.2.10. There is another notion of composable morphism betweenlax functors, called icon,
which yields a bicategory Bicatic of bicategories, lax functors, and icons. Icons are, however, more
restrictive than lax transformations, as their 1-cell components must be identities. Nonetheless,
this restriction is satisfied by loss models as defined in §5.3.2, and so morphisms of loss models will
be icons.
5.2.3. Coparameterized Bayesian lenses
With that categorical background out of the way, we are able to define copy-composite Bayesian
lenses, starting with the corresponding indexed bicategory. Letdisc denote the functorSet ÑCat
taking sets to discrete categories (cf. Definition 4.2.2).
Definition 5.2.11. We define the indexed bicategoryStat2 : Coparal
2pCqco op ÑBicat fibrewise
as follows.
(i) The 0-cells Stat2pXq0 of each fibre Stat2pXqare the objects C0 of C.
(ii) For each pair of 0-cells A, B, the hom-category Stat2pXqpA, Bqis defined to be the functor
category Cat
`
disc CpI, Xq, Coparar
2pCqpA, Bq
˘
.
184
(iii) For each 0-cell A, the identity functor idA : 1 ÑStat2pXqpA, Aqis the constant functor on
the identity on A in Coparar
2pCq; i.e. disc CpI, Xq !Ý Ñ1 idA
Ý Ý ÑCoparar
2pCqpA, Aq.
(iv) For each triple A, B, Cof 0-cells, the horizontal composition functor ˝A,B,C is defined by
˝A,B,C : Stat2pXqpB, Cqˆ Stat2pXqpA, Bq ¨¨¨
¨¨¨ “Ý ÑCat
`
disc CpI, Xq, Coparar
2pCqpB, Cq
˘
ˆCat
`
disc CpI, Xq, Coparar
2pCqpA, Bq
˘
¨¨¨
¨¨¨ ˆÝ ÑCat
`
disc CpI, Xq2, Coparar
2pCqpB, Cqˆ Coparar
2pCqpA, Bq
˘
¨¨¨
¨¨¨
Cat
´
,˝
¯
ÝÝÝÝÝÝÑCat
`
disc CpI, Xq, Coparar
2pCqpA, Cq
˘
¨¨¨
¨¨¨ “Ý ÑStat2pXqpA, Cq
where Cat p , ˝qindicates pre-composition with the universal (Cartesian) copying functor in
pCat, ˆ, 1qand post-composition with the horizontal composition functor of Coparar
2pCq.
For each pair of 0-cells X, Yin CoparalpCq, we define the reindexing pseudofunctor Stat2,X,Y :
CoparalpCqpX, Yqop ÑBicat
`
Stat2pY q, Stat2pXq
˘
as follows.
(a) For each 1-cell f in CoparalpCqpX, Yq, we obtain a pseudofunctor Stat2pfq: Stat2pY qÑ
Stat2pXqwhich acts as the identity on 0-cells.
(b) For each pair of 0-cellsA, Bin Stat2pY q, the functor Stat2pfqA,B is defined as the precompo-
sition functor Cat
`
disc CpI, f q, Coparar
2pCqpA, Bq
˘
, where p´q is the discarding functor
Coparal
2pCqÑ C of Proposition 5.2.6.
(c) For each 2-cell φ : f ñ f1 in Coparal
2pCqpX, Yq, the pseudonatural transformation
Stat2pφq: Stat2pf1qñ Stat2pfqis defined on 0-cells A : Stat2pY qby the discrete natural
transformation with components Stat2pφqA :“idA, and on 1-cells c : Stat2pY qpA, Bqby
the substitution natural transformation with constitutent 2-cells Stat2pφqc : Stat2pfqpcqñ
Stat2pf1qpcqin Stat2pXqwhich acts by replacing Cat
`
disc CpI, f q, Coparar
2pCqpA, Bq
˘
by Cat
`
disc CpI, f1 q, Coparar
2pCqpA, Bq
˘
; and which we might alternatively denote by
Cat
`
disc CpI, φq, Coparar
2pCqpA, Bq
˘
.
Notation 5.2.12. We will write f : A XÝÑ
M
‚ B to denote a state-dependent coparameterized channel
f with coparameter M and state-dependence on X.
Remark 5.2.13. We could give an alternative definition of Stat2, for which the definition above
would give a sub-indexed bicategory, by defining the state-dependence on the whole hom-category
185
Copara2pCqpI, Xqrather than just CpI, Xq. However, doing this would cause the reindexing
pseudofunctors to introduce coparameters (from the now-coparameterized priors), which would
contradict the type signature of coparameterized Bayesian inversion: imagine the equation of
Definition 5.2.18 below but without the discarding on the right-hand side.
Remark 5.2.14. Similarly, the same definitions would go through upon substituting CoparapCq
for Copara2pCq; but, as noted above, we need copy-composition for the relative entropy.
As we saw in §4.2.2, lenses in 1-category theory are morphisms in the fibrewise opposite of a
fibration. Analogously, our bicategorical Bayesian lenses are obtained as 1-cells in the bicategorical
Grothendieck construction of (the pointwise opposite of) the indexed bicategory Stat2; this yields
a 2-fibration. So as not to over-complicate the presentation, we do not present all the details of this
construction, and refer the reader instead to Baković [19, §6].
Definition 5.2.15. Fix a copy-discard category pC, b, Iq. We define the bicategory of coparameter-
ized Bayesian lenses inC, denoted BayesLens2pCqor simply BayesLens2, to be the bicategorical
Grothendieck construction of the pointwise opposite of the corresponding indexed bicategory
Stat2, with the following data:
(i) A 0-cell in BayesLens2 is a pair pX, Aqof an object X in Coparal
2pCqand an object A in
Stat2pXq; equivalently, a 0-cell in BayesLens2 is a pair of objects in C.
(ii) The hom-categoryBayesLens2
`
pX, Aq, pY, Bq
˘
is the product categoryCoparal
2pCqpX, Yqˆ
Stat2pXqpB, Aq.
(iii) The identity on pX, Aqis given by the pair pidX, idAq.
(iv) For each triple of 0-cells pX, Aq, pY, Bq, pZ, Cq, the horizontal composition functor is given
186
by
BayesLens2
`
pY, Bq, pZ, Cq
˘
ˆBayesLens2
`
pX, Aq, pY, Bq
˘
“ Coparal
2pCqpY, Zqˆ Stat2pY qpC, Bqˆ Coparal
2pCqpX, Yqˆ Stat2pXqpB, Aq
„Ý Ñ
ÿ
g:Coparal
2pCqpY,Zq
ÿ
f:Coparal
2pCqpX,Y q
Stat2pY qpC, Bqˆ Stat2pXqpB, Aq
ř
g
ř
f Stat2pfqC,Bˆid
Ý ÝÝÝÝÝÝÝÝÝÝÝÝÝ Ñ
ÿ
g:Coparal
2pCqpY,Zq
ÿ
f:Coparal
2pCqpX,Y q
Stat2pXqpC, Bqˆ Stat2pXqpB, Aq
ř
˝Coparal
2pCq˝Stat2pXq
Ý ÝÝÝÝÝÝÝÝÝÝÝÝÝ Ñ
ÿ
g˝f:Coparal
2pCqpX,Zq
Stat2pXqpC, Aq
„Ý ÑBayesLens2
`
pX, Aq, pZ, Cq
˘
where the functor in the penultimate line amounts to the product of the horizontal
composition functors on Coparal
2pCqand Stat2pXq.
Proposition 5.2.16. There is a projection pseudofunctorπLens : BayesLens2pCqÑ Coparal
2pCq
mapping each 0-cell pX, Aqto X, each 1-cell pf, f1qto f, and each 2-cell pφ, φ1qto φ. This
pseudofunctor is a 2-fibration in the sense of Baković [19, Def. 4.7].
Proof. The claim follows as a consequence of Baković [19, Theorem 6.2].
Remark 5.2.17. When C is symmetric monoidal,Stat2 acquires the structure of a monoidal indexed
bicategory (Definition 5.4.2 and Theorem 5.4.4), and hence BayesLens2 becomes a monoidal
bicategory (Corollary 5.4.5).
5.2.4. Coparameterized Bayesian updates compose optically
So that our generalized Bayesian lenses are worthy of the name, we should also confirm that
Bayesian inversions compose according to the lens pattern (‘optically’) in the coparameterized
setting. Such confirmation is the subject of the present section.
Definition 5.2.18. We say that a coparameterized channelγ : AÑ‚M bB admits Bayesian inversion
187
if there exists a dually coparameterized channel ρπ : BÑ‚A bM satisfying the graphical equation
γ
π
A M B
“ ρπ
γ
π
A M B
.
In this case, we say that ρπ is the Bayesian inversion of γ with respect to π.
With this definition, we can supply the desired result that “coparameterized Bayesian updates
compose optically”.
Theorem 5.2.19. Suppose pγ, γ:q : pA, AqÝÑ
M
| pB, Bq and pδ, δ:q : pB, BqÝ Ñ
N
| pC, Cq are
coparameterized Bayesian lenses in BayesLens2. Suppose also that π : IÑ‚A is a state on
A in the underlying category of channels C, such that γ:
π is a Bayesian inversion of γ with respect
to π, and such that δ:
γπ is a Bayesian inversion of δ with respect to pγπq ; where the notation p´q
represents discarding coparameters. Then γ:
π ‚δ:
γπ is a Bayesian inversion of δ ‚γ with respect to
π. (Here ‚denotes copy-composition.) Moreover, if pδ ‚γq:
π is any Bayesian inversion of δ ‚γ with
respect to π, then γ:
π ‚δ:
γπ is pδγπq -almost-surely equal to pδ ‚γq:
π: that is, pδ ‚γq:
π
pδγπq
„ γ:
π ‚δ:
γπ .
Proof. We only need to show that γ:
π ‚δ:
γπ is a Bayesian inversion of δ ‚γ with respect to π; the
‘moreover’ claim follows immediately because Bayesian inversions are almost surely unique (by
Proposition 4.1.29). Thus, δ ‚γ ‚π has the following depiction;
δ
B N C
γ
A M
π
188
Since γ:
π is a Bayesian inversion of γ with respect to π, this is equal to
δ
B N C
γ:
π
γ
π
A M
.
By the coassociativity of copying, this in turn is equal to
δ
B N C
γ:
π
γ
π
A M
.
189
And since δ:
γπ is a Bayesian inversion of δ with respect to pγπq , this is equal to
B
γ:
π
A M C
δ:
γπ
N
γ
π
δ
which establishes the result.
In order to satisfy this coparameterized Bayes’ rule, a Bayesian lens must be of ‘simple’ type.
Definition 5.2.20. We say that a coparameterized Bayesian lens pc, c1qis simple if its domain
and codomain are ‘diagonal’ (duplicate pairs of objects) and if the coparameter of c is equal to
the coparameter of c1. In this case, we can write the type of pc, c1qas pX, XqÝÑ
M
| pY, Yqor simply
XÝÑ
M
| Y .
Remark 5.2.21. In Remark 5.2.13, we explained that we should restrict the type of state-dependent
coparameterized morphisms so that they cohere with the coparameterized Bayes’ rule of Definition
5.2.18. The restriction here to simple lenses is by contrast not enforced by the type system, an
oversight which (like the failure to restrict to supports noted in Remark 4.1.20) is comparatively
inelegant, but which is forced upon us by the Grothendieck construction, which does not have a
place for such constraints. We expect that the use of (a bicategorical instance of) dependent optics
[43, 50, 276] would allow such a constraint to be enforced (alongside support objects), at the cost of
requiring yet more high-powered categorical machinery, of which there is probably enough in this
thesis. We therefore leave this avenue unexplored for now.
190
By analogy with Corollary 4.3.15, we have the following important consequence of Theorem
5.2.19.
Corollary 5.2.22. Suppose Coparal
2pCq:is a subbicategory ofCoparal
2pCqall of whose channels
admit Bayesian inversion. Then there is almost surely a pseudofunctor : : Coparal
2pCq: Ñ
BayesLens2 mapping each 1-cell to its almost-surely unique corresponding exact Bayesian lens.
Moreover, :is a section of the 2-fibration πLens : BayesLens2 ÑCoparal
2pCqinduced by the
bicategorical Grothendieck construction.
5.3. Statistical games for local approximate inference
5.3.1. Attaching losses to lenses
Statistical games are obtained by attaching to Bayesian lenses loss functions, representing ‘local’
quantifications of the performance of approximate inference systems. Because this performance
depends on the system’s context ( i.e., the prior π : IÑ‚X and the observed data b : B), a loss
function at its most concrete will be a function CpI, XqˆB ÑR. To internalize this type in C, we
may recall that, when C is the category sfKrn of s-finite kernels or the Kleisli category KℓpDď1q
of the subdistribution monad6, a density function pc : X ˆY Ñr0, 1sfor a channel c : XÑ‚Y
corresponds to an effect (or costate) X bY Ñ‚I. In this way, we can see a loss function as a kind of
state-dependent effect B XÝ Ñ‚ I (and not a coparameterized one).
Loss functions will compose by sum, and so we need to ask for the effects in C to form a monoid.
Moreover, we need this monoid to be ‘bilinear’ with respect to channels, so that Stat-reindexing
(cf. Definition 4.3.2) preserves sums. These conditions are formalized in the following definition.
Definition 5.3.1. Suppose pC, b, Iqis a copy-discard category. We say that C has bilinear effects if
the following conditions are satisfied:
(i) effect monoid : there is a natural transformation`: Cp´, IqˆCp“, Iqñ Cp´b“, Iqmaking
ř
A:C CpA, Iqinto a commutative monoid with unit 0 : IÑ‚I;
(ii) bilinearity: pg `g1q‚ ‚f “g ‚f `g1‚f for all effects g, g1and morphisms f such that
pg `g1q‚ ‚f exists.
Example 5.3.2. A trivial example of a category with bilinear effects is supplied by any Cartesian
category, such as Set, in which there is a unique effect for each object, so the effect monoid
6Weaken the definition of the distribution monad D : Set ÑSet so that distributions may sum to any number in the
unit interval.
191
structure is given only by the product of objects, and bilinearity follows from the terminality of the
unit object 1.
Example 5.3.3. We might hope that KℓpDď1qhas bilinear effects, but this is not the case, because
the sum of two effects may exceed 1: the effects only form a partial monoid7. But if M is any
monoid in Set, then there is a monad DM taking each set X to the set DM pXqof formal M-linear
combinations of elements of X. This is the free M-module on X, just as traditionally DX is the
free convex space on X, and the monad structure is obtained from the adjunction in the same
way [137, §2]. An effect Y Ñ‚I then corresponds to a function Y ÑM, and the monoid structure
follows from the monoid structure on M; bilinearity follows from the linearity of the (free) module
structure:
pg `g1q‚ ‚fpxq“
ÿ
y
`
gpyq` g1pyq
˘
¨fpy|xq
“
ÿ
y
gpyq¨ fpy|xq` g1pyq¨ fpy|xq
“
ÿ
y
gpyq¨ fpy|xq`
ÿ
y
g1pyq¨ fpy|xq
“g ‚fpxq` g1‚fpxq
Example 5.3.4. The category sfKrn of s-finite kernels [270] has bilinear effects. An effect Y Ñ‚I
is a measurable function Y Ñr0, 8s, and bilinearity follows from the linearity of integration:
pg `g1q‚ ‚fpxq“
ż
y
`
gpyq` g1pyq
˘
fpdy|xq
“
ż
y
gpyqfpdy|xq` g1pyqfpdy|xq
“
ż
y
gpyqfpdy|xq`
ż
y
g1pyqfpdy|xq
“g ‚fpxq` g1‚fpxq
We will typically assume sfKrn as our ambient C for the examples below.
Example 5.3.5. Given a category C with bilinear effects, we can lift the natural transformation `,
and hence the bilinear effect structure, to the fibres of StatC, using the universal property of the
7Indeed, an effect algebra is a kind of partial commutative monoid [141, §2], but we do not need the extra complication
here.
192
product of categories:
`X : StatpXqp´, Iqˆ StatpXqp“, Iq ù ùSet
`
CpI, Xq, Cp´, Iq
˘
ˆSet
`
CpI, Xq, Cp“, Iq
˘
p¨,¨q
ùùñSet
`
CpI, Xq, Cp´, Iqˆ Cp“, Iq
˘
Set
`
CpI,X q,`
˘
ù ùùùùùùùùù ñSet
`
CpI, Xq, Cp´b“ , Iq
˘
“
ù ñStatpXqp´b“ , Iq
Here, p¨, ¨qdenotes the pairing operation obtained from the universal property. In this way, each
StatpXqhas bilinear effects. Note that this lifting is (strictly) compatible with the reindexing ofStat,
so that `p´qdefines an indexed natural transformation. This means in particular that reindexing
distributes over sums : given state-dependent effects g, g1 : B YÝ Ñ‚I and a channel c : XÑ‚Y , we have
pg `Y g1qc “gc `X g1
c. We will thus generally omit the subscript from the lifted sum operation,
and just write `.
We are now ready to construct the bicategory of statistical games.
Definition 5.3.6. Suppose pC, b, Iq has bilinear effects, and let BayesLens2 denote the
corresponding bicategory of (copy-composite) Bayesian lenses. We will write SGameC to denote
the following bicategory of (copy-composite) statistical games in C:
• The 0-cells are the 0-cells pX, Aqof BayesLens2;
• the 1-cells, called statistical games , pX, Aq Ñ pY, Bqare triples pc, c1, Lcqconsisting of a
1-cell pc, c1q: pX, Aq ÞÑpY, Bqin BayesLens2 and a loss Lc : B XÝ Ñ‚ I in StatpXqpB, Iq;
• given 1-cells pc, c1, Lcq, pe, e1, Leq: pX, AqÑp Y, Bq, the 2-cells pc, Lcqñp e, Leqare pairs
pα, Kαqof a 2-cell α : pc, c1qñp e, e1qin BayesLens2 and a loss Kα : B XÝ Ñ‚ I such that
Lc “Le `Kα;
• the identity 2-cell on pc, c1, Lcqis pidpc,c1q, 0q;
• given 2-cells pα, Kαq : pc, c1, Lcq ñ pd, d1, Ldqand pβ, Kβq : pd, d1, Ldq ñ pe, e1, Leq,
their vertical composite is pβ ˝α, Kβ `Kαq, where ˝here denotes vertical composition in
BayesLens2;
• given 1-cellspc, c1, Lcq: pX, AqÑp Y, Bqand pd, d1, Ldq: pY, BqÑp Z, Cq, their horizontal
composite is
`
pd, d1q pc, c1q, Ld
c `Lc ˝d1
c
˘
; and
193
– given 2-cells pα, Kαq: pc, c1, Lcqñp e, e1, Leqand pβ, Kβq: pd, d1, Ldqñp f, f1, Lf q,
their horizontal composite is pβ  α, Kβ
c `Kα ˝d1
cq, where  here denotes horizontal
composition in BayesLens2.
Remark 5.3.7. In earlier work (such as versions 1 and 2 of our preprint [250]), we gave a more
elaborate but less satisfying definition of “statistical game”, as a Bayesian lens equipped with a
function from its ‘context’ to R (which we also called a loss function). The construction given here
shows that the complicated earlier notion of context, which follows the ideas of ‘Bayesian open
games’ [36], is actually unnecessary for the purposes of statistical games. Considering a Bayesian
lens in KℓpDqof type pX, AqÑp Y, Bq, its ‘context’ is an element of DX ˆSetpDY, DBq. By
comparison, a corresponding loss function of the type given above is equivalently a function with
domain DX ˆB, and so we have replaced the dependence on ‘continuations’ in SetpDY, DBq
with a simple dependence on B.
Theorem 5.3.8. Definition 5.3.6 generates a well-defined bicategory.
The proof of this result is that SGameC is obtained via a pair of bicategorical Grothendieck
constructions: first to obtain Bayesian lenses; and then to attach the loss functions. The proof
depends on the following intermediate result that our effect monoids can be ‘upgraded’ to monoidal
categories; we then use the delooping of this structure to associate (state-dependent) losses to
(state-dependent) channels, after discarding the coparameters of the latter.
Lemma 5.3.9. Suppose pC, b, Iqhas bilinear effects. Then, for each object B, CpB, Iqhas the
structure of a symmetric monoidal category. The objects of CpB, Iqare its elements, the effects.
If g, g1are two effects, then a morphism κ : g Ñg1is an effect such that g “g1`κ; the identity
morphism for each effect idg is then the constant 0 effect. Likewise, the tensor of two effects is their
sum, and the corresponding unit is the constant 0. Precomposition by any morphism c : AÑ‚B
preserves the monoidal category structure, making the presheaf Cp´, Iqinto a fibrewise-monoidal
indexed category C op ÑMonCat (cf. Remark 4.2.21).
As already indicated, this structure lifts to the fibres of Stat.
Corollary 5.3.10. For each object X in a category with bilinear effects, and for each object
B, StatpXqpB, Iqinherits the symmetric monoidal structure of CpB, Iq; note that morphisms of
state-dependent effects are likewise state-dependent, and that tensoring (summing) state-dependent
effects involves copying the parameterizing state. Moreover,Statp´qp“, Iqis a fibrewise-monoidal
indexed category ř
X:C op StatpXqop ÑMonCat.
194
Using this corollary, we can give the abstract proof of Theorem 5.3.8. There are two further
observations of note: first, that we can deloop a monoidal category into a bicategory with one
object; second, that we can extend Statp´qp“, Iqto Stat2 via discarding.
Proof of 5.3.8. Recall from Proposition 3.1.24 that every monoidal category M can be transformed
into a one-object bicategory, its delooping BM, with the 1-cells and 2-cells being the objects
and morphisms of M, vertical composition being composition in M, and horizontal composition
being the tensor. This delooping is functorial, giving a 2-functor B : MonCat ÑBicat which,
following Corollary 5.3.10, we can compose after Statp´qp“, Iq(taking its domain as a locally
discrete 2-category) to obtain indexed bicategories; we will assume this transformation henceforth.
Next, observe that we can extend the domain ofStatp´qp“, Iqto ř
X:Coparal
2pCqco op Stat2pXqco op
by discarding the coparameters of the (coparameterized) state-dependent channels as well as the
coparameter on any reindexing, as in the following diagram of indexed bicategories:
ř
X:Coparal
2pCqco op Stat2pXqco op
ř
X:C op StatpXqop Bicat
ř Stat2p´qp“,Iq
Statp´qp“,Iq
Here, the 2-cell indicates also discarding the coparameters of the ‘effects’ in Stat2p´qp“, Iq.
If we let L denote the composite functor in the diagram above, we can reason as follows:
L : ř
X:Coparal
2pCqco op Stat2pXqco op ÑBicat
sum/productś
X:Coparal
2pCqco op BicatStat2pXqco op
śş
ś
X:Coparal
2pCqco op 2Fib pStat2pXqq
forget
Coparal
2pCqco op ÑBicat op
G : Coparal
2pCqco op ÑBicat
where the first step uses the adjointness of (dependent) sums and products; the second applies the
bicategorical Grothendieck construction in the codomain; the third forgets the 2-fibrations, to leave
only the total bicategory; and the fourth step takes the pointwise opposite. We can thus write the
action of G as GpXq“
`ş
LpX, ´q
˘op.
Since each bicategory LpX, Bqhas only a single 0-cell, the 0-cells of eachGpXqare equivalently
just the objects of C, and the hom-categories GpXqpA, Bqare equivalent to the product categories
Stat2pXqpB, Aqˆ StatpXqpB, Iq. That is to say, a 1-cell A Ñ B in GpXqis a pair of a state-
dependent channel B XÝ Ñ‚ A along with a correspondingly state-dependent effect on its domain B.
We therefore seem to approach the notion of statistical game, but in fact we are already there:
195
SGameC is simply
ş
G, by the bicategorical Grothendieck construction. To see this is only a matter
of further unfolding the definition.
Remark 5.3.11. There are two notable details that the abstractness of the preceding proof obscures.
Firstly, the horizontal composition of effects in SGameC is strict. To see this, let pc, ιq: A ÑB
and pd, κq: B ÑC and pe, λq: C ÑD be 1-cells in GpXq, and for concision write the horizontal
composite of effects by concatenation, so that κι “κ `ι ˝d (by the Grothendieck construction).
Then strict associativity demands that λpκιq“p λκqι. This obtains as follows:
λpκιq“ λ `pκιq˝ e
“λ `pκ `ι ˝d q˝ e
“λ `pκ ˝e `ι ˝d ˝e q
“λ `pκ ˝e `ι ˝pd ˝eq q
“λ `pκ ˝e `ι ˝pe ˝op dq q
“pλ `κ ˝e q` ι ˝pe ˝op dq
“pλκqι
by Grothendieck
by Grothendieck
by bilinearity
by functoriality
by “pointwise opposite”
by monoid associativity
by Grothendieck
Since the identity effect is the constant 0, it is easy to see that horizontal composition is strictly
unital on effects:
0κ “0 `κ ˝id “κ “κ `0 ˝d “κ0
Secondly, note that the well-definedness of horizontal composition in SGameC depends
furthermore on the distributivity of reindexing over sums (cf. Example 5.3.5). Suppose we have
1-cells and 2-cells in SGameC as in the following diagram:
pX, Aq pY, Bq pZ, Cq
pc,Lcq
pc1,Lc1
q
pd,Ldq
pd1,Ld1
q
pα,Kαq pβ,Kβq
Then, writing  for horizontal composition inSGameC and ˝for composition inStat2 (and leaving
the discarding of coparameters implicit):
Ld1
 Lc1
“pLd `Kβq pLc `Kαq
“pLd `Kβqc `pLc `Kαq˝ dc
“Ld
c `Kβ
c `pLc ˝dcq`p Kα ˝dcq
“Ld
c `pLc ˝dcq` Kβ
c `pKα ˝dcq
“Ld  Lc `Kβ  Kα
ex hypothesi
by Grothendieck
by distributivity and bilinearity
by commutativity of the effect monoid
by Grothendieck
196
Remark 5.3.12. Of course, we don’t strictly need to use BayesLens2 in the preceding; the
structure equally makes sense if we work only with ‘marginalized’ lenses in BayesLens. In this
case, although BayesLens is a 1-category, one still obtains 2-cells between statistical games,
because it remains possible to consider their differences.
5.3.2. Inference systems and loss models
In the context of approximate inference, one often does not have a single statistical model to evaluate,
but a whole family of them. In particularly nice situations, this family is actually a subcategory
D of C, with the family of statistical models being all those that can be composed in D. The
problem of approximate inference can then be formalized as follows. Since both BayesLens2 and
SGameC were obtained by bicategorical Grothendieck constructions, we have a pair of 2-fibrations
SGameC
πLoss
Ý ÝÝ ÑBayesLens2
πLens
Ý ÝÝ ÑCoparal
2pCq. Each of πLoss, πLens, and the discarding functor
p´q can be restricted to the subcategory D. The inclusion p´q : D ãÑCoparal
2pDqrestricts to
a section of this restriction ofp´q ; the assignment of inversions to channels inD then corresponds
to a 2-section of the 2-fibration πLens (restricted to D); and the subsequent assignment of losses is a
further 2-section of πLoss. This situation is depicted in the following diagram of bicategories:
SGameD SGameC
BayesLens2|D BayesLens2
Coparal
2pDq Coparal
2pCq
D C
πLoss
πLens
πLoss|D
πLens|D
|D
This motivates the following definitions of inference system and loss model, although, for the sake of
our examples, we will explicitly allow the loss-assignment to belax: if L is a loss model and c and d
are composable lenses, then rather than an equality or natural isomorphismLpdq˛Lpcq– Lpdcq,
we will only require a natural transformation Lpdq˛ Lpcqñ Lpd  cq.
Before defining loss models and inference systems, it helps to recall the concept of essential
image: a generalization of the notion of image from functions to functors.
Definition 5.3.13 ([196]). Suppose F : C ÑD is an n-functor (a possibly weak homomorphism
of weak n-categories). The image of F is the smallest sub-n-category of D that contains Fpαqfor
all k-cells α in C, along with any pk `1q-cells relating images of composites and composites of
images, for all 0 ďk ďn. We say that a sub-n-category D is replete if, for any k-cells α in D
197
and β in C (with 0 ďk ăn) such that f : α ñβ is a pk `1q-isomorphism in C, then f is also a
pk `1q-isomorphism in D. The essential image of F, denoted impFq, is then the smallest replete
sub-n-category of D containing the image of F.
With these concepts in mind, we state our definitions.
Definition 5.3.14. Suppose pC, b, Iqis a copy-delete category. An inference system in C is a pair
pD, ℓqof a subcategory D ãÑC along with a section ℓ : D ÑBayesLens2|D of πLens|D, where
D is the essential image of the canonical lax inclusion p´q : D ãÑCoparal
2pDq.
Definition 5.3.15. Suppose pC, b, Iqhas bilinear effects and B is a subbicategory of BayesLens2.
A loss model for B is a lax section L of the restriction πLoss|B of πLoss to B. We say that L is a strict
loss model if it is in fact a strict 2-functor, and a strong loss model if it is in fact a pseudofunctor.
Remark 5.3.16. We may often be interested in loss models for which B is in fact the essential
image of an inference system, but we do not stipulate this requirement in the definition as it is not
necessary for the following development.
In order for two loss models F and G to be comparable, they must both be sections of the same
fibration of statistical games. One consequence of this is that both F and G must map each 0-cell
pX, Aqin the bicategory of lenses to the same 0-cell in the bicategory of games, which (by the
definition of the bicategory of games) must again be pX, Aq. In such circumstances, the relevant
type of morphism of lax functors is the icon, whose definition we now review.
Definition 5.3.17 (Johnson and Yau [145, Def. 4.6.2]). Suppose F and G are lax functors B ÑC
such that, for all b : B, F b“Gb. An icon (or identity component oplax natural transformation )
α : F ÑG consists of a family of natural transformations
Bpa, bq CpF a, F bq“ CpGa, Gbq
Fa,b
Ga,b
αa,b
for each pair a, bof 0-cells in B, satisfying coherence conditions corresponding to unity and oplax
naturality, and whose component 2-cells we write as αf : F fñGf for each 1-cell f in B.
Lax functors B ÑC and icons between them constitute the objects and morphisms of a category,
BicaticpB, Cq, which we can use to construct categories of loss models. Moreover, owing to the
monoidality of `, this category will be moreover monoidal: a property that we will use to define
198
the free energy loss model below. (Note that this monoidal structure, on thecategory of loss models,
is distinct from the monoidal structure that we will attach to loss models themselves in §5.4.)
Proposition 5.3.18. Loss models for B constitute the objects of a symmetric monoidal category
`
LosspBq, `, 0
˘
. The morphisms of LosspBqare icons between the corresponding lax functors, and
they compose accordingly. The monoidal structure is given by sums of losses.
Proof sketch. From Johnson and Yau [145, Theorem 4.6.13], we know that icons compose, forming
the morphisms of a category. Next, note that for any two loss models F and G and any k-cell
α (for any k Pt0, 1, 2u), Fpαqand Gpαqmust only differ on the loss component, and so we can
sum the losses; this gives the monoidal product. The monoidal unit is necessarily the constant 0
loss. Finally, observe that the structure is symmetric becauase effect monoids are commutative (by
Definition 5.3.1).
5.3.3. Examples
Each of our examples involves taking expectations of log-densities, and so to make sense of them it
first helps to understand what we mean by “taking expectations”.
Notation 5.3.19 (Expectations). Written as a function, a density p on X has the type X ÑR`;
written as an effect, the type is XÑ‚I. Given a measure or distribution π on X (equivalently, a
state π : IÑ‚X), we can compute the expectation of p under π as the composite p ‚π. We write the
resulting quantity as Eπrps, or more explicitly as Ex„π
“
ppxq
‰
. We can think of this expectation as
representing the ‘validity’ (or truth value) of the ‘predicate’ p given the state π [142].
5.3.3.1. Relative entropy and Bayesian inference
For our first example, we return to the subject with which we opened this paper: the compositional
structure of the relative entropy. We begin by giving a precise definition.
Definition 5.3.20. Suppose α, βare both measures on X, with α absolutely continuous with
respect to β. Then the relative entropy or Kullback-Leibler divergence from α to β is the quantity
DKLpα, βq:“Eα
”
log α
β
ı
, where α
β is the Radon-Nikodym derivative of α with respect to β.
Remark 5.3.21. When α and β admit density functions pα and pβ with respect to the same base
measure dx, then DKLpα, βqcan equally be computed as Ex„α
“
log pαpxq´ log pβpxq
‰
. It it this
form that we will adopt henceforth.
199
Proposition 5.3.22. Let B be a subbicategory of simple lenses in BayesLens2, all of whose
channels admit density functions with respect to a common measure and whose forward channels
admit Bayesian inversion (and whose forward and backward coparameters coincide), and with
only structural 2-cells. Then the relative entropy defines a strict loss model KL : B ÑSGame.
Given a lens pc, c1q: pX, Xq ÞÑpY, Yq, KL assigns the loss function KLpc, c1q: Y XÝ Ñ‚ I defined, for
π : IÑ‚X and y : Y , by the relative entropy KLpc, c1qπpyq:“DKL
`
c1
πpyq, c:
πpyq
˘
, where c:is the
exact inversion of c.
Proof. Being a section of πLoss|B, KL leaves lenses unchanged, only acting to attach loss functions.
It therefore suffices to check that this assignment of losses is strictly functorial. Writing ‚for
composition in C, ˝for horizontal composition in Stat2,  in BayesLens2, and ˛for horizontal
composition of losses in SGame, we have the following chain of equalities:
KL
`
pd, d1q pc, c1q
˘
πpzq“ E
px,m,y,nq„pc1˝d1cqπpzq
”
log ppc1˝d1cqπ px, m, y, n|zq
´log ppc:˝d:
cqπ
px, m, y, n|zq
ı
“ E
py,nq„d1c‚πpzq
E
px,mq„c1πpyq
”
log pc1π px, m|yqpd1c‚π py, n|zq
´log pc:
π
px, m|yqpd:
c‚π
py, n|zq
ı
“ E
py,nq„d1c‚πpzq
”
log pd1c‚π py, n|zq´ log pd:
c‚π
py, n|zq
` E
px,mq„c1πpyq
”
log pc1π px, m|yq´ log pc:
π
px, m|yq
ıı
“DKL
`
d1
c‚πpzq, d:
‚πpzq
˘
` E
py,nq„d1c‚πpzq
“
DKL
`
c1
πpyq, c:
πpyq
˘‰
“KLpd, d1qc‚πpzq`
`
KLpc, c1q˝ d1
c
˘
πpzq
“
`
KLpd, d1q˛ KLpc, c1q
˘
πpzq
The first line obtains by definition of KL and ; the second by definition of ˝; the third by the log
adjunction (log ab “log a `log b) and by linearity of E; the fourth by definition of DKL; the fifth
by definition of KL and of ˝; and the sixth by definition of ˛.
This establishes that KL
`
pd, d1q pc, c1q
˘
“KLpd, d1q˛ KLpc, c1qand hence that KL is strictly
functorial on 1-cells. Since we have assumed that the only 2-cells are the structural 2-cells ( e.g.,
the horizontal unitors), which do not result in any difference between the losses assigned to the
corresponding 1-cells, the only loss 2-cell available to be assigned is the 0 loss; which assignment is
easily seen to be vertically functorial. Hence KL is a strict 2-functor, and moreover a section of
πLoss|B as required.
200
Successfully playing a relative entropy game entails minimizing the divergence from the
approximate to the exact posterior. This divergence is minimized when the two coincide, and so
KL represents a form of approximate Bayesian inference.
Remark 5.3.23. We opened the chapter by observing that the relative entropy satisfies a chain
rule defined not on Bayesian lenses, but simply on pairs of channels: to formalize this simpler case,
we do not need the full machinery of statistical games (which is useful when we have bidirectional
inference systems); but we do need some of it.
If c and c1 are parallel channels XÑ‚Y , then DKL pcp´q, c1p´qqdefines an effect XÑ‚I. This
means we can use the statistical games idea to equip parallel (copy-composite) channels in C with
such non-state-dependent loss functions; and the relative entropy will again form a strict section
of the resulting Grothendieck fibration.
Therefore, let B be the bicategory whose 0-cells are the objects of C, but whose 1-cells and
2-cells are parallel pairs of 1-cells and 2-cells in Copara2pCq; equivalently, the subbicategory of
Copara2pCq2 which is diagonal on 0-cells.
Next, let K denote the indexed bicategory B co op ÑBicat obtained as the composite
B co op proj1
Ý ÝÝ ÑCopara2pCqco op
co op
ÝÝÝÝÑC op Cp´,Iq
Ý ÝÝÝ ÑMonCat BÝ ÑBicat
where proj1 indicates the projection of the 1st factor of the parallel pairs of 1-cells and 2-cells.
Applying the Grothendieck construction to K yields a 2-fibration
ş
K πK
Ý Ý ÑB. The 0-cells of
ş
K are the objects of C. The 1-cells X Ñ Y are triples pc, c1, Lqwhere c and c1 are parallel
coparameterized channels XÑ‚Y and L is an effect (loss function) XÑ‚I. Given composable 1-cells
pc, c1, Lq: X ÑY and pd, d1, Mq: Y ÑZ, their horizontal composite is defined on the parallel
channels as copy-composition, and on the loss functions asM ‚c `L (where ‚here is composition
in C). 2-cells are pairs of 2-cells in Copara2pCqand differences of losses.
Finally, the relative entropy DKL defines a strict section of πK, mapping the parallel pair pc, c1q
to
`
c, c1, DKLpc, c1q
˘
. Its chain rule is thus formalized by the horizontal composition in
ş
K.
5.3.3.2. Maximum likelihood estimation
A statistical system may be more interested in predicting observations than updating beliefs. This
is captured by the process of maximum or marginal likelihood estimation .
Definition 5.3.24. Let pc, c1q: pX, Xq ÞÑpY, Yqbe a simple lens whose forward channel c admits
a density function pc. Then its log marginal likelihood is the loss function given by the marginal
log evidence MLEpc, c1qπpyq:“´ log pc ‚πpyq.
201
Proposition 5.3.25. Let B be a subbicategory of lenses in BayesLens2 all of which admit density
functions with respect to a common measure, and with only structural 2-cells. Then the assignment
pc, c1qÞÑ MLEpc, c1qdefines a lax loss model MLE : B ÑSGame.
Proof. We adopt the notational conventions of the proof of Proposition 5.3.22. Observe that
MLE
`
pd, d1q pc, c1q
˘
πpzq“´ log pd ‚c ‚πpzq“ MLEpd, d1qc‚πpzq.
By definition, we have
`
MLEpd, d1q˛ MLEpc, c1q
˘
πpzq“ MLEpd, d1qc‚πpzq`
`
MLEpc, c1q˝ d1
c
˘
πpzq
and hence by substitution
`
MLEpd, d1q˛ MLEpc, c1q
˘
πpzq“ MLE
`
pd, d1q pc, c1q
˘
πpzq`
`
MLEpc, c1q˝ d1
c
˘
πpzq.
Therefore, MLEpc, c1q˝d1
c constitutes a 2-cell fromMLEpd, d1q˛MLEpc, c1qto MLE
`
pd, d1qpc, c1q
˘
,
and hence MLE is a lax functor. It is evidently moreover a section of πLoss|B, and, like KL, acts
trivially on the (purely structural) 2-cells.
Successfully playing a maximum likelihood game involves maximizing the log-likelihood that the
system assigns to its observations y : Y . This process amounts to choosing a channel c that assigns
high likelihood to likely observations, and thus encodes a valid model of the data distribution.
5.3.3.3. Autoencoders via the free energy
Many adaptive systems neither just infer nor just predict: they do both, building a model of their
observations that they also invert to update their beliefs. In machine learning, such systems are
known as autoencoders, as they ‘encode’ (infer) and ‘decode’ (predict), ‘autoassociatively’ [161]. In
a Bayesian context, they are known as variational autoencoders [154], and their loss function is the
free energy [80].
Definition 5.3.26. The free energy loss model is the sum of the relative entropy and the likelihood
loss models: FE :“KL `MLE. Given a simple lens pc, c1q: pX, Xq ÞÑpY, Yqadmitting Bayesian
inversion and with densities, FE assigns the loss function
FEpc, c1qπpyq“p KL `MLEqpc, c1qπpyq
“DKL
`
c1
πpyq, c:
πpyq
˘
´log pc ‚πpyq
202
Note that this means that optimizing the free energy is not guaranteed to optimize either KL
or MLE individually, although by definition FE is an upper bound on them both (and hence often
known in machine learning by the alternative name, the evidence upper bound , thinking of MLE as
encoding a measure of ‘evidence’).
Remark 5.3.27. Beyond its autoencoding impetus, another important property of the free energy
is its improved computational tractability compared to either the relative entropy or the likelihood
loss. This property is a consequence of the following fact: although obtained as the sum of terms
which both depend on an expensive marginalization8, the free energy itself does not. This can be
seen by expanding the definitions of the relative entropy and of c:
π and rearranging terms:
FEpc, c1qπpyq“ DKL
`
c1
πpyq, c:
πpyq
˘
´log pc ‚πpyq
“ E
px,mq„c1πpyq
“
log pc1π px, m|yq´ log pc:
π
px, m|yq
‰
´log pc ‚πpyq
“ E
px,mq„c1πpyq
“
log pc1π px, m|yq´ log pc:
π
px, m|yq´ log pc ‚πpyq
‰
“ E
px,mq„c1πpyq
“
log pc1π px, m|yq´ log pcpm, y|xqpπpxq
pc ‚πpyq ´log pc ‚πpyq
‰
“ E
px,mq„c1πpyq
“
log pc1π px, m|yq´ log pcpm, y|xq´ log pπpxq
‰
(5.5)
“DKL
`
c1
πpyq, πb1
˘
´ E
px,mq„c1πpyq
“
log pcpm, y|xq
‰
Here, 1 denotes the measure with density 1 everywhere. Note that when the coparameter is trivial,
FEpc, c1qπpyqreduces to
DKL
`
c1
πpyq, π
˘
´ E
x„c1πpyq
“
log pcpy|xq
‰
.
Remark 5.3.28. The name free energy is due to an analogy with the Helmholtz free energy in
thermodynamics, as we can write it as the difference between an (expected) energy and an entropy
term:
FEpc, c1qπpyq“ E
px,mq„c1πpyq
“
´log pcpm, y|xq´ log pπpxq
‰
´SXbM
“
c1
πpyq
‰
“ E
px,mq„c1πpyq
“
Epc,πqpx, m, yq
‰
´SXbM
“
c1
πpyq
‰
“U ´T S
where we call Epc,πq : X bM bY XÝ Ñ‚ I the energy, and where SXbM : I XbMÝ ÝÝÝ Ñ‚ I is the Shannon
entropy. The last equality makes the thermodynamic analogy: U here is the internal energy of the
system; T “1 is the temperature; and S is again the entropy.
8Evaluating the pushforward c ‚π involves marginalizing over the intermediate variable; and evaluating c:
πpyqalso
involves evaluating c ‚π.
203
5.3.3.4. The Laplace approximation
Although optimizing the free energy does not necessitate access to exact inversions, it does still
entail computing an expectation under the approximate inversion (cf. equation (5.5) of Remark
5.3.27 above), which may remain non-trivial. When one is interested in optimizing a model by
gradient descent, this becomes particularly pressing, as one needs to form an estimate of the
gradient of this expectation with respect to the parameters (which is not in general equal to the
expectation of the gradient of the energy). In machine learning, optimizing variational autoencoders
typically involves a “reparameterization trick” [155, §2.5] to circumvent this difficulty, but in the
context of neuroscientific modelling (where one is concerned with biological plausibility), this
option is not generally available.
An alternative strategy is to make simplifying assumptions, enabling the desired computations
without totally sacrificing biological realism. In the context of predictive coding, a typical such
assumption is that all measures are Gaussian [21, 33, 48, 104, 216]. This is motivated not only by
hypotheses about the nature of biological noise (related to the Central Limit Theorem), but also by
expediency, as a Gaussian distribution is determined by just two sufficient statistics: its mean and
variance. If one first restricts to lenses with Gaussian channels, and then to lenses whose inversion
morphisms are constrained to emit ‘tightly-peaked’ Gaussians (i.e., with small variances), then one
can eliminate the expectation from the expected energy, and simply evaluate the energy at the
posterior mean.
The conceptual justification for this approximation is due to Laplace [163, p.367], who observed
that, given a function of the form fpxq “en hpxq with h having a maximum at x0, the only
non-negligible contributions to its integral as n Ñ8 are those near to x09. Consequently, the
function h can be approximated by the quadratic form obtained from its its 2nd-order Taylor
expansion about x0, so that, in the one-dimensional (univariate) case,
ż
fpxqdx «en hpx0q
ż
e´ 1
2σ2 px´x0q2
dx
for σ “
`
n B2
xhpx0q
˘´1{2. Notably, the integrand on the right-hand side is a Gaussian function: it
has the form of the density of a normal distribution.
In the present context, we are generally interested in expectations of the form Ex„π
“
gpxq
‰
,
which correspond to integrals
ş
gpxqelog pπpxqdx. It is possible to extend the foregoing reasoning
to this case: supposing that log pπpxq9n hpxqfor some function h with a maximum at x0, then as
9A demonstration of this can be found on Wikipedia at https://en.wikipedia.org/w/index.php?
title=Laplace%27s_method&oldid=1154930495.
204
n Ñ8, we can approximate both g and h by their 2nd-order expansions, thereby approximating π
by a Gaussian and g by a quadratic form.
This method of approximating integrals is known as Laplace’s method, and it has been widely
applied in statistics10 [22, 101, 151, 280] [176, Chp. 27], in some circumstances, even yielding exact
posteriors [267, §10.2]. For further exposition (and more rigour) in the finite-dimensional case, we
refer the reader to Bruijn [46, Ch. 4] and Olver [200, §3.7]; for the general case in Banach spaces,
the reader may consult Piterbarg and Fatalov [211]. And for an analysis of the specific case of
approximating Bayesian posteriors (beyond the exact case), with consideration of the approximation
errors, one may refer to Kass, Tierney, and Kadane [150] or the technical report accompanying
Tierney and Kadane [263].
This latter situation is of course closely related to the matter at hand. Here, rather than
approximating the posterior by a Gaussian, we assume it to have Gaussian form.
Remark 5.3.29. We say that a channel c : XÑ‚Y is Gaussian if cpxqis a Gaussian measure for
every x in its domain. We denote the mean and variance of cpxqby µcpxqand Σcpxqrespectively,
and write its (log) density function as
log pcpy|xq“ 1
2
A
ϵcpy, xq, Σcpxq´1ϵcpy, xq
E
´log
a
p2πqn det Σcpxq
having also defined the ‘error’ function ϵc : Y ˆX ÑY by ϵcpy, xq“ y ´µcpxq. In §7.1, we give
a full definition of a category of (nonlinear) Gaussian channels.
We will still be concerned with approximating expectations Ex„dpyq
“
gpxq
‰
by the quadratic
expansion of g, and so to license Laplace’s method we need an analogue of the condition n Ñ8.
This will be supplied by the further assumption that Σdpyqhas small eigenvalues: that is, we work
in the limit tr pΣdpyqqÑ 0. With these two assumptions, we can write
E
x„dpyq
“
gpxq
‰
9
ż
x:X
gpxqexp
A
ϵdpx, yq, Σdpyq´1ϵdpx, yq
E
dx
and observe that as tr pΣdpyqq Ñ0, we must have tr
´
Σdpyq´1
¯
Ñ 8. Thus, by Laplace’s
reasoning, the contributions to the integral are only appreciably non-zero near the mean µdpyq.
This licenses the approximation of g by its quadratic expansion around µdpyq, and leads to the
following approximation of the free energy, known in the predictive coding literature as the
Laplace approximation [101]. (Consistent with the other examples in this chapter, we consider the
coparameterized case.)
10In statistics, making Gaussian assumptions about Bayesian posteriors, or equivalently using second-order approxima-
tions to log posteriors, is also known as variational Laplace [101].
205
Definition 5.3.30. A Cartesian space is an object X that is isomorphic to Rn for some n : N.
Proposition 5.3.31 (Laplacian free energy). Suppose pγ, ρq: pX, Xq ÞÑpY, Yqis a Bayesian lens
with Gaussian channels between finite-dimensional Cartesian spaces, for which, for all y : Y and
Gaussian priors π : IÑ‚X, the eigenvalues of Σρπ pyqare small. Then the free energy FEpγ, ρqπpyq
can be approximated by the Laplacian free energy
FEpγ, ρqπpyq« LFEpγ, ρqπpyq (5.6)
:“Epγ,πq
`
µρπ pyq, y
˘
´SXbM
“
ρπpyq
‰
(5.7)
“´ log pγpµρπ pyq, yq´ log pπpµρπ pyq|Xq´ SXbM
“
ρπpyq
‰
where we have written the argument of the density pγ in ‘function’ style; where p´qX denotes the
projection onto X; and where SXbM rρπpyqs“ Epx,mq„ρπpyqr´log pρπ px, m|yqsis the Shannon
entropy of ρπpyq. The approximation is valid when Σρπ satisfies
Σρπ pyq“
´
B2
px,mqEpγ,πq
¯
pµρπ pyq, yq´1 . (5.8)
We call Epγ,πqthe Laplacian energy .
Proof. Recall that we can write the free energy FEpγ, ρqπpyqas the difference between expected
energy and entropy:
FEpγ, ρqπpyq“ E
px,mq„ρπpyq
“
´log pγpm, y|xq´ log pπpxq
‰
´SXbM
“
ρπpyq
‰
“ E
px,mq„ρπpyq
“
Epγ,πqpx, m, yq
‰
´SX
“
ρπpyq
‰
Next, since the eigenvalues of Σρπ pyqare small for all y : Y , we can approximate the expected
energy by its second-order Taylor expansion around the mean µρπ pyq, following Laplace:
FEpγ, ρqπpyq« E
px,mq„ρπpyq
«
Epγ,πqpµρπ pyq, yq`
@
ϵρπ px, m, yq,
`
Bpx,mqEpγ,πq
˘
pµρπ pyq, yq
D
`1
2
A
ϵρπ px, m, yq,
´
B2
px,mqEpγ,πq
¯
pµρπ pyq, yq¨ ϵρπ px, m, yq
Eff
´SXbM
“
ρπpyq
‰
paq
“ Epγ,πqpµρπ pyq, yq`
B
E
px,mq„ρπpyq
“
ϵρπ px, m, yq
‰
,
`
Bpx,mqEpγ,πq
˘
pµρπ pyq, yq
F
`1
2 tr
”´
B2
px,mqEpγ,πq
¯
pµρπ pyq, yqΣρπ pyq
ı
´SXbM
“
ρπpyq
‰
pbq
“Epγ,πqpµρπ pyq, yq` 1
2 tr
”´
B2
px,mqEpγ,πq
¯
pµρπ pyq, yqΣρπ pyq
ı
´SXbM
“
ρπpyq
‰
206
where
´
B2
px,mqEpγ,πq
¯
pµρπ pyq, yqis the Hessian of Epγ,πq with respect to px, mqevaluated at
pµρπ pyq, yq. The equality marked paqholds first by the linearity of expectations and second because
E
px,mq„ρπpyq
«A
ϵρπ px, m, yq,
´
B2
px,mqEpγ,πq
¯
pµρπ pyq, yq¨ ϵρπ px, m, yq
Eff
“ E
px,mq„ρπpyq
«
tr
”´
B2
px,mqEpγ,πq
¯
pµρπ pyq, yqϵρπ px, m, yqϵρπ px, m, yqT
ıff
“tr
„´
B2
px,mqEpγ,πq
¯
pµρπ pyq, yq E
px,mq„ρπpyq
”
ϵρπ px, m, yqϵρπ px, m, yqT
ıȷ
“tr
”´
B2
px,mqEpγ,πq
¯
pµρπ pyq, yqΣρπ pyq
ı
(5.9)
where the first equality obtains because the trace of an outer product equals an inner product;
the second by linearity of the trace; and the third by the definition of the covariance Σρπ pyq. The
equality marked pbqabove then holds because Epx,mq„ρπpyq
“
ϵρπ px, m, yq
‰
“0.
Next, note that the entropy of a Gaussian measure depends only on its covariance,
SXbM
“
ρπpyq
‰
“1
2 log detp2π eΣρπ pyqq,
and that the energy Epγ,πqpµρπ pyq, yqdoes not depend on Σρπ pyq. We can therefore write down
directly the covariance Σ˚
ρπ pyqminimizing FEpγ, ρqπpyqas a function of y. We have
BΣρπ FEpγ, ρqπpyq
pbq
« 1
2
´
B2
px,mqEpγ,πq
¯
pµρπ pyq, yq` 1
2Σρπ
´1
by equation pbqabove. Setting BΣρπ FEpγ, ρqπpyq “0, we find the optimum as expressed by
equation (5.8):
Σ˚
ρπ pyq“
`
B2
xEpγ,πq
˘
pµρπ pyq, yq´1 .
Finally, by substituting Σ˚
ρπ pyqin equation (5.9), we obtain the desired expression, equation (5.6):
FEpγ, ρqπpyq« Epγ,πqpµρπ pyq, yq´ SXbM rρπpyqs“: LFEpγ, ρqπpyq.
Remark 5.3.32. The usual form of the Laplace model in the literature omits the coparameters. It
is of course easy to recover the non-coparameterized form by taking M “1.
As well as being an approximation to a particular statistical game, the Laplacian free energy
defines a lax loss model.
207
Proposition 5.3.33. Let B be a subbicategory of BayesLens2 of Gaussian lenses between
Cartesian spaces whose backward channels have small variance, and with only structural 2-cells11.
Then LFE defines a lax loss model B ÑSGame.
Proof. Again we follow the notational conventions of the proof of Proposition 5.3.22. Additionally,
if ω is a state on a tensor product such as X bY , we will write ωX and ωY to denote its X and Y
marginals. We will continue to write c to denote the result of discarding the coparameters of a
coparameterized channel c.
Observe that, by repeated application of the linearity ofE, the log adjunction, and the definitions
of ‚and ˝,
`
LFEpd, d1q˛ LFEpc, c1q
˘
πpzq
“LFEpd, d1qc‚πpzq`
`
LFEpc, c1q˝ d1
c
˘
πpyq
“LFEpd, d1qc‚πpzq` E
py,nq„d1c‚πpzq
“
LFEpc, c1qπpyq
‰
“´ log pd
`
µd1c‚π pzq, z
˘
´log pc ‚π
`
µd1c‚π pzqY
˘
` E
py,nq„d1c‚πpzq
”
log pd1c‚π py, n|zq´ log pc
`
µc1π pyq, y
˘
´log pπ
`
µc1π pyqX
˘
` E
px,mq„c1πpyq
“
log pc1π px, m|yq
‰ı
“´ log pd
`
µd1c‚π pzq, z
˘
´log pc ‚π
`
µd1c‚π pzqY
˘
` E
py,nq„d1c‚πpzq
“
´log pc
`
µc1π pyq, y
˘
´log pπ
`
µc1π pyqX
˘‰
` E
py,nq„d1c‚πpzq
”
log pd1c‚π py, n|zq` E
px,mq„c1πpyq
“
log pc1π px, m|yq
‰ı
“´ log pd
`
µd1c‚π pzq, z
˘
´log pc ‚π
`
µd1c‚π pzqY
˘
` E
py,nq„d1c‚πpzq
“
´log pc
`
µc1π pyq, y
˘
´log pπ
`
µc1π pyqX
˘‰
` E
py,nq„d1c‚πpzq
E
px,mq„c1πpyq
“
log pd1c‚π py, n|zq` log pc1π px, m|yq
‰
“´ log pd
`
µd1c‚π pzq, z
˘
´log pc ‚π
`
µd1c‚π pzqY
˘
` E
py,nq„d1c‚πpzq
“
´log pc
`
µc1π pyq, y
˘
´log pπ
`
µc1π pyqX
˘‰
` E
px,m,y,nq„pc1˝d1cqπpzq
“
´log ppc1˝d1cqπ px, m, y, n|zq
‰
“´ log pd
`
µd1c‚π pzq, z
˘
´log pc ‚π
`
µd1c‚π pzqY
˘
` E
py,nq„d1c‚πpzq
“
´log pc
`
µc1π pyq, y
˘
´log pπ
`
µc1π pyqX
˘‰
´SXMY N
“
pc1˝d1
cqπpzq
‰
11An example of B here is obtained by restricting BayesLens2 to the category FdGauss of Definition 7.1.9, and by
excluding all but the structural 2-cells
208
“´ log pd
`
µd1c‚π pzq, z
˘
´log pc ‚π
`
µd1c‚π pzqY
˘
` E
py,nq„d1c‚πpzq
“
Epc,πq
`
µc1π pyq, y
˘‰
´SXMY N
“
pc1˝d1
cqπpzq
‰
“Epd,c‚πqpµd1c‚π pzq, zq` E
py,nq„d1c‚πpzq
“
Epc,πq
`
µc1π pyq, y
˘‰
´SXMY N
“
pc1˝d1
cqπpzq
‰
where XMY Nis shorthand for X bM bY bN.
Now, writing Eµ
pc,πqpyq:“Epc,πq
`
µc1π pyq, y
˘
, by the Laplace assumption, we have
E
py,nq„d1c‚πpzq
“
Eµ
pc,πqpyq
‰
«Eµ
pc,πqpµd1c‚π pzqY q` 1
2 tr
”´
B2
yEµ
pc,πq
¯`
µd1c‚π pzqY
˘
Σd1c‚π pzqY Y
ı
and so we can write
`
LFEpd, d1q˛ LFEpc, c1q
˘
πpzq
«Epd,c‚πqpµd1c‚π pzq, zq` Eµ
pc,πqpµd1c‚π pzqY q´ SXMY N
“
pc1˝d1
cqπpzq
‰
`1
2 tr
”´
B2
yEµ
pc,πq
¯`
µd1c‚π pzqY
˘
Σd1c‚π pzqY Y
ı
“´ log pd
`
µd1c‚π pzq, z
˘
´log pc
`
µc1π pµd1c‚π pzqY q, µd1c‚π pzqY
˘
´log pπ
`
µc1π pµd1c‚π pzqY qX
˘
´SXMY N
“
pc1˝d1
cqπpzq
‰
´log pc ‚π
`
µd1c‚π pzqY
˘
`1
2 tr
”´
B2
yEµ
pc,πq
¯`
µd1c‚π pzqY
˘
Σd1c‚π pzqY Y
ı
“Epd‚c,πq
`
µpc1˝d1cqπ pzq, z
˘
´SXMY N
“
pc1˝d1
cqπpzq
‰
´log pc ‚π
`
µd1c‚π pzqY
˘
`1
2 tr
”´
B2
yEµ
pc,πq
¯`
µd1c‚π pzqY
˘
Σd1c‚π pzqY Y
ı
“LFE
`
pd, d1q pc, c1q
˘
πpzq´ log pc ‚π
`
µd1c‚π pzqY
˘
`1
2 tr
”´
B2
yEµ
pc,πq
¯`
µd1c‚π pzqY
˘
Σd1c‚π pzqY Y
ı
.
Therefore, if we define a loss function κ by
κπpzq:“1
2 tr
”´
B2
yEµ
pc,πq
¯`
µd1c‚π pzqY
˘
Σd1c‚π pzqY Y
ı
´log pc ‚π
`
µd1c‚π pzqY
˘
then κ constitutes a 2-cell LFEpd, d1q˛ LFEpc, c1qñ LFE
`
pd, d1q pc, c1q
˘
, as required.
Effectively, this proposition says that, under the stated conditions, the free energy and the
Laplacian free energy coincide. Consequently, successfully playing a Laplacian free energy game
has the same autoencoding effect as playing a free energy game in the sense of §5.3.3.3.
Remark 5.3.34. We formalized the idea of a Gaussian having small or tightly-peaked variance as
meaning its covariance matrix Σ has small eigenvalues. We do not specify precisely what ‘small’
means here: only, it must be enough to license the use of Laplace’s method. Of course, as the
209
eigenvalues approach 0, the Gaussian approaches a Dirac delta distribution. In this case, one may
truncate the approximating expansion at first order and just work with the means — in fact, the
inversions become deterministic — and indeed, this is the choice made in some of the predictive
coding literature [33].
5.4. Monoidal statistical games
In Remark 4.3.18, we noted that the canonical section :taking a channel c to the lens equipped
with its exact inversion c: is not monoidal, because inverting the tensor of two channels with
respect to a joint state is in general not the same as inverting the two channels independently with
respect to the marginals, owing to the possibility of correlations. At the same time, we know from
Proposition 4.3.11 that the category BayesLensC of non-coparameterized Bayesian lenses in C
is nonetheless a monoidal category (and it is moreover symmetric monoidal when C is); and we
saw in Corollary 5.3.10 that Stat, and hence BayesLensC, are additionally fibrewise monoidal.
In this section, we establish analogous results for copy-composite Bayesian lenses, and statistical
games and loss models in turn, as well as demonstrating that each of our loss models is accordingly
monoidal. This monoidal structure on loss models can then be used to measure the error obtained
by inverting channels independently with respect to the marginals of a joint prior.
Because statistical games are defined over copy-composite channels, our starting point must be
to establish a monoidal structure on Copara2pCq.
Proposition 5.4.1. If the copy-discard category C is symmetric monoidal, then Copara2pCq
inherits a monoidal structure pb, Iq, with the same unit object I as in C. On 1-cells f : A ÝÑ
M
B
and f1 : A1 Ý Ý Ñ
M1
B1, the tensor f bf1 : A bA1 ÝÝÝÝÑ
MbM1
B bB1is defined by
fA
f1
A1
M
M1
B
B1
.
On 2-cells φ : f ñg and φ1 : f1 ñg1, the tensor φ bφ1 : pf bf1qñp g bg1qis given by the
210
string diagram
φ
φ1
A
A1
M
M1
B
B1
N1
N
.
Proof. To establish that pCopara2pCq, b, Iqis a monoidal bicategory, we need to show that bis a
pseudofunctor Copara2pCqˆ Copara2pCqÑ Copara2pCqand that I induces a pseudofunctor
1 ÑCopara2pCq, such that the pair of pseudofunctors satisfies the relevant coherence data. We
will omit the coherence data, and only sketch that the pseudofunctor bis well defined, leaving a
full proof for later work. (In the sequel here, we will not make very much use of this tensor.)
First, we confirm that bis locally functorial, meaning that our definition gives a functor on
each pair of hom categories. We begin by noting that bis well-defined on 2-cells, that φ bφ1
satisfies that change of coparameter axiom for f bf1; this is immediate from instantiating the
axiom’s string diagram. Next, we note that bpreserves identity 2-cells; again, this is immediate
upon substituting identities into the defining diagram. We therefore turn to the preservation of
composites, which requires that pγ dφqbp γ1dφ1q“p γ bγ1qdp φ bφ1q, and which translates
to the following graphical equation:
φ γ
φ1 γ1
A
A1
M
M1
B
B1
O
O1
“
φ
φ1
A
A1
M
M1
B
B1
γ
φ1 O1
O
It is easy to see that this equation is satisfied: use the naturality of the symmetry of pC, b, Iq. This
establishes that bis locally functorial.
Next, we confirm that bis horizontally (pseudo) functorial. First, we note that idf bidf1 “
idfbf1 by the naturality of the symmetry of pC, b, Iq. Second, we exhibit a multiplication natural
211
isomorphism, witnessing pseudofunctoriality, with components µg,g1,f,f 1 : pg bg1q˝p f bf1qñ
pg ˝fqbp g1˝f1qfor all composable pairs of 1-cells g, fand g1, f1. Let these 1-cells be such that
pg bg1q˝p f bf1qhas the underlying depiction
fA g
C
N
M
B
f1A1
g1 C1
N1
M1
B1
and so pg ˝fqbp g1˝f1qhas the depiction
f1A1
g1 C1
N1
M1
B1
fA g
C
N
M
B
.
It is then easy to see that definingµg,g1,f,f 1 and its inverse µ´1
g,g1,f,f 1 as the 2-cells with the following
212
respective underlying depictions gives us the desired isomorphism:
N1
B1
N
B
M
M1
C
B1
M
N
C1
N1
B
M1
A
A1
and
N1
M1
B1
N
M
B
C
N
M
B
C1
N1
M1
B1
A
A1
.
The naturality of this definition is a consequence of the naturality of the symmetry of pC, b, Iq.
That this tensor satisfies the monoidal bicategory axioms — of associativity, unitality, and
coherence — follows from the fact that the monoidal structure pb, Iqsatisfies correspondingly
decategorified versions of these axioms; we leave the details to subsequent exposition.
Following the monoidal Grothendieck recipe, establishing that BayesLens2 is monoidal entails
establishing that Stat2 is a monoidal indexed bicategory. But first we must define the latter concept,
by categorifying Definition 4.2.19.
Definition 5.4.2. Suppose pB, b, Iqis a monoidal bicategory. We will say thatF : B co op ÑBicat
is amonoidal indexed bicategory when it is equipped with the structure of a weak monoid object in the
3-category of indexed bicategories, indexed pseudofunctors, indexed pseudonatural transformations,
and indexed modifications.
More explicitly, we will take F to be a monoidal indexed bicategory when it is equipped with
(i) an indexed pseudofunctor µ : Fp´qˆ Fp“qÑ Fp´b“q called the multiplication, i.e.,
(a) a family of pseudofunctors µX,Y : F XˆF YÑFpX bY q, along with
(b) for any 1-cells f : X Ñ X1 and g : Y Ñ Y 1 in B, a pseudonatural isomorphism
µf,g : µX1,Y 1 ˝pF fˆF gqñ Fpf bgq˝ µX,Y ;
(ii) a pseudofunctor η : 1 ÑF Icalled the unit;
as well as three indexed pseudonatural isomorphisms — an associator, a left unitor, and a right
unitor — which satisfy weak analogues of the coherence conditions for a monoidal indexed category
[189, §3.2], up to invertible indexed modifications.
213
Remark 5.4.3. Because it is not our main purpose, and because the coherence data for higher-
dimensional structures rapidly becomes cumbersome, the preceding definition only suggests the
form of this coherence data. Unfortunately, we are not presently aware of a full explicit definition
in the literature of the concept of monoidal indexed bicategory.
Using this notion, we can establish that Stat2 is monoidal.
Theorem 5.4.4. Stat2 is a monoidal indexed bicategory, in the explicit sense of Definition 5.4.2.
Proof sketch. We only check the explicit requirements of the preceding definition, and expect that
the higher coherence data is satisfied by the fact that each of our high-dimensional structures is
obtained from a well-behaved lower-dimensional one using canonical categorical machinery.
In this way, the multiplicationµ is given first by the family of pseudofunctorsµX,Y : Stat2pXqˆ
Stat2pY qÑ Stat2pX bY qwhich are defined on objects simply by tensor
µX,Y pA, Bq“ A bB
since the objects do not vary between the fibres of Stat2, and on hom categories by the functors
Stat2pXqpA, Bqˆ Stat2pY qpA1, B1q
“Cat
`
disc CpI, Xq, Coparar
2pCqpA, Bq
˘
ˆCat
`
disc CpI, Yq, Coparar
2pCqpA1, B1q
˘
–Cat
`
disc CpI, Xqˆ disc CpI, Yq, Coparar
2pCqpA, Bqˆ Coparar
2pCqpA1, B1q
˘
Catpdisc CpI,projXqˆdisc CpI,projY q,bq
Ý ÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝ ÑCat
`
disc CpI, XbY q2, Coparar
2pCqpA bA1, BbB1q
˘
Catp ,idq
ÝÝÝÝÝÝÑCatpdisc CpI, XbY q, Coparar
2pCqpA bA1, BbB1q
“Stat2pX bY qpA bA1, BbB1q.
where Cat p , idqindicates pre-composition with the universal (Cartesian) copying functor. For
all f : X ÑX1and g : Y ÑY 1in Coparal
2pCq, the pseudonatural isomorphisms
µf,g : µX1,Y 1 ˝
`
Stat2pfqˆ Stat2pgq
˘
ñStat2pf bgq˝ µX,Y
are obtained from the universal property of the productˆof categories. The unit η : 1 ÑStat2pIq
is the pseudofunctor mapping the unique object of 1 to the monoidal unit I. Associativity and
unitality of this monoidal structure follow from the functoriality of the construction, given the
monoidal structures on C and Cat.
214
Just as the monoidal Grothendieck construction induces a monoidal structure on categories of
lenses for monoidal pseudofunctors [189], we obtain a monoidal structure on the bicategory of
copy-composite bayesian lenses.
Corollary 5.4.5. The bicategory of copy-composite Bayesian lenses BayesLens2 is a monoidal
bicategory. The monoidal unit is the object pI, Iq. The tensor bis given on 0-cells by pX, Aqb
pX1, A1q:“pX bX1, AbA1q, and on hom-categories by
BayesLens2
`
pX, Aq, pY, Bq
˘
ˆBayesLens2
`
pX, Aq, pY, Bq
˘
“Coparal
2pCqpX, Yqˆ Stat2pXqpB, Aqˆ Coparal
2pCqpX1, Y1qˆ Stat2pX1qpB1, A1q
„Ý ÑCoparal
2pCqpX, Yqˆ Coparal
2pCqpX1, Y1qˆ Stat2pXqpB, Aqˆ Stat2pX1qpB1, A1q
bˆµ op
X,X1
ÝÝÝÝÝÝÑCoparal
2pCqpX bX1, YbY 1qˆ Stat2pX bX1qpB bB1, AbA1q
“BayesLens2
`
pX, Aqbp X1, A1q, pY, Bqbp Y 1, B1q
˘
.
And similarly, we obtain a monoidal structure on statistical games.
Proposition 5.4.6. The bicategory of copy-composite statistical games SGame is a monoidal
bicategory. The monoidal unit is the object pI, Iq. The tensor bis given on 0-cells as for the tensor
of Bayesian lenses, and on hom-categories by
SGame
`
pX, Aq, pY, Bq
˘
ˆSGame
`
pX1, A1q, pY 1, B1q
˘
“BayesLens2
`
pX, Aq, pY, Bq
˘
ˆStatpXqpB, Iq
ˆBayesLens2
`
pX1, A1q, pY 1, B1q
˘
ˆStatpX1qpB1, Iq
„Ý ÑBayesLens2
`
pX, Aq, pY, Bq
˘
ˆBayesLens2
`
pX1, A1q, pY 1, B1q
˘
ˆStatpXqpB, Iqˆ StatpX1qpB1, Iq
bˆµX,X1
ÝÝÝÝÝÝÑBayesLens2
`
pX, Aqbp X1, A1q, pY, Bqbp Y 1, B1q
˘
ˆStatpX bX1qpB bB1, IbIq
„Ý ÑSGame
`
pX, Aqbp X1, A1q, pY, Bqbp Y 1, B1q
˘
where here µ indicates the multiplication of the monoidal structure on Stat (cf. Proposition 4.3.6).
Having obtained a monoidal structure on statistical games, we are in a position to ask for
monoidal structures on inference systems and loss models:
Definition 5.4.7. A monoidal inference system is an inference system pD, ℓqfor which ℓ is a lax
monoidal pseudofunctor. A monoidal loss model is a loss model L which is a lax monoidal lax
functor.
215
To make sense of this definition, we need a notion of lax monoidal structure appropriate for
strong (pseudo-) and lax functors: a lax-functor generalization of the notion of lax monoidal
functor12 from Definition 3.1.11. Just as a lax monoidal structure on a functor is given by equipping
the functor with natural transformations, a lax monoidal structure on a lax functor is given by
equipping it with pseudonatural transformations. The general structure is given by Moeller and
Vasilakopoulou [189, §2.2] for the case of pseudofunctors; the lax case is similar.
In the following remark, we instantiate this structure for loss models.
Remark 5.4.8. A loss model L : B ÑSGame is lax monoidal when it is equipped with strong
transformations
B ˆB SGame ˆSGame
B SGame
bB bG
LˆL
L
λ and
1
B SGame
pI,I q
pI,I q
L
λ0
where bB and bG denote the monoidal products onB ãÑBayesLens2 and SGame respectively,
and when λ and λ0 are themselves equipped with invertible modifications satisfying coherence
axioms, as in Moeller and Vasilakopoulou [189, §2.2].
Note that, because L must be a (lax) section of the 2-fibration πLoss|B : SGame|B ÑB, the
unitor λ0 is forced to be trivial, picking out the identity on the monoidal unit pI, Iq. Likewise, the
laxator λ : Lp´qb Lp“qñ Lp´b“q must have 1-cell components which are identities:
LpX, Aqb LpX1, Aq“p X, Aqbp X1, A1q“p X bX1, AbA1q“ L
`
pX, Aqb LpX1, Aq
˘
The interesting structure is therefore entirely in the 2-cells. We follow the convention of [145,
Def. 4.2.1] that a strong transformation is a lax transformation with invertible 2-cell components.
Supposing that pc, c1q: pX, Aq ÞÑpY, Bqand pd, d1q: pX1, A1q ÞÑpY 1, B1qare 1-cells in B, the
corresponding 2-cell component of λ has the form λc,d : L
`
pc, c1qbp d, d1q
˘
ñLpc, c1qb Lpd, d1q,
hence filling the following square in SGame:
pX, Aqbp X1, A1q pY, Bqbp Y 1, B1q
pX, Aqbp X1, A1q pY, Bqbp Y 1, B1q
Lpc,c1qbLpd,d1q
Lppc,c1qbpd,d1qq
λc,d
Intuitively, these 2-cells witness the failure of the tensor Lpc, c1qb Lpd, d1qof the parts to account
for correlations that may be evident to the “whole system” L
`
pc, c1qbp d, d1q
˘
.
12Note that, although lax functors themselves generalize lax monoidal functors (as bicategories generalize monoidal
categories), lax monoidal lax functors are different again, adding another dimension (as monoidal functors add a
dimension to functors): a lax monoidal lax functor is equivalently a homomorphism of one-object tricategories.
216
Just as there is a notion of monoidal natural transformation accompanying the notion of monoidal
functor (recall Definition 3.1.13), there is a notion of monoidal icon between lax monoidal lax
functors13, from which we obtain a symmetric monoidal category of monoidal loss models.
Proposition 5.4.9. Monoidal loss models and monoidal icons form a subcategory MonLosspBqof
LosspBq, and the symmetric monoidal structure p`, 0qon the latter restricts to the former.
5.4.1. Examples
In this section, we present the monoidal structure on the loss models considered above. Because
loss models L are (lax) sections, following Remark 5.4.8, this monoidal structure is given in each
case by a lax natural family of 2-cells λc,d : L
`
pc, c1qbp d, d1q
˘
ñLpc, c1qb Lpd, d1q, for each
pair of lenses pc, c1q: pX, Aq ÞÑ pY, Bqand pd, d1q: pX1, A1q ÞÑ pY 1, B1q. Such a 2-cell λc,d is
itself given by a loss function of type B bB1XbX1
Ý ÝÝÝ Ñ‚ I satisfying the equation L
`
pc, c1qbpd, d1q
˘
“
Lpc, c1qb Lpd, d1q` λc,d; we can think of it as measuring the difference between the joint game
L
`
pc, c1qbp d, d1q
˘
and the “mean field” games Lpc, c1qand Lpd, d1qtaken together.
Following Johnson and Yau [145, Eq. 4.2.3], lax naturality requires that λ satisfy the following
equation of 2-cells, where K denotes the laxator (with respect to horizontal composition ˛) with
components Kpe, cq: Le ˛Lc ñLpe  cq:
pY, Bqbp Y 1, B1q
pX, Aqbp X1, A1q pZ, Cqbp Z1, C1q
Lpcbdq Lpebfq
L
´
pecqbpfdq
¯
LpecqbLpfdq
Kpebf,cbdq
λpec,fdq
“
pY, Bqbp Y 1, B1q
pX, Aqbp X1, A1q pY, Bqbp Y 1, B1q pZ, Cqbp Z1, C1q
Lpcbdq Lpebfq
LpecqbLpfdq
LcbLd LebLf
λpc,dq λpe,fq
Kpe,cqbKpf,dq
13The notion of monoidal icon can be obtained by weakening the notion of monoidal pseudonatural transformation
given by Moeller and Vasilakopoulou [189, §2.2].
217
Since vertical composition in SGame is given on losses by `, we can write this equation as
λpe  c, f dq` Kpe bf, cbdq
“λpe, fq˛ λpc, dq` Kpe, cqb Kpf, dq
“λpe, fqcbd `λpc, dq˝p e1bf1qcbd `Kpe, cqb Kpf, dq. (5.10)
In each of the examples below, therefore, we establish the definition of the laxatorλ and check that
it satisfies equation 5.10.
We will often use the notation p´qX to denote projection onto a factor X of a monoidal product.
5.4.1.1. Relative entropy
Proposition 5.4.10. The loss model KL of Proposition 5.3.22 is lax monoidal. Supposing that
pc, c1q: pX, Xq ÞÑpY, Yqand pd, d1q: pX1, X1q ÞÑpY 1, Y1qare lenses in B, the corresponding
component λKLpc, dqof the laxator is given, for ω : IÑ‚X bX1and py, y1q: Y bY 1, by
λKLpc, dqωpy, y1q:“ E
px,x1,m,m1q„
pc1
ωX bd1
ωX1qpy,y1q
„
log pωXbωX1px, x1q
pωpx, x1q
ȷ
`log ppcbdq ‚ωpy, y1q
ppcbdq ‚pωXbωX1qpy, y1q.
(Note that the first term has the form of a “posterior mutual information” and the second a
log-likelihood ratio.)
Proof. We have
`
KLpcqb KLpdq
˘
ωpy, y1q
“ E
px,mq„c1ωX pyq
”
log pc1ωX
px, m|yq´ log pc:
ωX
px, m|yq
ı
` E
px1,m1q„d1ωX1py1q
„
log pd1ωX1
px1, m1|y1q´ log pd:
ωX1
px1, m1|y1q
ȷ
“ E
px,x1,m,m1q„
pc1
ωX bd1
ωX1qpy,y1q
„
log pc1ωX bd1ωX1
px, x1, m, m1|y, y1q´ log pc:
ωX bd:
ωX1
px, x1, m, m1|y, y1q
ȷ
and
`
KLpc bdqωpy, y1q
“ E
px,x1,m,m1q„
pc1
ωX bd1
ωX1qpy,y1q
”
log pc1ωX bd1ωX1
px, x1, m, m1|y, y1q´ log ppcbdq:
ω
px, x1, m, m1|y, y1q
ı
.
218
Using Bayes’ rule, we can rewrite the exact inversions in these expressions, obtaining
`
KLpcqb KLpdq
˘
ωpy, y1q
“ E
px,x1,m,m1q„
pc1
ωX bd1
ωX1qpy,y1q
”
log pc1ωX bd1ωX1
px, x1, m, m1|y, y1q´ log pcpy, m|xq´ log pdpy1, m1|x1q
´log pωX pxq´ log pωX1px1q` log pc ‚ωX pyq` log pd ‚ωX1py1q
ı
and
`
KLpc bdqωpy, y1q
“ E
px,x1,m,m1q„
pc1
ωX bd1
ωX1qpy,y1q
”
log pc1ωX bd1ωX1
px, x1, m, m1|y, y1q´ log pcpy, m|xq´ log pdpy1, m1|x1q
´log pωpx, x1q` log ppcbdq ‚ωpy, y1q
ı
.
We define λKLpc, dqωpy, y1qas the difference from
`
KLpc bdqωpy, y1qto
`
KLpcqb KLpdq
˘
ωpy, y1q,
and so, with a little rearranging, we obtain the expression above:
λKLpc, dqωpy, y1q:“
`
KLpc bdqωpy, y1q´
`
KLpcqb KLpdq
˘
ωpy, y1q
“ E
px,x1,m,m1q„
pc1
ωX bd1
ωX1qpy,y1q
„
log pωXbωX1px, x1q
pωpx, x1q
ȷ
`log ppcbdq ‚ωpy, y1q
ppcbdq ‚pωXbωX1qpy, y1q.
Next, we need to validate lax naturality. Since KL is strict on losses, we need only check that
λKLpe  c, f dq“ λKLpe, fqcbd `λKLpc, dq˝p e1bf1qcbd .
By definition, we have
`
λKLpe, fqcbd
˘
ωpz, z1q
“ E
py,y1,n,n1q„
pe1
cbf1
dqωpz,z1q
«
log
ppcbdq ‚pωXbωX1qpy, y1q
ppcbdq ‚ωpy, y1q
ff
`log ppebfq ‚pcbdq ‚ωpz, z1q
ppebfq ‚pcbdq ‚pωXbωX1qpz, z1q
and
`
λKLpc, dq˝p e1bf1qcbd
˘
ωpz, z1q
“ E
py,y1,n,n1q„
pe1
cbf1
dqωpz,z1q
»
——– E
px,x1,m,m1q„
pc1
ωX bd1
ωX1qpy,y1q
„
log pωXbωX1px, x1q
pωpx, x1q
ȷ
`log ppcbdq ‚ωpy, y1q
ppcbdq ‚pωXbωX1qpy, y1q
fi
ffiffifl .
219
And so we also have
λKLpe  c, f dqωpz, z1q
“ E
px,x1,m,m1q„`
pc1˝e1
cqbpd1˝f1
dq
˘
ωpz,z1q
„
log pωXbωX1px, x1q
pωpx, x1q
ȷ
`log ppebfq ‚pcbdq ‚ωpz, z1q
ppebfq ‚pcbdq ‚pωXbωX1qpz, z1q
“
`
λKLpc, dq˝p e1bf1qcbd
˘
ωpz, z1q`
`
λKLpe, fqcbd
˘
ωpz, z1q
thereby establishing the lax naturality of λKL, by the commutativity of `.
Remark 5.4.11. Although KL is lax monoidal, its laxness arises from the state-dependence of the
inversions, and we saw in the opening of this chapter, and then more formally in Remark 5.3.23,
that in its simplest form the relative entropy does not depend on the inversions; in some sense, the
statistical game structure is extraneous.
In Remark 5.3.23, we saw that DKL defines a strict section of a 2-fibration
ş
K πK
Ý Ý ÑB, attaching
relative entropies to parallel pairs of channels and capturing their chain rule compositionally.
Since this section does not involve any inversions, we may thus wonder whether it is more than
lax monoidal: and indeed it is! DKL is in fact a strong monoidal section which is moreover
strict monoidal on the losses themselves. The laxator simply maps pc, c1, DKLpc, c1qq and
pd, d1, DKLpd, d1qqto pc bd, c1 bd1, DKLpc, c1q` DKLpd, d1qq; and indeed it is easy to verify
that DKLpc bd, c1bd1q“ DKLpc, c1q` DKLpd, d1q.
5.4.1.2. Maximum likelihood estimation
Proposition 5.4.12. The loss model MLE of Proposition 5.3.25 is lax monoidal. Supposing that
pc, c1q: pX, Xq ÞÑpY, Yqand pd, d1q: pX1, X1q ÞÑpY 1, Y1qare lenses in B, the corresponding
component λMLEpc, dqof the laxator is given, for ω : IÑ‚X bX1and py, y1q: Y bY 1, by
λMLEpc, dqωpy, y1q:“log
ppcbdq ‚pωXbωX1qpy, y1q
ppcbdq ‚ωpy, y1q .
Proof. To obtain the definition of λMLEpc, dq, we consider the difference from MLEpc bdqto
MLEpcqb MLEpdq:
λMLEpc, dqωpy, y1q:“MLEpc bdqωpy, y1q´
`
MLEpcqb MLEpdq
˘
ωpy, y1q
“ ´log ppcbdq ‚ωpy, y1q` log pc ‚ωX pyq´ log pd ‚ωX1py1q
“ log
ppcbdq ‚pωXbωX1qpy, y1q
ppcbdq ‚ωpy, y1q .
220
To demonstrate lax naturality, recall that MLE is a lax section, so we need to consider the
corresponding ˛-laxator. From Proposition 5.3.25, the laxator KMLEpe, cq: MLEpeq˛ MLEpcqñ
MLEpe  cqis given by KMLEpe, cq:“MLEpcq˝ e1
c. Next, observe that
λMLEpe  c, f dqωpz, z1q“ log
p`
pe‚cq bpf‚dq
˘
‚pωXbωX1qpz, z1q
p`
pe‚cq bpf‚dq
˘
‚ωpz, z1q
“log
ppebfq ‚pcbdq ‚pωXbωX1qpz, z1q
ppebfq ‚pcbdq ‚ωpz, z1q
“λMLEpe, fqpcbdq‚ωpz, z1q.
Consequently, we need to verify the equation
MLEpc bdq˝p e bf1qcbd “λMLEpc, dq˝p e1bf1qcbd `
`
MLEpcqb MLEpdq
˘
˝pe1bf1qcbd
which, by bilinearity of effects, is equivalent to verifying
MLEpc bdq“ λMLEpc, dq` MLEpcqb MLEpdq.
But, since `is commutative, this is satisfied by the definition of λMLEpc, dqas a 2-cell of type
MLEpc bdqñ MLEpcqb MLEpdq.
5.4.1.3. Free energy
Since KL and MLE are both lax monoidal, it follows that so is FE.
Corollary 5.4.13. The loss model FE of Definition 5.3.26 is lax monoidal. Supposing that pc, c1q:
pX, Xq ÞÑpY, Yqand pd, d1q: pX1, X1q ÞÑpY 1, Y1qare lenses in B, the corresponding component
λFEpc, dqof the laxator is given, for ω : IÑ‚X bX1and py, y1q: Y bY 1, by
λFEpc, dqωpy, y1q:“ E
px,x1q„pc1ωX bd1ωX1qpy,y1q
„
log pωXbωX1px, x1q
pωpx, x1q
ȷ
.
Proof. FE is defined as KL `MLE, and hence λFE is obtained as λKL `λMLE. Since `is functorial,
it preserves lax naturality, and so λFE is also lax natural. λFE is thus a strong transformation
FEp´qb FEp“qñ FEp´b“q , and hence FE is lax monoidal by Remark 5.4.8.
5.4.1.4. Laplacian free energy
In order to demonstrate that the lax monoidal structure on FE is not destroyed by the Laplace
approximation, we prove explicitly that LFE is also lax monoidal.
221
Proposition 5.4.14. The loss model LFE of Propositions 5.3.31 and 5.3.33 is lax monoidal.
Supposing that pc, c1q: pX, Xq ÞÑpY, Yqand pd, d1q: pX1, X1q ÞÑpY 1, Y1qare lenses in B, the
corresponding component λLFEpc, dqof the laxator is given, forω : IÑ‚X bX1and py, y1q: Y bY 1,
by
λLFEpc, dqωpy, y1q:“log pωXbωX1pµpcbdq1ω py, y1qXX 1q
pωpµpcbdq1ω py, y1qXX 1q
where µpcbdq1ω py, y1qXX 1 is the pX bX1q-mean of the Gaussian distribution pc1
ωX bd1
ωX1qpy, y1q.
Proof. We have
LFEpc bdqωpy, y1q
“´ log pcbdpµpc1bd1qω py, y1q, y, y1q´ log pωpµpc1bd1qω py, y1qXX 1q
´SXX 1MM 1
“
pc1bd1qωpy, y1q
‰
“´ log pcpµc1ωX
pyq, yq´ log pdpµd1ωX1
py1q, y1q´ pωpµpc1bd1qω py, y1qXX 1q
´SXM
“
c1
ωX pyq
‰
´SX1M1
”
d1
ωX1py1q
ı
and
`
LFEpcqb LFEpdq
˘
ωpy, y1q
“LFEpcqωX pyq` LFEpdqωX1py1q
“´ log pcpµc1ωX
pyq, yq´ pωX pµcωX pyqXq´ SXM
“
c1
ωX pyq
‰
´log pdpµd1ωX1
py1q, y1q´ pωX1pµd1ωX1
py1qX1q´ SX1M1
”
d1
ωX1py1q
ı
so that
λLFEpc, dqωpy, y1q“ LFEpc bdqωpy, y1q´
`
LFEpcqb LFEpdq
˘
ωpy, y1q
“log pωXbωX1pµpcbdq1ω py, y1qXX 1q
pωpµpcbdq1ω py, y1qXX 1q
as given above.
We need to verify lax naturality, which means checking the equation
λLFEpe  c, f dq`κpe bf, cbdq“ λLFEpe, fqcbd `λLFEpc, dq˝pe1bf1qcbd `κpe, cqbκpf, dq
where κ is the ˛-laxator with components κpe, cq: LFEpeq˛ LFEpcqñ LFEpe  cqgiven by
κpe, cqπpzq“ 1
2 tr
”´
B2
yEµ
pc,πq
¯`
µe1c‚π pzqY
˘
Σe1c‚π pzqY Y
ı
´log pc ‚π
`
µe1c‚π pzqY
˘
.
222
(see Proposition 5.3.33). We have
λLFEpe  c, f dq“ log
pωXbωX1pµpcbdq1ω pµpebfq1
pcbdq‚ω
pz, z1qY Y1qXX 1q
pωpµpcbdq1ω pµpebfq1
pcbdq‚ω
pz, z1qY Y1qXX 1q
“λLFEpc, dqωpµpebfq1
pcbdq‚ω
pz, z1qY Y1qXX 1q
and, by the Laplace approximation,
`
λLFEpc, dq˝p e1bf1qcbdqωpz, z1q
“ E
py,y1,n,n1q„
pe1
cbf1
dqωpz,z1q
”
λLFEpc, dqωpy, y1q
ı
« λLFEpc, dqωpµpebfq1
pcbdq‚ω
pz, z1qY Y1q
`1
2 tr
”´
B2
py,y1qλLFEpc, dqω
¯´
µpebfq1
pcbdq‚ω
pz, z1qY Y1
¯
Σpebfq1
pcbdq‚ω
pz, z1qpY Y1qpY Y1q
ı
.
We also have
`
κpe, cqb κpf, dq
˘
ωpz, z1q
“ κpe, cqωX pzq` κpf, dqωX1pz1q
“ 1
2 tr
”´
B2
yEµ
pc,ωXq
¯`
µe1c‚ωX
pzqY
˘
Σe1c‚ωX
pzqY Y
ı
´log pc ‚ωX
`
µe1c‚ωX pzqY
˘
`1
2 tr
„´
B2
y1Eµ
pd,ωX1q
¯`
µf1
d‚ωX1
pz1qY 1
˘
Σf1
d‚ωX1
pz1qY 1Y 1
ȷ
´log pd ‚ωX1
`
µf1
d‚ωX1
pz1qY 1
˘
“ 1
2 tr
”´
B2
py,y1qEµ
pcbd,ωXbωX1q
¯`
µpebfq1
pcbdq‚pωXbωX1q
pz, z1qY Y1
˘
Σpebfq1
pcbdq‚pωXbωX1q
pz, z1qpY Y1qpY Y1q
ı
´log ppcbdq ‚pωXbωX1q
`
µpebfq1
pcbdq‚pωXbωX1q
pz, z1qY Y1
˘
.
The left-hand side of the lax naturality equation is therefore given by
`
λLFEpe  c, f dq` κpe bf, cbdq
˘
ωpz, z1q
“ λLFEpc, dqωpµpebfq1
pcbdq‚ω
pz, z1qY Y1q
`1
2 tr
”´
B2
py,y1qEµ
pcbd,ωq
¯`
µpebfq1
pcbdq‚ω
pz, z1qY Y1
˘
Σpebfq1
pcbdq‚ω
pz, z1qpY Y1qpY Y1q
ı
´log ppcbdq‚ω
`
µpebfq1
pcbdq‚ω
pz, z1qY Y1
˘
223
while the right-hand side is given by
`
λLFEpe, fqcbd `λLFEpc, dq˝p e1bf1qcbd `κpe, cqb κpf, dq
˘
ωpz, z1q
“ log
ppcbdq ‚pωXbωX1qpµpebfq1
pcbdq‚ω
pz, z1qY Y1q
ppcbdq ‚ωpµpebfq1
pcbdq‚ω
pz, z1qY Y1q
`λLFEpc, dqωpµpebfq1
pcbdq‚ω
pz, z1qY Y1q
`1
2 tr
”´
B2
py,y1qλLFEpc, dqω
¯´
µpebfq1
pcbdq‚ω
pz, z1qY Y1
¯
Σpebfq1
pcbdq‚ω
pz, z1qpY Y1qpY Y1q
ı
`1
2 tr
”´
B2
py,y1qEµ
pcbd,ωXbωX1q
¯`
µpebfq1
pcbdq‚pωXbωX1q
pz, z1qY Y1
˘
Σpebfq1
pcbdq‚pωXbωX1q
pz, z1qpY Y1qpY Y1q
ı
´log ppcbdq ‚pωXbωX1q
`
µpebfq1
pcbdq‚pωXbωX1q
pz, z1qY Y1
˘
“´ log ppcbdq ‚ωpµpebfq1
pcbdq‚ω
pz, z1qY Y1q` λLFEpc, dqωpµpebfq1
pcbdq‚ω
pz, z1qY Y1q
`1
2 tr
”´
B2
py,y1qλLFEpc, dqω
¯´
µpebfq1
pcbdq‚ω
pz, z1qY Y1
¯
Σpebfq1
pcbdq‚ω
pz, z1qpY Y1qpY Y1q
ı
`1
2 tr
”´
B2
py,y1qEµ
pcbd,ωXbωX1q
¯`
µpebfq1
pcbdq‚pωXbωX1q
pz, z1qY Y1
˘
Σpebfq1
pcbdq‚pωXbωX1q
pz, z1qpY Y1qpY Y1q
ı
.
The difference from the left- to the right-hand side is thus
1
2 tr
”´
B2
py,y1qEµ
pcbd,ωq
¯`
µpebfq1
pcbdq‚ω
pz, z1qY Y1
˘
Σpebfq1
pcbdq‚ω
pz, z1qpY Y1qpY Y1q
ı
´1
2 tr
”´
B2
py,y1qEµ
pcbd,ωXbωX1q
¯`
µpebfq1
pcbdq‚pωXbωX1q
pz, z1qY Y1
˘
Σpebfq1
pcbdq‚pωXbωX1q
pz, z1qpY Y1qpY Y1q
ı
´1
2 tr
”´
B2
py,y1qλLFEpc, dqω
¯´
µpebfq1
pcbdq‚ω
pz, z1qY Y1
¯
Σpebfq1
pcbdq‚ω
pz, z1qpY Y1qpY Y1q
ı
.
Now, by definition Σpebfq1
pcbdq‚ω
“Σpebfq1
pcbdq‚pωXbωX1q
, and so by the linearity of the trace and of
derivation, this difference simplifies to
1
2 tr
”´
B2
py,y1q
´
Eµ
pcbd,ωq´Eµ
pcbd,ωXbωX1q´λLFEpc, dqω
¯¯
`
µpebfq1
pcbdq‚ω
pz, z1qY Y1
˘
Σpebfq1
pcbdq‚ω
pz, z1qpY Y1qpY Y1q
ı
.
224
Recall from the proof of Proposition 5.3.33 that Eµ
pc,πqpyq:“Epc,πq
`
µc1π pyq, y
˘
, and hence
`
Eµ
pcbd,ωq´Eµ
pcbd,ωXbωX1q
˘
py, y1q
“
`
Epcbd,ωq´Epcbd,ωXbωX1q
˘`
µpcbdq1ω py, y1q, y, y1˘
“´ log pωpµpcbdq1ω py, y1qXX 1q` log pωXbωX1pµpcbdq1ω py, y1qXX 1q
“log pωXbωX1pµpcbdq1ω py, y1qXX 1q
pωpµpcbdq1ω py, y1qXX 1q
“λLFEpc, dqωpy, y1q
so that Eµ
pcbd,ωq´Eµ
pcbd,ωXbωX1q´λLFEpc, dqω “0. This establishes that λLFE is lax natural.
5.5. Discussion
Having established the basic structure of statistical games and a handful of examples, there is much
more to be done, and so in this section we discuss a number of seemingly fruitful avenues of future
research.
An important such avenue is the link between this structure and the similar structure of
diegetic open (economic) games [51], a recent reformulation of compositional game theory [119],
which can also be understood as a constituting a fibration over lenses. Accordingly, the close
connection between game theory and reinforcement learning [25, 128] suggests that algorithms
for approximate inference (such as expectation-maximization) and reinforcement learning (such
as dynamic programming) are more than superficially similar. More broadly, we expect all three
of active inference, game theory, and reinforcement learning to fit into the general programme
of categorical systems theory [192] (with cybernetic extensions [54, 251]), and we expect that
reframing these disciplines in this way will elucidate their relationships. In Chapter 7, we supply
functorial dynamical semantics for approximate inference — a form of approximate inference
algorithm — but we leave the expression of this in systems-theoretic terms to future work. Likewise,
we leave to the future the study of the performance and convergence of algorithms built upon these
compositional foundations14.
Another avenue for further investigation concerns mathematical neatness. First, we seek an
abstract characterization of copy-composition and Copara2: Owen Lynch has suggested to us
14It is not clear that the fixed points of jointly optimizing the factors of a composite statistical game are the same
as those of the optimization of the composite. If one is only concerned with optimizing the inversions, then the
lens-like composition rule tells us that we may proceed by backward induction, first optimizing the factor nearest the
codomain, and then optimizing each remaining factor in turn back towards the domain. But the problem is harder if
we also wish to optimize the forward channels, as the inversion nearest the codomain still depends on the forward
channel nearest the domain.
225
that the computation by compilers of “static single-assignment form” (SSA) [152] by compilers may
have a similar structure, and so we expect an abstract characterization to capture both SSA and our
examples; we also hope that a more abstract approach will alleviate some of the higher-categorical
complexity resulting from the weakness of copy-composition. Second, the explicit constraint
defining simple coparameterized Bayesian lenses is inelegant; as indicated in Remark 5.2.21, we
expect that using dependent optics [43, 50, 276] may help to encode this constraint in the type
signature, at the cost of higher-powered mathematical machinery.
Finally, we seek further examples of loss models, and more abstract (and hopefully universal)
characterizations of those we already have; for example, it is known that the Shannon entropy has
a topological origin [40] via a “nonlinear derivation” [169], and we expect that we can follow this
connection further. In following this path, we expect to make use of the duality between algebra
and geometry [180, 194] (and their intersection in (quantitative) categorical logic [55, 140]), for
as we have already noted, loss functions have a natural algebraic structure. We consider such
investigations part of the nascent field of categorical information geometry.
226
6. Open dynamical systems, coalgebraically
In Chapter 3, we saw how to compose neural circuits together using an algebraic approach
to connectomics. These neural circuits are dynamical systems, formalized as sets of ordinary
differential equations. However, simply specifying these sets obscures the general compositional
structure of dynamical systems themselves, the revelation of which supports a subtler intertwining
of syntax and semantics, form and function—or, as it happens, algebra and coalgebra. In this
chapter we begin by introducing categorical language for describing general dynamical systems
‘behaviourally’. These systems will be ‘closed’ (non-interacting), and so we then explain how the
language of coalgebra, and specifically polynomial coalgebras, can be used to open them up.
However, traditional coalgebraic methods are restricted to discrete-time dynamical systems,
whereas we are also interested in the continuous-time systems that are commonly used in science,
such as our earlier neural circuits. This motivates the development of a class of generalized
polynomial coalgebras that model open systems governed by a general time monoid, and which
therefore encompass systems of dynamically interacting ordinary differential equations. In order to
account for stochastic dynamics, we generalize the situation still further, by redefining the category
of polynomial functors so that it can be instantiated in a nondeterministic setting. This will show us
how to define open Markov processes coalgebraically, and we also demonstrate related categories
of open random dynamical systems.
Finally, we use the polynomial setting to package these systems into monoidal bicategories of
‘hierarchical’ cybernetic systems, of which some are usefully generated differentially. In the next
chapter, these bicategories will provide the setting in which we cast the dynamical semantics of
approximate inference.
Remark 6.0.1. The story told in this chapter is of a form similar to that of categorical systems
theory [191, 192], in which systems on interfaces collect into (doubly) indexed (double) categories.
That story tells a general tale, but here we are interested in a specific case: coalgebraic systems
with polynomial interfaces whose time evolution is governed by an arbitrary monoid and which
may have non-determinism or side effects governed by a monad. Such systems appear to sit at a
227
sweet spot of scientific utility; in particular, the next chapter will use them to formalize models of
predictive coding. In future work, we intend to connect the two stories, expressing our generalized
polynomial coalgebras in the double-categorical framework.
6.1. Categorical background on dynamics and coalgebra
In this section, we introduce the background material needed for our development of open dynamical
systems as polynomial coalgebras.
6.1.1. Dynamical systems and Markov chains
We begin by recalling a ‘behavioural’ approach to dynamical systems popularized by Lawvere
and Schnauel [164] (who give a pedagogical account). These systems are ‘closed’ in the sense that
they do not require environmental interaction for their evolution. Later, when we consider open
systems, their ‘closures’ (induced by interaction with an environment) will constitute dynamical
systems of this form.
The evolution of dynamics is measured by time, and we will take time to be represented by
an arbitrary monoid pT, `, 0q. This allows us to consider time-evolution that is not necessarily
reversible, such as governed by N or R`, as well as reversible evolution that is properly governed
by groups such as Z or R. With this in mind, we give a classic definition of dynamical system, as a
T-action.
Remark 6.1.1. We will work in an abstract category E whose objects are considered to be “state
spaces”; its morphisms will determine the nature of the dynamical evolution. Therefore, for
deterministic systems, we can take E simply to be Set, or alternatively some other Cartesian
category or category of comonoid homomorphisms. For stochastic systems, we may take E to be
a copy-discard category such as KℓpDqor sfKrn, or some other category whose morphisms are
considered to be stochastic maps. For differential systems, we will require E to be equipped with a
tangent bundle endofunctor T; more on this in §6.3.2.
Definition 6.1.2. Let pT, `, 0qbe a monoid, representing time. Let X : E be some space, called
the state space . Then a closed dynamical system ϑ with state space X and time T is an action of
T on X. When T is also an object of E, then this amounts to a morphism ϑ : T ˆX ÑX (or
equivalently, a time-indexed family of X-endomorphisms, ϑptq: X ÑX), such that ϑp0q“ idX
and ϑps `tq“ ϑpsq˝ ϑptq. In this dynamical context, we will refer to the action axioms as the
flow conditions, as they ensure that the dynamics can ‘flow’.
228
Note that, in discrete time, this definition implies that a dynamical system is governed by a single
transition map .
Proposition 6.1.3. In discrete time T “N, any dynamical system ϑ is entirely determined by its
action at 1 : T. That is, letting the state space be X, we have ϑptq“ ϑp1q˝t where ϑp1q˝t means
“compose ϑp1q: X ÑX with itself t times”.
Proof. The proof is by induction on t : T. We must have ϑp0q“ idX and ϑpt `sq“ ϑptq˝ϑpsq. So
for any t, we must have ϑpt `1q“ ϑptq˝ ϑp1q. The result follows immediately; note for example
that ϑp2q“ ϑp1 `1q“ ϑp1q˝ ϑp1q.
An ordinary differential equation 9x “fpxqdefines a vector field x ÞÑ px, fpxqqon its state
space X, and its solutions xptqfor t : R define in turn a closed dynamical system, as the following
example sketches.
Example 6.1.4. Let T denote a tangent bundle functor E ÑE on the ambient category of spaces
E. Suppose X : U ÑTU is a vector field on U, with a corresponding solution (integral curve)
χx : R ÑU for all x : U; that is, χ1ptq“ Xpχxptqqand χxp0q“ x. Then letting the point x vary,
we obtain a map χ : R ˆU ÑU. This χ is a closed dynamical system with state space U and time
R.
So far, we have abstained from using much categorical language. But these closed dynamical
systems have a simple categorical representation.
Proposition 6.1.5. Closed dynamical systems with state spaces in E and time T are the objects of
the functor category CatpBT, Eq, where BT is the delooping of the monoid T. (Recall delooping
from Prop. 3.1.24.) Morphisms of dynamical systems are therefore natural transformations.
Proof. The category BT has a single object ˚and morphisms t : ˚Ñ˚ for each point t : T; the
identity is the monoidal unit 0 : T and composition is given by `. A functor ϑ : BT ÑE therefore
picks out an object ϑp˚q : E, and, for each t : T, a morphism ϑptq : ϑp˚q Ñϑp˚q, such that
the functoriality condition is satisfied. Functoriality requires that identities map to identities and
composition is preserved, so we require that ϑp0q“ idϑp˚qand that ϑps `tq“ ϑpsq˝ ϑptq. Hence
the data for a functor ϑ : BT ÑE amount to the data for a closed dynamical system in E with
time T, and the functoriality condition amounts precisely to the flow condition. A morphism of
closed dynamical systems f : ϑ Ñψ is a map on the state spaces f : ϑp˚qÑ ψp˚qthat commutes
229
with the flow, meaning that f satisfies f ˝ϑptq“ ψptq˝ f for all times t : T; this is precisely the
definition of a natural transformation f : ϑ Ñψ between the corresponding functors.
By changing the state space category E, this simple framework can represent different kinds of
dynamics. For example, by choosing E to be a category of stochastic channels, such as KℓpDqor
sfKrn, we obtain categories of closed Markov processes.
Example 6.1.6 (Closed Markov chains and Markov processes). A closed Markov chain is given by
a stochastic transition map XÑ‚X, typically interpreted as a Kleisli morphism X ÑPX for some
probability monad P : E ÑE (cf. §4.1.5 on probability monads). Following the discussion above, a
closed Markov chain is therefore an object in Cat
`
BN, KℓpPq
˘
. With more general time T, one
obtains closed Markov processes : objects in Cat
`
BT, KℓpPq
˘
. More explicitly, a closed Markov
process is a time-indexed family of Markov kernels; that is, a morphism ϑ : T ˆX ÑPX such
that, for all times s, t: T, ϑs`t “ϑs ‚ϑt as a morphism in KℓpPq. Note that composition ‚in
KℓpPqis typically given by the Chapman-Kolmogorov equation, so this means that
ϑs`tpy|xq“
ż
x1:X
ϑspy|x1qϑtpdx1|xq.
6.1.2. Coalgebra
We saw above that a closed discrete-time deterministic dynamical system is a functionX ÑX,
and that a closed discrete-time Markov chain is a function X ÑPX. This suggests a general
pattern for discrete-time dynamical systems, as morphisms X ÑF Xfor some endofunctor F:
such a morphism is called a coalgebra for the endofunctor F.
Definition 6.1.7. Let F : E ÑE be an endofunctor. A coalgebra for F, or F-coalgebra, is a pair
pX, cqof an object X : E and a morphism c : X ÑF X.
A morphism of F-coalgebras or coalgebra morphism pX, cqÑp X1, c1qis a morphism f : X Ñ
X1that commutes with the coalgebra structures; i.e., that makes the following diagram commute:
X X1
F X F X1
f
c c1
F f
F-coalgebras and their morphisms constitute a category, denoted CoalgpFq. The identity
morphism on pX, cqis simply the identity morphism idX : X ÑX.
230
Remark 6.1.8. In §3.4.1, we briefly discussed the notion of coalgebra for a comonad , which is a
coalgebra in the sense of the preceding definition that additionally satisfies axioms dual to those
defining algebras for a monad (Definition 3.4.16). In our dynamical applications, the endofunctors
not in general be comonads, and so it does not make sense to demand such axioms.
Remark 6.1.9. At the same time, the duality of algebra and coalgebra underlies the subtle powers
of the field of coalgebraic logic, in which the algebraic structure of logical syntax is used to define
constraints on or propositions about the behaviours of dynamical systems[62, 72, 138, 162, 207].
These tools are particularly useful in setting of formal verification, where it is desirable to prove
that systems behave according to a specification (for instance, for safety reasons).
With the notion ofF-coalgebra to hand, we immediately obtain categories of closed discrete-time
deterministic systems and Markov chains:
Example 6.1.10. The category of closed discrete-time deterministic dynamical systems in E is the
category Coalgpidqof coalgebras for the identity endofunctor idE : E ÑE.
Example 6.1.11. Let P : E ÑE be a probability monad on E. The category of Markov chains is
the category CoalgpPqof P-coalgebras.
Of course, polynomial functors are endofunctors Set ÑSet, so they come with a notion of
coalgebra, and we may ask how such objects behave.
Example 6.1.12. Suppose p : Set Ñ Set is a polynomial functor. A coalgebra for p is a
function c : X ÑpX for some set X. By Definition 3.5.2, we can write p as ř
i:pp1qypris, and
hence the p-coalgebra c has the form c : X Ñř
i:pp1qXpris. Such a function corresponds to a
choice, for each x : X, of an element of pp1qwhich we denote copxqand an associated function
cu
x : prc1pxqs ÑX. We can therefore write c equivalently as a pair pco, cuqwhere cu is the
coproduct ř
x cu
x : ř
x prcopxqsÑ X. We think of p as defining the interface of the dynamical
system represented by c, with pp1qencoding the set of possible ‘outputs’ or ‘configurations’ of the
system, each pristhe set of possible ‘inputs’ for the system when it is in configuration i : pp1q, and
X as the dynamical state space. The coalgebra c can then be understood as an open discrete-time
dynamical system: the map cu takes a state x : X and a corresponding input inprcopxqsand returns
the next state; and the map co takes a state x : X and returns the system’s corresponding output or
configuration copxq.
231
A pair of functions co : X Ñ pp1qand cu : ř
x prcopxqs ÑX is precisely a morphism
c : XyX Ñp of polynomials, and so we have established a mapping from p-coalgebras pX, cqto
morphisms XyX Ñp. In fact, we have a stronger result.
Proposition 6.1.13. There is an isomorphism of hom-sets PolypAyB, pq– SetpA, pBqnatural
in A, B, p, and hence adjunctions p´qyB % p´q˝B : Poly Ñ Set and Ayp´q % p ˝p´q :
Polyop ÑSet.
Proof sketch. In Example 6.1.12, we established a mapping SetpA, pBqÑ PolypAyB, pqfor the
case where A “ B; the general case is analogous. The inverse mapping follows directly from
Proposition 3.5.4. Naturality in A and B follows from naturality of pre-composition; naturality in
p follows from naturality of post-composition.
Polynomial coalgebras therefore constitute a type of open discrete-time dynamical systems. But
what if we want opencontinuous-time dynamical systems: do these fit into the coalgebra formalism?
In a different direction, what if we want open Markov chains ? In discrete time, we should be able to
consider coalgebras for composite endofunctors pP, but what if we want to do this in general time?
Let us turn now to answering these questions.
6.2. Open dynamical systems on polynomial interfaces
In this section, we begin by incorporating dynamical systems in general time into the coalgebraic
framework, before generalizing the notion of polynomial functor to incorporate ‘side-effects’ such
as randomness. The resulting framework will allow us to define types of system of interest, such as
open Markov processes, quite generally using coalgebraic methods, and in the subsequent sections
we will make much use of the newly available compositionality.
6.2.1. Deterministic systems in general time
In this section, let us suppose for simplicity that the ambient category E is Set. We will begin by
stating our general definition, before explaining the structures and intuitions that justify it.
Definition 6.2.1. A deterministic open dynamical system with interface p : Poly, state space
S : Set and time T : Set is a morphism β : SyS ÑrTy, psof polynomials, such that, for any
section σ : p Ñy of p, the induced morphism
SyS β
Ý ÑrTy, ps
rTy,σs
Ý ÝÝÝ ÑrTy, ys „Ý ÑyT
is a ◁-comonoid homomorphism.
232
To see how such a morphism β is like an ‘open’ version of the closed dynamical systems
of §6.1.1, note that by the tensor-hom adjunction, β can equivalently be written with the type
TybSyS Ñp. In turn, such a morphism corresponds to a pairpβo, βuq, where βo is the component
‘on configurations’ with the type T ˆS Ñpp1q, and βu is the component ‘on inputs’ with the
type ř
t:T
ř
s:S prβopt, sqsÑ S. We will call the map βo the output map, as it chooses an output
configuration for each state and moment in time; and we will call the map βu the update map , as it
takes a state s : S, a quantity of time t : T, and an input in prβopt, sqs, and returns a new state. We
might imagine the new state as being given by evolving the system from s for time t, and the input
as supplied while the system is in the configuration corresponding to ps, tq.
It is, however, not sufficient to consider merely such pairsβ “pβo, βuqto be our open dynamical
systems, for we need them to be like ‘open’ monoid actions: evolving for time t then for time s
must be equivalent to evolving for time t `s, given the same inputs. It is fairly easy to prove the
following proposition, whose proof we defer until after establishing the categories CoalgTppq,
when we prove it in an alternate form as Proposition 6.2.4.
Proposition 6.2.2. Comonoid homomorphisms SyS ÑyT correspond bijectively with closed
dynamical systems with state space S, in the sense given by functors BT ÑSet.
This establishes that seeking such a comonoid homomorphism will give us the monoid action
property that we seek, and so it remains to show that a composite comonoid homomorphism of the
form rTy, σs˝ β is a closed dynamical system with the “right inputs”. Unwinding this composite,
we find that the condition that it be a comonoid homomorphism corresponds to the requirement
that, for any t : T, the closure βσ : T ˆS ÑS of β by σ given by
βσptq:“S
βoptq˚σ
Ý ÝÝÝÝ Ñ
ÿ
s:S
prβopt, sqs
βu
ÝÑS
constitutes a closed dynamical system on S. The idea here is that σ gives the ‘context’ in which we
can make an open system closed, thereby formalizing the “given the same inputs” requirement
above.
With this conceptual framework in mind, we are in a position to render open dynamical systems
on p with time T into a category, which we will denote by CoalgTppq. Its objects will be pairs
pS, βqwith S and β an open dynamical on p with state space S; we will often write these pairs
equivalently as triples pS, βo, βuq, making explicit the output and update maps. Morphisms will be
maps of state spaces that commute with the dynamics:
233
Proposition 6.2.3. Open dynamical systems over p with time T form a category, denoted
CoalgTppq. Its morphisms are defined as follows. Let ϑ :“pX, ϑo, ϑuqand ψ :“pY, ψo, ψuqbe
two dynamical systems over p. A morphism f : ϑ Ñψ consists in a morphism f : X ÑY such
that, for any time t : T and section σ : pp1qÑ ř
i:pp1q
prisof p, the following naturality squares
commute:
X ř
x:X
prϑopt, xqs X
Y ř
y:Y
prψopt, yqs Y
ϑoptq˚σ ϑuptq
f f
ψoptq˚σ ψuptq
The identity morphism idϑ on the dynamical system ϑ is given by the identity morphism idX on
its state space X. Composition of morphisms of dynamical systems is given by composition of the
morphisms of the state spaces.
Proof. We need to check unitality and associativity of composition. This amounts to checking that
the composite naturality squares commute. But this follows immediately, since the composite of
two commutative diagrams along a common edge is again a commutative diagram.
We can alternatively state Proposition 6.2.2 as follows, noting that the polynomial y represents
the trivial interface, exposing no configuration to any environment nor receiving any signals from
it:
Proposition 6.2.4. CoalgT
idpyqis equivalent to the classical category CatpBT, Setqof closed
dynamical systems in Set with time T.
Proof. The trivial interface y corresponds to the trivial bundle id1 : 1 Ñ1. Therefore, a dynamical
system over y consists of a choice of state space S along with a trivial output map ϑo “ :
T ˆS Ñ1 and a time-indexed update map ϑu : T ˆS ÑS. This therefore has the form of a
classical closed dynamical system, so it remains to check the monoid action. There is only one
section of id1, which is again id1. Pulling this back along the unique map ϑoptq: S Ñ1 gives
ϑoptq˚id1 “idS. Therefore the requirement that, given any section σ of y, the maps ϑu ˝ϑoptq˚σ
form an action means in turn that so doesϑu : TˆS ÑS. Since the pullback of the unique section
id1 along the trivial output map ϑoptq“ : S Ñ1 of any dynamical system in CoalgT
idpyqis the
identity of the corresponding state space idS, a morphism f : pϑp˚q, ϑu, qÑp ψp˚q, ψu, qin
CoalgT
idpyqamounts precisely to a map f : ϑp˚qÑ ψp˚qon the state spaces in Set such that the
234
naturality condition f ˝ϑuptq“ ψuptq˝ f of Proposition 6.1.5 is satisfied, and every morphism in
CatpBT, Setqcorresponds to a morphism in CoalgT
idpyqin this way.
Now that we know that our concept of open dynamical system subsumes closed systems, let us
consider some more examples.
Example 6.2.5. Consider a dynamical system pS, ϑo, ϑuqwith outputs but no inputs. Such a
system has a ‘linear’ interface p :“Oy for some O; alternatively, we can write its interface p as
the ‘bundle’ idO : O ÑO. A section of this bundle must again be idO, and so ϑoptq˚idO “idS.
Once again, the update maps collect into to a closed dynamical system in CatpBT, Setq; just now
we have outputs ϑo : T ˆS Ñpp1q“ O exposed to the environment.
Proposition 6.2.6. When time is discrete, as with T “N, any open dynamical system pX, ϑo, ϑuq
over p is entirely determined by its components at1 : T. That is, we haveϑoptq“ ϑop1q: X Ñpp1q
and ϑuptq“ ϑup1q: ř
x:X prϑopxqsÑ X. A discrete-time open dynamical system is therefore a
triple pX, ϑo, ϑuq, where the two maps have types ϑo : X Ñpp1qand ϑu : ř
x:X prϑopxqsÑ X.
Proof. Suppose σ is a section of p. We require each closure ϑσ to satisfy the flow conditions, that
ϑσp0q“ idX and ϑσpt`sq“ ϑσptq˝ϑσpsq. In particular, we must haveϑσpt`1q“ ϑσptq˝ϑσp1q.
By induction, this means that we must haveϑσptq“ ϑσp1q˝t (compare Proposition 6.1.3). Therefore
we must in general have ϑoptq“ ϑop1qand ϑuptq“ ϑup1q.
Remark 6.2.7. Note that the preceding proposition means that the objects of CoalgNppqare
the objects of the traditional category Coalgppqof p-coalgebras. In fact, we have more than this:
CoalgNppq– Coalgppq; cf. Example 6.1.12 and Proposition 6.1.13.
Example 6.2.8. We can express ‘open’ vector fields in this framework. Suppose therefore thatX is
a differentiable manifold (and write X equally for its underlying set of points), and let 9x “fpx, aq
and b “gpxq, with f : X ˆA ÑTX and g : X ÑB. Then, as for the ‘closed’ vector fields
of Example 6.1.4, this induces an open dynamical system pX,
ş
f, gq : CoalgRpByAq, where
ş
f : R ˆX ˆA ÑX returns the pX, Aq-indexed solutions of f.
Example 6.2.9. The preceding example is easily extended to the case of a general polynomial
interface. Suppose similarly that 9x “fpx, axqand b “gpxq, now with f : ř
x:X prgpxqsÑ TX
and g : X Ñpp1q. Then we obtain an open dynamical system pX,
ş
f, gq: CoalgRppq, where
now
ş
f : R ˆř
x:X prgpxqsÑ X is the ‘update’ and g : X Ñpp1qthe ‘output’ map.
235
By letting the polynomialp vary, it is quite straightforward to extendCoalgTppqto an opindexed
category CoalgT.
Proposition 6.2.10. CoalgT extends to an opindexed category CoalgT : Poly Ñ Cat. On
objects (polynomials), it returns the categories above. On morphisms of polynomials, we simply
post-compose: given φ : p Ñq and β : SyS ÑrTy, ps, obtain SyS ÑrTy, psÑr Ty, qsin the
obvious way.
When we introduced Poly in §3.5, it was as a “syntax for interacting adaptive systems”, and
we know that we can understand Poly multicategorically, as it has a monoidal structure pb, yq
allowing us to place systems’ interfaces side-by-side (and which therefore gives us a multicategory,
OPoly by Proposition 3.3.4). We motivated our development of coalgebraic dynamical systems as
a compositional extension of the sets of ordinary differential equations that we used to formalize
rate-coded neural circuits (Definition 3.3.10), and we have seen that linear circuit diagrams embed
into Poly (Remark 4.2.33).
One may wonder, therefore, whether the opindexed categoriesCoalgT might supply the general
“semantics for interacting adaptive systems” that we seek: more precisely, is CoalgT a Poly-
algebra? This question can be answered affirmatively, as CoalgT is lax monoidal: more precisely,
it is a strong monoidal opindexed category.
Proposition 6.2.11. CoalgT is a monoidal opindexed category pPoly, b, yqÑp Cat, ˆ, 1q.
Proof. We need to define a natural family of functors µp,q : CoalgTppq ˆCoalgTpqq Ñ
CoalgTpp bqqconstituting the laxator, and a unit η : 1 ÑCoalgTpyq, along with associators α
and left and right unitors λ and ρ satisfying the pseudomonoid axioms of Definition 4.2.19.
The unit η : 1 ÑCoalgTpyqis given by the trivial system p1, !, !qwith the trivial state space
and the trivial interface: the output map is the unique map 1 Ñ1 (the identity); likewise, the
update map is the unique map 1 ˆ1 Ñ1. Note that 1 ˆ1 –1.
The laxator µp,q is given on objects pX, ϑq : CoalgTppqand pY, φq : CoalgTpqqby the
µp,qpϑ, φq:“
`
XY, pϑφq
˘
where the state space XY “X ˆY and pϑφqis the system given by
the right adjunct of
XY yXY bTy
„b y
ÝÝÝÝÑXyX bY yY bTy bTy
XyXbswapbTy
Ý ÝÝÝÝÝÝÝÝÝ ÑXyX bTy bY yY bTy
ϑ5bφ5
Ý ÝÝÝ Ñp bq
under the tensor-hom adjunction in Poly, where ϑ5and φ5are the corresponding left adjuncts of
ϑ and φ, and where „is the isomorphism XY yXY „Ý ÑXyX bY yY . On morphisms f : pX, ϑqÑ
236
pX1, ϑ1qand g : pY, φqÑp Y 1, φq, µp,q acts as µp,qpf, gq:“f ˆg; functoriality hence follows from
that of ˆ.
Next, we need to define µ on morphisms ζ : p Ñ p1 and ξ : q Ñ q1 of polynomials, giving
natural isomorphisms µζ,ξ : µp1,q1˝
`
CoalgTpζqˆCoalgTpξq
˘
ñCoalgTpζ bξq˝µp,q. But it is
easy to see in fact that µp1,q1 ˝
`
CoalgTpζqˆ CoalgTpξq
˘
“CoalgTpζ bξq˝ µp,q, as both sides
act by post-composing ζ bξ.
The associator is defined componentwise on objects as
αp,q,r :
´
pXY qZypXY qZ pϑbφqbψ
Ý ÝÝÝÝÝ Ñrppbqqbr,Tys
¯
ÞÑ
´
XpY ZqyXpY Zq ϑbpφbψq
Ý ÝÝÝÝÝ Ñrpbpqbrq, Tys
¯
and on morphisms as αp,q,r : pf ˆgqˆ h ÞÑf ˆpg ˆhq, implicitly using the associators of bon
Poly and ˆon Set.
Likewise, the left unitor is defined by
λp :
´
1Xy1X µy,ppη,ϑq
ÝÝÝÝÝÑry bp, Tys
¯
ÞÑ
´
XyX ϑÝ Ñrp, Tys
¯
implicitly using the left unitors of bon Poly and ˆon Set; and the right unitor is defined dually,
using the corresponding right unitors on Poly and Set.
That the associators and unitors satisfy the indexed monoidal category axioms follows from
the satisfaction of the monoidal category axioms by pPoly, b, yqand pSet, ˆ, 1q. (But it is easy,
though laborious, to verify this manually.)
Remark 6.2.12. We emphasize that the functor CoalgT is lax monoidal—the laxators are not
equivalences—since not all systems over the parallel interface p bq factor into a system over p
alongside a system over q.
With this indexed monoidal structure, we can show that, as we might hope from a general
semantics for interacting dynamical systems, Coalg subsumes our earlier linear circuit algebra of
rate-coded neural circuits.
Proposition 6.2.13. There is an inclusion ι of monoidal indexed categories as in the diagram
`
LinCirc, `, p0, 0q
˘
pSet, ˆ, 1q
pPoly, b, yq pCat, ˆ, 1q
R
CoalgR
ι
where R is the algebra from Proposition 3.3.12.
237
Proof sketch. ι is defined by a family of functors ιpno,niq : Rpno, niqÑ CoalgRpRnoyRni
q, where
each set Rpno, niqis treated as the corresponding discrete category; this means that ιpno,niq is
trivially functorial, and needs only be defined on objects (rate-coded neural circuits). Each such
circuit pλ, α, β, γ, Wqdefines an ‘open’ ordinary differential equation by Definition 3.3.10 with
inputs i : Rni. ιpno,niqis then defined by taking this ordinary differential equation to a corresponding
open dynamical system following Example 6.2.8, where the output space is the same as the state
space Rno and the output map is idRno .
We then need to check that this definition of ι is natural, meaning that the following diagram
commutes for each linear circuit diagram pA, Bq: pno, niqÑp mo, miq, where CoalgRpA, Bqis
defined by treating pA, Bqas a lens and hence a morphism of monomials of the type indicated.
Rpno, niq Rpmo, miq
CoalgRpRnoyRni
q CoalgRpRmoyRmi
q
RpA,Bq
CoalgRpA,Bq
ιpno,niq ιpmo,miq
To see that this diagram commutes, observe that we can write a rate-coded neural circuit κ as
a morphism RnoyTRno
ÑRnoyRni
of polynomials, where T is the tangent bundle functor; and
observe that the action of RpA, Bqis to post-compose the lens pA, Bqafter κ, as in RnoyTRno κÝ Ñ
RnoyRni pA,Bq
ÝÝÝÑRmoyRmi
. Now, ιpno,niq acts by taking κ to a system RnoyRno
ÑrTy, RnoyRni
s,
and CoalgRpA, Bqpost-composes rTy, pA, Bqs, so we obtain the system
RnoyRno ιpno,niqpκq
ÝÝÝÝÝÝÑrTy, RnoyRni
s
rTy,pA,Bqs
ÝÝÝÝÝÝÑrTy, RmoyRmi
s.
This is precisely the system obtained by applying ιpmo,miqto pA, Bq˝ κ, and hence ι is natural.
Finally, it is easy to check that ι is a monoidal natural transformation (Definition 3.1.13), which
by Moeller and Vasilakopoulou [189, Proposition 3.6] entails that ι is a morphism of monoidal
indexed categories. That it is additionally an inclusion follows from the evident fact that each
functor ιpno,niqis an embedding.
At some point during the preceding exposition, the reader may have wondered in what sense
these open dynamical systems are coalgebras. To answer this, recall from Proposition 6.1.13 that a
polynomial morphism SyS Ñq is equivalently a function S ÑqS and hence by Example 6.1.12
a q-coalgebra. Then, by setting q “rTy, ps, we see the connection immediately: the objects of
CoalgTppqare rTy, ps-coalgebras that satisfy the ◁-comonoid condition, and the morphisms of
CoalgTppqare coalgebra morphisms.
238
In the following subsection, we generalize the constructions above to allow for non-deterministic
(‘effectful’) feedback, using a generalization of the category Poly.
6.2.2. Polynomials with ‘effectful’ feedback, and open Markov processes
The category Poly of polynomial functors Set Ñ Set can be considered as a category of
‘deterministic’ polynomial interaction; notably, morphisms of such polynomials, which we take to
encode the coupling of systems’ interfaces, do not explicitly incorporate any kind of randomness
or uncertainty. Even if the universe is deterministic, however, the finiteness of systems and their
general inability to perceive the totality of their environments make it a convenient modelling
choice to suppose that systems’ interactions may be uncertain; this will be useful not only in
allowing for stochastic interactions between systems, but also to define stochastic dynamical
systems ‘internally’ to a category of polynomials.
To reach the desired generalization, we begin by recalling thatPoly is equivalent to the category
of Grothendieck lenses for the self-indexing of Set (Example 4.2.30). We define our categories of
generalized polynomials from this perspective, by considering Kleisli categories indexed by their
“deterministic subcategories”. This allows us to define categories of Grothendieck lenses which
behave like Poly when restricted to the deterministic case, but also admit uncertain inputs. In
order to apply the Grothendieck construction, we begin by defining an indexed category.
Definition 6.2.14. Suppose E is a category with all limits, and suppose M : E ÑE is a monad
on E. Define the indexed category EM {´: E op ÑCat as follows. On objects B : E, we define
EM {B to be the full subcategory of KℓpMq{B on those objects ιp : EÑ‚B which correspond
to maps E
p
Ý ÑB
ηB
Ý Ý ÑMB in the image of ι. Now suppose f : C Ñ B is a map in E. We
define EM {f : EM {B Ñ EM {C as follows. The functor EM {f takes objects ιp : EÑ‚B to
ιpf˚pq: f˚EÑ‚C where f˚p is the pullback of p along f in E, included into KℓpMqby ι.
To define the action of EM {f on morphisms α : pE, ιp: EÑ‚Bq Ñ pF, ιq: FÑ‚Bq, note
that since we must have ιq ‚α “ ιp, we can alternatively write α as the B-dependent sum
ř
b:B αb : ř
b:B prbsÑ ř
b:B Mqrbs. Then we can definepEM {fqpαqaccordingly as pEM {fqpαq:“
ř
c:C αfpcq : ř
c:C prfpcqsÑ ř
c:C Mqrfpcqs.
Definition 6.2.15. We define PolyM to be the category of Grothendieck lenses for EM {´. That
is, PolyM :“
ş
EM {´op, where the opposite is again taken pointwise.
Example 6.2.16. When E “Set and M “idSet, Definition 6.2.14 recovers our earlier definition
of Poly.
239
Example 6.2.17. When M is a monad on Set, we find that the objects of PolyM are the same
polynomial functors as constitute the objects of Poly. The morphisms f : p Ñq are pairs pf1, f7q,
where f1 : B ÑC is a function in Set and f7is a family of morphisms qrf1pxqsÑ‚prxsin KℓpMq,
making the following diagram commute:
ř
x:B Mprxs ř
b:B qrf1pxqs ř
y:C qrys
B B C
f7
qηB˚p
f1
{
Remark 6.2.18. Consequently, we can think of PolyM as a dependent version of the category of
M-monadic lenses, in the sense of Clarke et al. [65, §3.1.3].
Remark 6.2.19. Any monad pM, µ, ηqon Set induces a comonad p¯M, δ, ϵqon the category Poly
of polynomial functors Set ÑSet, and PolyM can be recovered as the coKleisli category of this
comonad, PolyM –coKℓp¯Mq. We heard of this idea from David Spivak.
On objects (polynomial functors), ¯M : Poly Ñ Poly acts to map p : ř
i:pp1qpris Ñpp1q
to ¯Mp : ř
i:pp1qMpris Ñpp1q. Given a morphism of polynomials φ : p Ñ q, ¯M returns the
morphism ¯Mpφqwhose forward component is againφ1 and whose backward component is defined
by ¯Mpφq7
i :“Mpφ7
iqfor each i in pp1q.
The counit ϵ : ¯M ñ idPoly is defined for each p as the morphism ϵp : ¯Mp Ñ p whose
forward component is idpp1qand whose backward component is given for each i : pp1qby the unit
ηpris : prisÑ Mprisof the monad M. Similarly, the comultiplication δ : ¯M ¯M ñ ¯M is defined for
each p as the morphism δp : ¯M ¯Mp Ñ ¯Mp whose forward component is again the identity and
whose backward components are given by the multiplication of the monad µ, i.e. pδ7
pqi :“µpris.
Finally, the coKleisli category coKℓp¯Mqhas the same objects as Poly. A morphism p Ñq in
coKℓp¯Mqis a morphism ¯Mp Ñq in Poly. Composition in coKℓp¯Mqis the usual composition in
the forward direction and Kleisli composition in the backward direction.
Remark 6.2.20. Since E is assumed to have all limits, it must have a product structure pˆ, 1q.
When M is additionally a monoidal monad (Definition 4.1.16), then PolyM acquires a tensor akin
to that defined for Poly in Proposition 3.5.7, and which we also denote by pb, Iq: the definition
only differs by substituting the structure pb, Iqon KℓpMqfor the product pˆ, 1qon Set. This
monoidal structure follows as before from the monoidal Grothendieck construction: EM {´is lax
monoidal, with laxator taking p : EM {B and q : EM {C to p bq : EM {pB bCq.
On the other hand, for PolyM also to have an internal hom rq, rsrequires each fibre of EM {´
to be closed with respect to the monoidal structure. In cases of particular interest, E will be
240
locally Cartesian closed, and restricting EM {´to the self-indexing E{´gives fibres which are thus
Cartesian closed. In these cases, we can think of the broader fibres ofEM {´, and thus PolyM itself,
as being ‘deterministically’ closed. This means, for the stochastic examplePolyP for P a probability
monad, we get an internal hom satisfying the adjunctionPolyPpp bq, rq– PolyPpp, rq, rsqonly
when the backwards components of morphisms p bq Ñr are ‘uncorrelated’ between p and q.
Remark 6.2.21. For PolyM to behave faithfully like the category Poly of polynomial functors of
sets and their morphisms, we should want the substitution functors EM {f : EM {C ÑEM {B to
have left and right adjoints (corresponding respectively to dependent sum and product). Although
we do not spell it out here, it is quite straightforward to exhibit concretely the left adjoints. On
the other hand, writing f˚ as shorthand for EM {f, we can see that a right adjoint only obtains
in restricted circumstances. Denote the putative right adjoint by Πf : EM {B ÑEM {C, and for
ιp : EÑ‚B suppose that pΠf Eqrysis given by the set of ‘partial sections’ σ : f´1tyuÑ ME of p
over f´1tyuas in the commutative diagram:
f´1tyu tyu
ME B C
f
{
ηB˚p
σ
Then we would need to exhibit a natural isomorphism EM {Bpf˚D, Eq– EM {CpD, Πf Eq. But
this will only obtain when the ‘backwards’ components h7
y : DrysÑ MpΠf Eqrysare in the image
of ι—otherwise, it is not generally possible to pull f´1tyuout of M.
Despite these restrictions, we do have enough structure at hand to instantiateCoalgT in PolyM .
The only piece remaining is the composition product ◁, but for our purposes it suffices to define
its action on objects, which is identical to its action on objects in Poly1, and then to consider
◁-comonoids in PolyM . The comonoid laws force the structure maps to be deterministic (i.e., in
the image of ι), and so ◁-comonoids in PolyM are just ◁-comonoids in PolyidSet.
Finally, we note that, even if the internal hom r´, ´sis not available in general, we can define
morphisms β : SyS ÑrTy, ps: these again just correspond to morphisms Ty bSyS Ñp, and the
condition that the backwards maps be uncorrelated between Ty and p is incontrovertibly satisfied
because Ty has a trivial exponent. Unwinding such aβ according to the definition ofPolyM indeed
gives precisely a pair pβo, βuqof the requisite types; and a comonoid homomorphismSyS ÑyT in
PolyM is precisely a functor BT ÑKℓpMq, thereby establishing equivalence between the objects
of CoalgTppqestablished in PolyM and the objects of CoalgT
Cppq.
1We leave the full exposition of ◁ in PolyM to future work.
241
Henceforth, therefore, we will write CoalgT
M to denote the instantiation of CoalgT in PolyM .
We will call the objects of CoalgT
M ppqpM-coalgebras with time T, and to get a sense of how, in
the case where M is a probability monad, they provide a notion of open Markov process, we can
read off the definition a little more explictly.
Proposition 6.2.22. A pM-coalgebra with time T consists of a triple ϑ :“pS, ϑo, ϑuqof a state
space S : E and two morphisms ϑo : T ˆS Ñpp1qand ϑu : ř
t:T
ř
s:S prϑopt, sqsÑ MS , such
that, for any section σ : pp1qÑ ř
i:pp1qprisof p in E, the maps ϑσ : T ˆS ÑMS given by
ÿ
t:T
S
ϑoptq˚σ
Ý ÝÝÝÝ Ñ
ÿ
t:T
ÿ
s:S
prϑopt, sqs ϑu
ÝÑMS
constitute an object in the functor category Cat
`
BT, KℓpMq
˘
, where BT is the delooping of T
and KℓpMqis the Kleisli category of M. (Once more, we call the closed system ϑσ, induced by a
section σ of p, the closure of ϑ by σ.)
Following Example 6.1.6 and the intuition of Example 6.2.9, we can see how this produces an
open version of a Markov process.
Since stochastic dynamical systems are often alternatively presented as random dynamical
systems, we now briefly consider how these can be incorporated into the coalgebraic framework.
6.2.3. Open random dynamical systems
In the analysis of stochastic systems, it is often fruitful to consider two perspectives: on one side,
one considers explicitly the evolution of the distribution of the states of the system, by following
(for instance) a Markov process, or Fokker-Planck equation. On the other side, one considers the
system as if it were a deterministic system, perturbed by noisy inputs, giving rise to the frameworks
of stochastic differential equations and associated random dynamical systems .
Whereas a (closed) Markov process is typically given by the action of a time monoid on an
object in a Kleisli category of a probability monad, a (closed) random dynamical system is given
by a bundle of closed dynamical systems, where the base system is equipped with a probability
measure which it preserves: the idea being that a random dynamical system can be thought of as
a ‘random’ choice of dynamical system on the total space at each moment in time, with the base
measure-preserving system being the source of the randomness [13].
This idea corresponds in non-dynamical settings to the notion of randomness pushback [109,
Def. 11.19], by which a stochastic map f : A ÑPB can be presented as a deterministic map
f5 : Ω ˆA ÑB where pΩ, ωqis a measure space such that, for any a : A, pushing ω forward
242
through f5p-, aqgives the state fpaq; that is, ω induces a random choice of map f5pω, -q: A ÑB.
Similarly, under nice conditions, random dynamical systems and Markov processes do coincide,
although they have different suitability in applications.
In this section, we sketch how the generalized-coalgebraic structures developed above extend also
to random dynamical systems. We begin by defining the concept of measure-preserving dynamical
system, which itself requires the notion of measure space (in order that measure can be preserved);
we define the corresponding category abstractly, using a notion of slice category dual to that of
Definition 3.2.10.
Definition 6.2.23. Suppose X is an object of a category E. The slice of E under X, denoted X{E,
is the category whose objects pA, iqare morphisms X iÝ ÑA out of X in E, and whose morphisms
f : pA, iqÑp B, jqare the evident triangles:
X
A B
f
i j
There is a projection functor F : X{E ÑE mapping each object pA, iqto A and each morphism
f : pA, iqÑp B, jqto f : A ÑB.
We can use this notion to define a notion of ‘pointed’ category.
Definition 6.2.24. Let pC, b, Iqbe a monoidal category, D be a subcategory D ãÑC, and let F
denote the projection I{C ÑC. We define the category D˚to be the pullback category over the
diagram D ãÑC FÐ ÝI{C.
The category D˚has objects ‘pointed’ by corresponding states in C, and its morphisms are those
that preserve these states. The category of measure spaces is obtained as an example accordingly.
Example 6.2.25. Consider the deterministic subcategory Meas ãÑsfKrn. The pointed category
Meas˚obtained from Definition 6.2.24 is the category whose objects are measure spaces pM, µq
with µ a measure 1Ñ‚M, and whose morphisms f : pM, µq Ñ pN, νqare measure-preserving
maps; i.e., measurable functions f : M ÑN such that ν “f ‚µ in sfKrn. Likewise, if P is a
probability monad on E, then we have E ãÑKℓpPqand hence can understand E˚as a category of
abstract measure spaces.
Proposition 6.2.26. There is a projection functor U : E˚ Ñ E taking measure spaces pB, βq
to the underlying spaces B and their morphisms f : pA, αq Ñ pB, βqto the underlying maps
243
f : A ÑPB. We will write B to refer to the space in E underlying a measure space pB, βq, in the
image of U.
Proof. The functor is obtained as the projection induced by the universal property of the pullback.
Definition 6.2.27. Let pB, βqbe a measure space in E ãÑKℓpPq. A closed metric or measure-
preserving dynamical system pϑ, βqon pB, βqwith time T is a closed dynamical system ϑ with
state space B : E such that, for allt : T, Pϑptq˝β “β; that is, each ϑptqis a pB, βq-endomorphism
in 1{KℓpPq.
Proposition 6.2.28. Closed measure-preserving dynamical systems in E with time T form the
objects of a category CatpBT, E˚qwhose morphisms f : pϑ, αqÑp ψ, βqare maps f : ϑp˚qÑ
ψp˚qin E between the state spaces that preserve both flow and measure, as in the following
commutative diagram, which also indicates their composition:
Pϑp˚q Pϑp˚q
1 Pψp˚q Pψp˚q 1
Pλp˚q Pλp˚q
α
β
γ
α
β
γ
Pϑptq
Pψptq
Pλptq
Pf Pf
Pg Pg
Proof. The identity morphism on a closed measure-preserving dynamical system is the identity
map on its state space. It is easy to check that composition as in the diagram above is thus both
associative and unital with respect to these identities.
As we indicated in the introduction to this section, closed random dynamical systems are bundles
of deterministic systems over metric systems.
Definition 6.2.29. Let pϑ, βqbe a closed measure-preserving dynamical system. A closed random
dynamical system over pϑ, βqis an object of the slice category CatpBT, Eq{ϑ; it is therefore a
bundle of the corresponding functors.
Example 6.2.30. The solutions Xpt, ω; x0q : R` ˆΩ ˆM Ñ M to a stochastic differential
equation dXt “fpt, Xtqdt `σpt, XtqdWt, where W : R`ˆΩ ÑM is a Wiener process in M,
244
define a random dynamical system R`ˆΩ ˆM ÑM : pt, ω, xqÞÑ Xpt, ω; x0qover the Wiener
base flow θ : R`ˆΩ ÑΩ : pt, ωqÞÑ Wps `t, ωq´ Wpt, ωqfor any s : R`.
We can use the same trick, of opening up closed systems along a polynomial interface, to define
a notion of open random dynamical system — although at this point we do not have an elegant
concise definition.
Definition 6.2.31. Let pθ, βqbe a closed measure-preserving dynamical system in E with time
T, and let p : PolyidE be a polynomial in E. Write Ω :“ θp˚qfor the state space of θ, and let
π : S ÑΩ be an object (bundle) in E{Ω. An open random dynamical system over pθ, βqon the
interface p with state space π : S ÑΩ and time T consists in a pair of morphismsϑo : TˆS Ñpp1q
and ϑu : ř
t:T
ř
s:S
prϑopt, sqs ÑS, such that, for any section σ : pp1q Ñ ř
i:pp1q
prisof p, the maps
ϑσ : T ˆS ÑS defined as
ÿ
t:T
S
ϑop´q˚σ
ÝÝÝÝÝÑ
ÿ
t:T
ÿ
s:S
prϑop´, sqs ϑu
ÝÑS
form a closed random dynamical system in CatpBT, Eq{θ, in the sense that, for all t : T and
sections σ, the following diagram commutes:
S ř
s:S
prϑopt, sqs S
Ω Ω
π π
θptq
ϑoptq˚σ ϑuptq
Proposition 6.2.32. Let pθ, βqbe a closed measure-preserving dynamical system in E with time
T, and let p : PolyidE be a polynomial in E. Open random dynamical systems over pθ, βqon
the interface p form the objects of a category RDynTpp, θq. Writing ϑ :“ pπX, ϑo, ϑuqand
ψ :“pπY , ψo, ψuq, a morphism f : ϑ Ñψ is a map f : X ÑY in E making the following diagram
commute for all times t : T and sections σ of p:
X ř
x:X
prϑopt, xqs X
Ω Ω
Y ř
y:Y
prψopt, yqs Y
πX πX
θptq
ϑoptq˚σ ϑuptq
ψoptq˚σ ψuptq
πY πY
f f
245
Identities are given by the identity maps on state-spaces. Composition is given by pasting of
diagrams.
Proposition 6.2.33. The categories RDynTpp, θqcollect into a doubly-indexed category of the
form RDynT : PolyidE ˆCatpBT, E˚qÑ Cat. By the universal property of the product ˆin
Cat, it suffices to define the actions of RDynT separately on morphisms of polynomials and on
morphisms of closed measure-preserving systems.
Suppose therefore that φ : p Ñ q is a morphism of polynomials. Then, for each measure-
preserving system pθ, βq: CatpBT, E˚q, we define the functor RDynTpφ, θq: RDynTpp, θqÑ
RDynTpq, θqas follows. Let ϑ :“ pπX : X Ñ Ω, ϑo, ϑuq : RDynTpp, θqbe an object (open
random dynamical system) in RDynTpp, θq. Then RDynTpφ, θqpϑqis defined as the triple
pπX, φ1 ˝ϑo, ϑu ˝φo˚φ7q : RDynTpq, θq, where the two maps are explicitly the following
composites:
T ˆX ϑo
ÝÑpp1q
φ1
ÝÑqp1q,
ÿ
t:T
ÿ
x:X
qrφ1 ˝ϑopt, xqs
ϑo˚φ7
Ý ÝÝÝ Ñ
ÿ
t:T
ÿ
x:X
prϑopt, xqs ϑu
ÝÑX .
On morphisms f : pπX : X ÑΩ, ϑo, ϑuqÑp πY : Y ÑΩ, ψo, ψuq, the image RDynTpφ, θqpfq:
RDynTpφ, θqpπX, ϑo, ϑuqÑ RDynTpφ, θqpπY , ψo, ψuqis given by the same underlying map
f : X ÑY of state spaces.
Next, suppose that ϕ : pθ, βq Ñ pθ1, β1q is a morphism of closed measure-preserving
dynamical systems, and let Ω1 :“ θ1p˚qbe the state space of the system θ1. By Proposition
6.2.28, the morphism ϕ corresponds to a map ϕ : Ω Ñ Ω1 on the state spaces that preserves
both flow and measure. Therefore, for each polynomial p : PolyidE , we define the functor
RDynTpp, ϕq : RDynTpp, θq ÑRDynTpp, θ1qby post-composition. That is, suppose given
open random dynamical systems and morphisms over pp, θqas in the diagram of Proposition 6.2.32.
Then RDynTpp, ϕqreturns the following diagram:
X ř
x:X
prϑopt, xqs X
Ω1 Ω1
Y ř
y:Y
prψopt, yqs Y
θ1ptq
ϑoptq˚σ ϑuptq
ψoptq˚σ ψuptq
f f
ϕ˝πY
ϕ˝πX
ϕ˝πY
ϕ˝πX
246
That is, RDynTpp, ϕqpϑq :“ pϕ ˝πX, ϑo, ϑuq and RDynTpp, ϕqpfq is given by the same
underlying map f : X ÑY on state spaces.
6.3. Cilia: monoidal bicategories of cybernetic systems
Whereas it is the morphisms (1-cells) of process-theoretic categories—such as categories of lenses,
or the categories of statistical games to be defined in Chapter 7—that represent open systems, it is
the objects (0-cells) of the opindexed categories CoalgT
M
2 that play this rôle; in fact, the objects of
CoalgT
M each represent both an open system and its (polynomial) interface. In order to supply
dynamical semantics for statistical games—functors from categories of statistical games to categories
of dynamical systems—we need to cleave the dynamical systems from their interfaces, making the
interfaces into 0-cells and systems into 1-cells between them, thereby letting the systems’ types
and composition match those of the games. Doing this is the job of this section, which we first
perform in the case of the general categories CoalgT
M , followed by the specific case of systems
generated differentially, as in the vector-field Examples 6.2.8 and 6.2.9.
6.3.1. Hierarchical bidirectional dynamical systems
To construct “hierarchical bidirectional systems”, we will associate to each pair of objectspA, Sq
and pB, Tqof a category of (for our purposes, Bayesian) lenses a polynomial vAyS, ByT wwhose
configurations correspond to lenses and whose inputs correspond to the lenses’ inputs. The
categories CoalgT
P
`
vAyS, ByT w
˘
will then form the hom-categories of bicategories of hierarchical
inference systems called cilia3, and it is in these bicategories that we will find our dynamical
semantics.
Throughout this subsection, we will fix a category C of stochastic channels, defined by C :“
KℓpPqas the Kleisli category of a probability monad P : E ÑE, which we will also take to define
a category PolyP of polynomials with stochastic feedback. We will assume P to be a monoidal
monad, and we will write the monoidal structure on C as pb, Iq. Finally, we will assume that C is
enriched in its underlying category of spaces E.
Definition 6.3.1. Let BayesLens be the category of Bayesian lenses in C. Then for any pair of
objects pA, Sqand pB, Tqin BayesLens, we define a polynomial vAyS, ByT win PolyP by
vAyS, ByT w:“
ÿ
l:BayesLens
`
pA,Sq,pB,T q
˘yCpI,AqˆT .
2or, more precisely, their corresponding opfibrations
ş
CoalgT
M
3‘Cilia’, because they “control optics”, like the ciliary muscles of the eye.
247
Remark 6.3.2. We can think of vAyS, ByT was an ‘external hom’ polynomial for BayesLens,
playing a rôle analogous to the internal hom rp, qsin PolyP. Its ‘bifunctorial’ structure—with
domain and codomain parts—is what enables cleaving systems from their interfaces, which are
given by these parts. The definition, and the following construction of the monoidal bicategory, are
inspired by the operad Org introduced by Spivak [240].
Remark 6.3.3. Note that vAyS, ByT wis strictly speaking a monomial, since it can be written
in the form IyJ for I “BayesLens
`
pA, Sq, pB, Tq
˘
and J “CpI, Aqˆ T. However, we have
written it in polynomial form with the view to extending it in future work to dependent lenses and
dependent optics [43, 276] and these generalized external homs will in fact be true polynomials.
Proposition 6.3.4. Definition 6.3.1 defines a functor BayesLensop ˆBayesLens ÑPolyP.
Suppose c :“pc1, c7q: pZ, Rq ÞÑpA, Sqand d :“pd1, d7q: pB, Tq ÞÑpC, Uqare Bayesian lenses.
We obtain a morphism of polynomials vc, dw: vAyS, ByT wÑv ZyR, CyU was follows. Since the
configurations of vAyS, ByT ware lenses pA, Sq ÞÑ pB, Tq, the forwards map acts by pre- and
post-composition:
vc, dw1 :“d  p´q c : BayesLens
`
pA, Sq, pB, Tq
˘
ÑBayesLens
`
pZ, Rq, pC, Uq
˘
l ÞÑd  l  c
For each such l, the backwards map vc, dw7
l has type CpI, Zqb U ÑCpI, Aqb T in C, and is
obtained by analogy with the backwards composition rule for Bayesian lenses. We define
vc, dw7
l :“CpI, Zqb U
c1˚bU
ÝÝÝÝÑCpI, Aqb U
bU
ÝÝÝÑCpI, Aqb CpI, Aqb U ¨¨¨
¨¨¨
CpI,Aqbl1˚bU
ÝÝÝÝÝÝÝÝÝÑCpI, Aqb CpI, Bqb U
CpI,Aqbd7bU
ÝÝÝÝÝÝÝÝÑCpI, Aqb CpU, Tqb U ¨¨¨
¨¨¨
CpI,AqbevU,T
ÝÝÝÝÝÝÝÝÑCpI, Aqb T
where l1 is the forwards part of the lensl : pA, Sq ÞÑpB, Tq, andc1˚ :“CpI, c1qand l1˚ :“CpI, l1q
are the push-forwards along c1 and l1, and evU,T is the evaluation map induced by the enrichment
of C in E.
Less abstractly, with C “KℓpPq, we can write vc, dw7
l as the following map in E, depicted as a
248
string diagram:
vc, dw7
l “
c1˚
l1˚
d5
PZ
U
PT
PA
str
Here, we have assumed that KℓpPqpI, Aq“ PA, and define d5 : PB ˆU ÑPT to be the image
of d7 : PB ÑKℓpPqpU, Tqunder the Cartesian closure of E, and str : PA ˆPT ÑP
`
PA ˆTq
the (left) strength of the monoidal monad P.
The morphism vc, dwl acts to ‘wrap’ the lensl by pre-composing with c and post-composing with
d. The backwards component vc, dw7
l therefore acts to take the inputs of the resulting composite
d  l  c to appropriate inputs for l; that is, it maps a pair pπ, uqto pc1 ‚π, d7
l1‚c1‚πpuqq.
Proof. We need to check that the mappings defined above respect identities and composition. It is
easy to see that the definition preserves identities: in the forwards direction, this follows from the
unitality of composition in BayesLens; in the backwards direction, because pushing forwards
along the identity is again the identity, and because the backwards component of the identity
Bayesian lens is the constant state-dependent morphism on the identity in C.
To check that the mapping preserves composition, we consider the contravariant and covariant
parts separately. Suppose b :“pb1, b7q: pY, Qq ÞÑpZ, Rqand e :“pe1, e7q: pC, Uq ÞÑpD, Vq
are Bayesian lenses. We consider the contravariant case first: we check that vc  b, ByT w “
vb, ByT w˝v c, ByT w. The forwards direction holds by pre-composition of lenses. In the backwards
direction, we note from the definition that only the forwards channel c1 plays a rôle in vc, ByT w7
l ,
and that rôle is again pre-composition. We therefore only need to check thatpc1 ‚b1q˚ “c1˚˝b1˚,
and this follows immediately from the functoriality of CpI, ´q.
We now consider the covariant case, that vAyS, e dw“v AyS, ew˝v AyS, dw. Once again, the
forwards direction holds by composition of lenses. For simplicity of exposition, we consider the
backwards direction (with C “KℓpPq) and reason graphically. In this case, the backwards map on
249
the right-hand side is given, for a lens l : pA, Sq ÞÑpB, Tqby the following string diagram:
l1˚
e5
PA
V
d5˚d1˚
str
PU
PA
It is easy to verify that the composition of backwards channels here is precisely the backwards
channel given by e  d—see Theorem 4.3.14—which establishes the result.
Remark 6.3.5. Above, we claimed that a monoidal monad P : E ÑE on a symmetric monoidal
category pE, ˆ, 1qis equipped with a (left) strength strX,Y : X ˆPY ÑP
`
X ˆY q, in the sense
of Definition 3.2.4. This can be obtained from the unit η and the laxator α of the monad as follows:
strX,Y : X ˆPY
ηXˆidPY
Ý ÝÝÝÝÝ ÑPX ˆPY
αX,Y
ÝÝÝÑPpX ˆY q
Using the monad laws, a strength obtained in this way can be shown to satisfy the following axioms
(that the strength commutes with the monad structure), and so one may say that P is a strong
monad:
A ˆB
A ˆPB PpA ˆBq
idA ˆηB
strA,B
ηAˆB
A ˆPP B PpA ˆPBq PP pA ˆBq
A ˆPB PpA ˆBq
AˆµB
strA,PB PpstrA,Bq
µAˆB
strA,B
Now that we have an ‘external hom’, we might expect also to have a corresponding ‘external
composition’, represented by a family of morphisms of polynomials; we establish such a family
now, and it will be important in our bicategorical construction.
Definition 6.3.6. We define an ‘external composition’ natural transformation c, with components
vAyS, ByT wbv ByT , CyU wÑv AyS, CyU w
250
given in the forwards direction by composition of Bayesian lenses. In the backwards direction, for
each pair of lenses c : pA, Sq ÞÑpB, Tqand d : pB, Tq ÞÑpC, Uq, we need a map
c7
c,d : CpI, Aqb U ÑCpI, Aqb T bCpI, Bqb U
˘
which we define as follows:
c7
c,d :“CpI, Aqb U
b
Ý ÝÝÝ ÑCpI, Aqb CpI, Aqb U bU ¨¨¨
¨¨¨
CpI,Aqbc1˚bUbU
ÝÝÝÝÝÝÝÝÝÝÝÑCpI, Aqb CpI, Bqb U bU ¨¨¨
¨¨¨
CpI,Aqb bCpI,BqbUbU
Ý ÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝ ÑCpI, Aqb CpI, Bqb CpI, Bqb U bU
¨¨¨
CpI,AqbCpI,Bqbd7bUbU
Ý ÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝ ÑCpI, Aqb CpI, Bqb CpU, Tqb Y bU
¨¨¨
CpI,AqbCpI,BqevU,T bU
Ý ÝÝÝÝÝÝÝÝÝÝÝÝÝÝ ÑCpI, Aqb CpI, Bqb T bU
¨¨¨
CpI,AqbswapbU
Ý ÝÝÝÝÝÝÝÝÝ ÑCpI, Aqb T bCpI, Bqb U
where c1˚and evU,T are as in 6.3.4.
With C “KℓpPq, we can equivalently (and more legibly) define c7
c,d by the following string
diagram:
c7
c,d :“
d5
c1˚
str
PA
PT
PB
U
PA
U
where d5and str are also as in Proposition 6.3.4.
We can therefore understand c7
c,d as mapping forward and backward inputs for the composite
lens d  c to appropriate inputs for the constituent lenses c and d; that is, c7
c,d maps pπ, uqto
pπ, d7
c1‚πpuq, c1 ‚π, uq. The resulting inputs to the lens c are therefore pπ, d7
c1‚πpuqq, and those to
d are pc1 ‚π, uq. (This is precisely as the law of lens composition stipulates: the forwards input to
d is obtained by pushing forwards through d; and the backwards input to c is obtained from the
backwards component of d.)
251
We leave to the reader the detailed proof that this definition produces a well-defined natural
transformation, noting only that the argument is analogous to that of Proposition 6.3.4: one observes
that, in the forwards direction, the definition is simply composition of Bayesian lenses (which is
immediately natural); in the backwards direction, one observes that the definition again mirrors
that of the backwards composition of Bayesian lenses.
Next, we establish the structure needed to make our bicategory monoidal.
Definition 6.3.7. We define a distributive law d of v´, “wover b, a natural transformation with
components
vAyS, ByT wbv A1yS1
, B1yT1
wÑv AyS bA1yS1
, ByT bB1yT1
w,
noting that AyS bA1yS1
“pA bA1qypSbS1qand ByT bB1yT1
“pB bB1qypTbT1q. The forwards
component is given simply by taking the tensor of the corresponding Bayesian lenses, using the
monoidal product (also denotedb) in BayesLens. Backwards, for each pair of lensesc : pA, Sq ÞÑ
pB, Tqand c1 : pA1, S1q ÞÑpB1, T1q, we need a map
d7
c,c1 : CpI, AbA1qb T bT1 ÑCpI, Aqˆ T ˆCpI, A1qˆ T1
for which we choose
CpI, AbA1qb T bT1 bTbT1
Ý ÝÝÝÝÝ ÑCpI, AbA1qb CpI, AbA1qb T bT1¨¨¨
¨¨¨
CpI,projAqbCpI,projA1qbTbT1
ÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÑCpI, Aqb CpI, A1qb T bT1¨¨¨
¨¨¨
CpI,AqbswapbT1
ÝÝÝÝÝÝÝÝÝÝÑCpI, Aqb T bCpI, A1qb T1
where swap is the symmetry of the tensor bin C. Note that d7
c,c1 so defined does not in fact depend
on either c or c1.
We now have everything we need to construct a monoidal bicategory CiliaT
P of dynamical
hierarchical inference systems in C, following the intuition outlined at the beginning of this section.
We call systems over such external hom polynomials cilia, as they “control optics”, akin to the
ciliary muscles of the eye. In future work, we will study the general structure of these categories
and their relationship to categorical systems theory [191, 192] and related work in categorical
cybernetics [51].
Definition 6.3.8. Let CiliaT
P denote the monoidal bicategory whose 0-cells are objects
pA, Sq in BayesLens, and whose hom-categories CiliaT
P
`
pA, Sq, pB, Tq
˘
are given by
252
CoalgT
P
`
vAyS, ByT w
˘
. The identity 1-cell idpA,Sq : pA, Sq Ñ pA, Sqon pA, Sqis given by
the system with trivial state space 1, trivial update map, and output map that constantly emits
the identity Bayesian lens pA, Sq ÞÑpA, Sq. The composition of a system pA, SqÑp B, Tqthen a
system pB, TqÑp C, Uqis defined by the functor
CiliaT
P
`
pA, Sq, pB, Tq
˘
ˆCiliaT
P
`
pB, Tq, pC, Uq
˘
“CoalgT
P
`
vAyS, ByT w
˘
ˆCoalgT
P
`
vByT , CyU w
˘
λÝ ÑCoalgT
P
`
vAyS, ByT wbv ByT , CyU w
˘
CoalgT
Ppcq
Ý ÝÝÝÝÝÝ ÑCoalgT
P
`
vAyS, CyU w
˘
“CiliaT
P
`
pA, Sq, pC, Uq
˘
where λ is the laxator and c is the external composition morphism of Definition 6.3.6.
The monoidal structurepb, yqon CiliaT
P derives from the structures onPolyP and BayesLens,
justifying our overloaded notation. On 0-cells, pA, Sqbp A1, S1q:“pA bA1, SbS1q. On 1-cells
pA, SqÑp B, Tqand pA1, S1qÑp B1, T1q, the tensor is given by
CiliaT
P
`
pA, Sq, pB, Tq
˘
ˆCiliaT
P
`
pA1, S1q, pB1, T1q
˘
“CoalgT
P
`
vAyS, ByT w
˘
ˆCoalgT
P
`
vA1yS1
, B1yT1
w
˘
λÝ ÑCoalgT
P
`
vAyS, ByT wbv A1yS1
, B1yT1
w
˘
CoalgT
Ppdq
Ý ÝÝÝÝÝÝ ÑCoalgT
P
`
vAyS bA1yS1
, ByT bB1yT1
w
˘
“CiliaT
P
`
pA, Sqbp A1, S1q, pB, Tqbp B1, T1q
˘
where d is the distributive law of Definition 6.3.7. The same functors
CiliaT
P
`
pA, Sq, pB, Tq
˘
ˆCiliaT
P
`
pA1, S1q, pB1, T1q
˘
ÑCiliaT
P
`
pA, SqbpA1, S1q, pB, TqbpB1, T1q
˘
induce the tensor of 2-cells; concretely, this is given on morphisms of dynamical systems by taking
the product of the corresponding morphisms between state spaces.
We do not give here a proof that this makes CiliaT
P into a well-defined monoidal bicategory;
briefly, the result follows from the facts that the external composition c and the tensor bare
appropriately associative and unital, that CoalgT
P is lax monoidal, that v´, “wis functorial in both
positions, and that v´, “wdistributes naturally over b.
Before we move on, it will be useful to spell out concretely the elements of a ‘cilium’ (a 1-cell)
pA, SqÑp B, Tqin CiliaT
P.
253
Proposition 6.3.9. Suppose P is a monad on a Cartesian closed category E. Then a 1-cell ϑ :
pA, SqÑp B, Tqin CiliaT
P is given by a tuple ϑ :“pX, ϑo
1, ϑo
2, ϑuqof
• a choice of state space X,
• a forwards output map ϑo
1 : T ˆX ˆA ÑPB in E,
• a backwards output map ϑo
2 : T ˆX ˆPA ˆT ÑPS in E, and
• an update map ϑu : T ˆX ˆPA ˆT ÑPX in E,
satisfying the ‘flow’ condition of Proposition 6.2.22.
Proof. The result follows immediately upon unpacking the definitions, using the Cartesian closure
of E.
6.3.2. Differential systems
Approximate inference doctrines describe how systems play statistical games, and are particularly
of interest when one asks how systems’ performance may improve during such game-playing. One
prominent method of performance improvement involves descending the gradient of a statistical
game’s loss function, and we will see below that this method is adopted by both the Laplace and
the Hebb-Laplace doctrines. The appearance of gradient descent prompts questions about the
connections between such statistical systems and other ‘cybernetic’ systems such as deep learners or
players of economic games, both of which may also involve gradient descent [51, 74]; indeed, it has
been proposed [54] that parameterized gradient descent should form the basis of a compositional
account of cybernetic systems in general4.
In order to incorporate gradient descent explicitly into our own compositional framework,
we follow the recipes above to define here first a category of differential systems opindexed by
polynomial interfaces and then a monoidal bicategory of differential hierarchical inference systems.
We then show how we can obtain dynamical from differential systems by integration, and sketch
how this induces a “change of base” from dynamical to differential hierarchical inference systems.
4Our own view on cybernetics is somewhat more general, since not all systems that may be seen as cybernetic are
explicitly structured as gradient-descenders, and nor even is explicit differential structure always apparent. In earlier
work, we suggested that statistical inference was perhaps more inherent to cybernetics [251], although today we
believe that a better, though more informal, definition of cybernetic system is perhaps “an intentionally-controlled
open dynamical system”. (Slightly more formally, we can understand this as “an open dynamical system clad in a
controller”, with the possible ‘cladding’ collected into a fibration over systems of each given type.) Nonetheless, we
acknowledge that this notion of “intentional control” may generally be reducible to a stationary action principle,
again indicating the importance of differential structure. We leave the statement and proof of this general principle
to future work.
254
Differential systems require differential structure, but we are here still concerned with statistical
systems whose time evolution is stochastic. This means that a differential system will be given by
a stochastic vector field : a stochastic section of the tangent bundle over the system’s state space.
However, as we have seen, the state spaces of stochastic systems are naturally found in a category
of measurable spaces, but such a categorical setting does not generally supply differential structure
too, and without this we cannot define tangent bundles. This poses our first hurdle.
We will not here entirely vault this hurdle, for the interplay of randomness and smoothness is
subtle and untangling it is not our purpose in this thesis. However, we can overcome it in a manner
which is satisfactory for our present needs, by noting that all our state spaces of later interest will
be Euclidean, meaning that we can equip them with their standard Borel measurable structure. In
future work, we hope to generalize this situation, possibly using the notion of relative monad [8].
Definition 6.3.10. Let Euc denote the category whose objects are finite-dimensional Euclidean
spaces Rn and whose morphisms are smooth maps between them.
Euclidean spaces are trivially manifolds: the tangent space over each point x P Rn is again
Rn. Hence, if X is a Euclidean space, then the tangent bundle TX ÑX is simply the projection
X ˆX ÑX mapping px, vqto x. As in general differential geometry, T yields a functor Euc Ñ
Euc.
Proposition 6.3.11. The tangent bundle functor T : Euc ÑEuc maps each Euclidean space Rn
to Rn ˆRn and each smooth map f : Rm ÑRn to its differential d f: Rm ˆRm ÑRn ˆRn,
which in turn maps px, vqto
`
fpxq, Bxfpvq
˘
, where Bxf denotes the (total) derivative of f at x,
which can be represented by its n ˆm Jacobian matrix.
Remark 6.3.12. Differentials compose by pushforward, which yields the chain rule of differential
calculus. Earlier we have seen that chain rules indicate the presence of a fibration, and indeed this
is also the case here: T is properly a functor into the fibration of vector bundles over the category
of spaces; composing this functor with the projection out of the fibration yields the endofunctor
we have sketched in the preceding proposition.
Ordinary differential equations define vector fields, which are (deterministic) sections of the
tangent bundle over a space; these are deterministic closed differential systems. We are interested
in open differential systems that may have effectful (e.g. stochastic) evolution: for openness, we will
use the trick of §6.2; for stochasticity, we will need stochastic sections , which means transporting
the tangent bundles into a category of stochastic maps and considering their sections there.
255
Proposition 6.3.13. There is a functor J : Euc ÑMeas that takes each Euclidean space and
exhibits it as a measurable space equipped with its standard Borel σ-algebra, and which takes each
smooth map and exhibits it as a measurable function. This functor preserves products.
Proposition 6.3.14 (Heunen et al. [131, §III.B]). There is a functor R : Meas ÑQBS which is
full and faithful when restricted to the subcategory Borel ãÑMeas of standard Borel spaces.
Using these functors, we can transport a tangent bundle πX : TX ÑX in Euc to QBS, as
RJπX. Then, if we let P : QBS ÑQBS denote the probability monad on quasi-Borel spaces
introduced in Example 4.1.31, we can take the sections of RJπX in KℓpPqto be the stochastic
vector fields over the space X. Moreover, since QBS is finitely complete and Cartesian closed, it is
sufficiently structured that we may instantiate the categoryPolyP of polynomials with P-effectful
feedback.
Using these two ideas, we may define our desired categories of stochastic differential systems.
Recall that morphisms AyB Ñp in PolyP correspond to morphisms AÑ‚pB in KℓpPq.
Notation 6.3.15. In this section, let us write pr´qto denote the functor RJ : Euc ÑQBS.
Definition 6.3.16. For each p : PolyP, define a category DiffSysppqas follows. Its objects are
pairs pM, mqof a Euclidean space M : Euc and a morphism m : ĂMy ĄTM Ñp of polynomials in
PolyP, such that for any section σ : p Ñy of p, the composite morphism σ ˝m : ĂMy ĄTM Ñy
corresponds to a stochastic section mσ : ĂMÑ‚ ĄTM of the tangent bundle TM ÑM under RJ.
A morphism α : pM, mqÑp M1, m1qin DiffSysppqis a smooth map α : M ÑM1in Euc such
that the following diagram commutes:
ĂM p ĄTM
ĂM1 pĆTM1
m
rα
m1
p ĂTα
We obtain a monoidal opindexed category from this data in much the same way as we did for
CoalgT.
Proposition 6.3.17. DiffSys defines an opindexed category PolyP ÑCat. Given a morphism
φ : p Ñ q of polynomials, DiffSyspφq : DiffSysppq Ñ DiffSyspqq acts on objects by
postcomposition and trivially on morphisms.
Proposition 6.3.18. The functor DiffSys is lax monoidal pPolyP, b, yqÑp Cat, ˆ, 1q.
256
Proof sketch. Note that T is strong monoidal, with TpR0q– R0 and TpMqˆ TpNq– TpM ˆNq,
that RJ preserves products, and that RJpR0q“ 1. The unitor 1 ÑDiffSyspyqis given by the
isomorphism ĂR0y
ĆTR0
– 1y1 – y induced by the strong monoidal structure of T. The laxator
λp,q : DiffSysppqˆ DiffSyspqqÑ DiffSyspp bqqis similarly determined: given objects m :
ĂMy ĄTM Ñ p and n : rNy ĄTN Ñ q, take their tensor m bn : pĂM b rNqy ĄTMbĄTN Ñ p bq
and precompose with the induced morphism p ČM ˆNqy
ČTpMˆNq Ñp ĂM b rNqy ĄTMbĄTM ; proceed
similarly on morphisms of differential systems. The satisfaction of the unitality and associativity
laws follows from the monoidality of T.
We now define a monoidal bicategory DiffCilia of differential hierarchical inference systems,
following the definition of Cilia above.
Definition 6.3.19. Let DiffCilia denote the monoidal bicategory whose 0-cells are the objects
pA, Sqof BayesLensKℓpPq and whose hom-categories DiffCilia
`
pA, Sq, pB, Tq
˘
are given by
DiffSys
`
vAyS, ByT w
˘
. The identity 1-cell idpA,Sq : pA, Sq Ñ pA, Sqon pA, Sqis given by
the differential system y ÑvAyS, ByT wwith state space R0, trivial backwards component, and
forwards component that picks the identity Bayesian lens onpA, Sq. The composition of differential
systems pA, SqÑp B, Tqthen pB, TqÑp C, Uqis defined by the functor
DiffCilia
`
pA, Sq, pB, Tq
˘
ˆDiffCilia
`
pB, Tq, pC, Uq
˘
“DiffSys
`
vAyS, ByT w
˘
ˆDiffSys
`
vByT , CyU w
˘
λÝ ÑDiffSys
`
vAyS, ByT wbv ByT , CyU w
˘
DiffSyspcq
ÝÝÝÝÝÝÑDiffSys
`
vAyS, CyU w
˘
“DiffCilia
`
pA, Sq, pC, Uq
˘
where λ is the laxator of Proposition 6.3.18 andc is the external composition morphism of Definition
6.3.6.
The monoidal structure pb, yqon DiffCilia is similarly defined following that of CiliaT
P. On
0-cells, pA, SqbpA1, S1q:“pAbA1, SbS1q. On 1-cells pA, SqÑp B, Tqand pA1, S1qÑp B1, T1q
(and their 2-cells), the tensor is given by the functors
DiffCilia
`
pA, Sq, pB, Tq
˘
ˆDiffCilia
`
pA1, S1q, pB1, T1q
˘
“DiffSys
`
vAyS, ByT w
˘
ˆDiffSys
`
vA1yS1
, B1yT1
w
˘
λÝ ÑDiffSys
`
vAyS, ByT wbv A1yS1
, B1yT1
w
˘
DiffSyspdq
Ý ÝÝÝÝÝÝ ÑDiffSysP
`
vAyS bA1yS1
, ByT bB1yT1
w
˘
“DiffCilia
`
pA, Sqbp A1, S1q, pB, Tqbp B1, T1q
˘
257
where d is the distributive law of Definition 6.3.7.
Following Proposition 6.3.9, we have the following characterization of a differential hierarchical
inference system pA, SqÑp B, Tqin KℓpPq.
Proposition 6.3.20. A 1-cell δ : pA, Sq Ñ pB, Tq in DiffCilia is given by a tuple δ :“
pX, δo
1, δo
2, δ7qof
• a choice of state space X : Euc;
• a forwards output map δo
1 : rX ˆA ÑPB,
• a backwards output map δo
2 : rX ˆPA ˆT ÑPS,
• a stochastic vector field δ7 : rX ˆPA ˆT ÑPĄTX.
At least for deterministic differential systems, we can obtain continuous-time dynamical systems
from differential systems by integration. We may then discretize these flows to give discrete-time
dynamical systems.
Proposition 6.3.21. For the purposes of this proposition, let P be the identity monad on a
finitely complete category E of manifolds, let pr´qbe the corresponding inclusion Euc ãÑ E,
and let DiffSys be instantiated accordingly. Then integration induces an indexed functor Flow :
DiffSys ÑCoalgR
P.
Proof. Suppose pM, mqis an object in DiffSysppq. The morphism m : ĂMy ĄTM Ñp consists of
functions m1 : ĂM Ñpp1qand m7 : ř
x:ĂM prm1pxqsÑ ĄTM. Since, for any section σ : p Ñy, the
induced map mσ : ĂM Ñ ĄTM is a vector field on a compact manifold, it generates a unique global
flow Flowppqpmqσ : R ˆĂM Ñ ĂM [167, Thm.s 12.9, 12.12], which factors as
ÿ
t:R
ĂM
m˚
1 σ
Ý ÝÝ Ñ
ÿ
t:R
ÿ
x:ĂM
prm1pxqs
Flowppqpmqu
ÝÝÝÝÝÝÝÑĂM .
We therefore define the system Flowppqpmqto have state space ĂM, output map m1 (for all t : R),
and update map Flowppqpmqu. Since Flowppqpmqσ is a flow for any section σ, it immediately
satisfies the monoid action condition. On morphisms α : m Ñ m1, we define Flowppqpαqby
the same underlying map on state spaces; this is again well-defined by the condition that α is
compatible with the tangent structure. Given a morphism φ : p Ñq of polynomials, both the
reindexing DiffSyspφqand CoalgR
Ppφqact by postcomposition, and so it is easy to see that
CoalgR
Ppφq˝ Flowppq– Flowpqq˝ DiffSysPpφqnaturally.
258
Remark 6.3.22. The question of integration of stochastic systems is more vexed and we will not
treat it in this thesis.
Not only may we integrate a differential system to obtain a continuous-time dynamical system,
we can also variously discretize the continuous-time system to obtain a discrete-time one.
Proposition 6.3.23. Any map f : T1 Ñ T of monoids induces an indexed functor (a natural
transformation) CoalgT
P ÑCoalgT1
P .
Proof. We first consider the induced functor CoalgT
Pppq ÑCoalgT1
P ppq, which we denote by
∆p
f . Note that we have a morphism rfy, ps: rTy, psÑr T1y, psof polynomials by substitution
(precomposition). A system β in CoalgT
P is a morphism SyS ÑrTy, psfor some S, and so we
define ∆p
f pβqto be rf, ps˝ β : SyS ÑrTy, psÑr T1y, ps. To see that this satisfies the monoid
action axiom, consider that the closure ∆p
f pβqσ for any section σ : p Ñy is given by
ÿ
t:T1
S
βopfptqq˚σ
Ý ÝÝÝÝÝÝ Ñ
ÿ
t:T1
ÿ
s:S
prβopfptq, sqs
βu
ÝÑS
which is an object in the functor category Cat
`
BT1, KℓpPq
˘
since f is a monoid homomorphism.
On morphisms of systems, the functor ∆p
f acts trivially.
To see that ∆f collects into an indexed functor, consider that it is defined on each polynomial p
by the contravariant action rf, psof the internal hom r´, “s, and that the reindexing CoalgTpφq
for any morphism φ of polynomials is similarly defined by the covariant action rTy, φs. By
the bifunctoriality of r´, “s, we have rT1y, φs˝r fy, ps “ rfy, φs “ rfy, qs˝r Ty, φs, and so
CoalgT1
P pφq˝ ∆p
f “∆q
f ˝CoalgT
P.
Corollary 6.3.24. For each k : R, the canonical inclusion ιk : N ãÑ R : i ÞÑ ki induces a
corresponding ‘discretization’ indexed functor Disck :“∆ι : CoalgR
P ÑCoalgN
P.
Remark 6.3.25. From Proposition 6.3.21 and Corollary 6.3.24 we obtain a family of composite
indexed functors DiffSys FlowÝ ÝÝ ÑCoalgR
P
Disck
ÝÝÝÑCoalgN
P taking each differential system to a
discrete-time dynamical system in C. Below, we will define approximate inference doctrines in
discrete time that arise from processes of (stochastic) gradient descent, and which therefore factor
through differential systems, but the form in which these are given—and in which they are found
in the informal literature (e.g., Bogacz [33])—is not obtained via the composite Disck ˝Flow for any
k, even though there is a free parameter k that plays the same rôle (intuitively, a ‘learning rate’).
Instead, one typically adopts the following scheme, sometimes known as Euler integration or the
Euler method .
259
Euler integration induces a family of indexed functors Eulerk : DiffSys Ñ CoalgN
P, for
k : R, which we illustrate for a single system pRn, mqover a fixed polynomial p, with m :
RnyRnˆRn
Ñ p. This system is determined by a pair of morphisms m1 : Rn Ñ pp1qand
m7 : ř
x:Rn prm1pxqsÑ‚Rn ˆRn, and we can write the action of m7as px, yqÞÑp x, vxpyqq.
Using these, we define a discrete-time dynamical system β over p with state space Rn. This
β is given by an output map βo, which we define to be equal to m1, βo :“m1, and an update
map βu : ř
x:Rn prβopxqsÑ‚Rn, which we define by px, yqÞÑ x `k vxpyq. Together, these define a
system in CoalgN
Pppq, and the collection of these systems β produces an indexed functor by the
definition Eulerkppqpmq:“β.
By contrast, the discrete-time system obtained viaDisck ˝Flow involves integrating a continuous-
time system for k units of real time for each unit of discrete time: although this in general produces
a more accurate simulation of the trajectories implied by the vector field, it is computationally
more arduous; to trade off simulation accuracy against computational feasibility, one may choose a
more sophisticated discretization scheme than that sketched above, or at least choose a “sufficiently
small” timescale k.
Finally, we can use the foregoing ideas to translate differential hierarchical inference systems to
dynamical hierarchical inference systems.
Corollary 6.3.26. The indexed functors Disck : CoalgR
P Ñ CoalgN
P, Flow : DiffSys Ñ
CoalgR
P, and Eulerk : DiffSys ÑCoalgN
P induce functors (respectively) HDisck : CiliaR
P Ñ
CiliaN
P, HFlow : DiffCilia ÑCiliaR
P and HEulerk : DiffCilia ÑCiliaN
P by change of base of
enrichment.
260
7. Approximate inference doctrines for
predictive coding
The construction of the predictive coding models that underlie the theory of the Bayesian brain
involves mapping a (‘generative’) statistical model, representing how the modeller believes the
brain to understand the world, to a dynamical system which plays the rôle of the neural circuits
which are hypothesized to instantiate that model. This dynamical system is then simulated and the
resulting trajectories studied: for instance, to compare with experimental neural or psychological
data, or to judge against a synthetic benchmark.
Typically, both the generative model and the resulting dynamical systems are ‘modular’, and
the mapping from the former to the latter preserves this structure: that is to say, predictive coding
forms an example of functorial semantics, of which we saw a rudimentary example in §3.3, when we
considered an algebra of rate-coded neural circuits. This chapter makes this functoriality explicit,
which we hope to have a useful scientific consequence: it often seems to be the case that researchers
manually derive complicated dynamical systems from their statistical models [21, 48, 76, 108, 148,
264, 265] [205, Chapter 5], but once functoriality is established, this manual labour is unnecessary;
the functor represents a machine with which the process may be automated.
We call such functors approximate inference doctrines . In defining them, we bring together the
statistical games of Chapter 5 (which supply the ‘syntax’ of generative models) and the open
dynamical systems of Chapter 6 (which supply the ‘semantics’), and we explain precisely how these
doctrines may factorize through the various components we have seen: the stochastic channels,
the inference systems, the loss models, the differential systems, and the cilia. This is the work of
§7.3, which also establishes the functoriality of predictive coding under the (Laplacian) free energy
principle. Before we get there, we construct some final pieces of technical machinery, aspects of
which we have seen before: stochastic channels with Gaussian noise, to model functions of the
form fpxq` ω with ω Gaussian-distributed (§7.1); and externally parameterized Bayesian lenses,
so that our constructions have the freedom to learn (§7.2).
261
7.1. Channels with Gaussian noise
Our motivating examples from the predictive coding literature in computational neuroscience are
defined over a subcategory of channels between Cartesian spaces with additive Gaussian noise
[33, 48, 101]; typically one writes x ÞÑfpxq` ω, with f : X ÑY a deterministic map and ω
sampled from a Gaussian distribution over Y . This choice is made, as we saw in §5.3.3.4, because
it permits some simplifying assumptions, and the resulting dynamical systems resemble known
neural circuits.
In this section, we develop some categorical language in which we can express such Gaussian
channels, expanding on the informal definition given in Remark 5.3.29. We do so by thinking of
x ÞÑfpxq` ω as a map parameterized by a noise source, and so to construct a category of such
channels, we can use the Para construction, following Proposition 3.2.3. Because the noise comes
from the parameter, we need a category whose objects are spaces equipped with measures. For this,
we can use the ‘pointing’ construction introduced in §6.2.3; as we saw in Example 6.2.25, this gives
us a category of measure spaces. The next step is to spell out an actegory structure that induces
the parameterization we seek.
Proposition 7.1.1. Suppose pC, b, Iqis a monoidal category, and supposeD ãÑC is a subcategory
to which the monoidal structure restricts. Then there is a D˚-actegory structure D˚ ÑCatpD, Dq
on D as follows. For each pM, µq: D˚, define pM, µq˚p´q : D ÑD by pM, µq˚ X :“M bp´q.
For each morphism f : pM, µqÑp N, νqin D˚, define f ˚p´q :“f bp´q.
Proof sketch. The action on morphisms is well-defined because each morphism f : pM, µq Ñ
pN, νqin D˚projects to a map f : M ÑN in D; it is clearly functorial. The unitor and associator
of the actegory structure are inherited from the monoidal structure, and they satisfy the actegory
axioms for the same reason.
Remark 7.1.2. Note that the construction of ˚is easily extended to an action on the whole of C.
We will however be concerned only with the action of D˚on D.
When we instantiate ˚in the context of Meas ãÑ sfKrn, the resulting Para bicategory
Parap˚qcan be thought of as a bicategory of maps each of which is equipped with an independent
noise source; the composition of maps takes the product of the noise sources, and 2-cells are
noise-source reparameterizations.
In this case, the actegory structure ˚is moreover symmetric monoidal, and the 1-categorical
truncation Parap˚q1 (cf. Proposition 3.2.8) is a copy-delete category, as we now sketch.
262
Proposition 7.1.3. Suppose pC, b, Iqis a symmetric monoidal copy-discard category, and let
the symmetry and copy-discard structure restrict to D ãÑC. Then Parap˚q1 is also a symmetric
monoidal copy-delete category.
Proof sketch. The monoidal structure is defined following Proposition 3.2.5. We need to define a
right costrength ρ with components pN, νq˚p X bY q „Ý ÑX bppN, νq˚ Y q. Since ˚is defined by
forgetting the pointing and taking the monoidal product, the costrength is given by the associator
and symmetry in D:
pN, νq˚pXbY q“ NbpXbY q „Ý ÑNbpY bXq „Ý ÑpNbY qbX „Ý ÑXbpNbY q“ XbppN, νq˚Y q
As the composite of natural isomorphisms, this definition gives again a natural isomorphism; the
rest of the monoidal structure follows from that of the monoidal product on C.
We now need to define a symmetry natural isomorphism βX,Y : X bY „Ý ÑY bX in Parap˚q.
This is given by the symmetry of the monoidal product inD, under the embedding ofD in Parap˚q
that takes every map to its parameterization by the monoidal unit.
The rest of the copy-delete structure is inherited similarly from C via D.
When C is a category of Markov kernels, we will typically think of the morphisms of Parap˚q1
as kernels whose uncertainty arises from a noisy parameter. To formalize this we can push forward
the noise to obtain again a morphism in C. This yields a functor Push : Parap˚q1 ÑC.
Proposition 7.1.4. There is a strict monoidal functor Push : Parap˚q1 ÑC. Given a morphism
in Parap˚q1 represented by f : X
pΩ,µq
ÝÝÝÑY , let Pushpfqbe the composite f ‚pµ bidXq: XÑ‚Y
in C.
Proof sketch. First, the given mapping preserves identities: the identity in Parap˚qis trivially
parameterized, and is therefore taken to the identity inC. The mapping also preserves composites, by
the naturality of the unitors of the symmetric monoidal structure onC. That is, givenf : X
pΩ,µq
ÝÝÝÑY
and g : Y
pΘ,νq
ÝÝÝÑZ, their composite g ˝f : X
pΘbΩ,νbµq
ÝÝÝÝÝÝÝÑZ is taken to
X „Ý Ñ‚1 b1 bX
νbµbidX
Ý ÝÝÝÝÝ Ñ‚ Θ bΩ bX
g˝f
ÝÝÑ‚ Z
where here g ˝f is treated as a morphism in C. Composing the images of g and f under the given
mapping gives
X „Ý Ñ‚1 bX
µbidX
Ý ÝÝÝ Ñ‚ Ω bX
f
Ý Ñ‚Y „Ý Ñ‚1 bY νbYÝ ÝÝ Ñ‚ Θ bY
g
Ý Ñ‚Z
263
which is equal to
X „Ý Ñ‚1 b1 bX
νbµbidX
Ý ÝÝÝÝÝ Ñ‚ Θ bΩ bX
idΘ bf
ÝÝÝÝÑ‚ Θ bY
g
Ý Ñ‚Z
which in turn is equal to the image of the composite above.
Since the monoidal structure on Parap˚qis inherited from that on C (with identical objects), the
embedding is strict monoidal.
Remark 7.1.5. Note that Push is not an embedding, since the mapping on hom sets need not
be injective: pushing forward the noise of two parallel morphisms in Parap˚q1 may yield equal
morphisms in C without the noise sources being isomorphic, and hence without the original
morphisms being equivalent in Parap˚q; that is to say, the parameterization of noise sources is
not generally unique.
We now restrict our attention to Gaussian morphisms in C “sfKrn.
Definition 7.1.6. We say that f : XÑ‚Y in sfKrn is Gaussian if, for any x : X, the measure fpxq
is Gaussian1. Similarly, we say that f : X
pΩ,µq
ÝÝÝÑY in Parap˚qis Gaussian if its image under Push
is Gaussian. We will write Gauss to denote the subcategory of sfKrn generated by Gaussian
kernels and their composites; likewise, we will write Gauss˚to denote the Gaussian subcategory
of Parap˚q. Given a separable Banach space X, we will write GausspXqfor the space of Gaussian
states on X.
Example 7.1.7. Random functions of the form x ÞÑ fpxq` ω, where ω : Ω is distributed
according to a Gaussian, are therefore morphisms in Gauss˚. Under the embedding into Gauss,
the corresponding kernel emits, for each x : X, a Gaussian distribution with mean fpxq` µω,
where µω is the mean of the Gaussian random variable ω, and variance the same as that of ω.
Remark 7.1.8. In general, Gaussian morphisms are not closed under composition: pushing a
Gaussian distribution forward along a nonlinear transformation will not generally result in another
Gaussian. For instance, consider the Gaussian functions x ÞÑ fpxq` ω and y ÞÑ gpyq` ω1.
Their composite in Gauss˚ is the morphism x ÞÑg
`
fpxq` ωq
˘
`ω1; even if g
`
fpxq` ωq
˘
is
Gaussian-distributed, the sum of two Gaussians is in general not Gaussian, and sog
`
fpxq`ωq
˘
`ω1
will not be Gaussian. This non-closure underlies the power of statistical models such as the
variational autoencoder, which are often constructed by pushing a Gaussian forward along a
1We admit Dirac delta distributions, and therefore deterministic morphisms, as Gaussian, since delta distributions can
be seen as Gaussians with infinite precision.
264
learnt nonlinear transformation [155], in order to approximate an unknown distribution; since
sampling from Gaussians is relatively straightforward, this method of approximation can be
computationally tractable. The Gauss construction here is an abstraction of the Gaussian-
preserving transformations of Shiebler [232], and is to be distinguished from the category with
the same name introduced by Fritz [109], whose morphisms are affine transformations (which do
preserve Gaussianness) and which are therefore closed under composition; there is nonetheless an
embedding of Fritz’s Gauss into our Gauss.
For Laplacian statistical games (in the image ofLFE), and for the associated approximate inference
doctrines, we are interested only in Gaussian channels between finite-dimensional Cartesian spaces
Rn for n : N.
Definition 7.1.9. Denote by FdGauss the full subcategory of Gauss spanned by the objects Rn
for n : N.
Proposition 7.1.10. Every channelc : XÑ‚Y in FdGauss admits a density functionpc : Y ˆX Ñ
r0, 1swith respect to the Lebesgue measure on Y . Moreover, since Y “ Rn for some n : N,
this density function is determined by two maps: the mean µc : X Ñ Rn, and the covariance
Σc : X ÑRnˆn in E. We call the pair pµu, Σcq: X ÑRn ˆRnˆn the statistical parameters of c
(to be distinguished from any parameterization in the category-theoretic sense of §3.2).
Proof. The density function pc : Y ˆX Ñr0, 1sis defined by
log pcpy|xq“ 1
2
A
ϵcpy, xq, Σcpxq´1 ϵcpy, xq
E
´log
a
p2πqn det Σcpxq
where the ‘error’ function ϵc : Y ˆX ÑY is defined by ϵcpy, xq:“y ´µcpxq.
7.2. Externally parameterized Bayesian lenses and statistical games
The statistical games of Chapter 5 are simply Bayesian lenses equipped with loss functions. Given a
statistical game, its lens is therefore fixed, and the only way to a high score on its loss is through its
openness to the environment—the dependence on a prior and an observation. But this seems like a
strange model of adaptive or cybernetic systems, which should also be free to change themselves
in order to improve their performance.
Indeed, this changing-oneself is at the heart of the construction of approximate inference
doctrines, and in order to incorporate it into the structure, there must be some more freedom in
265
the model: the freedom to choose the lens. This freedom is afforded by the use of parameterized
statistical games, and in particular, externally parameterized statistical games, in the sense of §3.2.2.
Remark 7.2.1. It is of course possible to define an actegorical (internal) parameterization of
statistical games, but this seems to prove more complicated than necessary for our purposes.
In advance of our use of external parameterization in the construction of approximate inference
doctrines, recall that we denote the external parameterization of an enriched category C in its base
of enrichment E by PC. This section is dedicated to exhibiting the external parameterizations
PBayesLens2 and PSGame of Bayesian lenses and statistical games, and the notion of
parameterized loss model.
Remark 7.2.2. Because BayesLens2 and SGame are both bicategories, they are weakly enriched
in Cat. Consequently, following Remark 3.2.12, P has the type Cat-Cat ÑpCat-Catq-Cat,
or equivalently, Bicat Ñ Tricat. This means that, in full generality, PBayesLens2 and
PSGame are tricategories: if B is a bicategory, then the hom-bicategory PBpa, bq is the
bicategory Cat{Bpa, bq. Because we are now working with weakened structures (weak enrichment,
bicategories, lax loss models), we take this to be a lax slice of Cat.
We pause to define this new notion, generalizing our earlier Definition 3.2.10 (slice category).
Definition 7.2.3. Suppose X is a 0-cell in a bicategory B. The lax slice of B over X, denoted B{X,
is the following bicategory. Its 0-cells are pairs pA, pqwhere A is a 0-cell and p is a 1-cell A ÑX
in B. A 1-cell pA, pqÑp B, qqis a pair pf, ϕqwhere f is a 1-cell A ÑB and ϕ is a 2-cell p ñq ˝f
in B, as in the diagram
A B
X
f
p q
ϕ .
A 2-cell pf, ϕqñp g, γqis a 2-cell α : f ñg in B such that
p q ˝f g ˝g
ϕ q˝α
“ p q ˝g
γ
.
(In this definition, ˝denotes horizontal composition in B.) The horizontal composition in B{X is
given in the obvious way by the horizontal composition of the relevant 1- and 2-cells. Likewise,
vertical composition in B{X is vertical composition in B. (It is easy to check that these definitions
all satisfy the relevant axioms, hence constituting a valid bicategory.)
266
We will see how this structure works in practice in our examples of parameterization below.
Remark 7.2.4. To avoid venturing into 3- and 4-dimensional category theory, we will restrict the
hom-bicategories of PBayesLens2 and PSGame to be locally discrete , with the parameterizing
objects being mere sets (treated as discrete categories). Strictly speaking, our parameterizing sets
will be the underlying sets of differential manifolds — specifically, the trivial manifoldsRn — and we
could treat them properly as parameterizing categories by using their groupoidal (path) structure,
but we do not pursue this here. (Alternatively, we could follow the idea of Proposition 3.2.8 and
truncate the hom-categories by quotienting by connected components; but this turns the 1-cells
into equivalence classes of functors, which are again more complicated than we have the need or
appetite for here.)
Restricting P to discrete parameterization means that we instantiate PBayesLens2 and
PSGame as follows. Both being constructed overCopara2pCq, we build up fromPCopara2pCq,
after first sketching the horizontal composition of externally parameterized bicategories.
Remark 7.2.5. Given a bicategory B, horizontal composition in PB is obtained from the strong
monoidal structure of the covariant self-indexing (which follows from the universal property of
the product of categories) and the horizontal composition in B. For each triple of 0-cells a, b, c: B,
the composition pseudofunctor is given by
PBpb, cqˆ PBpa, bq“ Cat{Bpb, cqˆ Cat{Bpa, bq ¨¨¨
¨¨¨ „Ý ÑCat{
`
Bpb, cqˆ Bpa, bq
˘ Cat{˝a,b,c
ÝÝÝÝÝÝÑCat{Bpa, cq“ PBpa, cq.
Observe that this generalizes the lower-dimensional case of Definition 3.2.11: first, we take the
product of the parameterizing functors, and then we compose in their codomain.
Example 7.2.6. The 0-cells of PCopara2pCqare the 0-cells of Copara2pCq, which are in turn
the objects of C. A 1-cell from X to Y is a choice of (discrete) parameterizing category (hence a
set) Θ, along with a functor Θ ÑCopara2pCqpX, Yq. More intuitively, we can think of such a
1-cell as a morphism in C that is both (externally) parameterized and (internally) coparameterized,
and write it as f : X ΘÝÑ
M
Y , denoting a 1-cell with parameter Θ (in the base of enrichment of C),
domain X, codomain Y , and coparameter M.
A 2-cell from f : X ΘÝÑ
M
Y to f1 : X Θ1
Ý Ý Ñ
M1
Y is a pair pϕ, φqof a functor ϕ : Θ ÑΘ1 and a
natural transformation φ : f ñ f1 ˝ϕ. The functor ϕ changes the parameterization; and the
267
natural transformation φ permits additionally a compatible change of coparameterization, being
given by a natural family of 2-cells in Copara2pCq
φθ :
`
fθ : X θÝÑ
M
Y
˘
ñ
`
f1ϕpθq : X
ϕpθq
ÝÝÑ
M1
Y
˘
indexed by the parameters θ : Θ . (With discrete parameterization, such a family is trivially
natural.) Recalling the definition of Copara2 in Theorem 5.2.1, this means that each component
φθ corresponds to a morphism X bM bY ÑN in C satisfying the change of coparameter axiom
with respect to fθ and f1ϕpθq.
Horizontal composition in PCopara2pCqis as sketched in Remark 7.2.5: given 1-cellsf : X ΘÝÑ
M
Y and g : Y ΩÝ Ñ
N
Z, their composite is the evident g ˝f : X ΘÝÑ
M
Y ΩÝ Ñ
N
Z whose parameter is the
product Ω ˆΘ and whose coparameter is the tensor of M and N. The horizontal composition
of 2-cells is likewise by first forming the product of their parameters. Vertical composition in
PCopara2pCqis given by the horizontal composition in each lax slice hom (bi)category.
The structure of PBayesLens2 and PSGame follows the same pattern.
Example 7.2.7. The 0-cells of PBayesLens2 are the same pairs pX, Aqas in BayesLens2. A
1-cell from pX, Aqto pY, Bqis a biparameterized Bayesian lens: a pair pc, c1qof a biparameterized
forwards channel c : X ΘÝÑ
M
‚ Y and a biparameterized inversion (state-dependent) channel c1 :
B
Θ;X
Ý ÝÝ Ñ
M1
‚ A; here we have denoted the state-dependence and the parameterization together as Θ; X.
(Note that in all our examples, the forwards and backwards coparameters will be equal,i.e., M “M1;
cf. Remark 5.2.21 on dependent optics.)
A 2-cell from pc, c1q: pX, Aq ΘÝÝÝÑ
M,M1
| pY, Bqto pd, d1q: pX, Aq ΩÝÝÝÑ
N,N1
| pY, Bqis a triple pα, α1, α1q
such that α is a functor Θ ÑΩ, pα, α1qis a 2-cell c ñd in PCopara2pCq(cf. Example 7.2.6), and
pα, α1qis a 2-cell c1 ñd1 in PStat2pXqpB, Aq. The latter means that α1 is a family of 2-cells in
Copara2pCqpB, Aq
α1θ
π :
`
c1θ
π : B
θ;π
Ý Ý Ñ
M1
A
˘
ñ
`
d1αpθq
π : B
αpθq;π
Ý ÝÝÝ Ñ
N1
A
˘
natural in θ : Θ and indexed by π : CpI, Xq. (The preceding example shows how this corresponds
to an indexed natural family of change-of-coparameter morphisms in C.)
Horizontal composition in PBayesLens2 is of course by taking the product of the parameters
and then applying horizontal composition in BayesLens2; and vertical composition is horizontal
composition in the lax slices making up each hom (bi)category.
268
Example 7.2.8. Statistical games are obtained by attaching loss functions to Bayesian lenses, and
hence to understand parameterized statistical games having elaborated parameterized Bayesian
lenses in the preceding example, it suffices to exhibit parameterized loss functions.
A parameterized statistical game pX, Aq ΘÝÝÝÑ
M,M1
pY, Bqconsists of a parameterized Bayesian
lens pX, Aq ΘÝÝÝÑ
M,M1
| pY, Bqalong with a parameterized loss function B
Θ;X
Ý ÝÝ Ñ‚ I in PStatpXqpB, Iq.
Since StatpXqpB, Iqis a discrete category, such a loss function is given by a function Θ0 Ñ
StatpXqpB, Iq, or equivalently (by the Cartesian closure of Set) a function Θ0 ˆCpI, Xq Ñ
CpB, Iq. In the case where C “sfKrn, this means a function Θ0 ˆsfKrnp1, Xqˆ B ÑR`
which is measurable in B.
A 2-cell from the parameterized statistical game pc, c1, Kq: pX, Aq ΘÝÝÝÑ
M,M 1
pY, Bqto pd, d1, Lq:
pX, Aq ΩÝÝÝÑ
N,N 1
pY, Bqis a quadruple pα, α1, α1, ˜αqwhere pα, α1, α1qis a 2-cell of Bayesian lenses
and ˜α is a family of parameterized loss functions B
Θ;X
Ý ÝÝ Ñ‚ I such that Kθ “Lαpθq`˜αθ, naturally in
θ : Θ.
Horizontal and vertical composition of parameterized statistical games and their 2-cells follow
the pattern of the preceding examples.
Because P is functorial, we can consider parameterized versions of the inference systems and
loss models that we introduced in §5.3.2. We can think of parameterization as introducing a ‘hole’
in a structure (such as an extra input to a system), and parameterized inference systems and loss
models are inference systems and loss models that account for (and possibly modulate) such holes.
Example 7.2.9. Suppose pD, ℓqis an inference system in C. P acts on the canonical inclusion
p´q : D ãÑCoparal
2pDqto return the inclusion Pp´q : PD ãÑPCoparal
2pDq, which maps
a parameterized channel d : X ΘÝ Ñ‚Y to its trivially coparameterized form d : X ΘÝ Ñ
I
‚Y .
ℓ then maps a channel d to a lens pd, ℓdq. If d is parameterized by Θ, then its inversion ℓd under ℓ
will be parameterized accordingly, so that the whole lens pd, ℓdqhas parameter Θ. This mapping is
the action of the pseudofunctor Pℓ : PD ÑPBayesLens2|D, induced by the parameterization
of ℓ.
However, in the next section, we will want approximate inference systems that do not just
preserve an existing parameterization, but which also add to it, equipping (possibly parameterized)
morphisms with inversions that may have their own distinct capacity for improvement or learning.
For this reason, we make the following definition.
269
Definition 7.2.10. Suppose pC, b, Iqis a copy-delete category. A parameterized inference system
in C is a pair pB, ℓqof a sub-bicategory B ãÑPC along with a (strong functorial) section ℓ : B Ñ
PBayesLens2|B of the restriction PpπLensq|B to B of the parameterized 2-fibration PpπLensq:
PBayesLens2 ÑPCoparal
2pCq, where B is the essential image of the restriction to B of the
(parameterized) canonical lax inclusion Pp´q : PC ãÑPCoparal
2pCq. We say lax parameterized
inference system when ℓ is a lax functor.
A trivial example of a lax parameterized inference system is obtained by taking the parameters
to be hom categories, and the choice functor to be the identity, as the following example shows.
Example 7.2.11. The following data define a lax parameterized inference system ℓ acting on
the entirety of PC. First, let PpX, Y, Mqdenote the full subcategory of Stat2pXqpY, Xqon those
objects (state-dependent morphisms) with coparameter M. Then ℓ is defined as follows.
(i) Each 0-cell X is mapped to the 0-cell pX, Xq.
(ii) Each 1-cellc : X ΘÝÑ
M
‚ Y is mapped to the parameterized lenspc, c1q: pX, Xq
ΘˆPpX,Y,M q
ÝÝÝÝÝÝÝÝÑ
M
| pY, Yq
whose forward channel is chosen by
Θ ˆPpX, Y, Mq
proj1
Ý ÝÝ ÑΘ cÝ ÑCoparal
2pCqpX, Yq
and whose inverse channel c1 : Y
PpX,Y,M q;X
Ý ÝÝÝÝÝÝÝ Ñ
M
‚ X is chosen by
Θ ˆPpX, Y, Mq
proj2
Ý ÝÝ ÑPpX, Y, MqãÑStat2pXqpY, Xq
(iii) Each 2-cell pa, αq:
`
c : X ΘÝÑ
M
‚ Y
˘
ñ
`
d : X Θ1
Ý Ý Ñ
M1
‚ Y
˘
is mapped to the 2-cell pa ˆα˚, α, αq,
where α˚ is the functor defined by post-composing with α taken as a family of 2-cells in
Coparar
2pCqand hence in PpX, Y, Mq.
Proof. First, we confirm that the mapping is well-defined on 1-cells (taking it to be evidently so on
0-cells): in general, the coparameters in PCopara2pCqmay depend on the parameters, but here
the parameters arise from the embedding p´q : PC ÑPCopara2pCq. The only coparameters
are therefore those that arise by copy-composition, and their type is thus not parameter-dependent.
It is therefore legitimate to map a 1-cellc : X ΘÝÑ
M
Y to a lens with type pX, Xq
ΘˆPpX,Y,M q
ÝÝÝÝÝÝÝÝÑ
M
| pY, Yq.
Next, we check well-definedness on 2-cells. Note that the 2-cell pa, αq:
`
c : X ΘÝÑ
M
‚ Y
˘
ñ
`
d :
X Θ1
Ý Ý Ñ
M1
‚ Y
˘
in PCoparal
2pCqis constituted by a family of morphisms αθ : X bM bY Ñ‚M1, and
that a 2-cell
`
Y ÝÑ
M
‚ X
˘
ñ
`
Y Ý Ý Ñ
M1
‚ X
˘
in Coparar
2pCqhas an underlying morphism of the same
270
type; hence each αθ witnesses such a 2-cell in Coparar
2pCq. In particular, for each π : IÑ‚X in C,
and for each state-dependent ρ : Y XÝÑ
M
‚ X, αθ yields a 2-cell from ρπ to
α˚pθ, ρqπ :“ ρπY
X
M1
αθ
.
The functor α˚is thus defined by mapping pθ, ρq: Θ ˆPpX, Y, Mqto α˚pθ, ρq: PpX, Y, M1q; its
own action on 2-cells is likewise by parameterized post-composition. Finally, note that d1is also
given by evaluation, and so α also defines an indexed natural family of 2-cells
αθ,ρ
π :
`
c1ρ
π “ρπ : Y ÝÑ
M
‚ X
˘
ñ
`
d1α˚pθ,ρq
π “α˚pθ, ρqπ : Y Ý Ý Ñ
M1
‚ X
˘
as required (cf. Example 7.2.7). Therefore, pa ˆα˚, α, αqdefines a 2-cell in PBayesLens2. This
is compatible with ℓ being a section of PπLens, as pa ˆα˚, α, αqÞÑp a, αq.
To establish lax unity, we need to exhibit a family of 2-cellspiX, iX1, i1
Xq: idpX,Xq ñpidX, id1
Xq
natural in X, where idpX,Xqis the identity lens onpX, Xqin PBayesLens2 with trivial parameter
1, idX is the likewise trivially parameterized identity on X in PC, and id1
X is the parameterized
state-dependent channel id1
x : X
1ˆPpX,X,1q
Ý ÝÝÝÝÝÝÝ Ñ
1
‚ X defined by the inclusion
1 ˆPpX, X,1q „Ý ÑPpX, X,1qãÑStat2pXqpX, Xq.
Clearly id1
X is not constantly the identity morphism, and this is why ℓ is only laxly unital. By
defining the functor iX : 1 Ñ1 ˆPpX, X,1qto pick the element idX, the 2-cell iX1 to be the
identity on idX, and likewise i1
x, we obtain the required family of witnesses.
Lax functoriality is witnessed by a family of 2-cells
pfdc, fdc1, f1
dcq: pd, d1q pc, c1qñp d ‚c, pd ‚cq1q
natural in c : X ΘÝÑ
M
‚ Y and d : Y ΦÝ Ñ
N
‚ Z. We define the functor
fdc : Θ ˆPpX, Y, Mqˆ Φ ˆPpY, Z, NqÑ Θ ˆΦ ˆPpX, Z, MbNq
by composition, fdcpθ, ρ, ϕ, χq:“pθ, ϕ, ρ˝χcθ q; it is the fact that not all morphisms XÑ‚Z factor
through Y that makes ℓ lax functorial. With fdc so defined, we can set both fdc1 and f1
dc to be
identity 2-cells, and thus obtain the requisite witnesses.
271
On the other hand, the only parameterized loss models we encounter will be those of §5.3.3
under the action of P. This is because the ability to change is part of the system itself, rather than
part of how we measure the system2: we do not seek to “move the goalpoasts”. (In future work, we
may consider systems whose performance is dependent on some broader context; but not here.)
Therefore, our parameterized loss models will all be of the following form.
Example 7.2.12. If L is a loss model forB, then its parameterizationPL assigns to a parameterized
Bayesian lens pc, c1q : pX, Aq ΘÝÝÝÑ
M,M1
| pY, Bqthe correspondingly parameterized statistical game
`
c, c1, Lpcq
˘
. The parameterized loss function Lpcq thus also has parameter Θ and depends
accordingly on it, with type Lpcq: B
Θ;X
Ý ÝÝ Ñ‚ I. For each θ : Θ, its component is the loss function
Lpcqθ : B XÝ Ñ‚ I which is assigned to the lens pcθ, c1θq by L (as a loss model applied to an
unparameterized lens).
Remark 7.2.13. Before we move on to examples of approximate inference doctrines, let us note the
similarity of the notions of externally parameterized lens (Example 7.2.7), cilia (Definition 6.3.8), and
differential cilia (Definition 6.3.19): both of the latter can be considered as externally parameterized
lenses with extra structure, where the extra structure is a morphism or family of morphisms back
into (an algebra of) the parameterizing object: in the case of differential cilia, this ‘algebra’ is the
tangent bundle; for (dynamical) cilia, it is trivial; and forgetting this extra structure returns a mere
external parameterization. Notably, the ‘input’ on the external hom polynomial defining both types
of cilia (Definition 6.3.1) corresponds precisely to the domain of the loss function of a statistical
game; and so the domains of the update maps of either type of cilia correspond to the domains of
parameterized loss functions. We will make use of this correspondence in defining approximate
inference doctrines in the next section.
7.3. Approximate inference doctrines
We are at last in a position to build the bridge between abstract statistical models and the dynamical
systems that play them: functors from a copy-discard category of parameterized channels to a
category of cilia that factorize through an inference system (modelling how the system inverts the
channels) and possibly a loss model (encoding how well the system is doing). In the round, we can
think of the resulting approximate inference doctrines as “dynamical algebras” for categories of
parameterized stochastic channels (considered as statistical models), which take the parameters
2Physically speaking, we adopt the ‘Schrödinger’ perspective on change rather than the ‘Heisenberg’ perspective.
272
as part of the dynamical state space so that they might improve themselves. This line of thinking
leads us to the following definitions.
Definition 7.3.1. Let pC, b, Iqbe a copy-discard category, and let pB, ℓqbe a parameterized
inference system in C.
(a) An approximate inference doctrine is a pseudofunctor B ÑCiliaT that factors through ℓ, as
B
Pp´q |B
ÝÝÝÝÝÑB ℓÝ Ñimpℓq DÝ ÑCiliaT .
We say lax approximate inference doctrine if D is instead a lax functor.
(b) An approximate inference doctrine with loss L is an approximate inference doctrine along with
a loss modelL for impℓq, a pseudofunctorDL : impLqÑ CiliaT, and an iconλ : D ñDL˝L,
as in the diagram
B B impℓq CiliaT
impLq
DℓPp´q |B
L
DL
λ .
We say lax approximate inference doctrine with loss if L and DL are lax functors.
(c) A differential approximate inference doctrine with loss L is an approximate inference doctrine
with loss L such that DL factors through a differential system, as in the diagram
B B impℓq CiliaT
impLq CiliaT
DiffCilia
DℓPp´q |B
DL
∇
ş
L
λ
δ
.
We say lax differential approximate inference doctrine when L,∇ and
ş
are lax functors.
The different factors of a differential approximate inference doctrine with loss encode the different
stages by which a dynamical system is constructed from a statistical model: the parameterized
inference system ℓ equips a parameterized channel with a parameterized inversion; the loss model
L equips the resulting lens with a loss function; the functor ∇ translates this statistical game to a
273
differential system, possibly representing gradient descent on the loss; and finally the functor
ş
turns this differential system into a dynamical system that ‘flows’, possibly by integration.
With these definitions to hand, we come to our motivating neuroscientific examples. First (§7.3.1),
we formalize predictive coding using the Laplace approximation to the free energy [21, 33, 104],
which we saw in §5.3.3.4 forms a lax loss model for Gaussian lenses. This approximation allows the
resulting dynamical systems to exhibit some biological plausibility, with prediction errors computed
linearly and the dynamical updates obtained as affine transformations of prediction errors. We call
this the Laplace doctrine .
Apart from requiring Gaussian channels, the Laplace doctrine is agnostic about how predictions
are actually generated, and it does not produce systems which are able to improve their predictions;
they have no ‘synaptic’ plasticity, and thus do not learn. To remedy this, our second example
of an approximate inference doctrine (§7.3.2) is more opinionated about the predictive forward
channels, restricting them to be of the form x ÞÑθ hpxq` ω where θ is a square matrix on Y , h
is a differentiable function X ÑY , and ω is distributed according to a Gaussian on Y ; compare
this with the form of the firing rate dynamics of rate-coded neural circuits in Definition 3.3.10.
The ‘synaptic’ parameter (or weight matrix) θ can then be learnt, and this is incorporated into the
state space of the systems produced by the corresponding Hebb-Laplace doctrine , which formalizes
another standard scheme in the neuroscience literature [33]. The name of this doctrine indicates
another aspect of the biological plausibility of this scheme: the θ-updates can be seen as a form of
Hebbian learning [127].
Remark 7.3.2. In what follows, in order to focus on exemplification, we omit a full treatment of
all the higher-categorical structure. This means that we do not consider the action of the doctrines
on 2-cells, and leave leave the full elaboration of the 2-dimensional structure to future work. Our
main concern in this final part is the scientific question of the compositional structure of predictive
coding, and one further mathematical consequence of this is that the inference systems on which
the doctrines are based will not be unital: the schemes that are presented in the literature involve
mappings which do not preserve identity channels.
7.3.1. Predictive coding circuits and the Laplace doctrine
Notation 7.3.3. Any category C embeds into its external parameterization PC by mapping every
morphism to its trivially parameterized form; in a mild abuse of notation, we will denote the image
of this embedding simply by C. In this section, we will work with the trivial parameterization of
274
the subcategory FdGauss of sfKrn of Gaussian kernels between finite-dimensional Cartesian
spaces (Definition 7.1.9). Hence, when we write FdGauss , it denotes the image of FdGauss
under Pp´q .
We begin by presenting the action of the Laplace doctrine on a (non-coparameterized3) Gaussian
channel c. Below, we will see how the resulting system is obtained from a differential approximate
inference doctrine with the Laplacian free energy loss.
Proposition 7.3.4. Suppose c : XÑ‚Y is a morphism in FdGauss, and fix a “learning rate”
λ : R. Then the following data define a system Lλpcq: pX, XqÑp Y, Yqin CiliaN, following the
representation of Proposition 6.3.9.
(i) the state space is X;
(ii) the forwards output map Lλpcqo
1 : X ˆX ÑGausspY qis defined by
Lλpcqo
1 : X ˆX
proj2
Ý ÝÝ ÑX cÝ ÑGausspY q;
(iii) the inverse output map Lλpcqo
2 : X ˆGausspXqˆ Y ÑGausspXqis defined by
Lλpcqo
2 : X ˆGausspXqˆ Y ÑR|X|ˆR|X|ˆ|X| ãÑGausspXq
px, π, yqÞÑ
`
x, Σc1px, π, yq
˘
where the inclusion picks the Gaussian state with the indicated statistical parameters, whose
covariance Σc1px, π, yq:“
`
B2
xEpc,πq
˘
px, yq´1 is defined following equation (5.8) of Lemma
5.3.31 (with trivial coparameterization M “1);
(iv) the update map Lλpcqu : X ˆGausspXqˆ Y ÑGpXqis defined by
Lλpcqu : X ˆGausspXqˆ Y ÑX ãÑGpXq
px, π, yqÞÑ x `λ Bxµcpx, yqT ηcpx, yq´ λ ηπpxq
where the inclusion X ãÑGpXqis given by the unit of the Giry monad G which takes each x : X
to the corresponding delta distribution, and where ηcpx, yq :“ Σcpxq´1 ϵcpy, xqand ηπpxq :“
Σ´1
π ϵπpxq.
3Note that all coparameterized channels of interest are obtained as the copy-composites of non-coparameterized
channels.
275
Remark 7.3.5. Note that the update map Lλpcqu is actually deterministic, in the sense that it is
defined as a deterministic map followed by the unit of the probability monad. However, the general
stochastic setting is necessary, because the composition of system depends on the composition
of Bayesian lenses; recall Definition 6.3.6, which defines the bidirectional composition of cilia.
Intuitively, we can consider a composite system Lλpdq˝ Lλpcqand note that the forward inputs to
the d component and the backward inputs to the c component will be sampled from the stochastic
outputs of c and d respectively. Because these inputs are passed to the corresponding update maps,
the updates inherit this stochasticity.
Remark 7.3.6. The terms ηcpx, yq“ Σcpxq´1 ϵcpy, xqin the update map of the Laplace doctrine
can be understood as precision-weighted error terms: the inverse covariance Σcpxq´1 encodes the
‘precision’ of the distribution (consider the univariate case); and the term ϵcpy, xq“ y ´µcpxq
encodes the ‘error’ between the observation y and the predicted mean µcpxq. The representation
of prediction errors is a hallmark of predictive coding schemes.
To define an approximate inference doctrine, we need a (parameterized) inference system. For
predictive coding, this will be obtained by assigning to each channelc an inversion whose parameter
represents the mean of the emitted posterior; this parameter will later be learned by the resulting
doctrine. In order for this assignment to be functorial, we restrict the posteriors emitted by this
inference system to have diagonal covariance, meaning that there will be no correlations between
dimensions. This formalizes what is known in the literature as a mean field assumption [48, 101],
without which those earlier works would not have been able to make implicit use of functoriality.
Proposition 7.3.7 (Mean field Laplace). As long as ‚denotes copy-composition, the following
data define a (non-unital) strong parameterized inference system ℓ on FdGauss. Each 0-cell
X is mapped to pX, Xq. Each 1-cell c : XÝÑ
M
‚ Y is mapped to the parameterized lens pc, c1q :
pX, XqXˆMÝ ÝÝÝ Ñ
M
| pY, Yqwhose forward channel is c and whose parameterized backward channel
c1 : Y
XˆM;X
ÝÝÝÝÝÑ
M
‚ X emits the Gaussian with mean px, mq: X ˆM determined by the parameter and
which minimizes the (mean-field) Laplacian free energy. Thus, writing
`
µx,m
c1π
pyq, Σx,m
c1π
pyq
˘
for the
statistical parameters of c1x,m
π pyq, ℓ assigns
µx,m
c1π
pyq:“px, mq and Σx,m
c1π
pyq:“
˜`
B2xEpc,πq
˘
px, m, yq´1 0
0
`
B2mEpc,πq
˘
px, m, yq´1
¸
.
276
where B2 denotes the diagonal Hessian 4 It is the diagonal structure of Σx,m
c1π
that justifies the
‘mean-field’ moniker.
Proof. First, we note that ℓ fails to be unital because, for each identity channel idX : X XÝ Ñ‚ , the
mean of the assigned inversion id1
X is determined by the parameter X, rather than the input. If
this parameter happens to equal to the input, then id1
X will actually act as the identity channel.
This is because we can understand the identity channel as the limit as σ Ñ0 of a Gaussian with
mean equal to the input x and variance σ1X (where 1X is the identity matrix on X). Informally,
we have Σx
pid1
Xqπ
px1q“
`
B2xEpidX,πqpx, x1q´1 “0, and so pid1
Xqx acts as the Dirac delta distribution
on the parameter x; but of course in general the parameter x need not equal the forward input.
Next, we show thatℓ is strongly functorial (as long as‚is always interpreted as copy-composition).
If c : XÝÑ
M
‚ Y and d : Y Ý Ñ
N
‚ Z are composable Gaussian channels, then the statistical parameters of
the composite approximate inversion c1˝d1
c : Z
Y NXM;X
Ý ÝÝÝÝÝÝ Ñ
MY N
‚ X are µy,n,x,m
pc1˝d1cqπ
pzq“p x, m, y, nqand
Σy,n,x,m
pc1˝d1cqπ
pzq“ diag
»
———–
`
B2xEpc,πq
˘
px, m, yq´1
`
B2mEpc,πq
˘
px, m, yq´1
`
B2yEpd,c‚πq
˘
px, m, y, n, zq´1
`
B2nEpd,c‚πq
˘
px, m, y, n, zq´1
fi
ffiffiffifl .
Note that, by interpreting ‚as copy-composition, we have
Epd,c‚πqpx, m, y, n, zq“´ log pdpn, z|yq´ log pcpm, y|xq´ log pπpxq.
On the other hand, ℓ assigns to d ‚c : XÝ ÝÝÝ Ñ
MY N
‚ Z the lens
`
d ‚c, pd ‚cq1˘
whose inversion
pd ‚cq1 : Z
XMY N;X
Ý ÝÝÝÝÝÝ Ñ
MY N
‚ X is defined by the statistical parameters µx,m,y,n
pd‚cq1π
pzq“p x, m, y, nqand
Σx,m,y,n
pd‚cq1π
pzq“ diag
»
———–
`
B2xEpd‚c,πq
˘
px, m, y, n, zq´1
`
B2mEpd‚c,πq
˘
px, m, y, n, zq´1
`
B2yEpd‚c,πq
˘
px, m, y, n, zq´1
`
B2nEpd‚c,πq
˘
px, m, y, n, zq´1
fi
ffiffiffifl
where
Epd‚c,πqpx, m, y, n, zq“´ log pdpn, z|yq´ log pcpm, y|xq´ log pπpxq
“Epd,c‚πqpx, m, y, n, zq.
Consequently, Σx,m,y,n
pd‚cq1π
pzq“ Σy,n,x,m
pc1˝d1cqπ
pzq. It therefore suffices to take the laxator ℓpdq ℓpcqñ
ℓpd ‚cqto be defined by the isomorphism pY ˆNqˆp X ˆMq „Ý ÑpX ˆMqˆp Y ˆNq.
4That is, B2xfpxqcan be represented as the matrix with diagonal equal to the diagonal of the Hessian matrix B2
xfpxq
and with all other coefficients 0.
277
Remark 7.3.8. Note that the preceding inference system requires ‚to be interpreted as copy-
composition everywhere, which is not strictly in accordance with our earlier usage (which mixed
copy-composition with ordinary composition in the state-dependence). Resolving this irregularity
is the subject of ongoing work.
Proposition 7.3.9. Stochastic gradient descent with respect to the mean parameter of Laplacian free
energy games in the image ofℓ yields a strong functor∇ : L ÑDiffCilia, where L is the essential
image of LFE restricted to the essential image of ℓ. If c :“pc, c1, Lcq: pX, Xq XˆMÝ ÝÝÝ Ñ
M
pY, Yqis
such a game (a 1-cell) inL, then ∇c is the differential cilium pX, Xq XˆMÝ ÝÝÝ ÑpY, Yqwith state space
equal to the parameter X ˆM defined as follows.
For eachpx, mq: XˆM, ∇c outputs the non-coparameterized Bayesian lensℓpcqx,m : pX, Xq ÞÑ
pY, Yqobtained by taking the dynamical state px, mqas the parameter of the lens and discarding
any coparameters.
The ‘update’ vector field p∇cqu : pX ˆMq ÑGausspXq ÑY Ñ‚TpX ˆMqis obtained
by taking the negative gradient of the loss function Lc : Y
GausspXˆMq;X
Ý ÝÝÝÝÝÝÝÝÝÝ Ñ‚ I with respect to the
posterior mean parameter, evaluated at the posterior mean:
pX ˆMqÑ GausspXqÑ Y ÑTpX ˆMq
px, m, π, yqÞÑ´
`
Bpx,mqEpc,πq
˘
px, m, yq.
(This yields a morphism in sfKrn via the embedding Meas ãÑsfKrn; it is clearly measurable as
it is a continuous function between Cartesian spaces.)
Proof. Since the state space X ˆM is the space of means of the Laplacian posteriors, the ‘update’
action of ∇c, the open vector field p∇cqu, is defined as the (negative) gradient of Lc with respect
to these means (so that the associated flow performs gradient descent). The parameterized loss
function Lc : Y
XˆM;X
ÝÝÝÝÝÑ‚ I encodes the Laplacian free energy associated to the parameterized lens
pc, c1q, and corresponds (by Example 5.3.4) to the function
X ˆM ÑGausspXqÑ Y Ñr0, 8s
px, m, π, yqÞÑ LFEpc, c1x,mqπpyq
where
LFEpc, c1x,mqπpyq“ Epc,πqpx, m, yq´ SXbM rc1x,m
π pyqs.
278
The entropy SXbM rc1x,m
π pyqsdoes not depend on the mean of c1x,m
π pyq, and so the gradient of
LFEpc, c1x,mqπpyqwith respect to px, mqis simply
`
Bpx,mqEpc,πq
˘
px, m, yq. Hence defining p∇cqu
as stated yields
p∇cqu : px, m, π, yqÞÑ´
`
Bpx,mqEpc,πq
˘
px, m, yq.
We now show that ∇ is strongly functorial with respect to composition of 1-cells in L. First, we
check that ∇ satisfies the strong unity axiom, which means we need a 2-isomorphism idpX,Xq ñ
∇pidpX,Xqqin DiffCilia. Note that the cilium idpX,Xqhas trivial state space 1, trivial update map,
and outputs the identity lens pX, Xq ÞÑpX, Xq. Likewise, the identity game idpX,Xq has trivial
parameter 1, loss function equal to 0, and lens being the (trivially coparameterized copy-composite)
identity lenspX, Xq ÞÑpX, Xq. Since the loss function is constantly0 with trivial parameter,∇ acts
to return a ciliumpX, Xq 1Ý ÑpX, Xqagain with trivial state space and which constantly outputs the
identity lens; its update map is likewise trivial. Therefore we take the 2-cell idpX,Xq ñ∇pidpX,Xqq
to be witnessed by the identity id1 : 1 Ñ1, which satisfies strong unity a fortiori .
Finally, we check that ∇ satisfies the strong functoriality axiom, meaning that we seek a 2-
isomorphism ∇pd, d1, Ldq˝ ∇pc, c1, Lcqñ ∇
`
pd, d1, Ldq˝p c, c1, Lcq
˘
for each pair of composable
Laplacian free energy games pc, c1, Lcq: pX, XqXˆMÝ ÝÝÝ Ñ
M
| pY, Yqand pd, d1, Ldq: pY, YqY ˆNÝÝÝÑ
N
| pZ, Zq.
Note that the composite game has the type pX, Xq
pY ˆNqˆpXˆMq
ÝÝÝÝÝÝÝÝÝÝÑ
MY N
| pZ, Zq, that by the universal
property of ˆwe have an isomorphism pY ˆNqˆp X ˆMq–p X ˆMqˆp Y ˆNq, and that
the product of Gaussians is again Gaussian. Note also that the parameterized loss function Ld ˝Lc
equals
pY ˆNqˆp X ˆMq Ñ GausspXq Ñ Z Ñ r 0, 8s
py, n, x, m, π, z q ÞÑ pLcqx,m
π ˝d1y,n
c‚π pzq`p Ldqy,n
c‚πpzq.
On the other hand, the update map of the composite cilium
`
∇pd, d1, Ldq˝ ∇pc, c1, Lcq
˘u equals
pX ˆMqˆp Y ˆNq Ñ GausspXq Ñ‚ Z Ñ T
`
pX ˆMqˆp Y ˆNq
˘
px, m, y, n, π, z q ÞÑ
`
p∇cuqx,m
π ‚d1y,n
c‚π pzq, p∇duqy,n
c‚πpzq
˘ .
The desired 2-isomorphism ∇pd, d1, Ldq˝ ∇pc, c1, Lcq ñ∇
`
pd, d1, Ldq˝p c, c1, Lcq
˘
is thus
witnessed by a map pY ˆNqÑp X ˆMqÑp X ˆMqˆp Y ˆNq, which we take to be the
symmetry swap of the categorical product. Computing the gradient of the L terms in Ld ˝Lc with
respect to the mean of the joint Gaussian pχ, ρqyields the update map
pY ˆNqˆp X ˆMq Ñ GausspXq Ñ‚ Z Ñ T
`
pY ˆNqˆp X ˆMq
˘
py, n, x, m, π, z q ÞÑ
`
p∇duqy,n
c‚πpzq, p∇cuqx,m
π ‚Ð Ýd
y,n
c‚πpzq
˘
279
which is clearly equal to
`
∇pd, d1, Ldq˝ ∇pc, c1, Lcq
˘u upon composition with swap. It therefore
only remains to check that the two cilia output the same Bayesian lenses pX, Xq ÞÑpZ, Zq, up to
swap. This follows from the strong functoriality of ℓ.
Remark 7.3.10. Although we have defined ∇ manually, we expect that it can alternatively be
obtained more abstractly, from a proper treatment of stochastic gradient descent applied to statistical
games. We leave this to future work.
Finally, to obtain the dynamical systems with which we started this subsection (in Proposition
7.3.4), we use Euler integration, using the functor Eulerλ of Remark 6.3.25.
Corollary 7.3.11. Fix a real number λ : R. By defining Lλ :“ Eulerλ ˝∇ ˝LFE one obtains
Laplacian predictive coding as a differential approximate inference doctrine, the Laplace doctrine
for the mean field Laplace inference system ℓ. The systems of Proposition 7.3.4 are obtained in its
image.
Proof. Suppose c : XÑ‚Y is a morphism in FdGauss. It is not coparameterized, so ℓ assigns to it
the parameter space X, which becomes the state space of the ciliumLλpcq. By definition, this cilium
emits the same lens — and therefore has the same output maps — as those given in Proposition
7.3.4. We therefore only need to check that
`
∇pcqu˘x
πpyq“´
`
BxEpc,πq
˘
px, yq
“Bxµcpx, yqT ηcpx, yq´ ηπpxq.
Recall from Proposition 5.3.31 that
Epc,πqpx, yq“´ log pcpy|xq´ log pπpxq
“´1
2
@
ϵcpy, xq, Σc´1ϵcpy, xq
D
´1
2
@
ϵπpxq, Σπ´1ϵπpxq
D
`log
b
p2πq|Y |det Σc `log
b
p2πq|X|det Σπ .
It is then a simple exercise in vector calculus to show that
´
`
BxEpc,πq
˘
px, yq“B xµcpx, yqT ηcpx, yq´ ηπpxq
as required.
280
7.3.2. Synaptic plasticity with the Hebb-Laplace doctrine
The Laplace doctrine constructs dynamical systems that produce progressively better posterior
approximations given a fixed forwards channel, but natural adaptive systems—brains in particular—
do more than this: they also refine the forwards channels themselves, in order to produce better
predictions. In doing so, these systems better realize the abstract nature of free energy games, for
which improving performance means improving both prediction as well as inversion. To be able to
improve the forwards channel requires allowing some freedom in its choice, which means giving it
a nontrivial parameterization.
The Hebb-Laplace doctrine that we introduce in this section therefore modifies the Laplace
doctrine by fixing a class of parameterized forwards channels and performing stochastic gradient
descent with respect to both these parameters as well as the posterior means; we call it the Hebb-
Laplace doctrine as the particular choice of forwards channels results in their parameter-updates
resembling the ‘local’ Hebbian plasticity known from neuroscience, in which the strength of the
connection between two neurons is adjusted according to their correlation [79, 118, 127, 218, 233].
(Here, we could think of the ‘neurons’ as encoding the level of activity along a basis vector.)
We begin by defining the category of these parameterized forwards channels, after which we
proceed by modifying the mean-field Laplace inference system and the Laplace doctrine accordingly.
Definition 7.3.12 (‘Neural’ channels). Let H denote the subbicategory of PFdGauss˚generated
by 1-cells XÑ‚Y of the form
ΘX ÑGauss˚pX, Yq
θ ÞÑ
´
x ÞÑθ hpxq` ω
¯
where X and Y are finite-dimensional Cartesian spaces, h is a differentiable map X ÑY , ΘX is
the vector space of square matrices on X, and ω is sampled from a Gaussian distribution on Y .
Proposition 7.3.13 (Mean field Hebb-Laplace). Taking ‚as copy-composition, the following data
define a (non-unital) strong parameterized inference system ℓ on H. Each 0-cell X is mapped
to pX, Xq. Each (parameterized) 1-cell c : X ΘÝÑ
M
‚ Y is mapped to the parameterized lens pc, c1q:
pX, Xq
ΘˆpXˆMq
Ý ÝÝÝÝÝÝ Ñ
M
| pY, Yqwhose forward channel is given by projecting Θ from Θ ˆpX ˆMqand
applying c, and whose backward channel is defined as in Proposition 7.3.7 (mean-field Laplacian
inference).
281
Proof. The only difference from Proposition 7.3.7 is in the forward channel; but these are just taken
from H, and so they compose strongly by assumption.
Like the Laplace doctrine, the Hebb-Laplace doctrine is obtained by stochastic gradient descent
with respect to the parameters.
Proposition 7.3.14. Let L denote the essential image of LFE restricted to the essential image of ℓ.
Let c :“pc, c1, Lcq: pX, Xq
ΘˆpXˆMq
Ý ÝÝÝÝÝÝ Ñ
M
pY, Yqbe a 1-cell in L. Then stochastic gradient descent
yields an identity-on-objects strong functor ∇ : L Ñ DiffCilia mapping c to the differential
cilium ∇pcq: pX, Xq
ΘˆpXˆMq
Ý ÝÝÝÝÝÝ ÑpY, Yqdefined as follows.
For each triple of parameters pθ, x, mq: Θ ˆpX ˆMq, ∇c outputs the non-coparameterized
Bayesian lens ℓpcqθ,x,m : pX, Xq ÞÑpY, Yqobtained by taking the dynamical state pθ, x, mqas the
parameter of the lens and discarding any coparameters.
The vector field p∇cqu is obtained by taking the gradient of the loss function Lc with respect to
the ‘synaptic’ parameter θ : Θ and the posterior mean px, mq: X ˆM, evaluated at pθ, x, mq:
Θ ˆpX ˆMqÑ GausspXqÑ Y ÑTpΘ ˆpX ˆMqq
pθ, x, m, π, yqÞÑ´
`
Bpθ,x,mqEpc,πq
˘
pθ, x, m, yq.
Proof. The proof is almost identical to that of Proposition 7.3.9: the sole difference is that now we
also take gradients with respect to the synaptic parameter θ : Θ, but the reasoning is otherwise the
same.
Finally, we obtain dynamical systems by Euler integration.
Definition 7.3.15. Fix a real number λ : R. The Hebb-Laplace doctrine is obtained as the composite
Hλ :“Eulerλ ˝∇ ˝LFE, yielding a differential approximate inference doctrine for the mean field
Hebb-Laplace inference system ℓ.
Corollary 7.3.16. Suppose c : X ΘÝ Ñ‚Y is a channel in H defined by cθpxq“ θ hpxq` ω, for some
differentiable h and Gaussian noise ω. Then the update map Hλpcqu is given by
Θ ˆX Ñ GausspXq Ñ Y Ñ Θ ˆX
pθ, x, π, y q ÞÑ
ˆ θ ´λ ηcθ px, yqhpxqT
x `λ BxhpxqT θT ηcθ px, yq´ λ ηπpxq
˙
where ηcθ px, yq“ Σ´1
cθ ϵcθ py, xqand ηπpxq“ Σ´1
π ϵπpxqare the precision-weighted error terms.
282
Proof. Following Corollary 7.3.11 (the Laplace doctrine), We just need to check that
`
Bpθ,xqEpcθ,πq
˘
px, yq“
ˆBθEpcθ,πq
BxEpcθ,πq
˙
px, yq“
ˆ ηcθ px, yqhpxqT
´BxhpxqT θT ηcθ px, yq` λ ηπpxq
˙
.
This amounts to verifying that Bxµcθ pxq“ θ Bxhpxqand that BθEpcθ,πqpx, yq“ ηcθ px, yqhpxqT .
The former holds by the linearity of derivation since µcθ pxq“ θ hpxqby definition; and the latter
holds because
BθEpcθ,πqpx, yq“ ´Bθ
2
@
ϵcθ py, xq, Σ´1
cθ ϵcθ py, xq
D
“´Bθ
2
@
y ´θ hpxq, Σ´1
cθ
`
y ´θ hpxq
˘D
“Σ´1
cθ
`
y ´θ hpxq
˘
hpxqT
“Σ´1
cθ ϵcθ py, xqhpxqT
“ηcθ px, yqhpxqT
as required.
Remark 7.3.17. From a biophysical point of view, the Hebb-Laplace doctrine so defined has
a notably suboptimal feature: the ‘synaptic’ forwards parameter θ : Θ is updated on the same
timescale λ as the parameter x : X that encodes the posterior mean, even though the latter
parameter is typically interpreted as encoding the activity of a population of neurons, which
therefore changes on a faster timescale than those neurons’ synapses. Not only is this important
for reasons of biological plausibility, but also for mathematical reasons: we should understand the
backwards activity as bundled over the forwards synapses, and a change in the parameter θ should
induce a corresponding ‘transport’ of x. An appropriately geometric treatment of compositional
approximate inference and predictive coding, resulting in bundles of open dynamical systems, is
again something that we leave to future work.
283
8. Future directions
A powerful motivation propelling the development of this thesis was the belief that science, and
particularly the cognitive sciences, will benefit from being supplied with well-typed compositional
foundations. In this final chapter, we survey a number of new vistas that we have glimpsed from
the vantage point of our results, and indicate routes that we might climb in order to see them better.
One important benefit of the categorical framework is that it helps us express ideas at a useful
level of abstraction, and thereby compare patterns across systems and phenomena of interest. As a
result, although our primary system of interest is the brain, we are aware that much of our work is
more diversely applicable, and so our survey here is similarly not restricted to neural systems. At
the same time, as neural systems are our finest examples of natural intelligence, we attempt to stay
grounded in current neuroscience.
Beyond the evident shortcomings of the work that we have presented—which we review
momentarily—we first consider how to use the categorical language of structure to incorporate
structure better into our models themselves (§8.1), with a particular focus on the brain’s “cognitive
maps” (§8.1.3). We will see that the compositional consideration of the structure of open systems
naturally leads us to consider societies of systems (§8.2), and hence the relationships between
compositional active inference and single- and multi-agent reinforcement learning and economic
game theory (§8.2.3), although our first priority in this section is the incorporation of action (§8.2.1)
and planning (§8.2.2) into the framework of statistical games. From our abstract vantage point, there
is little difference between societies of agents and collective natural systems such as ecosystems1,
and so we then consider the prospects for a compositional mathematics of life (§8.3). Finally, we
close with some thoughts on matters of fundamental theory (§8.4).
Before we wade into the thick of it, let us note three prominent examples of the aforementioned
evident shortcomings.
Firstly, the current presentation of copy-composite stochastic channels, and the bicategories of
lenses and statistical games that result from them, is quite inelegant: the necessity of coparameters
1After all, a single multicellular organism is itself a kind of society of agents.
284
introduces substantial complexity that is never repaid, because all coparameters arise from the
copy-composition of ordinary channels. This complexity infects the notion of approximate inference
doctrine, which could benefit both from simplification and from further exemplification, ideally by
examples drawn from beyond neuroscience.
Secondly, the generative models that we have considered are somehow ‘static’, despite our
interest in dynamical systems, and this warrants a satisfactory exploration of dynamical generative
models.
Thirdly, although we considered “lower level” neural circuit models in §3.3, we did not explicitly
connect our approximate inference doctrines to these more ‘biological’ models. A satisfactory
account of the Bayesian brain would of course span from abstract principles to detailed biology, a
relationship the elaboration of which we sadly we leave to future work.
Fortunately, although these three shortcomings may be pressing, we expect that the pursual of a
research programme akin to that sketched below would result in overcoming them.
8.1. Structured worlds
8.1.1. Bayesian sensor fusion
A situation that is common in natural embodied systems but which is not yet well treated by
current statistical and machine learning methods 2, particularly those that are most popular in
computational neuroscience, is that of sensor fusion. In this situation, one has a number of sensors
(such as cameras or retinal ganglion cells) which report spatially situated data, and where the sensor
fields overlap in the space; the problem is then how to combine these “local views” of the space
into a coherent global picture. Mathematically, fusing ‘local’ data into a ‘global’ representation is
the job of sheaves: a sheaf is a “spatially coherent data type”—something like a bundle for which
‘local’ sections can always be uniquely glued together into a global section—and sheaf theory and
the related fields of applied topology and cohomology allow us to judge when it is possible to
form a consensus, and quantify the obstructions to the formation of consensus; recent work has
also begun to suggest algorithms and dynamics by which we can construct consensus-forming
distributed sensor systems [123].
Sheaves therefore allow us to construct and to measure spatially structured data types, but missing
from the current sheaf-theoretic understanding of sensor fusion is a thorough treatment of belief
and uncertainty, especially from a Bayesian perspective. Since biological systems contain many
2This is beginning to change: recently, the use of sheaf-theoretic and other applied-topological devices has started to
penetrate machine learning [31, 266].
285
distributed sensor types, and each of these systems is constituted by many cells, the mathematics
of neural representations may be expected to be sheaf-theoretic. A first possible extension of the
work presented here, therefore, is to extend statistical games and approximate inference doctrines
(and hence the classes of model that they encompass) to structured data types such as sheaves.
Because statistical games and approximate inference doctrines are defined using lenses over an
abstract category of stochastic channels, we expect that the first step will be to consider categories
of channels between sheaves; recently, there has been work on categorifying lenses [63, 64], and we
expect that this may prove relevant here. We also expect that at this point the fibrational structure
of statistical games will again prove utile in order that losses may be correctly counted on any
overlaps. Fortunately, being similarly structured, sheaves and fibrations are natural partners, and so
we expect that a second spatial extension of the present work will be to exploit the latent geometric
structure of fibrations of statistical games.
In this context, we may also encounter connections to sheaf-theoretic approaches to ‘contextual-
ity’, in which answers to questions depend on (the topology of) how they are asked, and which
seems to lie at the heart of quantum nonlocality. It is notable that lenses originated in database
theory [34, 99] and that contextuality can also be observed in database systems [1, 58], and so at
this point, it may be possible to uncover the mathematical origins of ‘quantum-like’ psychological
effects [6, 49, 153], and relate them formally to other kinds of perceptual bistability that have
been interpreted in a Bayesian context [144, 170]. Sheaves come with a cohomology theory that
permits the quantification of the ‘disagreements’ that underlie such paradoxes [5, 45, 75], and
dynamical systems can be designed accordingly to minimize disagreements and thus seek consensus
[123–125]. We hope that these tools will supply new and mathematically enlightening models of
these psychological phenomena, while at the same time also suggesting new connections to work
on quantum-theoretic formulations of the free-energy framework itself [90, 91].
The adoption of a sheaf-theoretic framework in this way may furthermore illuminate connections
between computational neuroscience and machine learning. Graph neural networks [156, 157,
285], and their generalization in ‘geometric’ deep learning [44], are increasingly used to apply the
techniques of deep learning to arbitrarily structured domains, and, as indicated above, recent work
has found sheaves to supply a useful language for their study [31]. In a similar fashion, we expect
connections here to the structure of message passing algorithms [83, 190, 208, 283, 285] (also hinted
at by Sergeant-Perthuis [229]) and less conventional structured machine learning architectures
such as capsule networks [223]. Finally, each category of sheaves is naturally a topos [177], and
286
hence comes with its own rich internal language, modelling dependent type theory (cf. §2.3.4).
8.1.2. Learning structure and structured learning
Having considered the incorporation of structured data into the process of inference, we can
consider the incorporation of structure into the process of learning, and here we make an important
distinction between structured learning and learning structure. By the former, we mean extending
the process of learning to a structured setting (such as the sheaf-theoretic one of the preceding
section), whereas by the latter, we mean learning the underlying structures themselves. This latter
process is also known in the literature as structure learning [143, 260, 262], but in order to avoid
ambiguity, we swap the order of the two words.
The observation at the end of the preceding section, that each category of sheaves forms a topos,
is pertinent here, as dependent type theory formalizes a notion of logical ‘context’, containing
the “axioms that are valid in the present situation”, and determining which (non-tautological)
statements can be derived. In the categorical semantics of dependent type theory, the context is
witnessed by the object over which a slice category is defined, and so in some sense it defines
the “shape of the universe”. By the Grothendieck construction, there is a correspondence between
sheaves and certain bundles (objects of slice categories), and so (very roughly speaking) we can
think of structured inference and learning as taking place in appropriate slice categories.
In the same way that we motivated random dynamical systems (qua bundles, §6.2.3) through “pa-
rameterization by a noise source”, we can think of bundle morphisms as generalized parameterized
maps. The problem of learning structure then becomes a problem of generalized parameter-learning,
and much like this can be formalized by a ‘reparameterization’ in the Para construction (§3.2.1),
in this more general setting it is formalized by the “generalized reparameterization” of base-change
between topoi (cf. Remark 4.2.29). Base-change synthesizes notions of parallel transport, allowing
us to translate spatially-defined data coherently between spaces—and, in particular, along changes
of structure; recall our earlier remark about the importance of parallel transport to a biophysically-
plausible Hebb-Laplace doctrine (Remark 7.3.17). In this setting therefore, we expect that work on
functorial lenses [63], as well as work on functorial data migration [236, 242], may prove relevant.
At the same time, we expect this line of enquiry to clarify the relationships between our formalism
of approximate inference and other related work on the categorical foundations of cybernetics [51,
54], which have typically been studied in a differential rather than probabilistic setting [74]. We
expect the connection to be made via information geometry [9, 10, 195], where Bayesian inference
can be understood both using gradient descent [199] and as a kind of parallel transport [225].
287
8.1.3. Compositional cognitive cartography
Natural systems such as animals learn the structure of their environments as they explore them. We
will come below (§8.2.1) to the question of how to incorporate action—and hence exploration—into
the compositional framework that we have developed here, but meanwhile we note that the
topos-theoretic developments sketched above may provide a suitable setting in which to understand
the neural basis for navigation, and help explain how ostensibly ‘spatial’ navigation processes and
circuits are invariably involved in more abstract problem solving [24, 26, 27, 116, 178].
There are two key observations underlying this proposal. Firstly, a topos is not only a richly
structured category of spaces (or spatial types), but it can also be understood as a categorified space
itself [234]: in this context, we can call each categorified space a ‘little’ topos, and the category of
spaces itself is the corresponding ‘big’ topos; changes in spatial structure—witnessed by base-change
between little topoi—thus correspond to trajectories within the space represented by the big topos.
Secondly, under the free energy principle, there is a close relationship between beliefs about the
geometry of an environment and beliefs about expected future trajectories in that environment
[148]: fundamentally, this is also the idea underlying the “successsor representation” [78] of the
cognitive map, which says roughly that the brain’s representation of where it is is equivalently a
representation of where it soon expects to be [47, 256, 257]. Although there have been studies in the
informal scientific literature attempting to connect free-energy models of navigation, exploration,
and the cognitive map with the successor representation [185], and to place both of these in less
overtly spatial contexts [47, 205], there has not yet been a comprehensive mathematical treatment
explaining the structures that underlie this nexus.
By placing such a mathematical treatment in a topos-theoretic context, it may be possible to
make sense of the “logic of space” of topoi to explain why animals’ abstract problem-solving makes
use of their abilities for spatial navigation: in particular, proving a proposition is mathematically
analogous to finding a path from premise to conclusion. Moreover, in a spatial topos, the “truth
values” are no longer simply binary, but encode where a proposition is (believed to be) true; the
(sub)object classifier of a spatial topos encodes something like the “internal universe” of that topos,
or “the universe according to the system”.
To be successful, this mathematical treatment should be attentive to the results and proposals of
computational and theoretical neuroscience, and so we now turn to our second key observation: the
relationship between (believed) geometry and (expected) dynamics. This will require an extension
of statistical games and approximate inference to dynamical generative models; until this point,
288
our treatment has merely supplied inference (or ‘recognition’ [48]) dynamics to static models.
Through this extension, we should expect a connection to other work on dynamical inference, such
as filtering [105, 147] and particularly its emerging compositional treatment [271, 272].
Under the free-energy principle, and similarly under the successor representation, the expected
dynamics is a geodesic flow, which is by geodesy determined by beliefs about the spatial geometry.
But these beliefs in turn are not static: they depend on what the agent believes will happen [76,
186], and this has been suggested as an explanation for the ‘predictive’ nature of the cognitive
map [148]. The cognitive map has its central locus in the hippocampus [88, 193, 198], which we
may therefore understand as representing the base space over which the big topos is sliced; and
since changes-of-plan seem therefore to induce changes-of-base, we might see the ‘functional’
connectivity of the brain [245] as witnessing this mathematical structure.
Because the internal universe of the topos represented by the cognitive map is inherently
context-dependent, it seems to fit naturally with the subjectivist metaphysics implied by the free
energy framework—that the universe as experienced by an agent is a construction of that agent’s
internal model, as updated by approximate inference—and thus to provide a natural setting for the
mathematical study of phenomenology. Moreover, as categories of sheaves, agents’ internal topoi
encode the consensus formed by the distributed circuits and sensors that constitute their beliefs,
and this points a way towards understanding how societies of agents are able to inhabit shared
spaces about which they form a consensus between themselves: the mathematics of this shared
universe should be little different from the mathematics of a single agent’s internal universe.
Such multi-agent adaptive systems have been studied in the context of reinforcement learning (of
which more below), but this potential for the formation of ‘nested’ systems with shared universes
implied by consensus is not the only connection between cognitive cartography and reinforcement
learning, as it is in reinforcement learning that the successor representation originates. We therefore
hope that this line of enquiry may illuminate the relationship between reinforcement learning and
compositional active inference, to the basis of which we now turn.
8.2. Societies of systems
Adaptive agents being necessarily in interaction with an external environment, we saw in the
previous section how consideration of the compositional structure of agents’ internal maps of their
worlds easily leads to the consideration of societies of agents. However, in order for us to study
289
these, we first need to make the more basic step of incorporating action into the compositional
framework: a collection of purely passive agents is no society.
8.2.1. Active inference
The doctrines of approximate inference introduced in this thesis are inherently perceptual. As
we described in Remark 4.0.1, the forwards channel of a statistical game points “towards the
environment”, predicting the expected incoming sense-data, whereas the backwards channel points
from the environment into the agent, terminating in the agent’s most causally abstract beliefs. In
other contemporary work on categorical cybernetics, the orientation appears different: the forwards
channel of an open (economic) game, for instance, points along the direction of interaction in the
environment, in the direction of causality, from observations to actions [36, 119]; there is no room
for prediction and its inversion, and the two kinds of game seem somehow perpendicular.
In resolution of this apparent disagreement, we can observe that an open economic game does
have a perpendicular direction: a second3 dimension inhabited by the strategies. That is to say, an
open economic game is a lens externally parameterized by strategies, a function from the latter
to the former, and therefore formally much like our cilia (§6.3). This resemblence becomes even
closer when one considers the recent ‘diegetic’ formulation of open games, in which strategies
themselves can be updated using a backwards map from the arena of the game back into strategies
(or rather, strategy updates).
This suggests one way in which we can incorporate action and thereby shape the framework
of this thesis into a framework for active inference: the forwards channel should predict not only
sense-data incoming from the environment, but also the actions to be taken by the agent. Indeed
this matches the usual informal presentation of active inference, which adopts a channel of the
form XÑ‚S bA, where S is the space of sense-data, A the space of possible actions, and X the
‘latent’ space.
Yet at this point the formal similarity between compositional active inference and compositional
game theory again begins to recede, as a channel XÑ‚S bA is more like a “stochastic span” than
an open economic game’s player model Σ ÑrS, As. Moreover, we expect our active inference
systems to have a richer variety of patterns of interaction, being embodied in a world—in part,
this motivated our adoption of polynomial functors for structuring interaction. We therefore
expect the compositional theory of active inference to have forwards channels rather of the form
3Or third, if one remembers the monoidal structure.
290
XÑ‚ ř
a:A Sras, so that an agent’s sensorium depends on the configuration (or ‘action’) that it has
chosen.
This was the approach we sketched in our earlier work-in-progress on Polynomial Life [252],
where we suggested that polynomial functors supply a formalization of the notion of “Markov
blanket” used in the informal active inference literature to characterize the interaction boundaries
of adaptive systems [102, 158, 203] (a formalization that is situated at a useful level of technical
flexibility, being neither as abstract as the very general notion of interface adopted by categorical
systems theory [192], nor as concrete as simple products of spaces). In this way, we believe that a
fruitful direction in which to pursue a compositional theory of active inference is, like our theory
of open dynamical systems, as a Poly-algebra of statistical games. Fortunately, although the types
prove somewhat different, the structural resemblence between active inference and economic games
is maintained: in both cases, one has categories of lenses into the arena of interaction, indexed
by a category of interfaces, and thus in philosophical (and thus we expect also mathematical)
concordance with Myers’ double-categorical view of systems theory [192].
Once again, this line of enquiry naturally leads on to the consideration of multi-agent systems.
But before we come to that, there remain important questions about single-agent systems, and the
connection between single-agent active inference and the cousin of economic games, reinforcement
learning.
8.2.2. What is the type of a plan?
Each active inference system has an internal ‘latent’ state space equipped (by its generative model)
with a prior distribution, which represents the systems’s initial beliefs about the likelihood of
those states. As we have seen, the system can perceive, changing that distribution better to match
incoming sense data. And as we hope to see, it should also be able to act, affecting its environment
so that future states better match its initial beliefs. Perception and action are thus in general the
two dual ways in which a system can minimize its free energy, akin to the two degrees of freedom
available in a free energy game.
But a system that acts must necessarily be motivated towards some goal, even if that goal is
simply “stay alive” or “perform actiona”, and even though this goal may be adjusted by the system’s
perceptions. In order to realize its goal, whatever it may be, the system must enact a plan, however
trivial—and the informal literature on active inference encodes the plan into the system’s latent
prior. When it comes to static models, the prior may be simply a (static) distribution over the state
space itself; but in the dynamical case, it is typically a distribution over trajectories of states.
291
Such a distribution is often [76, 148] taken to encode likelihoods of hypothetical courses of action,
which one might call a policy4: the system then perceives and acts in order to implement its policy.
But the construction of this policy may involve a lot of data, such as the specification of goal states
and the accumulation of the “expected free energy” of trajectories in the context of those goals,
and so it seems unnecessarily crude to hide all of this data inside a single undifferentiated choice of
prior distribution.
This prompts us to ask, what is the form of this data, and how can we incorporate it into the
compositional framework? In other words, what is the type of a plan? These seem to us to be key
questions for future work.
8.2.3. Reinforcement learning, open games, and ecosystems
There is known to be a close relationship between active inference in Markov decision problems
(MDPs) and reinforcement learning [73], and it is through this relationship that one sees particularly
clearly the strangeness of encoding all the data of an agent’s policy in a single ‘prior’ state. This
relationship is seemingly not superficial, as there are hints of a deep structural connection.
First, recall that the standard algorithm for obtaining a Bellman-optimal policy for an MDP is
backward induction (otherwise known as dynamic programming ) [214, 284]5. It is now known that
backward induction is structured according a similar bidirectional pattern (the optic pattern) to
that of both Bayesian inference and reverse differentiation [128], and that MDPs themselves fit into
the associated general framework of open games [36] (which are governed by the same pattern).
Second, in the informal active inference approach to MDPs, the system in question counterfactually
evaluates policies using a backward-induction-like process, accumulating free energies in order to
score them [73]. It is this process that results in the prior discussed above, which is then updated
by the agent’s inference process. Future work will need to untangle this knot of interrelated
bidirectional processes; and as usual in categorical modelling, this means first writing them all
down precisely. We hope that, having done so, we will see how the whole picture emerges, and
how it relates to the developing geometric (or ‘diegetic’) framework in categorical cybernetics
[51] (possibly involving the further development of our notion of ‘cilia’ from §6.3). In particular,
since the free energy principle underlying active inference asserts a certain informal universality
(on which more in §8.3.1), we might also hope that the satisfactory development of compositional
4In the language of polynomial functors, this seems to be something like a distribution over the cofree comonad on the
system’s polynomial interface.
5Also see [29, 89, 106, 128, 261] for other presentations.
292
active inference might exhibit a universal property: that any other doctrine of cybernetic systems
factors uniquely through it.
The story of these connections will initially be told from the perspective of a single agent,
as backward induction only considers how to find a single policy for a single MDP; although
this policy may involve multiple agents, the implied global search entails a common controller:
the procedure doesn’t consider the factorisation of the agents. But casting this account into the
emerging framework of compositional active inference will point towards a bridge to multi-agent
reinforcement learning. For example, multi-agent RL often studies the emergence of collaboration,
and we might expect to see this represented in the formal structure, thereby understanding how to
incorporate the factorisation of agents into the compositional framework for backward induction
(which in turn may be helpful for designing collaborative ‘edge’ AI systems).
The resulting general account of multi-agent intelligence will encompass both reinforcement
learning and active inference, allowing us to understand their relative strengths and differences.
One seeming difference (at this early stage, and following our thinking above) is that compositional
active inference envisages the latent state spaces of agents as their “internal universes”, which
come along with sufficient structure that we might consider them as Umwelten (i.e., their subjective
worlds, in the sense of biosemiotics; see §8.3.2 below). Consequently, we should be able to study how
agents might come to consensus, thereby resolving their disagreements. And because agents are
embodied in a shared world within which they act, this process might involve planning cooperation,
at which point the teleological structure of compositional game theory may become important, as
cooperating agents will have to bet on spatiotemporally distributed actions. We hope therefore that
one distal outcome of this work will be a new and beneficial understanding of corporate activity.
Below, in §8.3, we will discuss how active inference and the free energy principle aim not only
to be theories of brains or other prominent intelligent systems, but rather universal theories of all
adaptive things. Consequently, their compositional treatment should extend in the ‘multi-agent’ case
not just to corporate activity, but to ecosystems more broadly. And, following the multicategorical
algebra latent throughout this thesis, it will undoubtedly prove natural, once we have considered a
single level of nesting of systems into ecosystems, to let the hierarchy continue to infinity, producing
a fractal-like structure. At this point, we should expect once more to make contact with topics such
as higher categories and type theory, particularly in operadic or opetopic (i.e., ‘directed’) forms;
synthetic approaches to mathematical physics; and iterated parameterization in categorical systems
theory.
293
It almost goes without saying that we should expect any framework resulting from this work to
capture existing models of collective active inference, such as recent work on spin glasses [129].
8.3. The mathematics of life
We move on to consider the relationships between compositional active inference and the
contemporary mathematics of life. We hope that compositional active inference may supply
part of the story of a modern theory of autopoiesis, the ability for life to recreate itself [274].
8.3.1. Bayesian mechanics and the free energy principle
Recently, it has been suggested in various venues [102, 204] that the free energy framework provides
a ‘universal’ way to understand the behaviour of adaptive systems, in the sense that, given a random
dynamical system, it may be possible to write down a generative model such that the dynamics
of the system can be modeled as performing inference on this model. In the language of the
conjectured compositional framework for active inference, we may be able to describe a canonical
statistical game that each given random dynamical system can be seen as playing.
If this is true, we should be able to express this canonicity precisely: in particular, it should
correspond to auniversal property. Since approximate inference doctrines already gives us functorial
ways to turn statistical games into dynamical systems, this suggests we should seek functors that
associate to each random dynamical system a statistical game; and we should expect these functors
to be adjoint (as morphisms of categories indexed by the systems’ interfaces). The desired universal
property would then be expressed by the adjunction. (Notably, adjunctions are at the heart of
recent synthetic approaches to mathematical physics [228].) This would constitute an important
mathematical step to establishing the universality of the free energy principle, or to establishing
the conditions that must be satisfied by any satisfactory successor.
Bayesian mechanics promises to build upon the nascent understanding of random dynamics via
inference [224] to supply a new theory of mechanics for statistical systems [215]. The present
formulation of Bayesian mechanics is constructed using mathematical tools from physics, but not
(yet) the kinds of compositional tool promoted in this thesis and further described above. We expect
that developments along the lines sketched here will unify the on-going development of Bayesian
mechanics (and the resulting understanding of non-equilibrium systems) with the new synthetic
understanding of mathematical physics. By casting all dynamics as abstract inference, we should
294
also expect this line of enquiry to begin to quantify the persistence of things and imbue much of
physics with an élan vital .
8.3.2. Biosemiotics
It is increasingly acknowledged that biological systems are characterized not only by information-
processing, but by communication [20]: an often overlooked fact about ‘information’ in the strict
mathematical sense is that it is only meaningful in context. In the original Nyquist-Hartley-Shannon
conception of information, this context is the communication of a predefined message over a noisy
channel [126, 197, 230]; but more generally, we might think of this context as simply “a question”, in
which case it is easy to see that information answering one question may not be useful in answering
another; or, in a more computational setting, we can see that the bits of an encrypted signal are
only useful in revealing the message if one has the decryption key.
Still, one often encounters descriptions of signals as containing n bits of information, without a
clear specification of about what . Mathematically, the confusion arises because information theory
is framed by classical probability, and the assumed context is always the problem of trying to
communicate a probability distribution over a pre-defined space X; and once the space is fixed, the
only question that can be asked is “what is the distribution?” (Mathematically, this is to say that in
the Markov category of classical stochastic channels, there are no non-trivial effects or costates.)
Yet, in the shared universe that we inhabit, there are more questions than this: in quantum
theory, for instance, one can ask many questions of the state of a system, by projecting the state
onto the subspace representing one’s question. (These projections are the non-trivial effects or
costates of quantum probability.) This act of projection is an act of interpretation of the message
encoded by the state at hand.
The emerging ‘biosemiotic’ reconceptualization of life explicitly acknowledges the importance
and universality of communication in context [20], proposing that in any such situation the
interpreting system necessarily has an internal representation of the external world (its Umwelt)
which is updated by interpreting incoming signals. We can in principle reconstruct the external
world by understanding it as “that which a collection of systems agrees about”: perhaps, then,
the shared universe (as determined topos-theoretically) of a fusion of active inference agents
is a good model of this ‘semiosphere’. It seems therefore that the mathematics resulting from
our work on internal universes and their interactions — and, more broadly, many of the formal
ingredients of compositional active inference — is well aligned with the informal structures of
biosemiotics, and so it may be desirable to re-express biosemiotics accordingly. In doing so, perhaps
295
the mathematics for a modern Bayesian subjectivist metaphysics will be found 6: for instance,
by expressing communication and its phenomenology as a geometric morphism (a generalized
base-change) between agents’ internal universes. More pragmatically, perhaps we will be able to
say precisely when some object may act as a symbol, and how systems may (learn to) manipulate
such symbols.
8.4. Fundamental theory
Future work connected to this thesis need not only be in applications; a number of purely theoretical
questions raise themselves, too.
8.4.1. Geometric methods for (structured) belief updating
The mathematics of ‘belief’ is in large part about replacing definite points with ‘fuzzier’ distributions
over them. In dependent type theory, we replace points with ‘terms’ (non-dependent terms are
exactly points): so a type theory with belief should somehow encompass “fuzzy terms”. Just as we
can replace points with distributions, we can replace dependent points with dependent distributions.
However, the standard replacement (moving from a category of functions to a category of stochastic
channels) obscures some of the ‘universal’ categorical structure that underpins the rules of type
theory. This standard replacement also misses something else: while it does allow for fuzzy terms,
it omits a model of fuzzy types; and we might well want to express beliefs about things whose
identity we are not quite sure. (This omission also seems to be related to the loss of universal
structure.)
There seem to be a couple of related resolutions to this puzzle. The first is to notice that replacing
points by distributions yields another space: the space of distributions over the original space; this
is akin to the move in dynamics from working with the random motion of states to working with
the deterministic motion of the distribution over states. This space of distributions has a particular
geometry (its information geometry ), and hence we should expect corresponding flavours of topos
and type theory. As we have indicated above, there is a move in fundamental mathematical physics
(cf. Schreiber [228]) to work ‘synthetically’, expressing concepts using the universal structures
of higher topoi. This has proven particularly fruitful in the context of differential systems, but
it is interesting that stochastic and differential structures bear a number of similarities 7: what
6Perhaps getting to the structural heart of the theory known as QBism [112, 113].
7Both conditional probability and differential calculus exhibit “chain rules” of similar types, which give rise to backwards
actions that compose via the lens rule: in the former case, Bayesian inversion; in the latter, reverse differentiation.
Categories that admit a differentiation operation have begun to be axiomatized (as differential categories [30]
296
are we to make of this? Does Bayesian inversion induce a canonical geometric morphism, by
which structured models may be coherently updated? We have already indicated above signs of a
relationship between inference and parallel transport; it seems that it may at least be fruitful to
consider ‘metric’ topoi, appropriately enriched.
The second resolution is to work with topoi as we work with random dynamical systems, by
noticing that randomness is often like “an uncertain parameterization”. By parameterizing a topos
with a category of noise sources, we may obtain a notion of “stochastic topos” in which the standard
operations of dependent type theory are available, but where each type and term may depend on
the realization of the noise source, thereby giving us notions of fuzzy term and fuzzy type. The
mathematics of such uncertainly parameterized topoi is as yet undeveloped, although we expect
that they should bear a relationship to the “topoi of beliefs” of the foregoing first resolution similar
to the relationship of Fokker-Planck to random dynamical systems.
Finally, we note that higher topoi behave abstractly somewhat like vector spaces (with sheaves like
categorified functionals). Since distributions are themselves like vectors, perhaps this observation
is a first step towards relating the resolutions.
8.4.2. Dynamics
Chapter 6 has supplied the beginnings of a compositional coalgebraic theory for open stochastic
and random dynamical systems in general time, and we hope that this theory could provide a
home for a modern account of non-equilibrium systems, with the category of polynomial functors
supplying a satisfactory account of these systems’ interfaces ( i.e., the boundaries across which
information flows, along which they compose, and through which they interact).
In this context, and in parallel to the abstract questions above, there are similar questions to be
asked specifically of dynamical systems. For instance, what is the precise relationship between
the category of Markov processes on an interface, and the category of random dynamical systems
on that interface? We know that categories of deterministic discrete-time polynomial coalgebras
are topoi [240], so does the same hold in general time? To what extent is the logic of our systems
related to coalgebraic logics [72, 138, 162, 207, 275]?
Besides these ‘parallel’ questions, there are a number of more technical ones. For instance, our
current definition of “Markov process on a polynomial interface” is somewhat inelegant, and we
and reverse-derivative categories [66]), and categories whose morphisms behave like stochastic channels are also
presently being axiomatized (in the framework of Markov categories [109]), but the connections between these
various formalisms are not yet clear. The similar structures indicate that the two families of axiomatisation may
have a common generalization.
297
seek to simplify it. Similarly, we believe that there is a better definition of “random dynamical
system on a polynomial interface” that may be obtained by a (different) generalization of the
category of polynomial functors, using random variables. And we know that a topology for the
cofree comonoid on an interface can be generated by the corresponding free monoid, which may
be relevant for understanding the topological structure of open systems. An important set of open
questions about open random dynamical systems in this framework comes from attempting to
import notions about random systems from the classical ‘closed’ setting: fundamentally, we ask,
does this framework indeed supply a satisfactory setting in which to understand stochastic systems
away from equilibrium?
8.4.3. Computation
The early 21st century understanding of biological systems as information-processing involves
treating them as computational, but remarkably lacks a precise concept of what it means for a
system to compute, other than in the context of artificial machines. To us, it seems that a crisper
understanding of computation in general might begin with the slogan that “computation is dynamics
plus semantics”, which is philosophically aligned with the semiotic understanding of biological
information-processing sketched above: for example, we know that attractor networks in the brain
can informally be understood as computational [11], but these are ‘continuous’ systems for which
we do not yet have a good corresponding concept of algorithm (and it is upon algorithms that our
current understanding is built). But what more is an algorithm than a description of a discrete-time
open dynamical system? The quality that makes an algorithm computational is that its states
or its outputs correspond to some quantity of interest, and that it reaches a fixed point (it halts)
at the target quantity when the computation is complete. If this intuition is correct, then a new
understanding of computation may follow the semiotic understanding of information-processing
that we propose above: perhaps we could say more precisely that computation is the dynamics of
semiosis? The time is right for such a reconceptualization, as human-made systems increasingly
move away from von Neumann architectures towards more biosimilar ones (such as memristors,
optical processors, neuromorphic technology, graph processors, or even many-core and mesh-based
evolutions of classical processors).
298
A. Auxiliary material
A.1. From monads to multicategories
The assignment of domain and codomain to the morphisms of a small category C constitutes a
pair of functions C1 Ñ C0, which we can write as a span, C0
codÐ Ý ÝC1
domÝÝÑC0. Similarly, the
assignment of domain and codomain to the morphisms of a multicategory M constitutes a span
M0
codÐ Ý ÝM1
domÝÝÑListpM0q. This observation was used by Leinster [168] to construct a general
framework for constructing multicategories, replacing List with an arbitrary ‘Cartesian’ monad T,
which opens the way to a connection between monad algebras and multicategory algebras. In this
section, we explore this connection, starting by defining categories of spans.
Definition A.1.1. Suppose A and B are two objects in a category C. We will write a span from A
to B as pX, xq: A xA
Ð Ý ÝX xB
Ý Ý ÑB, and call X the apex of the span and xA, xB its legs or projections.
The category of spans from A to B, denoted SpanpA, Bqhas spans pX, xqas objects, and the
morphisms f : pX, xqÑp X1, x1qare morphisms f : X ÑX1in C that commute with the spans,
as in the following diagram:
X
A B
X1
xA
x1
A
xB
x1
B
f
We can treat the categories SpanpA, Bqas the hom categories of a bicategory.
Definition A.1.2. Suppose C is a category with all pullbacks. The bicategory of spans in C, denoted
Span, has for objects the objects of C, and for hom-categories the categories SpanpA, Bqof spans
from A to B. Given spans pX, xq : A Ñ B and pY, yq : B Ñ C, their horizontal composite
299
pY, yq˝p X, xq: A ÑC is the pullback span defined by
X ˆB Y
X Y
A B C
projX
xA
projY
yCxB yB
{
.
If pX1, x1q : A Ñ B and pY 1, y1q : B Ñ C are also spans with f : pX, xq ñ pX1, x1qand
g : pY, yqñp Y 1, y1qvertical morphisms, the horizontal composite of f and g is also defined by
pullback as f ˆB g : pY, yq˝p X, xqñp Y 1, y1q˝p X1, x1q. The identity span on an object A is
pA, idq: A ù ùA ù ùA.
If the ambient category C is not clear from the context, we will write SpanC to denote the
bicategory of spans in C.
Remark A.1.3. Note that Span really is a bicategory rather than a 2-category: since the horizontal
composition of spans is defined by pullback, it is only defined up to isomorphism. Consequently,
the composition of spans can in general only be associative and unital up to isomorphism, rather
than the strict equality required by a 2-category.
Now, recall that ‘monad’ is another name for “monoid in a bicategory”, where the bicategory has
so far been taken to be Cat: but it need not be.
Remark A.1.4. Since CC is the endomorphism monoid on C in the bicategory Cat, we can
generalize the preceding definition of monad to any bicategory B: a monad in a bicategory B is
simply a monoid in B, as defined in Remark 3.4.9. That is, a monad in B is a monoid object in
the monoidal category
`
Bpb, bq, ˝, idb
˘
for some choice of 0-cell b : B, where ˝denotes horizontal
composition. Explicitly, a monadpt, µ, ηqin B is a 1-cellt : b Ñb, a multiplication 2-cellµ : t˝t ñt,
and a unit 2-cell η : idb ñt, such that the associativity and unitality diagrams commute in Bpb, bq:
ttt tt
tt t
µt
tµ
µ
µ
and
t tt t
t
µ
ηt tη
With this more general notion of monad, we obtain another monadic definition of “small category”,
to add to the explicit Definition 2.1.2 and the monad-algebraic Example 3.4.19.
Proposition A.1.5. Small categories are monads in SpanSet.
300
Proof. A monad inSpanSet is a choice of objectC0 and monoid inSpanSetpC0, C0q. Such a monoid
is a span of sets C : C0
codÐ Ý ÝC1
domÝÝÑC0 along with functions ‚: C1 ˆC0 C1 ÑC1 and id : C0 ÑC1.
The set C1 ˆC0 C1 is the apex of the pullback span C ˝C as in
C1 ˆC0 C1
C1 C1
C0 C0 C0
cod dom domcod
{
so that ‚and id make the following diagrams commute:
C1 ˆC0 C1
C1 C1
C0 C1 C0
cod dom
cod dom
‚ and
C0
C0 C1 C0
id
cod dom
This means that codpg ‚fq“ codpgqand dompg ‚fq“ dompfq, and codpidxq“ dompidxq“ x.
It is easy to check that pC, ‚, idqtherefore constitutes the data of a small category; moreover, the
functions ‚and id must satisfy the monoid axioms of associativity and (right and left) unitality,
which correspond directly to the categorical axioms of associativity and unitality.
As we indicated at the opening of this section, by generalizing to a category of ‘spans’ of the
form A Ð X Ñ T B, we can use the preceding result to produce generalized multicategories
whose morphisms have domains “in the shape of T”. Since the horizontal composition of spans is
by pullback, we need an extra condition on the monad T to ensure that pullbacks of T-spans are
well-defined. This condition is known as ‘Cartesianness’.
Definition A.1.6. A Cartesian natural transformation between functors F and G is a natural
transformation α : F ñG for which every naturality square is a pullback:
F a Ga
F b Gb
αa
GfF f
αb
{
A Cartesian monad is a monad pT : C Ñ C, µ, ηqsuch that C has all pullbacks, T preserves
these pullbacks (sending pullback squares to pullback squares), and µ and η are Cartesian natural
transformations.
301
Definition A.1.7. Suppose T is a monad on C. A T-span from A to B is a span from A to T Bin
C. The category of T-spans from A to B, denoted SpanT pA, Bqhas T-spans as objects, and the
morphisms f : pX, xqÑp X1, x1qare morphisms f : X ÑX1in C that commute with the spans,
as in the diagram
X
A T B
X1
xA
x1
A
xB
x1
B
f .
Definition A.1.8. Suppose pT, µ, ηqis a Cartesian monad on C. The bicategory of T-spans
in C, denoted SpanT , has for objects the objects of C, and for hom-categories the categories
SpanT pA, Bqof T-spans from A to B. Given T-spans pX, xq: A ÑB and pY, yq: B ÑC, their
horizontal composite pY, yq˝p X, xq: A ÑC is the outer T-span in the diagram
X ˆT BT Y
X T Y
A T B T T C
T C
projX
xA
projTY
T yCxB T yB
{
µC
.
If pX1, x1q : A Ñ B and pY 1, y1q : B Ñ C are also T-spans with f : pX, xq ñ pX1, x1qand
g : pY, yq ñ pY 1, y1qvertical morphisms, the horizontal composite of f and g is defined as
f ˆT BT gaccordingly. The identity span on an object A is A idA
Ð Ý ÝA
ηA
ÝÑT A.
With these notions to hand, the general concept of T-multicategory is easy to define.
Definition A.1.9 (Leinster [168, Def. 4.2.2]) . Suppose T is a Cartesian monad on C. A T-
multicategory is a monad in the bicategory SpanT of T-spans.
And of course we can recover our earlier examples of category shapes accordingly.
Example A.1.10. The identity monad on a category with all pullbacks is trivially a Cartesian
monad. Therefore, taking T “idSet to be the identity monad on Set, we immediately see that an
idSet-multicategory is a small category.
Example A.1.11 (Leinster [168, Examples 4.1.4 and 4.2.7]). The free monoid monad List : Set Ñ
Set is Cartesian. Unpacking the definitions, we find that a List-multicategory is precisely a
multicategory as in Definition 3.3.1.
302
At this point, we can sketch how multicategory algebras correspond to monad algebras, referring
the reader to Leinster [168, §4.3] for the details. The basic picture is that, ifT : C ÑC is a Cartesian
monad and M is a T-multicategory, then one can obtain functorially a monad TM on the slice
C{M0 of C over the object M0 of M-objects. The algebras α : TMpX, pqÑp X, pqof this monad
are morphisms α : TMX ÑX as in the commuting diagram
TMX X
T X M1
TM0 M0
p
coddomT p
{
α
where TMpX, pqis defined as the bundle TMX ÑM0 on the right leg of the pullback square.
To get a sense for how this works, consider the case where T “idSet: a T-multicategory is then
simply a small category C, and as Leinster [168, Example 4.3.2] shows, its algebras are functors
C ÑSet.
303
B. Bibliography
[1] Samson Abramsky and Giovanni Carù. “Non-Locality, Contextuality and Valuation Algebras:
A General Theory of Disagreement”. In: Philosophical Transactions of the Royal Society
A: Mathematical, Physical and Engineering Sciences 377.2157 (09/2019), p. 20190036. doi:
10.1098/rsta.2019.0036.
[2] Samson Abramsky and Bob Coecke. “A Categorical Semantics of Quantum Protocols”. In:
Logic in Computer Science, 2004. Proceedings of the 19th Annual IEEE Symposium On . IEEE.
2004, pp. 415–425.
[3] Samson Abramsky and Viktor Winschel. “Coalgebraic Analysis of Subgame-perfect
Equilibria in Infinite Games without Discounting”. 10/16/2012. doi: 10 . 1017 /
S0960129515000365. arXiv: 1210.4537 [cs.GT].
[4] Samson Abramsky et al. “Categorical Methods at the Crossroads (Dagstuhl Perspectives
Workshop 14182)”. In: (2014). In collab. with Marc Herbstritt, 15 pages.doi: 10.4230/
DAGREP.4.4.49. url: http://drops.dagstuhl.de/opus/volltexte/
2014/4618/ (visited on 12/23/2022).
[5] Samson Abramsky et al. “Contextuality, Cohomology and Paradox”. In: 24th EACSL Annual
Conference on Computer Science Logic (CSL 2015), Leibniz International Proceedings in
Informatics (LIPIcs), 41: 211-228, 2015 (02/10/2015). doi: 10 . 4230 / LIPIcs . CSL .
2015.211. arXiv: 1502.03097 [quant-ph].
[6] Diederik Aerts et al. “Quantum Cognition Beyond Hilbert Space: Fundamentals and
Applications”. In: Lecture Notes in Computer Science (2017), pp. 81–98. issn: 1611-3349.
doi: 10.1007/978-3-319-52289-0_7 .
[7] Danel Ahman and Tarmo Uustalu. “Directed Containers as Categories”. In: EPTCS 207,
2016, pp. 89-98 (04/05/2016). doi: 10.4204/EPTCS.207.5 . arXiv: 1604.01187
[cs.LO].
[8] Thosten Altenkirch, James Chapman, and Tarmo Uustalu. “Monads Need Not Be Endo-
functors”. In: Logical Methods in Computer Science Volume 11, Issue 1 (03/06/2015), p. 928.
issn: 1860-5974. doi: 10.2168/LMCS-11(1:3)2015 . url: https://lmcs.
episciences.org/928 (visited on 10/16/2023).
[9] Shun-ichi Amari. “Information Geometry”. In: Contemporary Mathematics . Ed. by Hanna
Nencka and Jean-Pierre Bourguignon. Vol. 203. Providence, Rhode Island: American
Mathematical Society, 1997, pp. 81–95. isbn: 978-0-8218-0607-4 978-0-8218-7794-4. doi:
304
10.1090/conm/203/02554 . url: http://www.ams.org/conm/203/
(visited on 12/23/2022).
[10] Shun’ichi Amari. Information Geometry and Its Applications . Applied Mathematical Sciences
volume 194. Japan: Springer, 2016. 374 pp. isbn: 978-4-431-55977-1.
[11] Daniel J Amit. Modeling Brain Function: The World of Attractor Neural Networks . Cambridge
University Press, 1992.
[12] Britt Anderson et al. “Category Theory for Cognitive Science”. In: Proceedings of the Annual
Meeting of the Cognitive Science Society . Vol. 44. 2022.
[13] Ludwig Arnold. Random Dynamical Systems . Springer Berlin Heidelberg, 1998. 612 pp.
isbn: 978-3-540-63758-5. doi: 10.1007/978- 3- 662- 12878- 7. url: https:
//doi.org/10.1007/978-3-662-12878-7 .
[14] Robert Atkey. “Syntax and Semantics of Quantitative Type Theory”. In: Proceedings of
the 33rd Annual ACM/IEEE Symposium on Logic in Computer Science . ACM, 07/2018. doi:
10.1145/3209108.3209189.
[15] Robert J. Aumann. “Borel Structures for Function Spaces”. In: Illinois Journal of Math-
ematics 5.4 (12/01/1961). issn: 0019-2082. doi: 10.1215/ijm/1255631584 . url:
https : / / projecteuclid . org / journals / illinois - journal -
of - mathematics / volume - 5 / issue - 4 / Borel - structures - for -
function - spaces / 10 . 1215 / ijm / 1255631584 . full(visited on
06/20/2023).
[16] John C. Baez and James Dolan. “Higher-dimensional Algebra and Topological Quantum
Field Theory”. In: Journal of Mathematical Physics 36.11 (11/1995), pp. 6073–6105. issn:
0022-2488, 1089-7658. doi: 10.1063/1.531236. url: http://aip.scitation.
org/doi/10.1063/1.531236 (visited on 12/23/2022).
[17] John C. Baez and James Dolan. “Higher-Dimensional Algebra III: N-Categories and the
Algebra of Opetopes”. In: Adv. Math. 135 (1998), 145-206. (02/10/1997). arXiv: q-alg/
9702014.
[18] John C. Baez et al. “Network Models”. In:Theory and Applications of Categories, Vol. 35, 2020,
No. 20, pp 700-744 (10/31/2017). arXiv: 1711.00037 [math.CT].
[19] Igor Baković. “Fibrations of Bicategories”. In: (2010). url: https://www2.irb.hr/
korisnici/ibakovic/groth2fib.pdf.
[20] Marcello Barbieri, ed. Introduction to Biosemiotics The New Biological Synthesis. The New
Biological Synthesis . Springer, 2007. isbn: 978-1-4020-4814-2.
[21] A. M. Bastos et al. “Canonical Microcircuits for Predictive Coding”. In:Neuron 76.4 (11/2012),
pp. 695–711. doi: 10.1016/j.neuron.2012.10.038. pmid: 23177956.
305
[22] Matthew J. Beal and Zoubin Ghahramani. “The Variational Bayesian EM Algorithm for
Incomplete Data: With Application to Scoring Graphical Model Structures”. In: Bayesian
Statistics. Vol. 7. Valencia: Oxford University Press, 2003.
[23] C. Glenn Begley and Lee M. Ellis. “Raise Standards for Preclinical Cancer Research”. In:
Nature 483.7391 (03/2012), pp. 531–533. doi: 10.1038/483531a.
[24] Timothy EJ Behrens et al. “What Is a Cognitive Map? Organizing Knowledge for Flexible
Behavior”. In: Neuron 100.2 (2018), pp. 490–509.
[25] Richard Bellman. “The Theory of Dynamic Programming”. In: Bulletin of the American
Mathematical Society 60.6 (1954), pp. 503–515. issn: 0273-0979, 1088-9485. doi: 10.1090/
S0002-9904-1954-09848-8 . url: https://www.ams.org/bull/1954-
60-06/S0002-9904-1954-09848-8/ (visited on 05/17/2023).
[26] Jacob LS Bellmund et al. “Navigating Cognition: Spatial Codes for Human Thinking”. In:
Science (New York, N.Y.) 362.6415 (2018, 2018-11), eaat6766. doi: 10.1126/science.
aat6766.
[27] Silvia Bernardi et al. “The Geometry of Abstraction in Hippocampus and Pre-Frontal Cortex”.
In: (09/2018). doi: 10.1101/408633.
[28] Martin Biehl, Felix A. Pollock, and Ryota Kanai. “A Technical Critique of the Free Energy
Principle as Presented in "Life as We Know It" and Related Works”. 01/12/2020. arXiv:
2001.06408v2 [q-bio.NC].
[29] Achim Blumensath and Viktor Winschel. “A Compositional Coalgebraic Semantics of
Strategic Games”. 12/22/2017. arXiv: 1712.08381v1 [cs.GT].
[30] R. F. Blute, J. R. B. Cockett, and R. A. G. Seely. “Differential Categories”. In: Math-
ematical Structures in Computer Science 16.06 (11/2006), p. 1049. doi: 10 . 1017 /
s0960129506005676.
[31] Cristian Bodnar et al. Neural Sheaf Diffusion: A Topological Perspective on Heterophily and
Oversmoothing in GNNs . 10/21/2022. arXiv: 2202.04579 [cs, math]. url: http:
//arxiv.org/abs/2202.04579 (visited on 12/23/2022). preprint.
[32] M Boerlin et al. “Predictive Coding of Dynamical Variables in Balanced Spiking Networks”.
In: PLoS computational biology 9.11 (2013), e1003258. url: http://journals.plos.
org/ploscompbiol/article?id=10.1371/journal.pcbi.1003258.
[33] Rafal Bogacz. “A Tutorial on the Free-Energy Framework for Modelling Perception and
Learning”. In: Journal of Mathematical Psychology 76 (02/2017), pp. 198–211. doi: 10.
1016/j.jmp.2015.11.003.
[34] Aaron Bohannon, Benjamin C Pierce, and Jeffrey A Vaughan. “Relational Lenses: A Language
for Updatable Views”. In: Proceedings of the Twenty-Fifth ACM SIGMOD-SIGACT-SIGART
Symposium on Principles of Database Systems . ACM. 2006, pp. 338–347.
306
[35] Guillaume Boisseau. “String Diagrams for Optics”. 02/11/2020. arXiv: 2002.11480v1
[math.CT].
[36] Joe Bolt, Jules Hedges, and Philipp Zahn. “Bayesian Open Games”. 10/08/2019. arXiv:
1910.03656v1 [cs.GT].
[37] Joe Bolt et al. “Interacting Conceptual Spaces I: Grammatical Composition of Concepts”. In:
Springer, 2019, 2017, pp. 151–181. arXiv: 1703.08314 [cs.LO].
[38] Evert A. Boonstra and Heleen A. Slagter. “The Dialectics of Free Energy Minimization”. In:
Frontiers in Systems Neuroscience 13 (09/10/2019), p. 42. issn: 1662-5137. doi: 10.3389/
fnsys.2019.00042 . url: https://www.frontiersin.org/article/
10.3389/fnsys.2019.00042/full (visited on 12/23/2022).
[39] Francis Borceux. Handbook of Categorical Algebra 2. Categories and Structures . Vol. 51.
Encyclopedia of Mathematics and Its Applications. Cambridge University Press, Cambridge,
1994. isbn: 978-0-521-44179-7.
[40] Tai-Danae Bradley. “Entropy as a Topological Operad Derivation”. In: Entropy 23.9
(09/09/2021), p. 1195. issn: 1099-4300. doi: 10 . 3390 / e23091195. url: https :
//www.mdpi.com/1099-4300/23/9/1195 (visited on 05/02/2023).
[41] Dylan Braithwaite and Jules Hedges. Dependent Bayesian Lenses: Categories of Bidirectional
Markov Kernels with Canonical Bayesian Inversion . 09/29/2022. arXiv: 2209.14728 [cs,
math, stat]. url: http : / / arxiv . org / abs / 2209 . 14728(visited on
12/17/2022). preprint.
[42] Dylan Braithwaite, Jules Hedges, and Toby St Clere Smithe. The Compositional Structure
of Bayesian Inference . 05/10/2023. arXiv: 2305.06112 [cs, math]. url: http:
//arxiv.org/abs/2305.06112 (visited on 05/11/2023). preprint.
[43] Dylan Braithwaite et al. Fibre Optics . 12/21/2021. arXiv: 2112.11145 [math.CT].
preprint.
[44] Michael M. Bronstein et al. Geometric Deep Learning: Grids, Groups, Graphs, Geodesics,
and Gauges . 05/02/2021. arXiv: 2104.13478 [cs, stat]. url: http://arxiv.
org/abs/2104.13478 (visited on 12/23/2022). preprint.
[45] Kenneth S. Brown. “Abstract Homotopy Theory and Generalized Sheaf Cohomology”.
In: Transactions of the American Mathematical Society 186 (1973), pp. 419–419. doi: 10.
1090/s0002-9947-1973-0341469-9 . JSTOR: 1996573. url: https://
www.jstor.org/stable/1996573.
[46] N. G. de Bruijn. Asymptotic Methods in Analysis . Dover ed. New York: Dover Publications,
1981. 200 pp. isbn: 978-0-486-64221-5.
[47] Iva K. Brunec and Ida Momennejad. “Predictive Representations in Hippocampal and
Prefrontal Hierarchies”. In: bioRxiv : the preprint server for biology (2019). doi: 10.1101/
307
786434. eprint: https://www.biorxiv.org/content/early/2019/
09/30/786434.full.pdf. url: https://www.biorxiv.org/content/
early/2019/09/30/786434.
[48] Christopher L Buckley et al. “The Free Energy Principle for Action and Perception: A
Mathematical Review”. In: Journal of Mathematical Psychology 81 (05/24/2017), pp. 55–79.
doi: 10.1016/j.jmp.2017.09.004. arXiv: 1705.09156v1 [q-bio.NC].
[49] Jerome R Busemeyer and Peter D Bruza. Quantum Models of Cognition and Decision .
Cambridge University Press, 2012.
[50] Matteo Capucci. Seeing Double through Dependent Optics . 04/22/2022. arXiv: 2204.10708
[math.CT]. preprint.
[51] Matteo Capucci. “Diegetic Representation of Feedback in Open Games”. In: Electronic
Proceedings in Theoretical Computer Science . Applied Category Theory 2022. Vol. 380.
08/07/2023, pp. 145–158. doi: 10.4204/EPTCS.380.9 . url: http://arxiv.
org/abs/2206.12338v3 (visited on 09/29/2023).
[52] Matteo Capucci and Bruno Gavranović. “Actegories for the Working Amthematician”.
03/30/2022. arXiv: 2203.16351 [math.CT].
[53] Matteo Capucci, Bruno Gavranović, and Toby St Clere Smithe. “Parameterized Categories
and Categories by Proxy”. In: Category Theory 2021 . Category Theory. Genoa, 2021. url:
https://www.youtube.com/watch?v=lYs6Bs6JK9Q.
[54] Matteo Capucci et al. “Towards Foundations of Categorical Cybernetics”. In: Electronic
Proceedings in Theoretical Computer Science . Applied Category Theory 2021. Vol. 372.
11/03/2022, pp. 235–248. doi: 10.4204/EPTCS.372.17 . url: http://arxiv.
org/abs/2105.06332v2 (visited on 09/29/2023).
[55] Olivia Caramello. A Topos-Theoretic Approach to Stone-type Dualities . 03/17/2011. arXiv:
1103.3493 [math]. url: http://arxiv.org/abs/1103.3493 (visited on
05/17/2023). preprint.
[56] Henri Cartan. “Variétés Analytiques Complexes et Cohomologie”. In: Colloque Sur Les
Fonctions de Plusieurs Variables Tenu a Bruxelles. 1953. url: https://www.inp.
nsk.su/~silagadz/Cartan.pdf.
[57] Henri Paul Cartan and Samuel Eilenberg. Homological Algebra . Princeton Landmarks in
Mathematics and Physics. Princeton, N.J: Princeton University Press, 1956. 390 pp. isbn:
978-0-691-04991-5.
[58] Giovanni Carù. “Logical and Topological Contextuality in Quantum Mechanics and Beyond”.
University of Oxford / University of Oxford / University of Oxford, 2019. url: https:
/ / ora . ox . ac . uk / objects / uuid : 9bc2335a - b627 - 463b - 9526 -
f4b881b0fbbf.
308
[59] Eugenia Cheng. “Weak N-Categories: Opetopic and Multitopic Foundations”. 04/21/2003.
arXiv: math/0304277.
[60] Kenta Cho and Bart Jacobs. “Disintegration and Bayesian Inversion via String Dia-
grams”. In: Math. Struct. Comp. Sci. 29 (2019) 938-971 (08/29/2017). doi: 10 . 1017 /
S0960129518000488. arXiv: 1709.00322v3 [cs.AI].
[61] Kenta Cho et al. “An Introduction to Effectus Theory”. 2015. arXiv: 1512 . 05813
[cs.LO].
[62] Corina Cirstea. “An Algebra-Coalgebra Framework for System Specification”. In: Electronic
Notes in Theoretical Computer Science 33 (2000), pp. 80–110.
[63] Bryce Clarke. “Internal Lenses as Functors and Cofunctors”. In: Electronic Proceedings
in Theoretical Computer Science 323 (09/15/2020), pp. 183–195. issn: 2075-2180. doi: 10.
4204/EPTCS.323.13 . url: http://arxiv.org/abs/2009.06835v1
(visited on 12/23/2022).
[64] Bryce Clarke and Matthew Di Meglio. An Introduction to Enriched Cofunctors . 09/02/2022.
arXiv: 2209.01144 [math]. url: http://arxiv.org/abs/2209.01144
(visited on 12/23/2022). preprint.
[65] Bryce Clarke et al. “Profunctor Optics, a Categorical Update”. 01/21/2020. arXiv: 2001.
07488v1 [cs.PL].
[66] Robin Cockett et al. “Reverse Derivative Categories”. In: CSL 2020 . 28th International
Conference on Computer Science Logic. 2019-10-15, 2020. arXiv: 1910 . 07065v1
[cs.LO].
[67] Bob Coecke. “The Mathematics of Text Structure”. 2019. arXiv: 1904.03478.
[68] Bob Coecke and Aleks Kissinger. “Categorical Quantum Mechanics I: Causal Quantum
Processes”. 10/19/2015. arXiv: 1510.05468v3 [quant-ph].
[69] Bob Coecke and Aleks Kissinger. “Categorical Quantum Mechanics II: Classical-Quantum
Interaction”. 05/27/2016. arXiv: 1605.08617v1 [quant-ph].
[70] Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. “Mathematical Foundations for a
Compositional Distributional Model of Meaning”. In: Lambek Festschirft, special issue of
Linguistic Analysis, 2010. (03/2010). eprint: 1003.4394. url: https://arxiv.org/
abs/1003.4394.
[71] Matteo Colombo and Cory Wright. “First Principles in the Life Sciences: The Free-Energy
Principle, Organicism, and Mechanism”. In: Synthese 198.S14 (06/2021), pp. 3463–3488.
issn: 0039-7857, 1573-0964. doi: 10.1007/s11229-018-01932-w . url: https:
//link.springer.com/10.1007/s11229- 018- 01932- w (visited on
12/23/2022).
309
[72] David Corfield. Coalgebraic Modal Logic . 2009. url: https://golem.ph.utexas.
edu/category/2009/09/coalgebraic_modal_logic.html.
[73] Lancelot Da Costa et al. “Reward Maximisation through Discrete Active Inference”.
07/11/2022. arXiv: 2009.08111 [cs.AI].
[74] Geoffrey S. H. Cruttwell et al. “Categorical Foundations of Gradient-Based Learning”. In:
Programming Languages and Systems . Springer International Publishing, 2022, pp. 1–28.
doi: 10.1007/978-3-030-99336-8_1 .
[75] Justin Curry. “Sheaves, Cosheaves and Applications”. 03/13/2013. arXiv: 1303.3255v2
[math.AT].
[76] Lancelot Da Costa et al. “Active Inference on Discrete State-Spaces: A Synthesis”. 01/20/2020.
arXiv: 2001.07203 [q-bio.NC].
[77] Brian Day and Ross Street. “Monoidal Bicategories and Hopf Algebroids”. In: Advances
in Mathematics 129.1 (07/1997), pp. 99–157. issn: 00018708. doi: 10 . 1006 / aima .
1997.1649 . url: https://linkinghub.elsevier.com/retrieve/
pii/S0001870897916492 (visited on 12/17/2022).
[78] Peter Dayan. “Improving Generalization for Temporal Difference Learning: The Successor
Representation”. In: Neural Computation 5.4 (07/1993), pp. 613–624. issn: 1530-888X. doi:
10.1162/neco.1993.5.4.613.
[79] Peter Dayan and Laurence F Abbott. Theoretical Neuroscience . Vol. 806. Cambridge, MA:
MIT Press, 2001.
[80] Peter Dayan et al. “The Helmholtz Machine”. In: Neural Computation 7.5 (09/1995), pp. 889–
904. doi: 10.1162/neco.1995.7.5.889 . url: http://dx.doi.org/10.
1162/neco.1995.7.5.889.
[81] Sophie Deneve. “Making Decisions with Unknown Sensory Reliability”. In: Frontiers in
neuroscience 6 (2012).url: http://www.ncbi.nlm.nih.gov/pmc/articles/
PMC3367295/.
[82] Sophie Denève and Christian K Machens. “Efficient Codes and Balanced Networks”. In:
Nature neuroscience 19.3 (02/2016), pp. 375–382. issn: 1546-1726. doi: 10.1038/nn.
4243.
[83] Bert de Vries and Karl J. Friston. “A Factor Graph Description of Deep Temporal Active
Inference”. In: Frontiers in Computational Neuroscience 11 (10/2017). doi: 10.3389/
fncom.2017.00095.
[84] A Ehresmann and J Vanbremeersch. “Hierarchical Evolutive Systems: A Mathematical
Model for Complex Systems”. In: Bulletin of Mathematical Biology 49.1 (1987), pp. 13–50.
issn: 00928240. doi: 10.1016/S0092-8240(87)80033-2. url: http://link.
springer.com/10.1016/S0092-8240(87)80033-2 (visited on 12/23/2022).
310
[85] A.C. Ehresmann and J.P. Vanbremeersch. Memory Evolutive Systems; Hierarchy, Emergence,
Cognition. Studies in Multidisciplinarity. Elsevier Science, 2007. isbn: 978-0-08-055541-6.
url: https://books.google.co.uk/books?id=OqcYQbY79GMC.
[86] Andrée C. Ehresmann and Jean-Paul Vanbremeersch. “The Memory Evolutive Systems as
a Model of Rosen’s Organisms – (Metabolic, Replication) Systems”. In: Axiomathes 16.1-2
(03/2006), pp. 137–154. issn: 1572-8390. doi: 10.1007/s10516-005-6001-0. url:
http://link.springer.com/10.1007/s10516-005-6001-0.
[87] Samuel Eilenberg and Saunders MacLane. “General Theory of Natural Equivalences”. In:
Transactions of the American Mathematical Society 58.0 (1945), pp. 231–294. issn: 0002-9947,
1088-6850. doi: 10 . 1090 / S0002 - 9947 - 1945 - 0013131 - 6. url: https :
//www.ams.org/tran/1945-058-00/S0002-9947-1945-0013131-
6/ (visited on 12/23/2022).
[88] Russell A Epstein et al. “The Cognitive Map in Humans: Spatial Navigation and Beyond”.
In: Nature Neuroscience 20.11 (10/2017), pp. 1504–1513. doi: 10.1038/nn.4656.
[89] Martín Escardó and Paulo Oliva. “Selection Functions, Bar Recursion and Backward
Induction”. In: Mathematical Structures in Computer Science 20.2 (03/2010), pp. 127–168. doi:
10.1017/s0960129509990351.
[90] Chris Fields, James F. Glazebrook, and Antonino Marciano. “The Physical Meaning of
the Holographic Principle”. In: Quanta 11.1 (11/21/2022), pp. 72–96. issn: 1314-7374.
doi: 10 . 12743 / quanta . v11i1 . 206. arXiv: 2210 . 16021 [gr-qc,
physics:hep-th, physics:quant-ph]. url: http : / / arxiv . org /
abs/2210.16021 (visited on 12/23/2022).
[91] Chris Fields et al. “A Free Energy Principle for Generic Quantum Systems”. In: Progress
in Biophysics and Molecular Biology 173 (09/2022), pp. 36–59. doi: 10 . 1016 / j .
pbiomolbio.2022.05.006.
[92] John Rupert Firth. “A Synopsis of Linguistic Theory, 1930-1955”. In: Studies in Linguistic
Analysis. Oxford: Basil Blackwell, 1957, pp. 1–32.
[93] Brendan Fong. “Decorated Cospans”. In: Theory and Applications of Categories 30.33 (2015),
pp. 1096–1120. arXiv: 1502.00872 . url: http://www.tac.mta.ca/tac/
volumes/30/33/30-33abs.html.
[94] Brendan Fong. “The Algebra of Open and Interconnected Systems”. University of Oxford,
2016.
[95] Brendan Fong. “Causal Theories: A Categorical Perspective on Bayesian Networks”.
University of Oxford, 2013-01-26, 2013. arXiv:1301.6201v1 [math.PR]. url: http:
//arxiv.org/abs/1301.6201.
[96] Brendan Fong and Michael Johnson. “Lenses and Learners”. In: In: J. Cheney, H-S. Ko
(eds.): Proceedings of the Eighth International Workshop on Bidirectional Transformations (Bx
311
2019), Philadelphia, PA, USA, June 4, 2019, published at http://ceur-ws.org (03/05/2019). arXiv:
1903.03671v2 [cs.LG].
[97] Brendan Fong and David I. Spivak. Seven Sketches in Compositionality: An Invitation to
Applied Category Theory . 2018. arXiv: 1803.05316v3 [math.CT].
[98] Brendan Fong and David I. Spivak. “Hypergraph Categories”. In: Journal of Pure and
Applied Algebra 223.11 (11/2019), pp. 4746–4777. issn: 00224049. doi: 10 . 1016 / j .
jpaa.2019.02.014 . arXiv: 1806.08304v3 [math.CT]. url: https://
linkinghub . elsevier . com / retrieve / pii / S0022404919300489
(visited on 12/17/2022).
[99] J. Nathan Foster et al. “Combinators for Bidirectional Tree Transformations”. In: ACM
Transactions on Programming Languages and Systems 29.3 (05/2007), p. 17. doi: 10.1145/
1232420.1232424.
[100] K. Friston. “A Theory of Cortical Responses”. In: Philos. Trans. R. Soc. Lond., B, Biol.
Sci. 360.1456 (04/2005), pp. 815–836. doi: 10 . 1098 / rstb . 2005 . 1622. pmid:
15937014.
[101] K. Friston et al. “Variational Free Energy and the Laplace Approximation”. In: Neuroimage
34.1 (01/2007), pp. 220–234. doi: 10.1016/j.neuroimage.2006.08.035. pmid:
17055746.
[102] Karl Friston. “A Free Energy Principle for a Particular Physics”. 06/24/2019. arXiv: 1906.
10184v1 [q-bio.NC].
[103] Karl Friston, Lancelot Da Costa, and Thomas Parr. “Some Interesting Observations on the
Free Energy Principle”. 02/05/2020. arXiv: 2002.04501v1 [q-bio.NC].
[104] Karl Friston and Stefan Kiebel. “Predictive Coding under the Free-Energy Principle”.
In: Philosophical Transactions of the Royal Society B: Biological Sciences 364.1521 (2009),
pp. 1211–1221. doi: 10 . 1098 / rstb . 2008 . 0300. url: http : / / m . rstb .
royalsocietypublishing.org/content/364/1521/1211.
[105] Karl Friston et al. “Generalised Filtering”. In: Mathematical Problems in Engineering 2010
(2010), pp. 1–34. doi: 10.1155/2010/621670.
[106] Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. “Reinforcement Learning or Active
Inference?” In: PLoS ONE 4.7 (07/2009). Ed. by Olaf Sporns, e6421. doi: 10 . 1371 /
journal.pone.0006421.
[107] Karl J. Friston and Klaas E. Stephan. “Free-Energy and the Brain”. In: Synthese. An
International Journal for Epistemology, Methodology and Philosophy of Science 159.3 (09/2007),
pp. 417–458. doi: 10.1007/s11229-007-9237-y . url: http://dx.doi.
org/10.1007/s11229-007-9237-y.
312
[108] Karl J. Friston et al. “Action and Behavior: A Free-Energy Formulation”. In: Biological
Cybernetics 102.3 (02/2010), pp. 227–260. doi: 10.1007/s00422-010-0364-z.
[109] Tobias Fritz. “A Synthetic Approach to Markov Kernels, Conditional Independence and
Theorems on Sufficient Statistics”. In: Advances in Mathematics 370.107239 (08/19/2019).
doi: 10.1016/j.aim.2020.107239. arXiv: 1908.07021v3 [math.ST].
[110] Tobias Fritz and Paolo Perrone. “Bimonoidal Structure of Probability Monads”. In:Electronic
Notes in Theoretical Computer Science 341 (12/2018), pp. 121–149. issn: 15710661. doi:
10.1016/j.entcs.2018.11.007 . arXiv: 1804.03527 [cs, math]. url:
http://arxiv.org/abs/1804.03527 (visited on 06/20/2023).
[111] Peng Fu, Kohei Kishida, and Peter Selinger. “Linear Dependent Type Theory for Quantum
Programming Languages”. 04/28/2020. arXiv: 2004.13472 [cs.PL].
[112] Christopher A. Fuchs. “Notwithstanding Bohr, the Reasons for QBism”. In:Mind and Matter
15(2), 245-300 (2017) (05/09/2017). arXiv: 1705.03483v2 [quant-ph].
[113] Christopher A. Fuchs, N. David Mermin, and Ruediger Schack. “An Introduction to QBism
with an Application to the Locality of Quantum Mechanics”. In: Am. J. Phys., Vol. 82,
(11/2013), No.8, August2014, 749–754. eprint: 1311.5253.
[114] Christopher A. Fuchs and Ruediger Schack. “A Quantum-Bayesian Route to Quantum-State
Space”. 12/2009. doi: 10 . 1007 / s10701 - 009 - 9404 - 8. arXiv: 0912 . 4252
[quant-ph].
[115] Maria Carla Galavotti. “Subjectivism, Objectivism and Objectivity in Bruno de Finetti’s
Bayesianism”. In: Foundations of Bayesianism . Ed. by David Corfield and Jon Williamson.
Red. by Dov M. Gabbay and Jon Barwise. Vol. 24. Dordrecht: Springer Netherlands, 2001,
pp. 161–174. isbn: 978-90-481-5920-8 978-94-017-1586-7. doi: 10.1007/978-94-017-
1586-7_7 . url: http://link.springer.com/10.1007/978-94-017-
1586-7_7 (visited on 12/23/2022).
[116] Mona M Garvert, Raymond J Dolan, and Timothy EJ Behrens. “A Map of Abstract Relational
Knowledge in the Human Hippocampal–Entorhinal Cortex”. In: eLife 6 (04/2017). issn:
2050-084X. doi: 10.7554/elife.17086.
[117] Wulfram Gerstner. “Spike-Response Model”. In: Scholarpedia 3.12 (2008), p. 1343.
[118] Wulfram Gerstner and Werner M. Kistler. “Mathematical Formulations of Hebbian Learning”.
In: Biological Cybernetics 87.5-6 (12/2002), pp. 404–415. issn: 0340-1200. doi: 10.1007/
s00422-002-0353-y . url: http://dx.doi.org/10.1007/s00422-
002-0353-y.
[119] Neil Ghani et al. “Compositional Game Theory”. In:Proceedings of the 33rd Annual ACM/IEEE
Symposium on Logic in Computer Science . LICS ’18: 33rd Annual ACM/IEEE Symposium on
Logic in Computer Science. Oxford United Kingdom: ACM, 07/09/2018, pp. 472–481. isbn:
313
978-1-4503-5583-4. doi: 10.1145/3209108.3209165. url: https://dl.acm.
org/doi/10.1145/3209108.3209165 (visited on 09/29/2023).
[120] Michèle Giry. “A Categorical Approach to Probability Theory”. In: Categorical Aspects of
Topology and Analysis . Ed. by B. Banaschewski. Red. by A. Dold and B. Eckmann. Vol. 915.
Berlin, Heidelberg: Springer Berlin Heidelberg, 1982, pp. 68–85. isbn: 978-3-540-11211-2
978-3-540-39041-1. doi: 10.1007/BFb0092872. url: http://link.springer.
com/10.1007/BFb0092872 (visited on 06/20/2023).
[121] Alexander Grothendieck. “Sur Quelques Points d’algèbre Homologique”. In: Tohoku
Mathematical Journal 9.2 (01/01/1957). issn: 0040-8735. doi: 10 . 2748 / tmj /
1178244839. url: https://projecteuclid.org/journals/tohoku-
mathematical - journal / volume - 9 / issue - 2 / Sur - quelques -
points - dalg % c3 % a8bre - homologique - I / 10 . 2748 / tmj /
1178244839.full (visited on 12/23/2022).
[122] Micah Halter et al. Compositional Scientific Computing with Catlab and SemanticModels .
06/29/2020. arXiv: 2005.04831 [cs, math]. url: http://arxiv.org/abs/
2005.04831 (visited on 12/23/2022). preprint.
[123] Jakob Hansen. “Laplacians of Cellular Sheaves: Theory and Applications”. PhD thesis.
University of Pennsylvania, 2020.
[124] Jakob Hansen and Robert Ghrist. “Learning Sheaf Laplacians from Smooth Signals”. In:
ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP). IEEE, 05/2019. doi: 10.1109/icassp.2019.8683709.
[125] Jakob Hansen and Robert Ghrist. “Opinion Dynamics on Discourse Sheaves”. 05/26/2020.
arXiv: 2005.12798 [math.DS].
[126] R. V. L. Hartley. “Transmission of Information 1”. In: Bell System Technical Journal 7.3
(07/1928), pp. 535–563. issn: 00058580. doi: 10 . 1002 / j . 1538 - 7305 . 1928 .
tb01236.x. url: https://ieeexplore.ieee.org/document/6769394
(visited on 12/23/2022).
[127] Donald Olding Hebb. The Organization of Behavior: A Neuropsychological Approach . John
Wiley & Sons, 1949.
[128] Jules Hedges and Riu Rodríguez Sakamoto. “Value Iteration Is Optic Composition”.
06/09/2022. arXiv: 2206.04547 [math.CT].
[129] Conor Heins et al. “Spin Glass Systems as Collective Active Inference”. 07/14/2022. arXiv:
2207.06970 [cond-mat.dis-nn].
[130] Chris Heunen, Mehrnoosh Sadrzadeh, and Edward Grefenstette, eds. Quantum Physics and
Linguistics: A Compositional, Diagrammatic Discourse . Oxford University Press, 02/2013.
isbn: 978-0-19-964629-6. url: http://ukcatalogue.oup.com/product/
9780199646296.do#.UPAFH4mLLqp.
314
[131] Chris Heunen et al. “A Convenient Category for Higher-Order Probability Theory”. In:
2017 32nd Annual ACM/IEEE Symposium on Logic in Computer Science (LICS) (2017-01-10,
2017-06). doi: 10.1109/lics.2017.8005137. arXiv: 1701.02547 [cs.PL].
[132] Sepp Hochreiter and Jürgen Schmidhuber. “Long Short-Term Memory”. In: Neural Compu-
tation 9.8 (11/01/1997), pp. 1735–1780. issn: 0899-7667, 1530-888X. doi: 10.1162/neco.
1997.9.8.1735. url: https://direct.mit.edu/neco/article/9/8/
1735-1780/6109 (visited on 12/17/2022).
[133] A. L. Hodgkin and A. F. Huxley. “A Quantitative Description of Membrane Current and
Its Application to Conduction and Excitation in Nerve”. In: The Journal of Physiology
117.4 (08/1952), pp. 500–544. issn: 0022-3751. doi: 10 . 1113 / jphysiol . 1952 .
sp004764. url: http : / / dx . doi . org / 10 . 1113 / jphysiol . 1952 .
sp004764.
[134] Matthew Hutson. “Artificial Intelligence Faces Reproducibility Crisis”. In: Science (New
York, N.Y.) 359.6377 (02/2018), pp. 725–726. issn: 0036-8075, 1095-9203. doi: 10.1126/
science.359.6377.725 . url: https://www.science.org/doi/10.
1126/science.359.6377.725.
[135] John P. A. Ioannidis. “Contradicted and Initially Stronger Effects in Highly Cited Clinical
Research”. In: JAMA 294.2 (07/13/2005), p. 218. issn: 0098-7484. doi: 10.1001/jama.
294.2.218 . url: http://jama.jamanetwork.com/article.aspx?
doi=10.1001/jama.294.2.218 (visited on 12/23/2022).
[136] Eugene M Izhikevich. “Neural Excitability, Spiking and Bursting”. In: International Journal
of Bifurcation and Chaos 10.06 (06/2000), pp. 1171–1266. issn: 1793-6551. doi: 10 .
1142 / s0218127400000840. url: http : / / dx . doi . org / 10 . 1142 /
S0218127400000840.
[137] Bart Jacobs. “Convexity, Duality and Effects”. In: Theoretical Computer Science . Ed. by
Cristian S. Calude and Vladimiro Sassone. Vol. 323. Berlin, Heidelberg: Springer Berlin
Heidelberg, 2010, pp. 1–19. isbn: 978-3-642-15239-9 978-3-642-15240-5. doi: 10.1007/
978-3-642-15240-5_1 . url: http://link.springer.com/10.1007/
978-3-642-15240-5_1 (visited on 05/12/2023).
[138] Bart Jacobs. Introduction to Coalgebra . Vol. 59. Cambridge University Press, 2017.
isbn: 978-1-107-17789-5. url: https : / / www . cambridge . org / gb /
academic/subjects/mathematics/logic-categories-and-sets/
introduction - coalgebra - towards - mathematics - states - and -
observation?format=HB&isbn=9781107177895.
[139] Bart Jacobs. “From Probability Monads to Commutative Effectuses”. In: Journal of Logical
and Algebraic Methods in Programming 94 (01/2018), pp. 200–237. doi: 10.1016/j.
jlamp.2016.11.006.
315
[140] Bart Jacobs. “A Recipe for State-and-Effect Triangles”. In: Logical Methods in Computer
Science 13.2 (2017-05-17, 2017), pp. 1860–5974. doi: 10.23638/LMCS-13(2:6)2017.
arXiv: 1703.09034 [cs.LO].
[141] Bart Jacobs and Bram Westerbaan. “An Effect-Theoretic Account of Lebesgue Integration”.
In: Electronic Notes in Theoretical Computer Science 319 (12/2015), pp. 239–253. doi: 10.
1016/j.entcs.2015.12.015.
[142] Bart Jacobs and Fabio Zanasi. “The Logical Essentials of Bayesian Reasoning”. In:Foundations
of Probabilistic Programming . Ed. by Gilles Barthe, Joost-Pieter Katoen, and Alexandra
Silva. 1st ed. Cambridge University Press, 12/03/2020, pp. 295–332. isbn: 978-1-108-77075-0
978-1-108-48851-8. doi: 10.1017/9781108770750.010 . url: https://www.
cambridge.org/core/product/identifier/9781108770750%23c9/
type/book_part (visited on 09/29/2023).
[143] Amirhossein Jafarian et al. “Structure Learning in Coupled Dynamical Systems and Dynamic
Causal Modelling”. In: Philosophical Transactions of the Royal Society A: Mathematical,
Physical and Engineering Sciences 377.2160 (10/2019), p. 20190048. doi: 10.1098/rsta.
2019.0048.
[144] Renaud Jardri and Sophie Deneve. “Computational Models of Hallucinations”. In: The
Neuroscience of Hallucinations . Springer, 2013, pp. 289–313.
[145] Niles Johnson and Donald Yau.2-Dimensional Categories. 06/17/2020. arXiv: 2002.06055
[math]. url: http://arxiv.org/abs/2002.06055 (visited on 11/03/2022).
preprint.
[146] Renaud Jolivet, Timothy J., and Wulfram Gerstner. “The Spike Response Model: A
Framework to Predict Neuronal Spike Trains”. In: Lecture Notes in Computer Science (2003),
pp. 846–853. issn: 0302-9743. doi: 10.1007/3-540-44989-2_101 . url: http:
//dx.doi.org/10.1007/3-540-44989-2_101.
[147] R. E. Kalman. “A New Approach to Linear Filtering and Prediction Problems”. In: Journal of
Basic Engineering 82.1 (1960), p. 35. doi: 10.1115/1.3662552. url: http://dx.
doi.org/10.1115/1.3662552.
[148] Raphael Kaplan and Karl J. Friston. “Planning and Navigation as Active Inference”. In:
Biological Cybernetics 112.4 (03/2018), pp. 323–343. doi: 10.1007/s00422- 018-
0753-2.
[149] Sayash Kapoor and Arvind Narayanan. Leakage and the Reproducibility Crisis in ML-based
Science. 07/14/2022. arXiv:2207.07048 [cs, stat]. url: http://arxiv.org/
abs/2207.07048 (visited on 12/23/2022). preprint.
[150] Robert E Kass, Luke Tierney, and Joseph B Kadane. “The Validity of Posterior Expansions
Based on Laplace’s Method”. In: Bayesian and Likelihood Methods in Statistics and
Econometrics: Essays in Honor of George A. Barnard . Ed. by S Geisser et al. Elsevier Science
Publishers B.V. (North-Holland), 1990.
316
[151] Robert E. Kass and Adrian E. Raftery. “Bayes Factors”. In: Journal of the American Statistical
Association 90.430 (06/1995), pp. 773–795. issn: 0162-1459, 1537-274X. doi: 10.1080/
01621459.1995.10476572. url: http://www.tandfonline.com/doi/
abs/10.1080/01621459.1995.10476572 (visited on 05/15/2023).
[152] Richard A. Kelsey. “A Correspondence between Continuation Passing Style and Static
Single Assignment Form”. In: Papers from the 1995 ACM SIGPLAN Workshop on Intermediate
Representations. POPL95: 22nd ACM Symposium on Principles of Programming Languages.
San Francisco California USA: ACM, 03/1995, pp. 13–22. isbn: 978-0-89791-754-4. doi:
10.1145/202529.202532 . url: https://dl.acm.org/doi/10.1145/
202529.202532 (visited on 05/02/2023).
[153] Andrei Khrennikov et al. “Quantum Models for Psychological Measurements: An Unsolved
Problem”. In: PLoS ONE 9.10 (10/2014). Ed. by Zhong-LinEditor Lu, e110909. issn: 1932-6203.
doi: 10.1371/journal.pone.0110909 . url: http://dx.doi.org/10.
1371/journal.pone.0110909.
[154] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes . 12/20/2013. arXiv:
1312.6114 [stat.ML]. preprint.
[155] Diederik P. Kingma. “Variational Inference & Deep Learning. A New Synthesis”. PhD thesis.
University of Amsterdam, 2017. url: https://hdl.handle.net/11245.1/
8e55e07f-e4be-458f-a929-2f9bc2d169e8 .
[156] Thomas N. Kipf and Max Welling. Variational Graph Auto-Encoders . 11/21/2016. arXiv:
1611.07308 [cs, stat]. url: http://arxiv.org/abs/1611.07308
(visited on 12/23/2022). preprint.
[157] Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional
Networks. 02/22/2017. arXiv: 1609.02907 [cs, stat]. url: http://arxiv.
org/abs/1609.02907 (visited on 12/23/2022). preprint.
[158] Michael Kirchhoff et al. “The Markov Blankets of Life: Autonomy, Active Inference and the
Free Energy Principle”. In:Journal of The Royal Society Interface 15.138 (01/2018), p. 20170792.
doi: 10.1098/rsif.2017.0792.
[159] Michael D. Kirchhoff. “Autopoiesis, Free Energy, and the Life–Mind Continuity Thesis”. In:
Synthese 195.6 (2018), pp. 2519–2540.issn: 0039-7857, 1573-0964.doi: 10.1007/s11229-
016- 1100-6. url: http://link.springer.com/10.1007/s11229-
016-1100-6 (visited on 12/23/2022).
[160] David C Knill and Alexandre Pouget. “The Bayesian Brain: The Role of Uncertainty in
Neural Coding and Computation”. In: TRENDS in Neurosciences 27.12 (2004), pp. 712–719.
doi: 10.1016/j.tins.2004.10.007. url: http://www.sciencedirect.
com/science/article/pii/S0166223604003352.
[161] Mark A. Kramer. “Nonlinear Principal Component Analysis Using Autoassociative Neural
Networks”. In: AIChE Journal 37.2 (02/1991), pp. 233–243. issn: 0001-1541, 1547-5905. doi:
317
10.1002/aic.690370209. url: https://onlinelibrary.wiley.com/
doi/10.1002/aic.690370209 (visited on 05/02/2023).
[162] Alexander Kurz. Logic Column 15: Coalgebras and Their Logics . 05/28/2006. arXiv: cs/
0605128. url: http://arxiv.org/abs/cs/0605128 (visited on 12/17/2022).
preprint.
[163] Pierre Simon Laplace. “Memoir on the Probability of the Causes of Events”. In: Statistical
Science 1.3 (1986), pp. 364–378. issn: 08834237. JSTOR: 2245476. url: http://www.
jstor.org/stable/2245476 (visited on 05/15/2023).
[164] F. W. Lawvere and S. H. Schnauel.Conceptual Mathematics : A First Introduction to Categories .
Cambridge, UK New York: Cambridge University Press, 2009. isbn: 978-0-511-80419-9. doi:
10.1017/CBO9780511804199.
[165] F. William Lawvere. “Functorial Semantics of Algebraic Theories”. In: Proceedings of the
National Academy of Sciences 50.5 (11/1963), pp. 869–872. issn: 0027-8424, 1091-6490. doi:
10.1073/pnas.50.5.869 . url: https://pnas.org/doi/full/10.
1073/pnas.50.5.869 (visited on 12/23/2022).
[166] F. William Lawvere. “An Elementary Theory of the Category of Sets”. In: Proceedings of
the National Academy of Sciences 52.6 (12/1964), pp. 1506–1511. issn: 0027-8424, 1091-6490.
doi: 10.1073/pnas.52.6.1506. url: https://pnas.org/doi/full/10.
1073/pnas.52.6.1506 (visited on 12/23/2022).
[167] John M. Lee. “Smooth Manifolds”. In: Introduction to Smooth Manifolds . New York, NY:
Springer New York, 2012, pp. 1–31. isbn: 978-1-4419-9982-5. doi: 10.1007/978-1-
4419-9982-5_1. url: https://doi.org/10.1007/978-1-4419-9982-
5_1.
[168] Tom Leinster. Higher Operads, Higher Categories . London Mathematical Society Lecture
Note Series 298. Cambridge University Press, Cambridge, 2004. isbn: 0-521-53215-9. doi:
10.1017/CBO9780511525896.
[169] Tom Leinster. Entropy and Diversity: The Axiomatic Approach . 1st ed. Cambridge University
Press, 04/30/2021. isbn: 978-1-108-96355-8 978-1-108-83270-0 978-1-108-96557-6. doi: 10.
1017 / 9781108963558. url: https : / / www . cambridge . org / core /
product/identifier/9781108963558/type/book (visited on 09/29/2023).
[170] Pantelis Leptourgos et al. “Circular Inference in Bistable Perception”. In:Journal of Vision 20.4
(04/21/2020), p. 12. issn: 1534-7362. doi: 10.1167/jov.20.4.12. url: https://
jov.arvojournals.org/article.aspx?articleid=2765046 (visited
on 12/23/2022).
[171] Eugene Lerman and David I. Spivak. “An Algebra of Open Continuous Time Dynamical
Systems and Networks”. 02/02/2016. arXiv: 1602.01017v2 [math.DS].
318
[172] Paul Blain Levy. “Locally Graded Categories”. University of Birmingham. 02/17/2019. url:
https://www.cs.bham.ac.uk/~pbl/papers/locgrade.pdf.
[173] Fosco Loregian and Emily Riehl. “Categorical Notions of Fibration”. 06/15/2018. arXiv:
1806.06129v2 [math.CT].
[174] Martin Lundfall. “A Diagram Model of Linear Dependent Type Theory”. 06/25/2018. arXiv:
1806.09593 [math.LO].
[175] Saunders Mac Lane. Categories for the Working Mathematician . 2nd ed. Graduate Texts in
Mathematics 5. New York: Springer-Verlag, 1998.isbn: 0-387-98403-8.
[176] David J. C. MacKay. Information Theory, Inference, and Learning Algorithms . Cambridge, UK
; New York: Cambridge University Press, 2003. 628 pp. isbn: 978-0-521-64298-9.
[177] Saunders MacLane and Ieke Moerdijk. Sheaves in Geometry and Logic: A First Introduction
to Topos Theory . Springer, 1992. isbn: 0-387-97710-4.
[178] Shirley Mark et al. “Transferring Structural Knowledge across Cognitive Maps in Humans
and Models”. In: Nature Communications 11.1 (09/2020). doi: 10.1038/s41467-020-
18254-6.
[179] David Marr. Vision: A Computational Investigation into the Human Representation and
Processing of Visual Information . Henry Holt and Co, 1982.
[180] Yoshihiro Maruyama. “Meaning and Duality: From Categorical Logic to Quantum Physics”.
PhD thesis. University of Oxford / University of Oxford, 2016. url: https://ora.ox.
ac.uk/objects/uuid:440a291d-7533-493d-b5aa-f6db30ca03cf .
[181] Michael V Mascagni, Arthur S Sherman, et al. “Numerical Methods for Neuronal
Modeling”. In: Methods in neuronal modeling 2 (1989). url: http : / / cox . iwr .
uni-heidelberg.de/teaching/numsimneuro_ss2011/mascagni_
sherman.pdf.
[182] Conor McBride. “I Got Plenty o’ Nuttin’”. In: A List of Successes That Can Change the World .
Springer International Publishing, 2016, pp. 207–233. doi: 10.1007/978-3-319-
30936-1_12.
[183] Yaared Al-Mehairi, Bob Coecke, and Martha Lewis. “Compositional Distributional Cogni-
tion”. In: International Symposium on Quantum Interaction . Springer, 08/12/2016, pp. 122–134.
doi: 10.1007/978-3-319-52289-0_10 . arXiv: 1608.03785 [cs.AI].
[184] Bartosz Milewski. Profunctor Optics: The Categorical View . 2017. url: https : / /
bartoszmilewski . com / 2017 / 07 / 07 / profunctor - optics - the -
categorical-view/.
319
[185] Beren Millidge and Christopher L. Buckley. Successor Representation Active Inference .
07/20/2022. arXiv: 2207.09897 [cs]. url: http://arxiv.org/abs/2207.
09897 (visited on 12/23/2022). preprint.
[186] Beren Millidge, Alexander Tschantz, and Christopher L Buckley. “Whence the Expected
Free Energy?” 04/17/2020. arXiv: 2004.08128 [cs.AI].
[187] Beren Millidge et al. Predictive Coding: Towards a Future of Deep Learning beyond
Backpropagation? 02/18/2022. arXiv: 2202.09467 [cs]. url: http://arxiv.
org/abs/2202.09467 (visited on 12/23/2022). preprint.
[188] Aaron Mobley et al. “A Survey on Data Reproducibility in Cancer Research Provides
Insights into Our Limited Ability to Translate Findings from the Laboratory to the Clinic”.
In: PLoS ONE 8.5 (05/15/2013). Ed. by Hirofumi Arakawa, e63221. issn: 1932-6203. doi:
10.1371/journal.pone.0063221 . url: https://dx.plos.org/10.
1371/journal.pone.0063221 (visited on 12/23/2022).
[189] Joe Moeller and Christina Vasilakopoulou. “Monoidal Grothendieck Construction”. In:
Theory and Applications of Categories 35.31 (2020), pp. 1159–1207. arXiv: 1809.00727v2
[math.CT].
[190] Jason Morton. “Belief Propagation in Monoidal Categories”. In: Proceedings of the 11th
Workshop on Quantum Physics and Logic . Vol. 172. EPTCS, 12/28/2014, pp. 262–269. doi:
10.4204/EPTCS.172.18. arXiv: 1405.2618 [math]. url: http://arxiv.
org/abs/1405.2618 (visited on 12/23/2022).
[191] David Jaz Myers. “Double Categories of Open Dynamical Systems (Extended Abstract)”. In:
Electronic Proceedings in Theoretical Computer Science 333 (02/08/2021), pp. 154–167. issn:
2075-2180. doi: 10.4204/EPTCS.333.11 . arXiv: 2005.05956 [math]. url:
http://arxiv.org/abs/2005.05956 (visited on 11/16/2022).
[192] David Jaz Myers. Categorical Systems Theory (Draft) . 2022. url: http://davidjaz.
com/Papers/DynamicalBook.pdf.
[193] Lynn Nadel and Lloyd MacDonald. “Hippocampus: Cognitive Map or Working Memory?”
In: Behavioral and Neural Biology 29.3 (1980), pp. 405–409.issn: 0163-1047. doi: 10.1016/
S0163- 1047(80)90430- 6. url: http://www.sciencedirect.com/
science/article/pii/S0163104780904306.
[194] Jet Nestruev. Smooth Manifolds and Observables . Second edition. Graduate Texts in
Mathematics 220. New York: Springer, 2020. 433 pp. isbn: 978-3-030-45649-8. doi: 10.
1007/978-3-030-45650-4 .
[195] Frank Nielsen. “An Elementary Introduction to Information Geometry”. 08/17/2018. arXiv:
1808.08271 [cs.LG].
[196] nLab authors. Essential Image. 05/2023. url: https://ncatlab.org/nlab/show/
essential+image.
320
[197] H. Nyquist. “Certain Topics in Telegraph Transmission Theory”. In: Transactions of the
American Institute of Electrical Engineers 47.2 (04/1928), pp. 617–644. issn: 0096-3860. doi:
10.1109/T- AIEE.1928.5055024 . url: http://ieeexplore.ieee.
org/document/5055024/ (visited on 12/23/2022).
[198] John O’Keefe. “Place Units in the Hippocampus of the Freely Moving Rat”. In: Experimental
neurology 51.1 (1976), pp. 78–109. url: http://www.sciencedirect.com/
science/article/pii/0014488676900558.
[199] Yann Ollivier. “The Extended Kalman Filter Is a Natural Gradient Descent in Trajectory
Space”. 01/03/2019. arXiv: 1901.00696v1 [math.OC].
[200] Frank W. J. Olver. Asymptotics and Special Functions . AKP Classics. Wellesley, Mass: A.K.
Peters, 1997. 572 pp. isbn: 978-1-56881-069-0.
[201] Open Science Collaboration. “Estimating the Reproducibility of Psychological Science”.
In: Science 349.6251 (08/28/2015), aac4716. issn: 0036-8075, 1095-9203. doi: 10.1126/
science.aac4716. url: https://www.science.org/doi/10.1126/
science.aac4716 (visited on 12/23/2022).
[202] Srdjan Ostojic and Nicolas Brunel. “From Spiking Neuron Models to Linear-Nonlinear
Models”. In: PLoS computational biology 7.1 (01/2011). Ed. by Peter E. Latham, e1001056.
doi: 10.1371/journal.pcbi.1001056 . url: http://dx.doi.org/10.
1371/journal.pcbi.1001056.
[203] Ensor Rafael Palacios et al. “Biological Self-organisation and Markov Blankets”. In: (11/2017).
doi: 10.1101/227181.
[204] Thomas Parr, Lancelot Da Costa, and Karl Friston. “Markov Blankets, Information Geometry
and Stochastic Thermodynamics”. In: Philosophical Transactions of the Royal Society A:
Mathematical, Physical and Engineering Sciences 378.2164 (12/2019), p. 20190159. doi: 10.
1098/rsta.2019.0159.
[205] Thomas Parr, Giovanni Pezzulo, and Karl J. Friston.Active Inference. The Free Energy Principle
in Mind, Brain, and Behavior . MIT Press, 2022, p. 288. isbn: 978-0-262-04535-3.
[206] Evan Patterson, David I. Spivak, and Dmitry Vagner. “Wiring Diagrams as Normal Forms
for Computing in Symmetric Monoidal Categories”. In: Electronic Proceedings in Theoretical
Computer Science 333 (02/08/2021), pp. 49–64. issn: 2075-2180. doi: 10.4204/EPTCS.
333.4 . arXiv: 2101.12046 [cs]. url: http://arxiv.org/abs/2101.
12046 (visited on 10/27/2022).
[207] Dusko Pavlovic, Michael Mislove, and James B Worrell. “Testing Semantics: Connecting
Processes and Process Logics”. In: International Conference on Algebraic Methodology and
Software Technology . Springer. Springer Berlin Heidelberg, 2006, pp. 308–322. doi: 10.
1007/11784180_24.
321
[208] Judea Pearl. “Reverend Bayes on Inference Engines: A Distributed Hierarchical Approach”.
In: Probabilistic and Causal Inference . Ed. by Hector Geffner, Rina Dechter, and Joseph Y.
Halpern. 1st ed. New York, NY, USA: ACM, 1982, pp. 129–138. isbn: 978-1-4503-9586-1.
doi: 10.1145/3501714.3501727 . url: https://dl.acm.org/doi/10.
1145/3501714.3501727 (visited on 12/23/2022).
[209] Steven Phillips and William H. Wilson. “Categorial Compositionality: A Category Theory
Explanation for the Systematicity of Human Cognition”. In: PLoS Computational Biology 6.7
(07/2010). Ed. by Karl J. Friston, e1000858.doi: 10.1371/journal.pcbi.1000858.
[210] Benjamin C. Pierce. Basic Category Theory for Computer Scientists . MIT Press, 1991.
[211] V I Piterbarg and V R Fatalov. “The Laplace Method for Probability Measures in
Banach Spaces”. In: Russian Mathematical Surveys 50.6 (12/31/1995), pp. 1151–1239.
issn: 0036-0279, 1468-4829. doi: 10 . 1070 / RM1995v050n06ABEH002635.
url: https : / / iopscience . iop . org / article / 10 . 1070 /
RM1995v050n06ABEH002635 (visited on 05/16/2023).
[212] Tomaso Poggio and Thomas Serre. “Models of Visual Cortex”. In: Scholarpedia 8.4 (2013),
p. 3516. issn: 1941-6016. doi: 10.4249/scholarpedia.3516 . url: http://
www.scholarpedia.org/article/Models_of_visual_cortex (visited
on 12/23/2022).
[213] The Univalent Foundations Program. Homotopy Type Theory: Univalent Foundations of
Mathematics. Institute for Advanced Study: https://homotopytypetheory.org/book, 2013.
eprint: http://saunders.phil.cmu.edu/book/hott- a4.pdf . url:
https://homotopytypetheory.org/book/.
[214] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming .
1st ed. Wiley Series in Probability and Statistics. Wiley, 04/15/1994. isbn: 978-0-471-
61977-2 978-0-470-31688-7. doi: 10 . 1002 / 9780470316887. url: https : / /
onlinelibrary.wiley.com/doi/book/10.1002/9780470316887
(visited on 12/23/2022).
[215] Maxwell J D Ramstead et al. “On Bayesian Mechanics: A Physics of and by Beliefs”.
05/23/2022. arXiv: 2205.11543 [cond-mat.stat-mech].
[216] R. P. Rao and D. H. Ballard. “Predictive Coding in the Visual Cortex: A Functional
Interpretation of Some Extra-Classical Receptive-Field Effects”. In: Nature Neuroscience 2.1
(01/1999), pp. 79–87. doi: 10.1038/4580. pmid: 10195184.
[217] Mitchell Riley. “Categories of Optics”. 09/03/2018. arXiv: 1809.00738v2 [math.CT].
[218] Edmund T. Rolls and Alessandro Treves.Neural Networks and Brain Function . 1st ed. Oxford
University Press, USA, 01/15/1998. isbn: 0-19-852432-3.
[219] Mario Román. “Open Diagrams via Coend Calculus”. 04/09/2020. arXiv: 2004.04526v2
[math.CT].
322
[220] Robert Rosen. “The Representation of Biological Systems from the Standpoint of the Theory
of Categories”. In: The Bulletin of Mathematical Biophysics 20.4 (12/1958), pp. 317–341.
issn: 0007-4985, 1522-9602. doi: 10.1007/BF02477890 . url: http://link.
springer.com/10.1007/BF02477890 (visited on 12/23/2022).
[221] Robert Rosenbaum. “On the Relationship between Predictive Coding and Backpropagation”.
In: PLOS ONE 17.3 (03/31/2022). Ed. by Gennady S. Cymbalyuk, e0266102. issn: 1932-6203.
doi: 10.1371/journal.pone.0266102. url: https://dx.plos.org/10.
1371/journal.pone.0266102 (visited on 12/23/2022).
[222] Dylan Rupel and David I. Spivak. The Operad of Temporal Wiring Diagrams: Formalizing
a Graphical Language for Discrete-Time Processes . 07/25/2013. arXiv: 1307.6894 [cs,
math, q-bio]. url: http : / / arxiv . org / abs / 1307 . 6894(visited on
09/19/2023). preprint.
[223] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. “Dynamic Routing Between Capsules”.
In: Advances in Neural Information Processing Systems . Ed. by I. Guyon et al. Vol. 30. Curran
Associates, Inc., 2017. url: https://proceedings.neurips.cc/paper/
2017/file/2cad8fa47bbef282badbb8de5374b894-Paper.pdf.
[224] Dalton A R Sakthivadivel. “Towards a Geometry and Analysis for Bayesian Mechanics”.
04/25/2022. arXiv: 2204.11900 [math-ph].
[225] Dalton A. R. Sakthivadivel. A Constraint Geometry for Inference and Integration . 04/18/2022.
arXiv: 2203 . 08119 [cond-mat, physics:math-ph]. url: http : / /
arxiv.org/abs/2203.08119 (visited on 12/23/2022). preprint.
[226] Ausra Saudargiene, Bernd Porr, and Florentin Wörgötter. “How the Shape of Pre- and
Postsynaptic Signals Can Influence STDP: A Biophysical Model.” In:Neural computation 16.3
(03/2004), pp. 595–625. doi: 10.1162/089976604772744929. pmid: 15006093.
url: http://dx.doi.org/10.1162/089976604772744929.
[227] Evan S. Schaffer, Srdjan Ostojic, and L. F. Abbott. “A Complex-Valued Firing-Rate Model
That Approximates the Dynamics of Spiking Networks”. In: PLoS computational biology
9.10 (10/2013). Ed. by Bard Ermentrout, e1003301. doi: 10.1371/journal.pcbi.
1003301. url: http://dx.doi.org/10.1371/journal.pcbi.1003301.
[228] Urs Schreiber. “Differential Cohomology in a Cohesive Infinity-Topos”. 10/29/2013. arXiv:
1310.7930 [math-ph].
[229] Grégoire Sergeant-Perthuis. Regionalized Optimization . 05/19/2022. arXiv: 2201.11876
[cs, math]. url: http : / / arxiv . org / abs / 2201 . 11876(visited on
12/23/2022). preprint.
[230] C. E. Shannon. “A Mathematical Theory of Communication”. In: Bell System Technical
Journal 27.3 (07/1948), pp. 379–423. issn: 00058580. doi: 10.1002/j.1538-7305.
1948.tb01338.x . url: https://ieeexplore.ieee.org/document/
6773024 (visited on 12/23/2022).
323
[231] Brandon Shapiro and David I. Spivak. “Dynamic Categories, Dynamic Operads: From Deep
Learning to Prediction Markets”. 05/08/2022. arXiv: 2205.03906 [math.CT].
[232] Dan Shiebler. “Categorical Stochastic Processes and Likelihood”. In: Compositionality 3,
1 (2021) (05/10/2020). doi: 10.32408/compositionality-3-1 . arXiv: 2005.
04735 [cs.AI].
[233] Harel Shouval. “Models of Synaptic Plasticity”. In: Scholarpedia 2.7 (2007), p. 1605. issn:
1941-6016. doi: 10.4249/scholarpedia.1605. url: http://dx.doi.org/
10.4249/scholarpedia.1605.
[234] Michael Shulman. “Homotopy Type Theory: The Logic of Space”. In: New Spaces for
Mathematics and Physics . Ed. by M. Anel and G. Catren. Cambridge University Press,
2017-03-08, 2017. arXiv: 1703.03007 [math.CT]. url: http://arxiv.org/
abs/1703.03007.
[235] David I Spivak and Nelson Niu. Polynomial Functors: A General Theory of Interac-
tion. (In press), 2021. url: https : / / raw . githubusercontent . com /
ToposInstitute/poly/main/Book-Poly.pdf.
[236] David I. Spivak. “Functorial Data Migration”. In: Information and Computation 217 (2012),
pp. 31–51. issn: 0890-5401. doi: 10.1016/j.ic.2012.05.001 . url: https:
//doi.org/10.1016/j.ic.2012.05.001.
[237] David I. Spivak. The Operad of Wiring Diagrams: Formalizing a Graphical Language for
Databases, Recursion, and Plug-and-Play Circuits . 05/01/2013. arXiv: 1305.0297 [cs,
math]. url: http://arxiv.org/abs/1305.0297 (visited on 12/23/2022).
preprint.
[238] David I. Spivak. Generalized Lens Categories via Functors C Op → Cat. 08/06/2019. arXiv:
1908.02202 [math.CT]. preprint.
[239] David I. Spivak. Poly: An Abundant Categorical Setting for Mode-Dependent Dynamics .
05/05/2020. arXiv: 2005.01894 [math.CT]. preprint.
[240] David I. Spivak. “Learners’ Languages”. In: Electronic Proceedings in Theoretical Computer
Science 372 (03/01/2021), pp. 14–28. doi: 10.4204/EPTCS.372.2 . arXiv: 2103.
01189 [math.CT].
[241] David I. Spivak. A Reference for Categorical Structures on Poly . 02/01/2022. arXiv: 2202.
00534 [math.CT]. preprint.
[242] David I. Spivak. Functorial Aggregation. 01/31/2022. arXiv: 2111.10968 [cs, math].
url: http://arxiv.org/abs/2111.10968 (visited on 12/23/2022). preprint.
[243] David I. Spivak, Patrick Schultz, and Dylan Rupel. “String Diagrams for Traced and Compact
Categories Are Oriented 1-Cobordisms”. In: J. Pure Appl. Algebra 221 (2017), no. 8, pp. 2064-
324
2110 (08/05/2015). doi: 10.1016/j.jpaa.2016.10.009 . arXiv: 1508.01069
[math.CT].
[244] David I. Spivak and Joshua Tan. “Nesting of Dynamic Systems and Mode-Dependent
Networks”. 02/25/2015. arXiv: 1502.07380 [math.DS].
[245] Olaf Sporns. “Brain Connectivity”. In: Scholarpedia 2.10 (2007), p. 4695. issn: 1941-6016. doi:
10.4249/scholarpedia.4695 . url: http://dx.doi.org/10.4249/
scholarpedia.4695.
[246] Mandyam Veerambudi Srinivasan, Simon Barry Laughlin, and A Dubs. “Predictive Coding:
A Fresh View of Inhibition in the Retina”. In:Proceedings of the Royal Society of London. Series
B. Biological Sciences 216.1205 (11/22/1982), pp. 427–459. issn: 0080-4649, 2053-9193. doi:
10.1098/rspb.1982.0085 . url: https://royalsocietypublishing.
org/doi/10.1098/rspb.1982.0085 (visited on 12/23/2022).
[247] Toby St Clere Smithe. “Radically Compositional Cognitive Concepts”. 11/14/2019. arXiv:
1911.06602 [q-bio.NC].
[248] Toby St Clere Smithe. Bayesian Updates Compose Optically . 05/31/2020. arXiv: 2006 .
01631 [math.CT]. preprint.
[249] Toby St Clere Smithe. “Compositional Active Inference”. In: Finding the Right Abstractions.
Topos Institute, 05/12/2021.
[250] Toby St Clere Smithe. Compositional Active Inference I: Bayesian Lenses. Statistical Games .
09/09/2021. arXiv: 2109.04461 [math.ST]. preprint.
[251] Toby St Clere Smithe. “Cyber Kittens, or Some First Steps Towards Categorical Cybernetics”.
In: Electronic Proceedings in Theoretical Computer Science . Applied Category Theory 2020.
Vol. 333. 02/08/2021, pp. 108–124. doi: 10.4204/EPTCS.333.8 . url: http://
arxiv.org/abs/2101.10483v1 (visited on 09/29/2023).
[252] Toby St Clere Smithe. “Polynomial Life: The Structure of Adaptive Systems”. In: Fourth
International Conference on Applied Category Theory (ACT 2021) . Ed. by K. Kishida.
Vol. EPTCS 370. 2021, pp. 133–147. doi: 10.4204/EPTCS.370.28.
[253] Toby St Clere Smithe. “Compositional Active Inference II: Polynomial Dynamics. Approxi-
mate Inference Doctrines”. 08/25/2022. arXiv: 2208.12173 [nlin.AO].
[254] Toby St Clere Smithe. “Open Dynamical Systems as Coalgebras for Polynomial Func-
tors, with Application to Predictive Processing”. 06/08/2022. arXiv: 2206 . 03868
[math.CT].
[255] Toby St Clere Smithe and Simon M Stringer. “The Role of Idiothetic Signals, Landmarks, and
Conjunctive Representations in the Development of Place and Head-Direction Cells: A Self-
Organizing Neural Network Model”. In: Cerebral Cortex Communications 3.1 (01/01/2022),
tgab052. issn: 2632-7376. doi: 10 . 1093 / texcom / tgab052. url: https : / /
325
academic.oup.com/cercorcomms/article/doi/10.1093/texcom/
tgab052/6358621 (visited on 12/23/2022).
[256] Kimberly L Stachenfeld, Matthew Botvinick, and Samuel J Gershman. “Design Principles of
the Hippocampal Cognitive Map”. In: Advances in Neural Information Processing Systems
27. Ed. by Z. Ghahramani et al. Curran Associates, Inc., 2014, pp. 2528–2536. url: http:
//papers.nips.cc/paper/5340- design- principles- of- the-
hippocampal-cognitive-map.pdf.
[257] Kimberly Lauren Stachenfeld, Matthew M Botvinick, and Samuel J Gershman. “The
Hippocampus as a Predictive Map”. In: (12/2016). doi: 10.1101/097170. url: http:
//dx.doi.org/10.1101/097170.
[258] Sam Staton. “Commutative Semantics for Probabilistic Programming”. In: Programming
Languages and Systems . Springer Berlin Heidelberg, 2017, pp. 855–879. doi: 10.1007/
978-3-662-54434-1_32 .
[259] Dario Maxmilian Stein. “Structural Foundations for Probabilistic Programming Languages”.
University of Oxford, 2021. url: https://dario-stein.de/notes/thesis.
pdf.
[260] Christopher Summerfield, Fabrice Luyckx, and Hannah Sheahan. “Structure Learning and
the Posterior Parietal Cortex”. In: Progress in Neurobiology (10/2019), p. 101717. doi: 10.
1016/j.pneurobio.2019.101717.
[261] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction . 2nd ed.
Vol. 1. 1. MIT press Cambridge, 2018.
[262] D Gowanlock R Tervo, Joshua B Tenenbaum, and Samuel J Gershman. “Toward the Neural
Implementation of Structure Learning”. In: Current Opinion in Neurobiology 37 (04/2016),
pp. 99–105. issn: 0959-4388. doi: 10.1016/j.conb.2016.01.014 . url: http:
//dx.doi.org/10.1016/j.conb.2016.01.014.
[263] Luke Tierney and Joseph B. Kadane. “Accurate Approximations for Posterior Moments
and Marginal Densities”. In: Journal of the American Statistical Association 81.393 (03/1986),
pp. 82–86. issn: 0162-1459, 1537-274X. doi: 10.1080/01621459.1986.10478240.
url: http://www.tandfonline.com/doi/abs/10.1080/01621459.
1986.10478240 (visited on 05/15/2023).
[264] Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. “Learning Action-Oriented
Models through Active Inference”. In: (09/2019). doi: 10.1101/764969.
[265] Kai Ueltzhöffer. “Deep Active Inference”. In: Biological Cybernetics 112.6 (2017-09-07, 2018-
10), pp. 547–573. doi: 10.1007/s00422-018-0785-7 . arXiv: 1709.02341
[q-bio.NC].
326
[266] Dániel Unyi et al. Utility of Equivariant Message Passing in Cortical Mesh Segmentation .
06/15/2022. arXiv: 2206.03164 [cs]. url: http://arxiv.org/abs/2206.
03164 (visited on 12/23/2022). preprint.
[267] Aad W. van der Vaart.Asymptotic Statistics. 1. paperback ed., 8. printing. Cambridge Series in
Statistical and Probabilistic Mathematics. Cambridge: Cambridge Univ. Press, 2007. 443 pp.
isbn: 978-0-521-78450-4 978-0-521-49603-2.
[268] Dmitry Vagner, David I. Spivak, and Eugene Lerman. “Algebras of Open Dynamical Systems
on the Operad of Wiring Diagrams”. In: Theory and Applications of Categories 30 (2015),
Paper No. 51, 1793–1822. issn: 1201-561X.
[269] Matthijs Vákár. Syntax and Semantics of Linear Dependent Types . 01/16/2015. arXiv: 1405.
0033 [cs, math]. url: http://arxiv.org/abs/1405.0033 (visited on
12/17/2022). preprint.
[270] Matthijs Vákár and Luke Ong. On S-Finite Measures and Kernels . 10/03/2018. arXiv: 1810.
01837 [math.PR]. preprint.
[271] Frank van der Meulen. Introduction to Automatic Backward Filtering Forward Guiding .
10/31/2022. arXiv: 2203.04155 [math, stat]. url: http://arxiv.org/
abs/2203.04155 (visited on 12/23/2022). preprint.
[272] Frank van der Meulen and Moritz Schauer. Automatic Backward Filtering Forward Guiding
for Markov Processes and Graphical Models . 10/31/2022. arXiv: 2010.03509 [stat].
url: http://arxiv.org/abs/2010.03509 (visited on 12/23/2022). preprint.
[273] Carl van Vreeswijk and Haim Sompolinsky. “Chaos in Neuronal Networks with Balanced
Excitatory and Inhibitory Activity”. In: Science (New York, N.Y.) 274.5293 (1996), pp. 1724–
1726. doi: 10.1126/science.274.5293.1724 . pmid: 8939866. url: http:
//www.ncbi.nlm.nih.gov/pubmed/8939866.
[274] F. G. Varela, H. R. Maturana, and R. Uribe. “Autopoiesis: The Organization of Living Systems,
Its Characterization and a Model”. In: George J. Klir. Facets of Systems Science . Boston, MA:
Springer US, 1991, pp. 559–569.isbn: 978-1-4899-0720-2 978-1-4899-0718-9.doi: 10.1007/
978-1-4899-0718-9_40 . url: http://link.springer.com/10.1007/
978-1-4899-0718-9_40 (visited on 12/23/2022).
[275] Yde Venema. “Automata and Fixed Point Logic: A Coalgebraic Perspective”. In: Information
and Computation 204.4 (04/2006), pp. 637–678. doi: 10.1016/j.ic.2005.06.003.
[276] Pietro Vertechi. “Dependent Optics”. In: Electronic Proceedings in Theoretical Computer
Science. Applied Category Theory 2022. Vol. 380. 08/07/2023, pp. 128–144. doi: 10.4204/
EPTCS.380.8. arXiv: 2204.09547 [math.CT]. url: http://arxiv.org/
abs/2204.09547v4 (visited on 09/29/2023).
327
[277] Andre Videla and Matteo Capucci. Lenses for Composable Servers . 03/29/2022. arXiv: 2203.
15633 [cs]. url: http : / / arxiv . org / abs / 2203 . 15633(visited on
12/17/2022). preprint.
[278] James C. R. Whittington and Rafal Bogacz. “An Approximation of the Error Backpropagation
Algorithm in a Predictive Coding Network with Local Hebbian Synaptic Plasticity”. In:
Neural Computation 29.5 (05/01/2017), pp. 1229–1262. issn: 0899-7667, 1530-888X. doi:
10 . 1162 / NECO _ a _ 00949. url: https : / / direct . mit . edu / neco /
article/29/5/1229/8261/An- Approximation- of- the- Error-
Backpropagation (visited on 12/23/2022).
[279] James C. R. Whittington et al. “The Tolman-Eichenbaum Machine: Unifying Space and
Relational Memory through Generalisation in the Hippocampal Formation”. In: (09/2019).
doi: 10.1101/770495 . eprint: https://www.biorxiv.org/content/
biorxiv/early/2019/09/24/770495.full-text.pdf.
[280] C.K.I. Williams and D. Barber. “Bayesian Classification with Gaussian Processes”. In: IEEE
Transactions on Pattern Analysis and Machine Intelligence 20.12 (12/1998), pp. 1342–1351.
issn: 01628828. doi: 10.1109/34.735807. url: http://ieeexplore.ieee.
org/document/735807/ (visited on 05/15/2023).
[281] Daniel Williams. “Is the Brain an Organ for Free Energy Minimisation?” In: Philosophical
Studies 179.5 (05/2022), pp. 1693–1714. issn: 0031-8116, 1573-0883. doi: 10 . 1007 /
s11098-021-01722-0 . url: https://link.springer.com/10.1007/
s11098-021-01722-0 (visited on 12/23/2022).
[282] Donald Yau. Operads of Wiring Diagrams . Vol. 2192. Lecture Notes in Mathematics. Cham:
Springer International Publishing, 2018. isbn: 978-3-319-95000-6 978-3-319-95001-3. doi:
10.1007/978-3-319-95001-3 . arXiv: 1512.01602 [math]. url: http:
//link.springer.com/10.1007/978- 3- 319- 95001- 3 (visited on
09/11/2023).
[283] J.S. Yedidia, W.T. Freeman, and Y. Weiss. “Constructing Free-Energy Approximations and
Generalized Belief Propagation Algorithms”. In: IEEE Transactions on Information Theory
51.7 (07/2005), pp. 2282–2312. issn: 0018-9448. doi: 10.1109/TIT.2005.850085 .
url: http://ieeexplore.ieee.org/document/1459044/ (visited on
12/23/2022).
[284] Ernst Zermelo. “Über Eine Anwendung Der Mengenlehre Auf Die Theorie Des Schachspiels”.
In: Proceedings of the Fifth International Congress of Mathematicians . Vol. 2. Cambridge
University Press, 1913, pp. 501–504.
[285] Jie Zhou et al. “Graph Neural Networks: A Review of Methods and Applications”. 12/20/2018.
arXiv: 1812.08434v4 [cs.LG].
328