Learning, Planning, and Control in a Monolithic
Neural Event Inference Architecture
MartinV.Butz1 DavidBilkey2a DaniaHumaidan1 AlistairKnott2b
SebastianOtte1
1CognitiveModelingGroup
ComputerScienceDepartment
UniversityofTübingen
Sand14,72076Tübingen,Germany
2DepartmentofaPsychology/bComputerScience
UniversityofOtago
P.O.Box56,Dunedin,NewZealand
Abstract
We introduce REPRISE, a REtrospective and PRospective Inference SchEme,
whichlearnstemporalevent-predictivemodelsofdynamicalsystems. REPRISE
inferstheunobservablecontextualeventstateandaccompanyingtemporalpredic-
tivemodelsthatbestexplaintherecentlyencounteredsensorimotorexperiences
retrospectively. Meanwhile,itoptimizesupcomingmotoractivitiesprospectively
in a goal-directed manner. Here, REPRISE is implemented by a recurrent neu-
ralnetwork(RNN),whichlearnstemporalforwardmodelsofthesensorimotor
contingencies generated by different simulated dynamic vehicles. The RNN is
augmentedwithcontextualneurons, whichenabletheencodingofdistinct, but
related,sensorimotordynamicsascompacteventcodes. WeshowthatREPRISE
concurrently learns to separate and approximate the encountered sensorimotor
dynamics: itanalyzessensorimotorerrorsignalsadaptingbothinternalcontextual
neuralactivitiesandconnectionweightvalues. Moreover,weshowthatREPRISE
canexploitthelearnedmodeltoinducegoal-directed,model-predictivecontrol,
thatis,approximateactiveinference: Givenagoalstate,thesystemimaginesa
motor command sequence optimizing it with the prospective objective to mini-
mizethedistancetothegoal. TheRNNactivitiesthuscontinuouslyimaginethe
upcomingfutureandreflectontherecentpast,optimizingthepredictivemodel,
thehiddenneuralstateactivities,andtheupcomingmotoractivities. Asaresult,
event-predictiveneuralencodingsdevelop,whichallowtheinvocationofhighly
effectiveandadaptivegoal-directedsensorimotorcontrol.
1 Introduction
Thepredictivebrainperspectiveandactiveinferenceprincipleshavestronglyinfluencedcognitive
scienceoverthelastyears(Bar,2009;ButzandKutter,2017;Clark,2016;Friston,2009;Hohwy,
2013). Although predictive encodings have shown to yield promising results in artificial neural
networks focusing on vision (Rao and Ballard, 1999), it remains highly challenging to realize
these principles in scalable, temporal dynamic artificial neural network models, and particularly
modelsthatenableflexible,goal-directedplanning(butseeNajninandBanerjee,2017forarecent,
Preprint.Workinprogress.
9102
yaM
2
]GL.sc[
2v21470.9081:viXra
promisingapproachinphonologicalspeechproduction).Moreover,itremainsunclearhowabstracted,
hierarchicalstructuresmaybedevelopedeffectively(BotvinickandWeinstein,2014;McClelland
et al., 2010) – structures that are believed to be essential for enabling the generation of flexible,
adaptivegoal-directedbehaviorbymeansofhierarchical,model-basedplanningandreinforcement
learning(Botvinicketal.,2009).
Despitetherecentremarkablesuccessesinplayingsomewhatchallengingcomputergamesandthe
boardgameGO(Mnihetal.,2015;Silveretal.,2016),neuralnetworksgenerallystillseemtolacka
deeperunderstandingoftheunderlyingproblemdomain.Asaresult,thedevelopedsystemsarerather
inflexible,forexample,whenmultiple,differenttasksneedtobesolvedbythesamearchitecture
orwhentherewardfunctionchanges. Therearecertainlystrategiesthatcanhelp—suchasmore
effectiveepisodicreplayortask-specificweightandneuralmanipulations(Kirkpatricketal.,2016).
Nonetheless,deepandrecurrentneuralnetworksdonotyetlearnandthinkwiththeflexibilityof
humans(Lakeetal.,2017).
Weintroduceanovelretrospectiveandprospectivetemporalinferencescheme(REPRISE),whichwe
implementinrecurrentartificialneuralnetworks(RNNs). REPRISEcombinesweightadaptation
(i.e. modelinference)withneuralactivityadaptation(i.e. contextualhiddenstateinference)and
model-predictive,anticipatorycontrolandbehavior(i.e. activeinference). Asaresult,REPRISE
doesnotoptimizeastaticrewardfunction,butitcanflexiblyplancontext-andtask-dependently.
Moreover,REPRISEoffersafirststeptowardsthedevelopmentofhierarchicalhidden,generative
structures,whichcanbecloselyrelatedtotheconceptofeventcognition.
Theeventcognitionprinciplecomesfromcognitivepsychology. Itwasshownthathumanshave
astrongtendencytosegmentacontinuoussensorimotorstreamintomeaningfuleventsandevent-
transitions,leadingtotheproposalofaneventsegmentationtheory(EST)(RadvanskyandZacks,
2014;ZacksandTversky,2001;Zacksetal.,2007). Concurrently,thetheoryofeventcoding(TEC)
hasproposedintegrativeaction-effectcodes,referringtothemaseventcodes(Hommeletal.,2001).
Similarly, forward-inverse control schemes have been put forward as models of human behavior
(Wolpert and Kawato, 1998; Wolpert and Flanagan, 2016), where the involved forward-inverse
modelsessentiallyencodeinteractionevents. Eventhememorizationofexperiencedepisodesappears
event-segmentedandevent-focused(Richmondetal.,2017). Moreover,memorizedeventscanbe
usednotonlyforprocessingcurrentsensorimotorinformation,butalsoforreflectingonthepast,
forimaginingpotentialfutures,orevenforreasoningaboutfullyhypotheticalevents(Bar,2009;
BucknerandCarroll,2007;Schacteretal.,2012). Combinedwiththepredictivecodingperspective
on cognition, our mind appears to have the tendency to cluster sensorimotor contingencies into
predictiveevents(Butz,2016).
REPRISEoffersanRNN-basedneuralimplementationthatshowstheemergenttendencytocluster
differenttypesofpredictivelyencodedsensorimotordynamicsintocompacteventcodeswithoutthe
provisionofevent-typeorevent-boundaryinformation. RelatedworkwithRNNshasimplemented
hierarchical RNN architectures that develop symbol-like encodings in the deeper RNN layer via
gradientdescent(Tani,2003). Later,theprocesswastermedanerrorregressionschemeandwas
closelyrelatedtothefreeenergyprinciple(Murataetal.,2017). Relatedworkhasalsobeenput
forwardwhensettinginternalhiddenstates—oftenreferredtoasparametricbiasneurons—to
induceparticularbehavioralprimitivesandsequencesthereofbysuitablytrainedhierarchicalRNN
architectures(Arieetal.,2009;Tani,1996;Sugitaetal.,2011). Interestingly,theactivitiesofthe
parametricbiasneuronswereshowntoexhibitcompositional,pre-linguisticstructures(Sugitaand
Butz,2011;Sugitaetal.,2011),whichwerelinkedwithalanguageproductionsystemelsewhere
(SugitaandTani,2005).
REPRISEcombinesgoal-directed,active-inference-basedcontrolwiththelearningofstablehidden
contextualstates—withoutthehelpofepisodictraining,theprovisionofinteractiontypes,orboundary
signalsbetweeninteractions. Werelatetheinferenceofthesehiddenstateswithevent-predictive
encodings,whichtendtocontrastdistinctsensorimotordynamics. Somewhatsimilarstablestates
havebeeninferredrecentlyinaspikingneuralnetworkarchitecture,wherefreeenergyminimization
techniqueswereappliedtoestablishtemporarybindingsinadistributednetwork(Pittietal.,2017).
Moreover,stochasticsearchwasusedinthesespikingnetworkstoinducegoal-directedbehavior.
InREPRISE,context-adaptivegoal-directedbehaviorisgeneratedviagradient-based,prospective
inference, which is closely related to active inference with respect to the free energy formalism
(Friston,2009). Asaresult,REPRISEadaptsitsinternalstateanditsmotorbehaviorinsuchaway
2
that approximately optimal goal-directed behavior is generated. From a control perspective, the
systemcanbesaidtoapproximatemodel-predictivecontrol(CamachoandBordons,1999),where
themodelislearnedbyandthenencodedinanRNN.
Asafirstevaluationscenario, wetrainREPRISEtocontrolthreedifferenttypesof“vehicles”in
a simple but dynamic 2D simulated environment. Crucially, we show that REPRISE is able to
distinguishthethreetypesofvehiclesandcontrolthemeffectivelyinagoal-directedmannereven
whennoinformationaboutthevehicleidentityoreventhefactthattherearethreedifferentvehicles
isprovided. Webelievethatthismethodmaybeverywell-suitedtolearnevent-orientedabstractions
andeventhierarchies,butfutureworkisnecessarytoscalethesystemandapplyittomorechallenging
scenarios.
2 SystemArchitectureandInferenceMechanisms
Wenowdetailthemechanismsimplementedbyourretrospectiveandprospectivetemporalinference
scheme(REPRISE),whichisimplementedinanRNN.REPRISEinferstheunobservablecurrent
eventcontext(herethecontrolledvehicle),whichbestexplainstherecentsensorimotorexperiences,
retrospectively.Meanwhile,itinfersmotorcontrolcommandsprospectivelyinagoal-directedmanner.
Wetherebybuildonourpreviouswork,whichhadaccomplishedprospective,activemotorcontrol
inference(Otteetal.,2017a,b)butnotretrospectiveinference. OurresultssuggestthatREPRISEcan
learnandapplyboth,effectivegoal-directedcontrolandevent-oriented,systemstateinference.
InordertointroduceREPRISE,wedistinguishbetweentheactual(notdirectlyobservable)dynamical
systemφandthemodelΦofthissystem,whichisencodedbyanRNN.Focusingonadiscrete-time
dynamical system, at a certain point in time t, the (not directly observable) current state of the
dynamicalsystemφmaybedenotedbyϑt,suchthattheprogressionthroughtimeisdeterminedby
φ
ϑt (cid:55)−−→ϑt+1. (1)
2.1 TemporalForwardModel
ThemodelΦistrainedtoapproximatethesedynamics,inferringitsparametersfromsensorimotor
experiencesduringlearning.However,seeingthatwearedealingwithadynamic,partiallyobservable
Markovdecisionprocess(POMDP)(SuttonandBarto,1998),thetruesystemstateϑtistypicallynot
directlydeduciblefromcurrentobservablesst ∈Rn. Thus,thedynamicalsystem’sinternalstateσt
mustbeinferredineachiterationfromthecurrentobservablesst,thecurrentmotoractivitiesdenoted
byxt ∈Rk,andtheprevioussystemstateestimateσt−1. Withthehelpofthesystem’smodelΦ,the
nextsystemstateσt+1andtheconsequentsensoryexpectations˜st+1aredeterminedby
Φ
(st,σt−1,xt)(cid:55)−−→(˜st+1,σt), (2)
wherethemappingΦessentiallymodelsthetemporalforwarddynamicsofthesystem. Thus,the
next system state and sensory expectations depend on the current sensor (st) and motor control
(xt)activitiesaswellas,inprinciple,ontheentirestatehistory,whichisencodedinthe(hidden)
statecomponents(σt−1)incompressedform. Figure1showstheinputandoutputsignalsthatare
processedviathemodelΦ.
Whilelearningthemodel,thatis,whilepursuingmodelinference,thesystemattemptstominimize
thesquaredlossbetweenpredictedandencounteredsensoryinformationovertime,thatis,
T n
(cid:88)(cid:88)1
L= (˜st−st)2, (3)
2 i i
t=1 i=1
summingtheaccumulatedlossesoverthegatheredexperiences{s ,...,s }.
1 T
2.2 MultipleDynamicalSystems
In this paper we consider the challenge when not only a single dynamical system needs to be
controlled,butanensembleofmultipledynamicalsystemsφ={φ ,...,φ },whichcausedifferent
1 u
sensorimotorcontingenciesovertime.Thesesystemsdifferfromeachotherconcerningtheirbehavior,
3
F
Figure1: Illustrationoftherecurrent,temporalpredictivesensorimotorforwardmodelΦ’sinputto
outputprocessingactivities,includingneuralcontextualsignalsct.
but share the same input, state, and output dimensions. During model inference, the model Φ is
trained to approximate all of these dynamical systems within one monolithic RNN architecture.
As a result, the challenge is to approximate the particular dynamical system φ that is currently
i
active,givenobservedstatest andcontrolcommandsxt,inordertobeabletoaccuratelypredict
future sensorimotor dynamics and thus to control the dynamical system itself in an anticipatory,
goal-directedmanner.
WhenevaluatingREPRISEwewillfirstprovidetheidentityofthecurrentlyactivedynamicalsystem
φ inadditionalcontextinputneuronsc∈Ru,whichissimplyaddedasadditionalinputandwhich
i
isinitiallyencodedasaone-hotvector(i-thcomponentissetto1,restto0). Thisencodingisclosely
related to parametric bias neurons, which can be viewed as an indicator of the current event the
systemissituatedin(Sugitaetal.,2011;Tani,2017). Duringgoal-directedcontrol,however,wewill
inferthevaluesofthisvectoronline. Lateron,wewillinferthisvectoralsoduringtraining(never
providinginformationaboutthecurrentvehicleidentity)andwillshowthatthedifferentvehicles
tendtobeencodedseparatelyintheprovidedcontextualneurons.
2.3 REPRISE
Givenanimaginedactionsequence,aninitialstate,andtheidentityofthecurrentdynamicalsystem,
the RNN can predict a state progression that is expected when executing the action sequence by
meansofthelearnedtemporalforwardmodelΦ. Tocontrolthesystemeffectively, however, the
inversemappingisrequired,thatis,anactionsequenceneedstobeinferredtoapproachadesired
goal-state(orfollowasequenceofgoal-states)fromaninitialstate. Thisbecomesevenmoredifficult
whentheidentityofthecurrentactualdynamicalsystemφ isunknownandhastobeinferredaswell.
i
InthissectionweintroducetheREPRISEalgorithm—aconcurrentretrospectiveandprospective
inferencescheme,whichsolvesthetwofoldsystemidentificationandgoal-directedcontrolproblem.
Figure2showsthedynamicprocessesREPRISEunfoldsfortwoconsecutivetimesteps. Duringeach
step,botharetrospectiveandaprospectiveinferencephaseisexecuted.
Intheretrospectivephase,thegradientispropagatedRtimestepsintothepast,toreflectonthestates
that were justexperienced. Thegradient is fed bythe discrepancy between previously predicted
system states˜st−i, with i ∈ 0,...,R, and the actually observed system states st−i, minimizing
the quadratic loss over this time horizon (3). The discrepancy is then mapped onto the assumed
contextinputct−i—essentiallyasub-vectorofst−i—indicatingthedynamicalsystemφ thatis
i
presumablycurrentlyactive. BecauseREPRISEisdesignedtodistinguishdifferentevents,cissetto
aconstantvalueoverthetimehorizonR—essentiallysummingupthegradientsignalsreceivedby
ct−1...ct−R whenapplyinggradientdescent. Additionally,theerrorgradientcanbeusedtoadapt
theRNN’shiddenstateattimestept−R−1,thatis,σt−R−1,suchthatitbetterfitsthechanging
contextinput. Asaresult,theRNNavoidsdisadvantageousorevenundefinedsensoryinput,motor
command, hidden state combinations. After neural activity adaptation via gradient descent with
learningratesη andη ,respectively(weapplyAdam,KingmaandBa,2014),theneuralactivities
c σ
arepropagatedforwardagaintothepresenttimestep,withrespecttotheinferredhiddenstateand
contextinput,andthealreadyrecordedmotorcommandsandobservedsystemstates,yieldingan
updatedσt. Thisretrospectiveneuralactivityinferencecyclemaybeexecutedrtimes.
4
x˜t x˜t+1 ... x˜t+P91
˜st
1
9R+1 ... ˜st 191 ˜st
1
˜st
1
+1 ˜st
1
+2 ... ˜st
1
+P91 ˜st
1
+P
σt9R91 σt93 ... σt91 σt σt+1 ... σt+P91
st9R+1 ... st91 st ?st+1 ?st+2 ... ?st+P91 ?st+P
... ... ... ... ... ... ...
˜st r9R+1 ... ˜st r91 ˜st r ˜st p +1 ˜st p +2 ... ˜st p +P91 ˜st p +P
σt9R91 σt93 ... σt91 σt σt+1 ... σt+3
st9R+1 ... st91 st ?st+1 ?st+2 ... ?st+P91 ?st+P
c˜t9R ... c˜t91 x˜t x˜t+1 ... x˜t+P91 x˜t+P
˜s1 t9R+2 ... ˜st 1 ˜st 1 +1 ˜st 1 +2 ˜st 1 +3 ... ˜st 1 +P ˜st 1 +P+1
σt9R σt92 ... σt σt+1 σt+2 ... σt+P
st9R+2 ... st st+1 ?st+2 ?st+3 ... ?st+P ?st+P+1
... ... ... ... ... ... ...
˜sr t9R+2 ... ˜st r ˜st r +1 ˜st p +2 ˜st p +3 ... ˜st p +P ˜st p +P+1
σt9R σt92 ... σt σt+1 σt+2 ... σt+4
st9R+2 ... st st+1 ?st+2 ?st+3 ... ?st+P ?st+P+1
c˜t9R+1 ... c˜t x˜t+1 x˜t+2 ... x˜t+P
Figure2: IllustrationofREPRISEfortwoconsecutivetimestepst(toppart)andt+1(bottom
part). NotethatthereisonlyoneRNN,whoseactivitiesσarebufferedovertime. Theright(green
shaded)boxesillustratefutureimaginationsactivelyinferringprospectivemotoractivities,while
the left boxes (gray shaded) show retrospections about the recent past for system state inference
(includingeventstatect(cid:48) andhiddenstateσt). Blacklinesindicatecontextandinformationforward
flow, while the red lines indicate gradient flow. x˜t(cid:48) and c˜t(cid:48) refer to the action and context input
vectors,respectively,foraparticulartimestept(cid:48).˜st(cid:48) referstoaparticularsensorypredictioninthe
τ
τ-thoptimizationcycle,whereass(cid:63)t(cid:48) referstoadesiredsensorygoalstate.
Additionally,retrospectiveweightadaptationisappliedduringmodellearning,againfocusingon
minimizingthequadraticloss(3). Thisessentiallycorrespondstostandardback-propagationthrough
timelearning. NotethatweusuallyuseadeeperretrospectivetimehorizonR formodellearning
w
andasmallerlearningrateη ,whencomparedtoη andη . OtherwisetheRNNwouldbehavemore
w c σ
likeanadaptivefilterasitwouldnotlearnthe(versatile)modelcharacteristicsbutratherover-fitthe
recentsignalshape.
Intheprospectivephase,neuralactivitiesareprojectedP timestepsintothefuture,startingwiththe
inferredcurrentinternalsystemstateσtandhypotheticallyexecutingasequenceofmotorcommands
x˜t+i,whichwasinferredpreviously. Thediscrepanciesbetweenthepredictedfuture˜st+ianddesired
goalstatesequencess(cid:63)t+i,withi∈1,...,P,arethenpropagatedbackwardsthroughtimefromthe
imaginedfuturebacktothepresenttimestep, whilethegradientisprojectedontotheindividual
anticipatedmotoractivitysequencex˜t+i,effectivelyoptimizingitinthelightofthecurrentsystem
stateestimatesandthedesiredgoalstate. Thisprospectiveinferencecycleisexecutedptimes.
Aftertheretro-andprospectiveinferencephases,theinferredmotoractivityxt isexecutedbythe
systemφandtheforwardRNNisupdatedvia(2),closingtheprocessingloop. ThenREPRISEis
repeatedinthefollowingtimestep(t+1).1
3 SystemEvaluations
Ourexperimentsarebasedonatwodimensionaldynamicalsystemsimulationwithu=3typesof
“vehicles”,constitutingthreedynamicalsystems:
• φ isamulti-copter-likevehicle,whichwecallrocket,
1
• φ isastaticomnidirectionalvehicle,whichwecallstepper,and
2
1REPRISEJavacodecanbefoundinthefollowinggithubrepository:
https://github.com/CognitiveModeling/2019-ModeInferencePaperCode.
5
0.01
0.001
0 500 1000 1500 2000 2500 3000
rorre
.derp
Online Learning Performance Comparison
RNN 27: pred.error
RNN 36: pred.error
RNN 54: pred.error
LSTM 08: pred.error
LSTM 16: pred.error
LSTM 24: pred.error
number of epochs (with 2k control steps each)
Figure3: LearningprogresscomparingstandardRNNsandLSTMswithdifferentnumbersofhidden
units. Learningrateisreducedbyafactorof.1after1kand2kepochs.
• φ isadynamical,omnidirectionalglidingvehicle,whichwecallglider.
3
Therocketisinfluencedbysimulatedgravityandundergoesinertia. Ithastwopropulsionmotorsthat
arespreadata45◦anglefromtheverticalaxisonbothsides,inducingthrustforcesintherespective
direction. Thetwoothermotorinputsareignoredinthecaseoftherocket. Thestepperhasfour
thrustmotorsthatarespreadat45◦and135◦anglefromtheverticalaxistobothsides,inducingsteps
intheoppositedirection. Finally,thegliderhasthesamefourthrustmotorsasthestepper. However,
incontrasttothestepper,thegliderundergoesinertiawithoutanyfriction. Eachmotorunitcanbe
throttledwithintheinterval[0,1]. Uponinvocation,eachvehicleispositionedinarectangularfree
spaceofsize3×2units. Itissurroundedbyborders,whichblockthevehicle.
All presented evaluations below are based on ten independently trained networks, averaging the
achieved results. Here, we evaluate REPRISE when the context neuron activities are set during
trainingtodistinctone-hotvectorvaluesforthedifferentvehicles. InSection4,wetheninvestigate
learningandperformancewhencontextvaluesareinferredduringlearningaswell.
3.1 ModelLearning
Duringtraining,stochasticback-propagationthroughtimeoptimizedtheweightsoftheconsidered
RNN architectures based on simulated sensorimotor experiences, learning in a self-supervised
manner. Experiencesweregeneratedbyexecutingpseudo-randommotorcommandsx ∈ [0,1]4,
where motor command generation was such that sufficient upwards thrust was generated and a
reasonableexplorationofthecompleterectangularfreespacewaslooselyensured.
Ateachtimestepthenetworkisfedwiththecurrentpositionofthevehicle(s∈{[−1.5,1.5],[0,2]}),
thecurrentfourmotorcommands(activitiesofthefourthrustmotorsasforces;fortherocket,the
secondtwomotorvalueshavenoeffect;massofvehiclesissetto.1),andathreebitone-hotvector,
whichindicatesthevehiclethatiscurrentlycontrolled,i.e.,whichφ applies. Thenetworkpredicts
i
thevehicle’sresultingchangeinposition.
WetrainedtheconsideredRNNsin3000independentepochs,consistingof2000pseudo-random
controlstepseach. Weappliedback-propagationthroughtimeevery50iterationsandAdamasthe
weightadaptationmechanism(KingmaandBa,2014). Thelearningratewasannealed,suchthat
η =.001,η =10−4,η =10−5duringthefirst,second,andthird1000epochs,respectively. (First
6
andsecondmomentsmoothingfactorsweresettothestandardvaluesβ =0.9,β =0.999.) Each
1 2
vehiclewassimulatedfor2000timesteps(i.e. oneepoch),afterwhichthehiddenstateoftheRNN
wasresettozeroandanewvehiclewasinitialized.
Figure3contraststhesensorypredictionerrordevelopmentduringlearningforseveralRNNarchi-
tectures, showingaveragedmeanerrorsandstandarddeviationsacross20independentlyweight-
initialized(normallydistributedvalueswithstandarddeviation0.1)networks. StandardRNNswith
onehiddenlayerof27(1026),36(1692),and54neurons(3510weights)performconsistentlyworse
thanlongshort-termmemory(LSTM)RNNswithforgetgatesandpeepholeconnections(Gersetal.,
2002). While16hiddenmemorycells(1680weights)clearlyoutperform8hiddenmemorycells
(584weights),theadvantageofyetanother8hiddencells,thatis,24cellsintotal(3288weights)is
lesspronounced.
3.2 REPRISEPerformance
ToevaluatetherobustnessandabilitiesofREPRISE,includingallrelevantsettings,wecontrastthe
resultingcontrolperformanceoftheRNNwith36hiddenneuronswiththeLSTMwith16hidden
units,whichhaveapproximatelyanequalnumberofweights(1692versus1680,respectively). Each
networkwastestedtoreachasequenceof50uniformlyrandomlypositionedtargetswithinacentered
innerareaofsize1.5×1.5units,i.e. s(cid:63) ∈ [−.75,.75]×[.25,1.75]oftherectangularenvironment.
Thereby,thesimulationisdividedintoasequenceofdiscrete‘events’,wheretheagent‘becomes’
oneofthevehiclesφ for150timesteps. Oneoftheagent’stasksistoinferwhichoftheseeventsis
i
underwayatanygiventime. Thevaluesinthetablesbelowareaveragesoverthe20independently
trainednetworksand50consideredtargets,wherebythetargetpositionsandvehiclesuccessions
werethesameforallruns.
WeappliedAdaminallinferenceprocesses. ProspectiveinferencewasalwaysP = 7stepsinto
thefuture,executingtheinferencecyclep = 20times. Elsewhere,wehavestudiedthisinfluence
(without retrospective inference), showing that even longer prospective inferences are generally
possible(Otteetal.,2017a). Detailedevaluationswereruncontrastingdifferentlearningratesη and
c
η fortheretrospectivecontextcandsystemstateσinference. Inourstandardsetting,retrospective
σ
inferencecoveredR=20timestepsintothepast,whiler =20inferencecycleswereperformed. In
ourexperience,theretrospectivehorizonRmayvaryquiteabitstillyieldingrobustresults,while
thenumberofgradientdescentcyclesneedstobesufficientlylarger >2,butagainaratherwide
valuerangeyieldedcomparableresults. Notethatduringoptimizationthemotorcommandsandthe
contextinputswereclampedtotheirvaluerange[0,1],andtheneuralhiddenstatesσwereclamped
inaccordancetotherangeoftherespectiveneurons’activationfunction.
Figure4showstypicalflightsequencesgeneratedbyanLSTMcontrolledbytheREPRISEalgorithm,
inteniterationsteps. Althoughgliderandrocketinitiallyslightlyovershootthetarget,theyquickly
zoomin. Forthestepper,theprojectedpathislessdirect,whichisprobablypartiallythecasebecause
thegoalissimplynotdirectlyreachableinsevensteps. Itshouldbenotedthatalthoughtheimages
suggestthatthemotoreffortwhilestayingatthegoalisminimized,thisisnotalwaysthecase,as
thereiscurrentlynoincentiveinthesystemthatstressesmotoreffortminimization.
Tables1and2showtheaveragedistancetothegoallocationthatremainedafter150timesteps,
thatis,controliterationsaftertargetonset. Thefirstrowofresultsshowstheperformancewhenthe
contextbitsaresettothecorrectvalues(nostateinference)whilethehiddenstatesoftheLSTM
areadaptedwithvaryinglearningratesη . Sincetheinformationaboutwhichvehicleiscurrently
σ
beingcontrolledisprovided,a(muchsimpler)activemotorinferenceproblemissolved,yielding
robustandaccurategoalreachingbehavior. ThenextfourrowsshowtheperformanceofREPRISE
whenthecontextinformation,thatis,whichvehicleiscurrentlybeingcontrolled,isnotprovided.
Resultsfordifferentlearningratecombinationsareshown: η determinesthestrengthofadaptingthe
c
contextualneuralstatesc,whileη controlsthehiddenneuralstateσadaptations. Clearly,overly
σ
largeorsmallvaluesyieldmediocreperformance. However,quitealargevaluerangeyieldsrobust
targetreachingbehavior. Consistentlythebestsettingiswithη =.01andη =.001,adaptingthe
c σ
contextbitstentimesfasterthanthehiddenstates,whichismostlikelythecasebecausewithout
propercontextinputs,overlyfasthiddenstateadaptationswillleadtounstablebehavior. Thus,most
robustperformanceisreachedwhenbothcontextandhiddenstateactivitiesareadapted,yielding
performancethatisactuallycompetitive—intheRNNcaseevensuperior—totheonewhenthe
contextinformationisprovided! Insum,withsufficientlysmallstateinferencelearningratesη ,the
σ
7
Figure 4: Typical flight sequence for the three vehicle types controlled by REPRISE, showing 8
screenshotsofglider,rocket,andstepper,whichare10timestepsapartsuccessivelyintheupper,
middle,andbottomtworows,respectively. Thegreentargetisapproached. Theredlinesshowthe
currenttrajectoryanticipationofREPRISE.
additionalstateinference(muchharderproblem,nocontextbitinformationprovided)doesnotaffect
performanceinanegativemanner!
Table1additionallyshowstheperformancedifferenceswhenfocusinginonthethreedifferentvehicle
typesintheLSTMcase. Whiletheparameterdependenciesareverysimilar,theresultsindicatethat
itwasmostdifficulttomovetherockettowardsandkeepitclosetothegoallocation. Thisismost
likelyduetothefactthatgravityneedstobecontinuouslycounteractedinthecaseoftherocket,but
notinthecaseofthestepperorglider.
8
Table1: LSTM:Distancetotargetafter150controlsteps
Average η =0 η =1e-4 η =.001 η =.01 η =.1
σ σ σ σ σ
cset 0.006 0.006 0.006 0.007 0.051
η =1e-4 0.082 0.057 0.034 - -
c
η =.001 0.039 0.024 0.011 0.011 -
c
η =.01 0.024 - 0.006 0.007 0.055
c
η =.1 0.025 - - 0.008 0.038
c
Rocket η =0 η =1e-4 η =.001 η =.01 η =.1
σ σ σ σ σ
cset 0.014 0.014 0.013 0.015 0.095
η =1e-4 0.069 0.052 0.022 - -
c
η =.001 0.041 0.037 0.013 0.011 -
c
η =.01 0.026 - 0.006 0.011 0.069
c
η =.1 0.028 - - 0.010 0.051
c
Stepper η =0 η =1e-4 η =.001 η =.01 η =.1
σ σ σ σ σ
cset 0.002 0.001 0.001 0.001 0.006
η =1e-4 0.063 0.043 0.022 - -
c
η =.001 0.037 0.017 0.008 0.014 -
c
η =.01 0.024 - 0.007 0.005 0.035
c
η =.1 0.025 - - 0.004 0.024
c
Glider η =0 η =1e-4 η =.001 η =.01 η =.1
σ σ σ σ σ
cset 0.004 0.004 0.004 0.004 0.053
η =1e-4 0.116 0.078 0.060 - -
c
η =.001 0.040 0.017 0.012 0.009 -
c
η =.01 0.021 - 0.005 0.006 0.061
c
η =.1 0.021 - - 0.010 0.039
c
Table2: RNN:Distancetotargetafter150controlsteps
Average η =0 η =1e-4 η =.001 η =.01 η =.1
σ σ σ σ σ
cset 0.037 0.039 0.037 0.035 0.036
η =1e-4 0.162 0.019 0.019 - -
c
η =.001 0.038 0.052 0.020 0.018 -
c
η =.01 0.040 - 0.017 0.019 0.029
c
η =.1 0.169 - - 0.044 0.034
c
Table3showsfortheLSTMcasethattheaveragedistancetothetargetobjectoverthe150steps
averagedoverallvehiclesissmallestwhenthecontextinformationisprovided. Thiswasindeed
thecaseforallthreevehicles(notshown). Thisisexpectableascontextinferenceinevitablyyields
erroneousbehaviorduringthefirstcontrolsteps,confirmingthattheswitchinthevehicleidentity
causesinitialdisruptions,whicharequicklystabilized.
Asafinalevaluation,Figure5showstheinferredcontextinputactivationsforthethreevehicles,
contrastingagainLSTMwithRNNperformance. TheresultsindicatethattheLSTMarchitectureis
better-suitedtoinfertheunderlyingcontrolsystem,seeingthatthecorrectvehiclewinsinallthree
casesandthewinnerismoreseparatedfromthetwoalternativeswhencontrastedwiththeRNN
performance. Clearly,though,theresultsareverynoisyandfarfromoptimal. Itwasobservedthat
oncethegoalhasbeenreached,theestimatessometimesdriftedofftowardsmoreincorrectestimates
—probablybecausethesensorimotorinformationisnotsufficientlyinformative. Thisobservationin
particularsuggeststhatcontextestimationstabilityshouldbeimprovedbyallowingcontextswitches
only only when error signals suggest to do so. Moreover, active motor inference may be further
optimized for the purpose of maintaining high context estimation certainty (Friston et al., 2015),
whichshouldleadtothegenerationofmotorcommandsthatminimizeuncertaintiesinthemodel
stateestimatesσ.
9
Table3: LSTM:Averageaccumulateddistancetotarget
Average η =0 η =1e-4 η =.001 η =.01 η =.1
σ σ σ σ σ
cset 0.061 0.060 0.060 0.061 0.109
η =1e-4 0.157 0.139 0.109 - -
c
η =.001 0.106 0.100 0.079 0.082 -
c
η =.01 0.090 - 0.072 0.077 0.127
c
η =.1 0.092 - - 0.077 0.115
c
1
0.8
0.6
0.4
0.2
0
LSTM 16: Rocket Stepper Glider RNN 36: Rocket Stepper Glider
yitivitca
txetnoc
derrefni
Context Inference Values
LSTM 16 inference: Rocket
Stepper
Glider
RNN 36 inference: Rocket
Stepper
Glider
Figure5: Inferredvaluesofthecontextvaluescwhenindicatorvalueswereprovidedduringtraining.
4 EmergentEventEncodings
Despitetheapparentlyrathernoisysignalsencounteredduringretrospectivecontextinferenceabove,
we were still very interested in whether REPRISE may be able to infer suitable context vector
activitiescalsoduringmodellearning. Theexperimentalsetupisidenticaltotheonedetailedabove,
exceptforthefactthatnocontextualinformationisprovidedatanypoint,neitheraboutwhichvehicle
iscurrentlybeingcontrollednoraboutavehicleswitchoraboutdistinctvehicleidentities. Asaresult,
contextstatecinferenceneedstobeappliedduringlearningaswell.
4.1 ControlPerformance
Again, we trained ten networks independently (with different weight initializations) on the task.
Duringtraining,vehicleswitchesoccurredeveryV steps(hereV =205). Contextvectorc,which
wekeepatlengththree,wasupdatedwitharateanddepthofR =2steps,repeatingthisretrospective
c
adaptationr =5timeswithalearningrateofη =.1. Weightswereupdatedwithalearningrate
c
of η = 10−4 and R = 30 steps in depth. Moreover, every 2000 steps, all neural activities in
w w
thenetworkwereresettozeroinordertoavoiduncontrolledactivityincrease. Duringtesting,we
generated100successivegoals(e.g. epochs),whichwererandomlyplacedinaninnerreachable
spaceofsize1.5×1.5. Typically,weadmittedG = 150stepstoreachthegoalandswitchedthe
vehicleconcurrentlywiththegoalswitch(V =150). Figure6showsthecontextguessesduringa
flightsequencecontrolledbyREPRISE.
Whentestingactiveinference-basedmodel-predictivecontrol, theEuclideandistancetothegoal
locations averaged over all 100 goals and all 10 networks reached a value of e = .0027 with an
10
adaptation rate of η = .1. Nearly the same error value was reached when η = .01, while the
c c
distanceincreasedtoe = .0057ande = .1406withη = .001andη = .0001,respectively. The
c c
reacheddistanceisclearlysmallerthan(i)whencontextinferenceisswitchedoffcompletely,which
yieldse=.0998,(ii)whenthelearningrateduringmodellearningisloweredtoη =.01yielding
c
e = .0200,or(iii)whenthevehicleswitchoccursmorefrequently,e.g. everyfivesteps,thanthe
contextadaptation,e.g. R =20,yieldinge=.0429. Ontheotherhand,whenthevehicleswitchV
c
duringlearningoccursrandomlybetweenevery20thand30thtimestepandR =2,acomparable
c
errorofe=.0032isachieved.
Interestingly,asshownabove,weachievedabestminimalerrorofe=.006whenthecontextvalues
were set to one-hot vectors during training, which is larger than e = .0027 achieved here. This
impliesthatthenetworkdevelopedcontextualstateindicatorsduringlearningthataremoresuitable
forthetaskthantheone-hotvectors. Thisinterpretationisanalyzedfurtherinthefollowingsection.
Theresultsimplyfurthermorethatareasonablewideparameterrangeyieldscomparableperformance.
Nonetheless,contextstateadaptationsneedtooccurmorefrequentlythanvehicleswitches(i.e. event
changes)andneedtobeadaptedwithasufficientlylargeadaptationrate,e.g.,η =.1.
c
4.2 CurrentControlledVehicleInference
Seeing the improved control performance when context inference is switched on during training
(i.e. noone-hotvectors),weanalyzedthedevelopmentofthecontextstateestimatesduringtesting
further. Herewefocusonreportingtheresultsforoneofthetennetworks,whichserveswellfor
illustratingthemainpoints. Qualitatively,theresultslooksimilarfortheothernetworks. Figure7
showstheadaptedcontextvaluescbeforeagoalswitch,thatis,attheendofeachofthe100epochs,
coloredaccordingtothevehiclethatwasjustcontrolled. Theresultsshowthreeobservableclusters,
indicatingthatthenetworkhaslearnedtoseparatethesensorimotordynamicsofthethreevehicles
intodistinctbutsomewhatoverlappingclusters.
Furtheranalysesrevealedthatthecontextstateestimatescandriftseverelyoncethetargetisreached.
ThisisparticularlythecasebecauseinthesetupStepperandGlideressentiallyjustneedtoremain
still at the goal state, which they can achieve by sending out zero motor commands but also by
sendingoutequallystrongonestoallfourmotors. Similarly,therocketcanmimicgravitybyits
ineffectivedownwardsthrustmotors. Thus,particularlyatthegoal,thesensorimotorsignalabout
whichvehicleiscurrentlycontrolledcanbecomeambiguous. Figure6showssuchanexemplary
case,whilethestepperiscontrolled. Duringtheinitialsteps,thesignalfromthepreviousvehicle
stillinfluencesthegradient. Then,thesignaltendsapproximatelytowardsthepreferredcenterofthe
stepper’scontextstatevector. Afterreachingthegoal,however,adriftofthevectorcanbenoticed.
Toavoidrecordingthecontextguessattheendafterithasdrifted,werecordedthecontextguessat
thetimestepwhenthedistancetothegoalwasminimal. Figure7showsthecorrespondingresults,
revealingsomewhatfocusedclustering.Onecannoticethatthenetworkisseparatingthecontextcode
fortheRocket(inred)well,butsometimesthecontextguessesgetconfusedbetweentheStepper(in
green)andtheGlider(inblue). Toanalyzethisobservationfurther,wehavecalculatedtheEuclidean
distancesbetweenthecentersoftheclustersforthetenindependentlytrainednetworksTherelative
distancebetweenthecenteroftheRocketclusterandthecenteroftheGliderclusterwasthelargest
onaverage(d=.415),followedbythedistancefromtheRockettotheStepper(d=.348),whilethe
distancebetweenGliderandStepperwasshortest(d=.237). ThegravityeffectontheRocketand
thelackoftwoofthemotorsseemstorequirethemostdistinctencoding. Ontheotherhand,the
distancebetweenGliderandStepperisthesmallestonaverage,probablyduetothefactthatboth
have4motorsanddonotexperiencegravity.
4.3 SystemDynamics
Runningthenetworkina“contextfree”mannerdidnotonlyraisethequestionofhowthecontext
estimateschangeovertimeandwheretotheyconverge,butalsohowthesedynamicscorrelatewith
goalreachingbehaviorandtheunfoldingsensorimotorpredictionerrordynamics. Inparticular,if
furtherconceptualabstractionsofthesensorimotordynamicsaretobefostered,aneventboundary
signalintheformofameasurable,significantincreaseinpredictionerror(akintoGumbschetal.,
2017a)uponvehiclechangewouldbeveryuseful.
11
1.00
0.75
0.50
0 100 200 300 400
Time
eulav
x
txetnoC
Rocket
Stepper
Glider
1.0
0.5
0.0
0 100 200 300 400
Time
eulav
y
txetnoC
1.0
0.5
0.0
0 100 200 300 400
Time
eulav
z
txetnoC
Figure6: Typicaldevelopmentofthethreecontextguessneuralvalueswhilecontrolling(unknow-
ingly) the three vehicles consecutively — each one for 150 steps. The context is adapted via
retrospectiveinference. Thecontextguessesforthethreevehiclesarewell-separated. However,quite
someuncertaintyintheestimatesisinferable.
Figure8showsexemplaryresultsfor≈ 750timesteps, plottingtheEuclideandistancebetween
the vehicle and the target, the current prediction error, as well as the corresponding trajectories
ofthevehicleestimates,coloredandmarkedbythecurrentlycontrolledvehicle(unknowntothe
system).Itiseasytoseehowthepredictionerrorstronglyincreasesaftereachvehicleandgoalswitch
(V,G=150). Moreover,theerrordecreaseswhenthegoalisreachedandthenstaysratherconstant
atalowbutnotnecessarily0level. Thecontextstateestimatesfurthermoreconfirmthereporteddrift
behavior. Moreover,particularlythesecondtimetherocketiscontrolled,thecontextstateestimates
reachextremevalues.Gradientsthatgobeyondtheirboundariesof[0,1]arecurrentlyignored.Future
versionsshouldconsidercontinuouslydifferentiablecontextstateactivationfunctions,ratherthan
inducinghardboundaries.
Figure 9 shows typical prediction error dynamics when vehicle and goal switches (V = 100,
G=150)occurasynchronously. Itcanbeseenthatbothgoalswitchesandvehicleswitchescause
temporaryincreasesinpredictionerror. Thus,errordynamicsanalysesmaybeusedtodetectsurprise
signals,whichtendtoindicateeventboundaries(herevehicleswitches)(Butz,2016;Gumbschetal.,
2017a).
5 Summary,Conclusions,andFuturePerspectives
WehaveshownthatREPRISEmaintainsANNactivitiesthatreflectthepast,continuouslyoptimizing
its internal (generative) hidden state estimates (both, ct−i and σt−i) about its own body and the
environment. Meanwhile,REPRISEprojectsitsownstateintothefuture,thusoptimizingupcoming
environmental interactions (that is, motor commands xt+i) under consideration of comparisons
betweenimaginedfuturehiddenandactualstateestimations(thatis,σt+i,and˜st+i)anddesired
futuregoalstates(thatis,s(cid:63)t+i). Wehavedevelopedthissystemasafirststeptowardssensorimotor-
grounded,event-orientedabstractions. Essentially,thecontextvectorccanbeinterpretedassignaling
12
Rocket Stepper Glider
0.8
0.6
z
a
x
is 0.4
0.2
0.0
0.2
0.4
0.0 0.2 0 y . 4 axis 0.6 0.8 1.0 1.0 0.8 0.6 x axis
Figure7: Contextguessesplottedwhenthegoallocationisreached. Thestarsymbolmarksthe
centerofthecluster.
thecontextual“event”thesystemiscurrentlyin. Wehaveshownthattheeventwasinferableafter
modellearningbutthatsuitableeventencodingscanalsoemergeduringlearningwheninferenceis
appliedtoboth,contextualestimatesandmodelweights.
Inparticular,theresultsconfirmthatREPRISEisabletoidentifythecurrentlycontrolledvehicle
withouteverbeinginformedaboutavehicleswitch,thevehicleidentities,orthefactthattherewere
three types of vehicles. As long as the vehicle type is switched less frequently than the context
stateestimatesareadapted,andaslongasthelearningrateissufficientlylarge,verygoodcontrol
performancewasachieved. Moreover,distinctcontextualencodingsdevelopedforeachvehicleina
self-organizedmanner. Theresultingnetworkstructurecouldbeusedtominimizepredictionerror
anddistancetotarget,concurrentlywithdeducingthecurrentlycontrolledvehicle,byperforming
prospective,goal-directedbehaviorwhileretrospectivelyinferringappropriatecontextualstates.
Elsewhere,Butz(2016,2017)hasproposedthatpredictiveencodingsmaybesuitablycompressed
intostableeventandeventboundarycodes,whichmayleadtoconceptualabstractions. REPRISE
achieves this for the first time in an RNN-based control architecture, inferring contextual neural
activitiesonthefly. Weseeacloserelationofthesecontextualneuralactivitiesandstableevent
codes. However,herewehavefocusedonanalyzingsensorimotordynamics,wheredistinctevents
arecharacterizedbydistinctsensorimotorregimesgeneratedbycontrollingthreedifferentvehicles.
Similareventencodingsmaybedevelopedinotherscenarios,suchaswhenmanipulatingobjects,
usingtools,orevenwhenformingplacefieldsfornavigatingthroughanenvironment. Inallcases,it
appearsthatparticularsensorimotorregimesapply—especiallywhenencodingobjectsinmanipulator-
relativeframesofreference(Calinonetal.,2010;Gumbschetal.,2017b). Tofosterthedevelopment
ofsuchencodingsfurther,unexpectedchangesinpredictionerrorsshouldbeusedasanindicator
signalforaneventchange(Butz,2016;Gumbschetal.,2017b). Thegatheredresultsinthisrespect
suggestthatthesesignalsareindeedavailableinREPRISE.Oncetheautomaticlearningofevent
encodingsisachieved,event-predictivecognitiononthecompactevent-encodinglevelwillbecome
possible,potentiallyofferingasteptowardsconceptualandcompositionallyre-combinableevent
schemaabstractions.
Itshouldbekeptinmindthattheimplementedprocessescurrentlyfullyrelyonerrorbackpropagation
through time. Implementations of probabilistic inference processes along similar lines are well-
imaginable. In addition, training is currently accomplished by providing pseudo-random motor
commands. Curiosity-driven learning, potentially focusing on expected information gain, is a
veryattractivealternative,whichisindeedalsogenerallycompatiblewithfreeenergy-basedactive
inference(Friston,2009;Oudeyeretal.,2007;Pittietal.,2017;Schmidhuber,1991).
Another challenge lies in compressing identified events further when particular trajectories and
dynamicsbecomeimportantforachievingparticulargoals,suchaswhenopeningadoororwhen
learningtorideabicycle. Theadditionofevent-specificcontrolroutineoptimizationtechniques
13
1.0
0.8
0.6
0.4
0.2
0.0
0 100 200 300 400 500 600 700
Time
ecnatsid
naedilcuE
Euclidean distance between the current position and target
Rocket Stepper Glider
0.04
0.03
0.02
0.01
0.00
0.01
0.02
0 100 200 300 400 500 600 700
Time
rorre
noitiderP
Prediction error
1.0 1.0 1.0
0.75 0.75
z
axis 0.5
z
axis 0.5
z
axis 0.50
z
axis 0.5
z
axis 0.50
0.25 0.25
0.0 0.0 0.00 0.0 0.00
0.2 y 50 a . x 5 i 00 s .751.001.00. x 80 . a 6 xis 0.000 y . 2 ax 50 i . s 500.75 1.0 0.8 x 0 . a 6 xis 0.000 y . a 2 x 50 is .500.75 1.0 0. x 80 . a 6 xis 0. y 4 a 0 x .6 is 0.81.0 1.0 0 x .5 0 a . x 0 is 0 y .9 a 0 x 0 is .951.001.0 0 x .5 a 0 x .0 is
Figure8: TheEuclideandistancebetweenthevehicleandthetargetateachtimestep,theprediction
errorofthenetworkandthechangesinthecontextguessstepbystep.
Figure9: Whenswitchingthevehicleasynchronously(V = 100)tothegoal(G = 150),sudden
increasesinpredictionerrorstillindicatevehiclesswitches—albeitlessstronglyincomparisonto
combinedtargetandvehicleswitches.
seemtobewithinthegraspofREPRISE.Thesetechniquesmayfocusonachievingparticularevent
transitions as goals and may be implemented by means of policy gradient techniques (Stulp and
Sigaud,2013),Inthiscase,butalsoforimprovingsystemscalabilityandfocusinglearningingeneral,
theidentifiedeventboundarysignalsmaybeparticularlyuseful.
Finally,webelievethattheexplorationofdeeperhierarchies,akintothenetworksdiscussedinTani
(2017), but with slower, event-adaptive dynamics in deeper levels of the hierarchy, constitutes a
highlyimportantnextresearchstep. ByfocusingRNNlearningfurtheronpredictingtheoccurrence
ofeventtransitions,itmaybepossibletodevelopconceptualabstractionsthatcanbesuitablylinked
withlinguisticstructuresthatverbalizeexecutableenvironmentalinteractions(Schrodtetal.,2017).
14
Acknowledgments
Funding from a Feodor Lynen Research Fellowship of the Humboldt Foundation is gratefully
acknowledged.
References
Hiroaki Arie, Tetsuro Endo, Takafumi Arakaki, Shigeki Sugano, and Jun Tani. Creating novel
goal-directedactionsatcriticality: Aneuro-roboticexperiment. NewMathematicsandNatural
Computation,05(01):307–334,2009. doi: 10.1142/S1793005709001283.
MosheBar. Predictions: Auniversalprincipleintheoperationofthehumanbrain. Philosophical
Transactions of the Royal Society B: Biological Sciences, 364(1521):1181–1182, 2009. doi:
10.1098/rstb.2008.0321.
MatthewBotvinickandAriWeinstein. Model-basedhierarchicalreinforcementlearningandhuman
actioncontrol. PhilosophicalTransactionsoftheRoyalSocietyofLondonB:BiologicalSciences,
369(1655),2014. ISSN0962-8436. doi: 10.1098/rstb.2013.0480.
MatthewBotvinick,YaelNiv,andAndrewC.Barto. Hierarchicallyorganizedbehavioranditsneural
foundations: Areinforcementlearningperspective. Cognition,113(3):262–280,2009. ISSN
0010-0277. doi: 10.1016/j.cognition.2008.08.011.
RandyL.BucknerandDanielC.Carroll. Self-projectionandthebrain. TrendsinCognitiveSciences,
11:49–57,2007.
MartinV.Butz. Towardsaunifiedsub-symboliccomputationaltheoryofcognition. Frontiersin
Psychology,7(925),2016. ISSN1664-1078. doi: 10.3389/fpsyg.2016.00925.
MartinV.Butz. Whichstructuresareoutthere? learningpredictivecompositionalconceptsbased
onsocialsensorimotorexplorations. MINDGroup,FrankfurtamMain,2017. doi: 10.15502/
9783958573093.
MartinV.ButzandEstherF.Kutter. HowtheMindComesIntoBeing: IntroducingCognitiveScience
fromaFunctionalandComputationalPerspective. OxfordUniversityPress,Oxford,UK,2017.
Sylvain Calinon, Florent D’halluin, Eric L. Sauser, Darwin G. Caldwell, and Aude G. Billard.
Learningandreproductionofgesturesbyimitation. RoboticsAutomationMagazine,IEEE,17(2):
44–54,2010. ISSN1070-9932. doi: 10.1109/MRA.2010.936947.
E.F.CamachoandC.Bordons. ModelPredictiveControl. Springer-Verlag,1999. ISBN3-540-
76241-8.
AndyClark. SurfingUncertainty: Prediction,actionandtheembodiedmind. OxfordUniversity
Press,Oxford,UK,2016.
KarlFriston. Thefree-energyprinciple: aroughguidetothebrain? TrendsinCognitiveSciences,13
(7):293–301,2009. ISSN1364-6613. doi: 10.1016/j.tics.2009.04.005.
Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas FitzGerald, and
GiovanniPezzulo. Activeinferenceandepistemicvalue. CognitiveNeuroscience,6:187–214,
2015. doi: 10.1080/17588928.2015.1020053.
FelixA.Gers,NicolN.Schraudolph,andJürgenSchmidhuber. LearningprecisetimingwithLSTM
recurrentnetworks. JournalofMachineLearningResearch,3:115–143,2002.
ChristianGumbsch,SebastianOtte,andMartinV.Butz. Acomputationalmodelforthedynamical
learningofeventtaxonomies. InProceedingsofthe39thAnnualMeetingoftheCognitiveScience
Society,pages452–457.CognitiveScienceSociety,2017a.
ChristianGumbsch,SebastianOtte,andMartinV.Butz. Acomputationalmodelforthedynamical
learningofeventtaxonomies. Proceedingsofthe39thAnnualMeetingoftheCognitiveScience
Society,pages452–457,2017b.
15
JakobHohwy. ThePredictiveMind. OxfordUniversityPress,Oxford,UK,2013.
BernhardHommel,JochenMüsseler,GisaAschersleben,andWolfgangPrinz. Thetheoryofevent
coding(TEC):Aframeworkforperceptionandactionplanning. BehavioralandBrainSciences,
24:849–878,2001.
DiederikP.KingmaandJimmyL.Ba. Adam: Amethodforstochasticoptimization. ArXive-prints,
abs/1412.6980,2014.
JamesKirkpatrick,RazvanPascanu,NeilC.Rabinowitz,JoelVeness,GuillaumeDesjardins,An-
dreiA.Rusu,KieranMilan,JohnQuan,TiagoRamalho,AgnieszkaGrabska-Barwinska,Demis
Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic
forgettinginneuralnetworks. CoRR,abs/1612.00796,2016.
BrendenM.Lake,TomerD.Ullman,JoshuaB.Tenenbaum,andSamuelJ.Gershman. Building
machinesthatlearnandthinklikepeople. BehavioralandBrainSciences,2017. doi: 10.1017/
S0140525X16001837.
JamesL.McClelland,MatthewM.Botvinick,DavidC.Noelle,DavidC.Plaut,TimothyT.Rogers,
MarkS.Seidenberg,andLindaB.Smith. Lettingstructureemerge: connectionistanddynamical
systems approaches to cognition. Trends in Cognitive Sciences, 14(8):348–356, 2010. ISSN
1364-6613. doi: 10.1016/j.tics.2010.06.002.
VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiA.Rusu,JoelVeness,MarcG.Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
CharlesBeattie,AmirSadik,IoannisAntonoglou,HelenKing,DharshanKumaran,DaanWierstra,
Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature,518(7540):529–533,February2015. ISSN0028-0836. doi: 10.1038/nature14236.
S. Murata, Y. Yamashita, H. Arie, T. Ogata, S. Sugano, and J. Tani. Learning to perceive the
worldasprobabilisticordeterministicviainteractionwithothers: Aneuro-roboticsexperiment.
IEEETransactionsonNeuralNetworksandLearningSystems,28(4):830–848,April2017. ISSN
2162-237X. doi: 10.1109/TNNLS.2015.2492140.
ShamimaNajninandBonnyBanerjee. Apredictivecodingframeworkforadevelopmentalagent:
Speechmotorskillacquisitionandspeechproduction. SpeechCommunication,92:24–41,Septem-
ber2017. ISSN0167-6393. doi: 10.1016/j.specom.2017.05.002.
SebastianOtte,TheresaSchmitt,KarlFriston,andMartinV.Butz. Inferringadaptivegoal-directed
behaviorwithinrecurrentneuralnetworks. 26thInternationalConferenceonArtificialNeural
Networks(ICANN17),pages227–235,2017a.
SebastianOtte,AdrianZwiener,andMartinV.Butz. Inherentlyconstraint-awarecontrolofmany-
jointrobotarmswithinverserecurrentmodels. 26thInternationalConferenceonArtificialNeural
Networks(ICANN17),pages262–270,2017b.
P.-Y.Oudeyer,F.Kaplan,andV.V.Hafner. Intrinsicmotivationsystemsforautonomousmental
development. IEEETransactionsonEvolutionaryComputation,11:265–286,2007. ISSN1089-
778X. doi: 10.1109/TEVC.2006.890271.
AlexandrePitti,PhilippeGaussier,andMathiasQuoy. Iterativefree-energyoptimizationforrecurrent
neuralnetworks(inferno). PLOSONE,12(3):1–33,032017. doi: 10.1371/journal.pone.0173684.
GabrielA.RadvanskyandJeffreyM.Zacks. Eventcognition. OxfordUniversityPress,Oxford,UK,
2014.
RajeshP.N.RaoandDanaH.Ballard. Predictivecodinginthevisualcortex: afunctionalinterpre-
tationofsomeextra-classicalreceptive-fieldeffects. NatureNeuroscience,2(1):79–87,January
1999. doi: 10.1038/4580.
LaurenL.Richmond, DavidA.Gold, andJeffreyM.Zacks. Eventperception: Translationsand
applications. 6(2):111–120,2017. ISSN2211-3681. doi: 10.1016/j.jarmac.2016.11.002.
16
DanielL.Schacter,DonnaRoseAddis,DemisHassabis,VictoriaC.Martin,R.NathanSpreng,and
KarlK.Szpunar. Thefutureofmemory: Remembering,imagining,andthebrain. Neuron,76(4):
677–694,November2012. ISSN0896-6273. doi: 10.1016/j.neuron.2012.11.001.
JurgenSchmidhuber. Apossibilityforimplementingcuriosityandboredominmodel-buildingneural
controllers. Proceedingsofthefirstinternationalconferenceonsimulationofadaptivebehavior:
Fromanimalstoanimats,pages222–227,1991.
FabianSchrodt,JanKneissler,StephanEhrenfeld,andMartinV.Butz. Mariobecomescognitive.
TopicsinCognitiveScience,9(2):343–373,2017. doi: 10.1111/tops.12252.
DavidSilver,AjaHuang,ChrisJ.Maddison,ArthurGuez,LaurentSifre,GeorgevandenDriessche,
JulianSchrittwieser,IoannisAntonoglou,VedaPanneershelvam,MarcLanctot,SanderDieleman,
DominikGrewe,JohnNham,NalKalchbrenner,IlyaSutskever,TimothyLillicrap,Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go
with deep neural networks and tree search. Nature, 529(7587):484–489, 2016. doi: 10.1038/
nature16961.
Freek Stulp and Olivier Sigaud. Robot skill learning: From reinforcement learning to evolution
strategies. Paladyn,JournalofBehavioralRobotics,4:49–61,2013. doi: 10.2478/pjbr-2013-0003.
YuuyaSugitaandMartinV.Butz. Compositionalityandembodimentinharmony. InPierre-Yves
Oudeyer,editor,AMDNewsletter,volume8,pages8–9.IEEECIS,2011.
Yuuya Sugita and Jun Tani. Learning semantic combinatoriality from the interaction between
linguisticandbehavioralprocesses. AdaptiveBehavior,13:33–52,2005.
Yuuya Sugita, Jun Tani, and Martin V Butz. Simultaneously emerging braitenberg codes and
compositionality. AdaptiveBehavior,19:295–316,2011. doi: 10.1177/1059712311416871.
Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. MIT Press,
Cambridge,MA,1998.
JunTani. Model-basedlearningformobilerobotnavigationfromthedynamicalsystemsperspective.
IEEETransactions.System,ManandCybernetics(PartB),SpecialIssueonLearningAutonomous
Systems,26(3):421–436,1996.
Jun Tani. Learning to generate articulated behavior through the bottom-up and the top-down
interactionprocesses. NeuralNetworks,16:11–23,2003.
JunTani. ExploringRoboticMinds. OxfordUniversityPress,Oxford,UK,2017.
DanielM.WolpertandJ.RandallFlanagan.Computationsunderlyingsensorimotorlearning.Current
OpinioninNeurobiology,37:7–11,2016. ISSN0959-4388. doi: 10.1016/j.conb.2015.12.003.
DanielM.WolpertandM.Kawato. Multiplepairedforwardandinversemodelsformotorcontrol.
NeuralNetworks,11:1317–1329,1998. ISSN0893-6080. doi: 10.1016/S0893-6080(98)00066-5.
JeffreyM.ZacksandBarbaraTversky. Eventstructureinperceptionandconception. Psychological
Bulletin, 127(1):3–21, 2001. ISSN 1939-1455(Electronic);0033-2909(Print). doi: 10.1037/
0033-2909.127.1.3.
JeffreyM.Zacks,NicoleK.Speer,KhenaM.Swallow,ToddS.Braver,andJeremyR.Reynolds.
Eventperception: Amind-brainperspective. PsychologicalBulletin,133(2):273–293,2007. doi:
10.1037/0033-2909.133.2.273.
17