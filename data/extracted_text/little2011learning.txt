1
Learning in embodied action-perception loops
through exploration
Daniel Y. Little and Friedrich T. Sommer
Abstract—Although exploratory behaviorsare ubiquitous in the animalkingdom, their computational underpinnings arestill largely
unknown.BehavioralPsychologyhasidentifiedlearningasaprimarydriveunderlyingmanyexploratorybehaviors.Explorationisseen
asameansforananimaltogathersensorydatausefulforreducingitsignoranceabouttheenvironment.Whilerelatedproblemshave
beenaddressedinDataMiningandReinforcementLearning,thecomputationalmodelingoflearning-drivenexplorationbyembodied
agentsislargelyunrepresented.
Here,weproposeacomputationaltheoryforlearning-drivenexplorationbasedontheconceptofmissinginformationthatallowsan
agenttoidentifyinformativeactionsusingBayesianinference.Wedemonstratethatwhenembodimentconstraintsarehigh,agents
mustactivelycoordinatetheiractionstolearnefficiently.Comparedtoearlierapproaches,ourexplorationpolicyyieldsmoreefficient
learning across a range of worlds with diverse structures. The improved learning in turn affords greater success in general tasks
including navigation and reward gathering. We conclude by discussing how the proposed theory relates to previous information-
theoreticobjectivesofbehavior,suchaspredictiveinformationandthefreeenergyprinciple,andhowitmightcontributetoageneral
theoryofexploratorybehavior.
IndexTerms—Knowledgeacquisition,Informationtheory,Controltheory,Machinelearning,Psychology,Computationalneuroscience.
(cid:70)
1 INTRODUCTION the search for new rewards (exploration) [1], [7], [32],
EXPLORATORY behaviors have been observed and [34], [58], [71]. The emphasis in the RL literature on
studied in diverse species across the animal king- reward acquisition, however, stands in contrast to the
dom. As one example, approach and investigation of dominantpsychologicaltheoriesofexploration.Quoting
novel stimuli have been studied in vertebrates ranging D.E.Berlyne,apioneerinthepsychologyofexploration:
from fish to birds, reptiles, and mammals [21], [40], As knowledge accumulated about the condi-
[41], [60], [68], [75]. As another, open field and maze tions that govern exploratory behavior and
experimental paradigms for studying locomotive explo- about how quickly it appears after birth, it
ration in mice and rats have recently been adapted to seemed less and less likely that this behavior
behavioral studies in zebrafish [64], [65]. Indeed, ex- could be derivative of hunger, thirst, sexual
ploratory behaviors have even been described across a appetite, pain, fear of pain, and the like, or
rangeofinvertebrates[12],[26],[31],[52].Theprevalence thatstimulisoughtthroughexplorationarewel-
ofexploratorybehaviorsacrossanimalspeciessuggestsa comed because they have previously accompa-
fundamentalevolutionaryadvantage,largelybelievedto nied satisfaction of these drives. [11]
derive from the utility of information acquired through
Berlyne further suggested “the most acute motivational
such behaviors [33], [50], [51], [55], [56].
problems . . . are those in which the perceptual and
Computational models of exploratory behavior, de-
intellectual activities are engaged in for their own sake
veloped predominantly in the field of Reinforcement
and not simply as aids to handle practical problems”
Learning(RL),havelargelyfocusedontheroleofexplo-
[10]. In fact, a consensus has emerged in behavioral
rationintheacquisitionofexternalrewards[1],[7],[32],
psychologythatlearningrepresentstheprimarydriveof
[34], [69], [70], [71]. An agent that strictly maximizes the
exploratory behaviors [2], [37], [52], [62]. To address this
acquisitionofknownrewardsmightfallshortinfinding
gap between computational modeling and behavioral
new, previously unknown, sources of reward. Reward
psychology, we introduce here a mathematical frame-
maximization therefore requires balancing between di-
work for studying how behavior effects learning and
rected harvesting of known rewards (exploitation) and
develop a novel model of learning-driven exploration.
• D.Y. Little is with the Department of Molecular and Cellular Biology, In Computational Neuroscience, machine learning
UniversityofCaliforniaatBerkeley,Berkeley,CA94720. techniqueshavebeensuccessfullyappliedtowardsmod-
E-mail:dylittle@berkeley.edu
eling how the brain might learn the structure underly-
• F.T. Sommer is with the Redwood Center for Theoretical Neuroscience,
UniversityofCalifornia,Berkeley,CA94720 ing sensory signals, e.g., [15], [16], [18], [36], [47], [54],
E-mail:fsommer@berkeley.edu [59]. Generally, these methods focus on passive learning
where the learning system can not directly effect the
1102
ceD
9
]GL.sc[
2v5211.2111:viXra
2
sensory input it receives. Exploration, in contrast, is from state s to state s’ when it chooses action a):
inherently active, and can only occur in the context of a p a,s (s(cid:48)|Θ)=Θ a,s,s(cid:48) (1)
closed-actionperceptionloop.Learninginclosedaction-
(cid:88)
Θ =1
perception loops differs from passive learning in two a,s,s(cid:48)
important aspects [22]. First, a learning agent’s internal s(cid:48)
model of the world must keep track of how actions For any fixed action, Θ a,:,: is a (two-dimensional)
change the sensory input. Sensorimotor contingencies, stochastic matrix describing a Markov process. Each
such as the way visual scenes change as we shift our column in this matrix Θ a,s,: defines a transition distri-
gaze or move our head, must be taken into account to bution which is a categorical (finite discrete) distribution
properly attribute changes in sensory signals to their specifying the likelihoods for the next state.
causes.Thisisperhapsreflectedinneuroanatomywhere The CMC provides a simple mathematical framework
tight sensory-motor integration has been reported at all for modeling exploration in embodied action-perception
levels of the brain [23], [24]. Though often taken for loops. At every time step, an exploring agent is allowed
granted, sensor-motor contingencies must actually be tofreelyselectanyactiona∈A.Thelearningtaskofthe
learned during the course of development as is elo- exploring agent is to build from observed transitions an
quentlyexpressedintheexplorativebehaviorsofyoung estimate, the internal model Θ(cid:98), of the true CMC kernel,
infants (e.g., grasping and manipulating objects during the world Θ. We assume that the explorer begins with
proprioceptive exploration or bringing them into visual limited information about the world in the form of a
view during intermodal exploration) [45], [48], [57]. prior and must improve its estimate by acting in the
The second crucial aspect of learning in a closed world and gathering observations. The states can be
action-perception loop is that actions direct the acqui- directly observed by the agent, i.e. the system is not
sition of sensory data. To discover what is inside an hidden. In the CMC framework, an agent’s immediate
unfamiliar box, a curious child must open it. To learn ability to interact with and observe the world is limited
about the world, scientists perform experiments. Direct- by the current state. This restriction models the embod-
ing the acquisition of data is particularly important for iment of the agent. To ameliorate the myopia imparted
embodied agents whose actuators and sensors are phys- by its embodiment, an agent can coordinate its actions
icallyconfined.Sincethemostinformativedatamaynot over time. Our primary question is how action policies
always be accessible to a physical sensor, embodiment can optimize the speed and efficiency of learning in
may constrain an exploring agent and require that it embodied action-perception loops.
coordinates its actions to retrieve useful data.
2.2 Information-theoreticassessmentoflearning
In the model we propose here, an agent moving
betweendiscretestatesinaworldhastolearnhowitsac- To assess an agent’s success towards learning, we de-
tions influence its state transitions. The underlying tran- termine the missing information I M in its internal model
sition dynamics are governed by a Controllable Markov (as proposed by Pfaffelhuber [49]). We do this by first
Chain (CMC). Within this simple framework, various calculating the Kullback-Leibler (KL) divergence of the
utility functions for guiding exploratory behaviors will internal model from the world for each transition distri-
be studied, as well as several methods for coordinating bution:
actionsovertime.Thedifferentexploratorystrategiesare N (cid:32) (cid:33)
compared in their rate of learning. D KL (Θ a,s,: (cid:107)Θ(cid:98)s,a,: ):=
(cid:88)
Θ s,a,s(cid:48) log 2
Θ s,a,s(cid:48)
s(cid:48)=1
Θ(cid:98)s,a,s(cid:48)
The KL-divergence can be interpreted as the expected
2 MODEL amount of information in bits, lost when observations
(followingthetruedistribution)arecommunicatedusing
2.1 Mathematical framework for embodied active an encoding scheme optimized for the estimated distri-
learning bution [13]. The loss is large when the two distributions
differ greatly and zero when they are identical. The
Controllable Markov chains (CMCs) are a simple extension
missing information is then calculated as the sum of the
of Markov chains that incorporate a control variable for
KL-divergences for each transition distribution:
switchingbetweendifferenttransitiondistributions[19].
(cid:88)
Formally, a CMC is a 3-tuple (S,A,Θ) where: I
M
(Θ(cid:107)Θ(cid:98)):= D
KL
(Θ
s,a,:
||Θ(cid:98)s,a,: ) (2)
• S is a finite set of the possible states of the system s∈S,a∈A
(for example, the possible location of an agent in its We will use missing information (2) to assess learning
world). N =|S| and to compare the performance of different explorative
• A is a finite set of the possible control values, i.e., strategies.Steeperdecreasesinmissinginformationover
the actions the agent can choose. M =|A| time represent faster learning and thus more efficient
• Θ is a 3-dimensional CMC kernel describing the explorative strategies. Table 1 has been included as a
probability of transitions between states given an reference guide for the various measures discussed in
action (for example, the probability an agent moves this manuscript for assessing or guiding exploration.
3
TABLE1
TableofInformationMeasures
Nameusedhere,Abbreviation(EquationNumber) Nameusedin[Reference] Mathematicalexpression
(cid:88)
MissingInformation, IM(2) MissingInformation [49] DKL(Θs,a,:||Θ(cid:98)s,a,:)
s∈S,a∈A
InformationGain, IG(10)
IM(Θ(cid:98))−IM(Θ(cid:98)a,s,s∗
)
(cid:104) (cid:105)
PredictedInformationGain, PIG(13) InformationGain [44] E s∗|a,s,(cid:126)da,s DKL(Θ(cid:98)a,s,:||Θ(cid:98) a a , , s s , ,: s∗ )
(cid:88)
PosteriorExpectedInformationGain, PEIG(18) KL−Divergence [67]orSurprise [29] DKL(Θ(cid:98) p a a ,s s , t : (cid:107)Θ(cid:98)c a u ,s r , r : ent)
s∈S,a∈A
PredictedModeChange, PMC(19) ProbabilityGain [44] (cid:88) Θ(cid:98)a,s,s∗ (cid:104) m
s
a
(cid:48)
xΘ(cid:98) a
a
,
,
s
s
,
,
s
s
∗
(cid:48)
−m
s
a
(cid:48)
xΘ(cid:98)a,s,s(cid:48) (cid:105)
s∗
(cid:34) (cid:35)
PredictedL1Change, PLC(20) Impact [44] (cid:88) Θ(cid:98)s,a,s∗ N 1 (cid:88)(cid:12) (cid:12) (cid:12) Θ(cid:98) a a , , s s, ,s s∗ (cid:48) −Θ(cid:98)a,s,s(cid:48) (cid:12) (cid:12) (cid:12)
s∗ s(cid:48)
2.3 Bayesianinferencelearninginanagent independent estimation of each transition distribution:
(cid:90)
During exploration, a learning agent will gather data in Θ(cid:98)a,s,s(cid:48) = Θ
a,s,s(cid:48)
f(Θ|(cid:126)d)dΘ
the form of experienced state transitions. At every time Θ
(cid:90)
step, it can update its internal model Θ(cid:98) with its last = Θ f (Θ |(cid:126)d )dΘ
a,s,s(cid:48) a,s a,s,: a,s a,s,:
observation. Θa,s,:
Assuming the transition probabilities are drawn from =E Θa,s,:|(cid:126)da,s [Θ a,s,s(cid:48) ] (4)
a prior distribution f (a distribution over CMC kernels),
and letting (cid:126)d be the history of observed transitions, the In the following theorem, we demonstrate that the
Bayesian estimate Θ(cid:98)s,a,s(cid:48) for the probability of transi- Bayesian estimator minimizes the expected missing in-
tioning to state s(cid:48) from state s under action a is given formation and thus is the best estimate under our objec-
by: tive function (2).
Theorem 1: Let Θ and Φ be CMC kernels. Θ describes
the ground truth environment generated from a prior
Θ(cid:98)a,s,s(cid:48) :=p
a,s
(s(cid:48)|(cid:126)d)
distribution. Φ is any internal model of the agent. Then
(cid:90)
= p (s(cid:48),Θ|(cid:126)d)dΘ the expected missing information between Θ and an
Θ a,s internal model Φ, given data (cid:126)d, is minimized by the
(cid:90)
= p (s(cid:48)|Θ,(cid:126)d)f(Θ|(cid:126)d)dΘ Bayesian estimate Θ(cid:98):
a,s
Θ
=
(cid:90)
p (s(cid:48)|Θ)f(Θ|(cid:126)d)dΘ
Θ(cid:98) =argminE
Θ|(cid:126)d
[I
M
(Θ(cid:107)Φ)] (5)
a,s Φ
Θ
(cid:90)
= Θ f(Θ|(cid:126)d)dΘ Proof: Since missing information is simply the sum
a,s,s(cid:48)
of the KL-divergence for each transition kernel (2), min-
Θ
=E Θ [Θ a,s,s(cid:48) |(cid:126)d]:=E Θ|(cid:126)d [Θ a,s,s(cid:48) ] (3) imizing missing information is equivalent to indepen-
dently minimizing these KL-divergences:
Fordiscretepriorstheaboveintegralswouldbereplaced argminE [D (Θ (cid:107)Φ )]
withsummations.Atthebeginningofexploration,when Φa,s,:
Θa,s,:|(cid:126)d KL a,s,: a,s,:
the history (cid:126)d is empty, f(Θ|(cid:126)d) is simply the prior f(Θ).
=argminE
(cid:34) (cid:88)
Θ log
(cid:18) Θ a,s,s(cid:48) (cid:19)(cid:35)
E
is
qu
eq
at
u
i
i
o
v
n
ale
(3
n
)
t
d
to
em
th
o
e
ns
e
tr
x
a
p
te
e
s
cte
t
d
hat
va
t
l
h
u
e
e
B
o
a
f
y
t
e
h
s
e
ian
tru
e
e
sti
C
m
M
at
C
e
Φa,s,:
Θa,s,:|(cid:126)d
s(cid:48)
a,s,s(cid:48) 2 Φ
a,s,s(cid:48)
(cid:34) (cid:35)
kernel given the data. If we further assume that each (cid:88)
=argminE Θ log Θ
transition distribution Θ
a,s,:
is independently drawn
Φa,s,:
Θa,s,:|(cid:126)d
s(cid:48)
a,s,s(cid:48) 2 a,s,s(cid:48)
from the marginals f a,s of the distribution f and we (cid:34) (cid:35)
l
w
et
he
(cid:126)d
n
a,s
ta
b
k
e
in
t
g
he
ac
h
ti
i
o
st
n
or
a
y
i
o
n
f
s
s
t
t
a
a
t
t
e
e
s
tr
,
a
(
n
3
s
)
iti
s
o
im
ns
pl
e
i
x
fi
p
e
e
s
ri
t
e
o
nc
t
e
h
d
e
−E Θa,s,:|(cid:126)d
(cid:88)
Θ a,s,s(cid:48) log 2 Φ a,s,s(cid:48)
s(cid:48)
4
The three different classes of test environments to be
1 2 3 4 5 6
investigated will be called Dense Worlds, Mazes, and 1-
a=‘up’ 2-3 Worlds. For each class, random CMCs are generated
7 8 9 10 11 12 bydrawingthetransitiondistributionsfromspecificdis-
tributions. These generative distributions are also given
13 14 15 16 17 18 totheagentsaspriorsforperformingBayesianinference.
1)DenseWorldscorrespondtocompletedirectedprob-
a=‘left’
19 20 21 22 23 24 ability graphs with N = 10 states and M = 4 actions.
Each transition distribution is independently drawn
25 26 27 28 29 30 from a Dirichlet distribution:
(cid:81) Θ α−1
Θ ∼Dir(1):= s(cid:48) s,a,s(cid:48) (6)
31 32 33 34 35 36 s,a,,: B(α)
Γ(α)N
Fig. 1. Example maze. The 36 states correspond to B(α):=
Γ(Nα)
rooms in a maze. The 4 actions correspond to noisy
(cid:90) ∞
translationsinthecardinaldirections.Twotransitiondistri- Γ(α):= tα−1e−tdt
butionsaredepicted,eachbyasetof4arrowsemanating 0
from their starting state. Flat-headed arrows represent The Dirichlet distribution is the conjugate prior of the
translations into walls, resulting in staying in the same categorical distribution (1) and thus a natural distri-
room. Dashed arrows represent translation into a portal bution for generating CMCs. It is parametrized by a
(blue lines) leading to the base state (blue target). The concentrationfactorαthatdetermineshowmuchweight
shadingofanarrowindicatestheprobabilityofthetransi- in the Dirichlet distribution is centered at the midpoint
tion(darkercolorrepresentshigherprobability). of the simplex, the space of all possible transition dis-
tributions. The midpoint corresponds to the uniform
(cid:34) (cid:35) categoricaldistribution.ForDenseWorlds,weuseacon-
(cid:88) centration parameter α = 1 which results in a uniform
=argmin −E Θ log Φ
Φa,s,:
Θa,s,:|(cid:126)d
s(cid:48)
a,s,s(cid:48) 2 a,s,s(cid:48)
distribution over the simplex. An example is depicted
(cid:88) graphically in Fig. S1.
=argmin − E [Θ ]log Φ
Φa,s,:
s(cid:48)
Θa,s,:|(cid:126)d a,s,s(cid:48) 2 a,s,s(cid:48) The Bayesian estimate for Dense Worlds has the fol-
(cid:104) (cid:105) lowing closed-form expression:
=argminH E [Θ ];Φ
Here H[θ
Φ
;
a
φ
,s
]
,:
denotes
Θa
t
,
h
s,
e
:|(cid:126)d
cros
a
s
,s
-
,
e
:
ntro
a
p
,s
y
,:
[13]. Then, by
Θ(cid:98)a,s,s(cid:48) =
1+ (cid:80)
K
K k=
+
1 δ
N
s(cid:48),(cid:126)da,s[k]
(7)
Gibb’s inequality [13] we conclude: where K is the length of the history (cid:126)d and δ is the
a,s x,y
(cid:104) (cid:105) Kronecker delta (i.e. δ is 1 if x = y and 0 otherwise).
x,y
argminH E [Θ ];Φ
Φa,s,:
Θa,s,:|(cid:126)d a,s,: a,s,: Equation (7) reveals that the Bayesian estimate is sim-
ply the relative frequencies of the observed data with
=E [Θ ]
Θa,s,:|(cid:126)d a,s,:
the addition of one fictitious count per transition. The
=Θ(cid:98)a,s,: ((cid:126)d) incorporation of this fictitious observation is referred to
as Laplace smoothing and is often performed to avoid
over-fitting [39]. The derivation of Laplace smoothing
The analytical form for the Bayesian estimate will de- from Bayesian inference over a Dirichlet prior is a well
pendontheprior.Inthefollowingsection,weintroduce known result [38].
three classes of CMCs which will be considered in this 2) Mazes consist of N = 36 states corresponding
study and specify the Bayesian estimates for each. to rooms in a randomly generated 6 by 6 maze and
M =4 actions corresponding to noisy translations, each
biased towards one of the four cardinal directions ”up”,
2.4 Three test environments for studying explo-
”down”, ”left” and ”right”. An example is depicted in
ration
Fig. 1. Walking into a wall causes the agent to remain in
In the course of exploration, the data an agent accumu- its current location. There are 30 transporters randomly
lates will depend on both its behavioral strategy as well distributed amongst the walls which lead to a base
as the world structure. Studying diverse environments, state. Each maze has a single, randomly chosen base
i.e., CMCs that differ greatly in structure, will help us state (concentric rings in Fig. 1). All transitions that
to investigate how world structure effects the relative do not correspond to a single translation are assigned
performance of different exploratory strategies and to a probability of zero. The non-zero probabilities are
identify action policies that produce efficient learning drawn from a Dirichlet distribution with concentration
under broad conditions. parameter α = 0.25. The highest probability is assigned
5
to the state corresponding to the cardinal direction of is given by:
1
theaction.Thesmallconcentrationparameterdistributes
more probability weight in the corners of the simplex
Θ(cid:98)a,s,s(cid:48) =
a
correspondingtodeterministictransitions.Thisresultsin If a,s→s(cid:48) has not been observed but a,s→1 has, then
Maze transitions have strong biases towards an actions the Bayesian estimate is given by:
associated cardinal direction.
Agents in Mazes must estimate the non-zero transi- 1−
|S∗|
tions using the Dirichlet prior without knowledge of
Θ(cid:98)a,s,s(cid:48) =
N −
a
T
each action’s assigned cardinal direction. Similar to (7),
Here T is the number of target states that have already
the Bayesian estimate for maze transitions is given by:
been observed:
Θ(cid:98)a,s,s(cid:48) = 0.25
K
+
+
(cid:80)
0
K k
.2
=
5
1
·
δ
N
s(cid:48),(cid:126)da,s[k] (8) T :=|{s∗ ∈(cid:126)d a,s }|
a,s Finally, if neither a,s → s(cid:48) nor a,s → 1 have been
where N is the number of non-zero probability states observed, then the Bayesian estimate is:
a,s
in the transition distribution Θ a,s,: . As with Dense  1−0.75a
W sm o o rl o d th s, e t d he hi B st a o y g e r s a ia m n . estimate (8) for mazes is a Laplace  1+ (cid:0)(cid:0)a− T 1(cid:1) −1 (cid:1) ∗0.75a · a 1 if s(cid:48) =1
3) 1-2-3 Worlds consists of N = 20 states and M = 3 Θ(cid:98)a,s,s(cid:48) =
a
d
c
e
t
t
i
e
o
r
n
m
s.
in
I
i
n
sti
a
ca
g
l
i
l
v
y
en
to
s
a
tat
s
e
i
,
ng
ac
le
tio
ta
n
rg
a
e
=
t s
1
tat
m
e,
ov
a
e
c
s
tio
th
n
e
a
ag
=
en
2
t  1− (cid:16) T
a
+Θ(cid:98)a,s,1 (cid:17)
otherwise
N −T −1
brings the agent with probability 0.5 to one of two
possible target states, and action a = 3 brings the
agent with probability 0.333 to one of 3 potential target
3 RESULTS
states.Thetargetstatesarerandomlyandindependently 3.1 Assessing the information-theoretic value of
selected for each action taken in each state. To create an plannedactions
absorbing state, the probability that state 1 is among the
The central question to be addressed is how actions
targets of action a is set to 1 − 0.75a. The probability
effectthelearningprocessinembodiedaction-perception
for all other states to be selected as targets is uniform.
loops. Ideally, actions should be chosen so that the
Explicitly,lettingΩ bethesetofalladmissibletransition
a missing information (2) decreases as fast as possible. As
distributions for action a:
discussedinSection2.3,theBayesianestimateminimizes
Ω :={θ ∈IRN| (cid:88) θ =1andθ ∈{0, 1 }∀s(cid:48)} the expected missing information. We will assume that
a s(cid:48) s(cid:48) a an agent continually updates its internal model accord-
s(cid:48)
ingly from the observations it receives. The Bayesian
thetransitiondistributionsaredrawnfromthefollowing
estimate, however, does not indicate which action will
distribution:
optimize the utility of future data. Towards this objec-
 0 if Θ ∈/ Ω tive, an agent should try to predict the impact a new

1
(cid:0)
−
N
0
−
.
1
7
(cid:1)
5a
else
a
i
,
f
s,
Θ
:
a,s,1
a
=
a
1
o
ca
b
l
s
l
er
th
v
e
ati
d
o
e
n
cr
w
ea
i
s
ll
e
h
in
av
m
e
i
o
s
n
sin
i
g
ts
in
m
f
i
o
s
r
s
m
in
a
g
tio
in
n
fo
b
r
e
m
tw
at
e
io
en
n.
t
W
wo
e
p(Θ a,s,: )= a−1 (9) internal models the information gain (I G ). Letting Θ(cid:98) be

1−(
(cid:0)
1
N
−
−1
0
(cid:1)
.75a)
otherwise
a
u
a,
p
c
s
d
u
→
a
r
t
r
e
e
s
d
n
∗
t
m
t
m
o
o
o
(cid:126)d
d
d
,
e
e
l
t
l
h
d
e
d
e
e
r
i
r
n
iv
i
f
v
o
e
e
d
r
d
m
f
f
a
r
r
o
t
o
i
m
o
m
n
a
d
g
d
a
a
d
t
i
i
a
n
n
(cid:126)
g
d
fo
a
a
r
n
n
t
d
h
o
is
b
Θ(cid:98)
s
o
a
e
b
,
r
s
s
v
,s
e
a
∗
r
t
v
i
b
o
a
e
n
tio
a
o
n
n
f
a
is:
If this process results in a non ergodic CMC, it is
discarded and a new CMC is generated. A CMC as I G (a,s,s∗):=I M (Θ(cid:107)Θ(cid:98))−I M (Θ(cid:107)Θ(cid:98) a,s,s∗ )
ergodic if, for every ordered pair of states, there exist =D
KL
(Θ
a,s,:
(cid:107)Θ(cid:98)a,s,: )−D
KL
(Θ
a,s,:
(cid:107)Θ(cid:98) a
a
,
,
s
s
,
,:
s∗ )
a fi 1 n - r 2 s - t a 3 c s t W t i a o t o n e rl w d p s o il l l i i s c e y v d e e u n p n t i u d ct a e e l r d ly w i r n h e i a F c c i h g h . a t S h n 2 e . a s g e e c n o t nd st . a A rt n in e g xa a m t t p h l e e = (cid:88) s(cid:48) Θ a,s,s(cid:48) log 2 Θ Θ (cid:98)a a , , s s , , s s (cid:48) (cid:48) −Θ a,s,s(cid:48) log 2 Θ Θ (cid:98) a a a , , , s s s , , , s s s ∗ (cid:48) (cid:48)
Ma B z a e y s es a i n an d i D nf e e n r s e e nc W e o in rl 1 d - s 2- b 3 e W ca o u r s l e ds o d f i i f t f s er d s i g sc re re a t t e ly p fr r o io m r. = (cid:88) Θ a,s,s(cid:48) log2 Θ(cid:98) a a , , s s , , s s ∗ (cid:48) (10)
Essentially, state transitions that have been observed are s(cid:48)
Θ(cid:98)a,s,s(cid:48)
accurately estimated, while the remaining probability Calculating the information gained from taking action a
weightisdistributedacrossthosestatesthathavenotyet in state s would therefore require knowing Θ as well as
beenexperienced(preferentiallytostate1anduniformly s∗. An agent can only infer former and can only know
across other states). Explicitly, if a,s → s(cid:48) has been pre- thelatterafterithasexecutedtheaction.Inthefollowing
viously observed, then the Bayesian estimate for Θ a,s,s(cid:48) theorem, however, we derive a closed-form expression
6
Dense Worlds Mazes 1-2-3 Worlds
0
10
−1
10
−2
10
−2
10
−2
10
−4
10
−2 −2 0
10 10 10
)stib(
niaG
noitamrofnI
detciderP
Realized Information Gain (bits) Realized Information Gain (bits) Realized Information Gain (bits)
)stib(
niaG
noitamrofnI
detciderP
)stib(
niaG
noitamrofnI
detciderP
successive
observations
Fig.2. Accuracyofpredictedinformationgain.Theaveragepredictedinformationgainisplottedagainsttheaverage
realized information gain. Averages are taken over 200 CMCs, N × M transition distributions, and 50 trials. Error
bars depict standard deviations (only plotted above the mean for 1-2-3 Worlds). The arrow indicates the direction of
increasingnumbersofobservations(top-right=none,bottom-left=19).Theunitylinesaredrawningray.
for the expected information gain, which we shall call
the predicted information gain (PIG). Notice, (11) can be computed from previously col-
Theorem 2: Let Θ be a CMC kernel whose transition lected data alone. For each class of environments, Fig. 2
distributions are independently generated from prior compares the average PIG with the average realized
distributions. If an agent is in state s and has previously information gain as successive observation are drawn
collected data (cid:126)d, then the expected information gain for from a transition distribution and used to update a
taking action a and observing the resultant state S∗ is Bayesian estimate. In accordance with Theorem 2, in all
given by: three environments PIG accurately predicts the average
information gain. Thus, theoretically and empirically,
PIG(a,s):=E [I (a,s,s∗)]
s∗,Θ|(cid:126)d G PIGrepresentsanaccurateestimateoftheaveragegains
= (cid:88) Θ(cid:98)a,s,s∗ D
KL
(Θ(cid:98)a,s,: (cid:107)Θ(cid:98) a
a
,
,
s
s
,
,:
s∗ ) (11) towards the learning objective functions that an agent
can expect to receive for taking a planned action in a
s∗
particular state.
where Θ(cid:98) is the current internal model of the agent and
Interestingly, the equation for computing PIG, RHS
Θ(cid:98)a,s,s∗
is what the internal model would become if it of (11), has been previously considered in the field of
were updated with an observation s∗ resulting from a psychology, where it was applied to describe human
prospective new action a. behavior during hypothesis testing [35], [44], [46]. To
Proof: our knowledge, however, its equality to the expected
E [I (a,s,s∗)] decrease in missing information (Theorem 2) has not
s∗,Θ|(cid:126)d G
been previously shown.
=E (cid:34) (cid:88) Θ log2 (cid:32) Θ(cid:98) a a , , s s , , s s ∗ (cid:48) (cid:33)(cid:35)
s∗,Θ|(cid:126)d
s(cid:48)
a,s,s(cid:48)
Θ(cid:98)a,s,s(cid:48) 3.2 Control learners: unembodied and random ac-
=E (cid:34) (cid:88) Θ log2 (cid:32) Θ(cid:98) a a , , s s , , s s ∗ (cid:48) (cid:33)(cid:35) tion
s∗,Θa,s,:|(cid:126)da,s
s(cid:48)
a,s,s(cid:48)
Θ(cid:98)a,s,s(cid:48)
During exploration, an embodied agent can choose its
action but is bound to the state that resulted from its
=E (cid:34) E (cid:34) (cid:88) Θ log2 (cid:32) Θ(cid:98) a a , , s s , , s s ∗ (cid:48) (cid:33)(cid:35)(cid:35) last transition. A simple exploratory strategy would be
s∗|(cid:126)da,s Θ|(cid:126)da,s,s∗
s(cid:48)
a,s,s(cid:48)
Θ(cid:98)a,s,s(cid:48) toalwaysselectactionsuniformlyrandomly.Wewilluse
=E (cid:34) (cid:88) E [Θ ]log2 (cid:32) Θ(cid:98) a a , , s s , , s s ∗ (cid:48) (cid:33)(cid:35) s le u a c r h ni a ng ra p n e d r o f m orm ac a t n io c n e s r t e r p a r t e e s g e y nt a in s g a a b n a a se iv li e ne ex c p o l n o t r r e o r. l for
s∗|(cid:126)da,s
s(cid:48)
Θ|(cid:126)da,s,s∗ a,s,s(cid:48)
Θ(cid:98)a,s,s(cid:48) In contrast to embodied agents, one can also consider
=E
s∗|(cid:126)da,s
(cid:34) (cid:88)
s(cid:48)
Θ(cid:98) a
a
,
,
s
s
,
,
s
s
∗
(cid:48)
log
2
(cid:32) Θ
Θ
(cid:98)
(cid:98)
a a
a
, ,
,
s s
s
, ,
,
s s
s
∗ (cid:48)
(cid:48)
(cid:33)(cid:35) a l
u
o n
n
c
e
a u
m
te n
b
e t
o
m o
d
b
i
o a
e
d
d
i n ed e
a
w
g
a
e
g
n
s e
t
t n
,
a t t
o
e t
p
h b
t
a
i
e
m
t fo
i
i r
z
s e
at
a
i
t l
o
a l
n
k ow in
o
e g
f
d a
le
n to
ar
a
n
a c
i
r t
n
b io
g
it n r .
b
ar
e
F i
c
l o
o
y r
m
r a
e
e n
s
-
=E
s∗|(cid:126)da,s
(cid:104) D
KL
(Θ(cid:98)a,s,: (cid:107)Θ(cid:98) a
a
,
,
s
s
,
,:
s∗ ) (cid:105) m
sa
u
m
c
p
h
lin
si
g
m
p
p
r
l
o
e
b
r
le
a
m
s i
[
t
49
d
].
e
S
co
in
m
ce
po
th
se
e
s
PI
in
G
to
for
an
ea
i
c
n
h
d
t
e
r
p
a
e
n
n
si
d
ti
e
o
n
n
t
= (cid:88) p(s∗|(cid:126)d
a,s
)D
KL
(Θ(cid:98)a,s,: (cid:107)Θ(cid:98) a
a
,
,
s
s
,
,:
s∗ ) distributiondecreasesmonotonicallyoversuccessiveob-
s∗ servations(Fig.2),learningbyanunembodiedagentcan
= (cid:88) Θ(cid:98)a,s,s∗ D
KL
(Θ(cid:98)a,s,: (cid:107)Θ(cid:98) a
a
,
,
s
s
,
,:
s∗ ) be optimized by always sampling from the state and
action pair with the highest PIG. Thus, learning can be
s∗
7
Dense Worlds Mazes 1-2-3 Worlds
140 200
20
120
150 100
15
80
100
10
60
40
5 50
20
0 0 0
0 1000 2000 3000 0 1000 2000 3000 0 500 1000
Time (steps)
)stib(
noitamrofnI
gnissiM
Unembodied
Random
Time (steps) Time (steps)
)stib(
noitamrofnI
gnissiM
)stib(
noitamrofnI
gnissiM
Fig.3. Learningcurvesforcontrolstrategies.Theaveragemissinginformationisplottedoverexplorationtimeforthe
unembodied positive control and random action baseline control. Standard errors are plotted as dotted lines above
andbelowlearningcurves.(n=200)
optimized in a greedy fashion: is embodied but the other is not. Thus differences of
their performance reflect the embodiment constraint on
(a,s) :=argmaxPIG(a,s) (12)
Unemb. learning. As shown in Fig. 4 the performance difference
(a,s)
is largest in Maze worlds, moderate though significant
The learning curves of the unembodied agent will serve
in 1-2-3 Worlds and smallest in Dense Worlds (p<0.001
here as a positive control as it represents an upper bound
forMazesand1-2-3Worlds,p>0.001forDenseWorlds).
for the performance of embodied agents.
Toquantifytheembodimentconstraintfacedinaworld,
An initial comparison between random action and
we define an embodiment index as the relative differ-
the unembodied control highlights a notable difference
ence between the areas under the learning curves for
among the three classes of environments (Fig. 3). Specif-
PIG(greedy)andtheunembodiedcontrolwhichaverage
ically, the performance margin between the two controls
0.02forDenseWorlds,2.59forMazes,and1.27for1-2-3
is significant in Mazes and 1-2-3 Worlds (p < 0.001),
Worlds.
but not in Dense Worlds (p > 0.01). The significance
Also of particular interest, the comparison between
was assessed by post-hoc analysis of Friedman’s test
PIG(greedy) and random action provides further in-
[25] comparing the areas under the two learning curves.
sight differentiating the three classes of worlds (Fig. 4).
Despite using a naive strategy, the random actor is
WhereasPIG(greedy)yieldednoimprovementoverran-
essentially reaching maximum performance in Dense
dom action in Dense Worlds and Mazes (p > 0.001),
Worlds, suggesting that exploration of this environment
it significantly improved learning in 1-2-3 Worlds(p <
is fairly easy. The difference in performance between
0.001), demonstrating that agents benefitted from the
randomactionandtheunembodiedcontroloffersanini-
information-theoretic utility function only in 1-2-3
tialinsightintotheconstraintsexperiencedbyembodied
Worlds.
agents. A directed exploration strategy may help bridge
GreedymaximizationofPIGconsidersonlytheimme-
this gap.
diategainsavailableandfailstoaccountfortheeffectan
action can have on future utility. In particular, when the
3.3 ExplorationstrategiesbasedonPIG
potential for information gain is unevenly distributed,
GiventhatPIGcanbecomputedbyanagentusingonly it may be necessary to coordinate actions over time
thedataithasalreadycollected(alongwithitsprior),we to obtain remote but informative observations. Forward
wondered whether it could be used as a utility function estimation of total future PIG over multiple time steps
to guide exploration. Since greedy maximization of PIG is intractable as the number of action sequences and
is optimal for the unembodied agent, one might con- state outcomes increases exponentially with time. To
sider a similar greedy strategy for an embodied agent guide an agent towards maximizing long-term gains of
(PIG(greedy)). The key difference would be that the PIG, we instead employ a back-propagation approach
embodied agent can only select its action but not its previously developed in the field of economics, Value-
current state: Iteration (VI) [8]. The estimation starts at a distant time
point (initialized as τ = 0) in the future with initial
a :=argmaxPIG(a,s) (13)
PIG(greedy) values equal to the PIG for each state-action pair:
a
The performance comparison between PIG(greedy) (13) Q (a,s):=I (a,s) (14)
0 G
and the unembodied control (12) is of particular inter-
est because the two strategies differ only in that one Then propagating backwards in time, we maintain a
8
Dense Worlds Mazes 1-2-3 Worlds
140 200
20 120
150
100
15
80
100
10
60
40
5 50
20
0 0 0
0 1000 2000 3000 0 1000 2000 3000 0 500 1000
Time (steps)
)stib(
noitamrofnI
gnissiM
PIG(greedy)
PIG(VI)
PIG(VI+)
Unembodied
Random
Time (steps) Time (steps)
)stib(
noitamrofnI
gnissiM
)stib(
noitamrofnI
gnissiM
Fig. 4. Coordinating exploration using predicted information gain. The average missing information is plotted over
exploration time for greedy and value-iterated (VI) maximization of PIG. The standard control strategies and the VI+
positivecontrolarealsodepicted.Standarderrorsareplottedasdottedlinesaboveandbelowlearningcurves.(n=200)
running total of estimated future value by: is relatively small compared to the gains made over
random or greedy behaviors (Fig. 4). Altogether these
V (s):=maxQ (a,s)
τ τ results suggest that PIG(VI) may be an effective strategy
a
Q
τ−1
(a,s):=I
G
(a,s)+γ (cid:88) Θ(cid:98)s,a,s(cid:48) ·V
τ
(s(cid:48)) (15) employable by embodied agents for coordinating explo-
rative actions towards learning.
s(cid:48)∈S
From the results so far the picture emerges that the
Here, 0 ≤ γ ≤ 1, is a discount factor reducing the
three classes of environments offer very different chal-
value of gains obtained further in the future. When
lengesfortheexploringagent.DenseWorldsareeasyto
γ < 1, backward propagation can be continued until
explore. Mazes require policies that coordinate actions
convergence.Alternatively,itcansimplybeexecutedfor
over time but exhibit little sensitivity to the particu-
a predefined number of steps. Choosing the latter with
lar choice in utility function. 1-2-3 Worlds also require
γ = 1, we construct a behavioral policy (PIG(VI)) for
coordination of actions over time, though to a lesser
an agent that coordinates its actions under VI towards
extent than Mazes. Unlike in Mazes, however, agents
maximizing PIG:
in 1-2-3 Worlds strongly benefit from the information-
theoretically derived utility function PIG.
a :=argmaxQ (a,s); (16)
PIG(VI) −10
a
Comparing the learning curves in Fig. 4 for PIG(VI) 3.4 Structuralfeaturesofthethreeworlds
and PIG(greedy) in the three classes of worlds we find
We next asked how structural differences in the three
thatcoordinationofactionsyieldedthegreatestlearning
classes of environments correlated with the above dif-
gains in Mazes, with moderate gains also seen in 1-2-
ferences in exploration performance. In particular we
3 Worlds. In Dense Worlds PIG(VI), like PIG(greedy)
considered two structural features of the worlds, their
andrandomaction,essentiallyreachedmaximallearning
tendency to draw agents into a biased distribution over
performance.Alongwiththeresultsfortheembodiment
statesandhowtightlyanactioncontrolsthefuturestates
index above, these results support the hypothesis that
of the agent.
worlds with high embodiment constraint require agents
State bias: To assess how strongly a world biases the
to coordinate their actions over several time steps to
state distribution of agents we consider the equilibrium
achieve efficient exploration.
distribution under an undirected action policy, random
Convergence and optimality of the VI algorithm can
action. The equilibrium distribution Ψ is the limit dis-
be guaranteed [8], but only if the utility function is
tribution over states after many time steps. To quantify
stationary and the true world structure is known. To
thebiasofthisdistribution,wecomputeastructureindex
assess the impairment resulting from the use of the
(SI) as the relative difference between its entropy H(Ψ)
internal model in VI (15), we constructed a second
and the entropy of the uniform distribution H(U):
positive control, PIG(VI+), which is given the true CMC
kernel Θ for use during coordinated maximization of H(U)−H(Ψ)
PIG under VI. Under this strategy, Θ is used only to co- SI(Ψ):=
H(U)
ordinate the selection of actions and is not incorporated
into the Bayesian estimate or the PIG utility function. where:
(cid:88)
ComparingthePIG(VI)agenttothePIG(VI+)control,we H(p(s)):=− p(s)log (p(s))
2
find that they only differ in Mazes, and this difference
s∈S
9
5
4
3
2
1
Dense Worlds
Mazes 0
1-2-3 Worlds
−1
0 0.5 1
(a)
1.5
Dense Worlds
Mazes
1 1-2-3 Worlds
0.5
0
2 4 6 8 10
(b)
xednI
tnemidobmE
)stib(
] S
;
A[IM
t
0
AsshowninFig.5b,anactioninaMazeor1-2-3Worlds
has significantly more impact on future states than an
action in Dense Worlds. Controllability is required for
effective coordination of actions, such as under PIG(VI).
In Mazes, where actions can significantly effect states
far into the future, agents yielded the largest gains
fromcoordinatedactions.However,controllability,while
necessary,isnotsufficientforcoordinatedactionstohave
thepotentialofimprovinglearning.Forexample,anon-
ergodic world might have high controllability but not
allow an embodied agent to ever reach a large isolated
set of states, regardless of whether it coordinated its
actions or not. In such a world, an unembodied agent
could reach the isolated states and thereby gain a learn-
Structure Index ing opportunity inaccessible to any embodied agent.
3.5 Comparisontopreviousexplorativestrategies
WhileexplorationintheRLliteraturehaslargelyfocused
on its role in reward acquisition, many of the principles
developed to induce exploration can be implemented in
ourframework.Inthissection,wecomparethesevarious
methods to PIG(VI) under our learning objective.
Random action is perhaps the most common explo-
ration strategy used in RL. As we have already seen
in Fig. 4, random action is only efficient for exploring
Dense Worlds. In addition to undirected random action,
the following directed exploration strategies have been
developed in the RL literature. The learning curves of
the various strategies are plotted in Fig. 6.
Least Taken Action (LTA): Under LTA, an agent will
always choose the action that has been performed least
often in the current state [7], [58], [61]. Like random
Time (steps)
action, LTA yields uniform sampling of actions in each
state. Consistently, LTA fails to significantly improve on
the learning rates seen under random action (p > 0.001
Fig. 5. Quantifying the structure of the worlds. (a) The
for all three environments).
embodiment index, defined in Section 3.3, is plotted
Counter-Based Exploration (CB): Whereas LTA actively
againstthestructureindexforeachof200DenseWorlds,
samples actions uniformly, CB attempts to induce a
Mazes, and 1-2-3 Worlds. (b) The average controllability,
uniform sampling across states. To do this, it maintains
asmeasuredbythemutualinformationbetweenanaction
a count of the occurrences of each state, and chooses its
and a future state, is plotted as a function of the number
action to minimize the expected count of the resultant
oftimestepsthestateliesinthefuture(n=200).Theerror
state [71]. As shown in Fig. 6, CB performs even worse
barsdepictstandarddeviations.
than random action in Dense Worlds and 1-2-3 Worlds
(p<0.001).ItdoesoutperformrandomactionsinMazes
but falls far short of the performance seen by PIG(VI)
Thestructureindexvaluesfor200worldsineachclassof (p<0.001).
environment are plotted against the embodiment index Q-learning on Posterior Expected Information Gain
(defined in section 3.3) in Fig. 5a. As depicted, the (PEIG(Q)): Stork et al. [67] developed a utility function
embodimentindexcorrelatesstronglywiththestructure U to measure past changes in the internal model,
Storck
index. Thus, the state bias seems to represent a signifi- whichtheyusedtoguideexplorationunderaQ-learning
cantchallengeembodiedagentsfaceduringexploration. algorithm [69]. Let τ be the most recent time step in
Controllability: To measure the capacity for an agent thepastoverwhichtheinternalmodelforthetransition
to control its state trajectory we computed the mutual distribution Θ a,s,: changed:
informationbetweenarandomactionandafuturestate:
τ :=max{t|s(t)=s,a(t)=a,t<|(cid:126)d|}
(cid:18) (cid:19)
MI[A ,S |s ]= (cid:88) p(a ,s |s )log p(s t |a 0 ,s 0 ) Then, considering the internal model before and after
0 t 0
a0∈A,st∈S
0 t 0 2 p(s t |s 0 ) this time step (Θ(cid:98)τ and Θ(cid:98)τ+1 respectively), and the
10
Dense Worlds Mazes 1-2-3 Worlds
140 200
20
120
150
100
15
80
100
10
60
40
5 50
20
0 0 0
0 1000 2000 3000 0 1000 2000 3000 0 500 1000
Time (steps)
)stib(
noitamrofnI
gnissiM
PIG(VI)
LTA
CB
PEIG(Q)
Unemb.
Random
Time (steps) Time (steps)
)stib(
noitamrofnI
gnissiM
)stib(
noitamrofnI
gnissiM
Fig.6. Comparisontopreviousexplorationstrategies.TheaveragemissinginformationisplottedovertimeforPIG(VI)
agentsalongwiththreeexplorationstrategiesfromtheliterature:leasttakenaction(LTA)[7],[58],[61],counter-based
(CB)[71],andQ-Learningonposteriorexpectedinformationgain(PEIG(Q))[67].Thestandardcontrolstrategiesare
alsoshown.Standarderrorsareplottedasdottedlinesaboveandbelowlearningcurves.(n=200)
data collected up to this point
(cid:126)dτ+1,
the utility function that Q-learning is ineffective at coordinating actions
defined by Storck et al. is: during exploration. The second cross-over strategy is
PEIG(VI) which applies the VI algorithm to Storck et
U
Storck
:=D
KL
[Θ(cid:98) τ
a,
+
s,
1
:
(cid:107)Θ(cid:98) τ
a,s,:
] (17)
al.’sutilityfunction.PEIG(VI)matchedPIG(VI)inMazes
Note, both Θ(cid:98)τ and Θ(cid:98)τ+1 are internal models previously (p > 0.001) but not 1-2-3 Worlds (p < 0.001), suggesting
(orcurrently)heldbytheagent.Inthefollowingderiva- that the posterior information gain is a reasonable pre-
tion, we demonstrate that U is equivalent to the dictorforfutureinformationgainunderaDirichletprior
Storck
posterior expected information gained (PEIG). but not a Discrete prior.
(cid:104) (cid:105)
PEIG(a,s):=E
Θ|(cid:126)dτ+1
I
M
(Θ(cid:107)Θ(cid:98) τ)−I
M
(Θ(cid:107)Θ(cid:98) τ+1)
3.6 ComparisontoutilityfunctionsfromPsychology
= E (cid:34) (cid:88) Θ log (cid:32) Θ(cid:98) τ a, + s, 1 s(cid:48) (cid:33)(cid:35) Inspired by independent findings in the field of Psy-
Θ|(cid:126)dτ+1 a,s,s(cid:48) 2 Θ(cid:98)τ chology that PIG can describe human behavior during
s(cid:48) a,s,s(cid:48)
= (cid:88) s(cid:48) E Θ|(cid:126)dτ+1 [Θ a,s,s(cid:48) ]log 2 (cid:32) Θ Θ (cid:98) (cid:98) τ a τ a , , + s s , , 1 s s (cid:48) (cid:48) (cid:33) h a a l r y s e p o m o d th e e a e v s s e u i l s o re p t s e e s d o ti f n in t g h , t e h w d is e if c i f n e o r v n e e t n e s c x ti e t ga [ b 4 t e e 4 t d ] w , e t [ w 4 e 6 n o ]. t o h L t e i h k e c e r u P r m r I e G e n a , t s b u a o r n t e d h s
= (cid:88) Θ(cid:98) τ a, + s, 1 s(cid:48) log 2 (cid:32) Θ Θ (cid:98) (cid:98) τ a τ , + s, 1 s(cid:48) (cid:33) hy P p r o e t d h ic e t t e i d cal m f o u d t e ur c e ha i n n g te e rn (P a M l m C o ) d p e r l e s d : icts the height dif-
s(cid:48) a,s,s(cid:48) ference between the modes of the current and future
= D KL [Θ(cid:98) τ a, + s, 1 : (cid:107)Θ(cid:98) τ a,s,: ] (18) internal models [6], [44]:
Thus, PEIG is a posterior analogue to our PIG utility PMC(a,s)= (cid:88) Θ(cid:98)s,a,s∗ (cid:104) maxΘ(cid:98) a
a
,
,
s
s
,
,
s
s
∗
(cid:48)
−maxΘ(cid:98)a,s,s(cid:48) (cid:105)
function. Q-learning is a model-free approach to max- s(cid:48) s(cid:48)
s∗
imizing long-term gains of a utility function [69]. Fol- (19)
lowing Storck et al., we tested the combination of PEIG Predicted L1 change (PLC) predicts the average L1 dis-
andQ-learning(PEIG(Q))inourtestenvironments.Sur- tance between the current and future internal models
prisingly,PEIG(Q)performsevenworse,atleastinitially, [35]:
than random action in all three environments (p<0.001 (cid:34) (cid:35)
f
s
o
u
r
ch
C
,
M
it
C
fa
s
il
a
s
n
t
d
o
1
y
-
i
2
e
-
l
3
d
W
th
o
e
rl
l
d
ea
s,
rn
p
in
>
g p
0.
e
0
r
0
f
1
or
f
m
or
an
M
ce
az
s
e
e
s
e
)
n
. A
by
s PLC(a,s)= (cid:88) Θ(cid:98)s,a,s∗
N
1 (cid:88)(cid:12) (cid:12)
(cid:12)
Θ(cid:98) a
a
,
,
s
s
,
,
s
s
∗
(cid:48)
−Θ(cid:98)a,s,s(cid:48) (cid:12) (cid:12)
(cid:12)
(20)
s∗ s(cid:48)
PIG(VI) in Mazes in 1-2-3 Worlds.
Altogether, Fig. 6 demonstrates that PIG(VI) outper- Note,PMCandPLCdifferfromPIGinthenormused
forms the previous explorative strategies at learning to quantify differences between CMC kernels. Consider-
structured worlds. To further compare the principles
inganarbitrarynormd,theclaimanalogoustoTheorem
of PIG(VI) and PEIG(Q), we introduce two cross-over 2 would be:
strategies that borrow from each of them. The first is (cid:88)
PIG(Q)whichappliesQ-learningtothePIGutilityfunc-
Θ(cid:98) c
a
u
,s
r
,
r
s
e
∗
ntd(Θ(cid:98) future (cid:107)Θ(cid:98) current)
s∗
tion. The learning performance of PIG(Q) is similar to (cid:104) (cid:105)
PEIG(Q), falling short of PIG(VI) (Fig. S3). This suggests
=E
s∗,Θ|(cid:126)d
d(Θ(cid:107)Θ(cid:98) current)−d(Θ(cid:107)Θ(cid:98) future)
11
Dense Worlds Mazes 1-2-3 Worlds
140 200
20 120
150
100
15
80
100
10
60
40
5 50
20
0 0 0
0 1000 2000 3000 0 1000 2000 3000 0 500 1000
Time (steps)
)stib(
noitamrofnI
gnissiM
PIG(VI)
PMC(VI)
PLC(VI)
Unembodied
Random
Time (steps) Time (steps)
)stib(
noitamrofnI
gnissiM
)stib(
noitamrofnI
gnissiM
Fig. 7. Comparison between utility functions. The average missing information is plotted over time for agents that
employ VI to maximize long-term gains in the three objective function, PIG, PMC, or PLC. The standard control
strategiesarealsoshown.Standarderrorsareplottedasdottedlinesaboveandbelowlearningcurves.(n=200)
This claim states that the expected difference between CMC. Each agent is tested in ten randomly generated
the current and future internal model equals the ex- reward structures.
pected change in difference with respect to the ground At several time points during exploration, the agent
truth. While this claim holds when d is the norm used is stopped and its internal models assessed for general
in PIG (Theorem 2), it does not generally hold for either utility. For each task, we next derive the behavioral
of the norms used in PMC or PLC. policy that optimizes performance under the internal
To our knowledge, neither PIG, PMC nor PLC have model. The derived policy is then tested in the world
previously been applied to sequences of observations or (i.e. under the true CMC kernel), and the expected path
to embodied action perception loops. We tested agents length or acquired reward for that policy is determined.
that attempt to maximize PMC or PLC using VI. As Fig. Asapositivecontrol,wealsoderiveanobjectiveoptimal
7 reveals, PIG(VI) proved again to be the best performer policy that maximizes the realized performance for the
overall. In particular, PIG(VI) significantly outperforms trueCMCkernel.Thedifferenceinrealizedperformance
PMC(VI) in all three environments, and PLC(VI) in 1- between the subjective and objective policies is used
2-3 Worlds (p < 0.001). Nevertheless, PMC and PLC as a measure of navigational loss or reward loss. High
achieved significant improvements over the baseline navigational loss means the agents policy took many
controlinMazesand1-2-3Worlds,highlightingtheben- moretimestepstoreachthetargetstatethantheoptimal
efit of value iteration across different utility functions. policy.Highrewardlossmeanstheagentspolicyyielded
Interestingly, when performance was measured by an significantly fewer rewards than the optimal policy.
L1 distance instead of missing information, PIG(VI) still Fig. 8 depicts the average rank in navigational and
outperformed PMC(VI) and PLC(VI) in 1-2-3 Worlds reward loss for the different explorative strategy. Signif-
(Fig. S4). icance bounds (p = 0.001) around PIG(VI) were deter-
mined by post-hoc analysis of Friedman’s test [25]. In
3.7 Generalizedutilityofexploration all environments, for both navigation and reward acqui-
sition, PIG(VI) always grouped with the top performers
Fromabehavioralperspective,learningrepresentsafun-
(p>0.001), excepting positive controls. PIG(VI) was the
damental and primary drive [2], [37]. The evolutionary
only strategy to do so. Thus, the explorative strategy
advantage of such an exploratory drive likely rests on
that optimized learning under the missing information
the general utility of the acquired internal model [33],
objective function gave the agent an advantage in a
[50],[51],[55],[56].Totestthis,weassessedtheabilityof
range of independent tasks.
the agents to use their internal models, derived through
exploration, to accomplish an array of goal-directed
4 DISCUSSION
tasks. We consider two groups of tasks: navigation and
reward acquisition. In this manuscript we introduced a parsimonious math-
Navigation: Starting at any given state, the agent has ematical framework for studying learning-driven explo-
to navigate to any given target state with the minimal ration by embodied agents based on information the-
number of steps. ory, Bayesian inference and controllable Markov chains
Reward Acquisition: For every starting state, the agent (CMCs). We compared agents that utilized different
has to gather as much reward as possible over 100 time exploration strategies towards optimizing learning. To
steps.Rewardvaluesaredrawnfromastandardnormal understand how learning performance depends on the
distributionandrandomlyassignedtoeverystateinthe structure of the world, three classes of environments
12
20 18 18
16 16
14 14
15
12 12
10 10
8 8
10
6 6
4 4
5 2 2
5 10 15 20 0 5 10 15 20 0 5 10 15 20
mean rank (reward loss) mean rank (reward loss) mean rank (reward loss)
)ssol
noitagivan(
knar
naem
)ssol
noitagivan(
knar
naem
)ssol
noitagivan(
knar
naem
Dense Worlds Mazes 1-2-3 Worlds
Fig. 8. Demonstration of generalized utility. For each world (n=200), explorative strategies are ranked for av-
erage navigational loss (averaged across N start states and N target states) and average reward loss (aver-
aged across N start states and 10 randomly generated reward distributions). The average ranks are plotted
with standard deviations. Strategies lying outside the pair of solid green lines differ significantly from PIG(VI) in
navigational loss. Strategies lying outside the pair of solid blue lines differ significantly from PIG(VI) in reward loss
(p < 0.0001). The different utility functions and heuristics are distinguished by color: PIG(green), PEIG(magenta),
PMC(blue), PLC(cyan), LTA(orange), CB(yellow). The different coordination methods are distinguished by symbol:
Greedy(squares), VI(circles), VI+(diamonds), Heuristic Strategies(asterisks). The two standard controls are depicted
asfollows:Unembodied(black),Random(red).
were considered that challenge the learning agent in potentially be captured by the discount factor of VI.
different ways. We found that fast learning could be Interesting future work may lie in accounting for the
achieved by an exploration strategy that coordinated effectofsuchmonotonicdecreasesinestimatesoffuture
actions towards long-term maximization of predicted learning gains.
information gain (PIG). Inaddition,thelearningagentdoesnothaveaccessto
the true transition distributions for performing VI and
has to rely instead on its evolving internal model. The
4.1 Potentiallimitationstoourapproach
impairmentcausedbythisrelianceontheinternalmodel
The optimality of the Bayesian Estimate (Theorem 1) wasdirectlyassessedwithapositivecontrolPIG(VI+).A
and the accuracy of PIG (Theorem 2) both require a comparisonofPIG(VI)againstthiscontrol(Fig.4)shows
prior distribution on the transition distributions. For performance impairment only in Mazes and it is rather
biological agents, such priors could have been learned small compared to the improvements offered by VI.
fromearlierexplorationofrelatedenvironments,ormay Finally, it might be argued that the use of missing
represent hardwired beliefs optimized by evolutionary information as a measure of learning unfairly advan-
pressures.Asanotherpossibility,anagentcouldattempt taged the PIG utility function. Interestingly, however,
to simultaneously learn a prior while exploring its envi- PIG under VI was not only the fastest learner, but also
ronment. Indeed, additional results (Fig. S5) show that demonstrated the greatest capacity for accomplishing
themaximum-likelihoodestimationoftheconcentration goal-directed tasks. Furthermore, it even outperformed
parameter for Dense Worlds and Mazes enables explo- other strategies, including PLC(VI), under an L1 objec-
ration that quickly matches the performance of agents tive function (Fig. S4).
given accurate priors. Nevertheless, biological agents
may not always have access to an accurate prior for an
4.2 RelatedworkinReinforcementLearning
environment. For such cases, future work is required to
understandexplorationunderfalsepriorsandhowthey CMCsarecloselyrelatedtothemorecommonlystudied
couldyieldsub-optimalbutperhapsbiologicallyrealistic Markov Decision Processes (MDPs) used in Reinforce-
exploratory behaviors. ment Learning. MDPs differ from CMCs in that they
As another potential limitation of our approach, the explicitlyincludeastationaryrewardfunctionassociated
VIalgorithmisonlyoptimalfordynamicprocesseswith witheachtransition[19],[69].RLresearchofexploration
known stationary transition probabilities and stationary usually focusses on its role in balancing exploitative
utilities [8]. In contrast, any utility function, including behaviorsduringrewardmaximization.Severalmethods
PIG, that attempts to capture the progress in learning of for inducing exploratory behavior in RL agents have
an agent will necessarily change over time. This caveat been developed. Heuristic strategies such as random
may be partially alleviated by the fact that PIG changes action, least taken action, and counter-based algorithms
only for the sampled distributions. Furthermore, PIG arecommonlyemployedintheRLliterature.Whilesuch
decreasesinamonotonicfashion(seeFig.2)whichcould strategiesmaybeusefulinRL,ourresultsshowthatthey
13
are inefficient for learning the dynamics of structured can help identify new behavioral tasks for disambiguat-
worlds. ing the role of these measures in human behavior.
Incontrasttotheseheuristicstrategiesforexploration, Itti and Baldi recently developed an information the-
several principled approaches have been proposed for oretic measure closely related to PEIG for modeling
inducing exploratory actions to maximize rewards. For bottom-up visual saliency and predicting gaze attention
example, the BEETLE algorithm models reward as a [5],[29],[30].Intheirmodel,aBaysianlearnermaintains
partially observable MDP and derives an analytic so- aprobabilisticbeliefstructureoverthelow-levelfeatures
lution to optimize rewards [53]. Similarly, the BOSS of a video. Attention is believed to be attracted to loca-
approach maintains a posterior distribution over MDPs tions in the visual scene that exhibit high Surprise. Like
from which it periodically samples for selecting actions PEIG, Surprise quantifies changes in posterior beliefs by
that maximize reward gains ”optimistically” from the a summed Kullback-Leibler divergence. Several poten-
samples [3]. These strategies focus exclusively on ex- tialextensionsofthisworkaresuggestedbyourresults.
trinsically motivated exploration and do not address First, it may be useful to model the active nature of
exploration driven by learning for its own sake. data acquisition during visual scene analysis. In Itti and
Finally, several studies have investigated intrinsically Baldi’s model, all features are updated for all location
motivated learning under the RL framework. For ex- of the visual scene regardless of current gaze location or
ample, Singh et al. [63] have demonstrated that RL gaze trajectory. Differences in accuity between the fovea
guidedbysaliency,anintrinsicmotivationderivedfrom and periphery however suggest that gaze location will
changes in stimulus intensity, can promote the learning have a significant effect on which low-level features can
of reusable skills. As mentioned previously, Storck et al. betransmittedbytheretina[74].Second,ourcomparison
introduced the combination of Q-learning and PEIG as between the PIG and PEIG utility functions (Figs. 6 and
an intrinsic motivator of learning [67]. In their study, S3) suggests that predicting where future change might
PEIG(Q) outperformed random action only over long occur,maybemoreefficientthanfocusingattentiononly
time scales. At shorter time scales, random action per- onthoselocationswherechangehasoccuredinthepast.
formed better. Interestingly, we found exactly the same A model that anticipates Surprise, as PIG anticipates
trend, initially slow learning with eventual catching-up, information gain, may be better able to explain some
when we applied PEIG(Q) to exploration in our test aspects of human attention. For example, if a moving
environments (Fig. 6). subject disappears behind an obscuring object, viewers
mayanticipatethereemergenceofthesubjectandattend
thefaredgeoftheobscurer.Incorporatingtheseinsights
4.3 RelatedworkinPsychology
into new models of visual saliency and attention could
In the Psychology literature, PIG, as well as PMC and be an interesting course of future research.
PLC, were directly introduced as measures of the ex-
pecteddifferencebetweenacurrentandfuturebelief[6],
4.4 Information-theoreticmodelsofbehavior
[35], [44], [46]. Here, in contrast, we derived PIG, using
Bayesianinference,fromtheexpectedchangeinmissing The field of behavioral modeling has recently seen
information with respect to a ground truth (Theorem 2). increased utilization of information-theoretic concepts.
Analogous theorems do not hold for PMC or PLC. For These approaches can be grouped under three guiding
example,theexpectedchangeinL1distancebetweenan principles. The first group uses information theory to
internal model and the true structure is not equivalent quantifythecomplexityofabehavioralpolicy,withhigh
to the expected L1 distance between successive internal complexitygenerallyconsideredundesirable.Tishbyand
models. This might explain why PIG(VI) outperformed Polani for example, considered RL maximization of re-
PLC(VI)evenunderanL1measureoflearning(Fig.S4). wards under such complexity constraints [73]. While
We applied the PIG principle to the learning of a full we did not consider complexity constraints on our be-
model of the world. The Psychology literature, in con- havioral strategies in the current work, it may be an
trast, focusses on specific questions (hypothesis testing) interesting topic for future studies.
regarding the data. In addition, this prior literature has The second common principle seeks to maximize pre-
notconsideredsequencesofactionsorembodiedsensor- dictive information [4], [66], [72] (not to be confused with
motor loops. predicted information gain, PIG). Predictive informa-
It has been shown that human behavior during hy- tion, which has also been termed excess entropy [14],
pothesis testing can be explained by a model that estimates the amount of information a known variable
maximizes PIG [44], [46]. This suggests that the PIG (or past variable) contains regarding an unknown (or
information-theoretic measure may have biological sig- future) variable. For example, in simulated robots, Ay
nificance. The behavioral studies, however, could not et al. demonstrated that complex and interesting behav-
distinguish between the different utility functions (PIG, iors can emerge by choosing control parameters that
PMC and PLC) in their ability to explain human behav- maximizethepredictiveinformationbetweensuccessive
ior [44]. Perhaps our finding that 1-2-3 Worlds give rise sensory inputs [4]. The information bottleneck approach
to large differences between the three utility functions introduced by Tishby et al. [72] combines predictive
14
information and complexity constraints, maximizing the theories may be capturing distinct behavioral modes,
informationbetweenacompressedinternalvariableand with ”curiosity”-theory underlying investigatory explo-
future state progression subject to a constraint on the ration and ”boredom”-theory underlying play. In chil-
complexity of generating the internal variable from sen- dren, exploration often occurs in two stages, inspection
sory inputs. Recently, Still extended the information to understand what is perceived, followed by play to
bottleneck method to incorporate actions [66]. maintain changing stimulation [28]. These distinctions
Both Ay et al. and Still describe the behaviors that nicely correspond to the differences between our ap-
result from their models as exploratory. Their objective proach and the predictive information approach of Ay
of high predictive information selects actions such that et al. [4] and Still [66]. In particular, we hypothesize
the resulting sensory input changes often but in a pre- that our approach, which emphasizes the acquisition of
dictable way. We therefore call this form of exploration information, corresponds to curiosity-driven investiga-
stimulation-driven. Predictive information can only be tion. In contrast, we propose that predictive information
high when the sensory feedback can be predicted, and a la Ay et al. and Still, which rehearses the internal
thusstimulation-drivenexplorationreliesonanaccurate model in a wide range, may correspond with play.
internal model. In contrast, the learning objective we Further, the proposed method of additively combining
introduced here drives actions most strongly when the these two principles (Section 4.4), may naturally capture
internal model can be improved and this drive weakens the transition between investigation and play seen in
as it becomes more accurate. Thus, learning-driven and children during exploration.
stimulation-drivenexplorationcontrasteachotherwhile Even in the domain of curiosity-driven exploration,
being very interdependent. Indeed, a simple additive there are many varied theories [37]. Early theories
combination of the two objectives may naturally lead to viewed curiosity as a drive to maintain a specific level
a smooth transitioning between the two types of explo- of arousal. These were followed by theories interpret-
ration, directed by the expected accuracy of the internal ing curiosity as a response to intermediate levels of
model. In the next section we suggest a correspondence incongruencebetweenexpectationsandperceptions,and
of these two computational principles of exploration later by theories interpreting curiosity as a motivation
with two distinct modes of behavior distinguished in tomasterone’senvironment.Loewensteindevelopedan
psychology and behavioral research. Information Gap Theory and suggested that curiosity is
Finally,theFree-Energy(FE)hypothesisintroducedby an aversive reaction to missing information [37]. More
Fristonproposesthattheminimizationoffree-energy,an recently, Silvia proposed that curiosity is composed of
information-theoreticboundonsurprise,offersaunified two appraisal components, complexity and comprehen-
variational principle for governing both the learning sibility. For Silvia complexity is broadly defined, and in-
of an internal model as well as actions [17]. Friston cludes novelty, ambiguity, obscurity, mystery, etc. Com-
notes that under this principle agents should act to prehensibility appraises whether something can be un-
minimize the number of states they visit. This stands in derstood. It is interesting how well these two appraisals
stark contrast to both learning-driven and stimulation- match information-theoretic concepts, complexity being
driven exploration. During learning-driven exploration, captured by entropy, and comprehensibility by infor-
an agent will seek out novel states where missing infor- mation gain [49]. Indeed, predicted information gain
mation is high. During stimulation-driven exploration, mightbeabletoexplainthedualappraisalsofcuriosity-
an agent will actively seek to maintain high variation driven exploration proposed by Silvia. PIG is bounded
in its sensory inputs. Nevertheless, as Friston argues, byentropyandthushighvaluesrequirehighcomplexity.
reduced state entropy may be valuable in dangerous At the same time, PIG equals the expected decrease
environments where few states permit survival. The in missing information and thus may be equivalent to
balance between cautionary and exploratory behaviors expected comprehensibility.
would be an interesting topic for future research. All told, our results add to a bigger picture of explo-
ration in which the theories for its different aspects fit
togetherlikepiecesofapuzzle.Thisinvitesfuturework
4.5 Towardsageneraltheoryofexploration
for integrating these pieces into a more comprehensive
With the work of Berlyne [11], Psychologists began to theory of exploration and ultimately of autonomous
dissect the complex domains of behavior and motiva- behavior.
tion that comprise exploration. A distinction between
ACKNOWLEDGEMENTS
play (or diversive exploration) and investigation (or
specificexploration)grewoutoftwocompetingtheories The authors wish to thank Nihat Ay, Susanne Still, Jim
of exploration. As reviewed by Hutt [27], ”curiosity”- Crutchfield, Reza Moazzezi, and the Redwood Center
theory proposed that exploration is a consummatory for Theoretical Neuroscience for useful discussions. D.Y.
response to curiosity-inducing stimuli [9], [42]. In con- Little is supported by the Redwood Center for Theoret-
trast, ”boredom”-theory held that exploration was an ical and Computational Neuroscience Graduate Student
instrumental response for stimulus change [20], [43]. To Endowment. This work was supported in part by the
ameliorate this opposition, Hutt suggested that the two National Science Foundation grant CNS-0855272.
15
REFERENCES
[31] H. S. Jennings, Behavior of the lower organisms,. New York,The
Columbiauniversitypress,TheMacmillancompany,agents,1906.
[1] P. Abbeel and A. Ng, “Exploration and apprenticeship learning [32] L.Kaelbling,M.Littman,andA.Moore,“Reinforcementlearning:
inreinforcementlearning,”inProceedingsofthe22ndinternational Asurvey,”Arxivpreprintcs/9605103,1996.
conferenceonMachinelearning. ACM,2005,pp.1–8. [33] R.KaplanandS.Kaplan,“Cognitionandenvironment:function-
[2] J. Archer and L. Birke, Exploration in animals and humans. Van ingofanuncertainworld.ulrich’sbookstore,”AnnArbor,1983.
NostrandReinhold(UK)Co.Ltd.,1983.
[34] M. Kawato and K. Samejima, “Efficient reinforcement learning:
[3] J. Asmuth, L. Li, M. Littman, A. Nouri, and D. Wingate, “A computationaltheories,neuroscienceandrobotics,”Currentopin-
bayesian sampling approach to exploration in reinforcement ioninneurobiology,vol.17,no.2,pp.205–212,2007.
learning,” in Proceedings of the Twenty-Fifth Conference on Uncer-
[35] J.KlaymanandY.Ha,“Confirmation,disconfirmation,andinfor-
taintyinArtificialIntelligence. AUAIPress,2009,pp.19–26.
mationinhypothesistesting.”Psychologicalreview,vol.94,no.2,
[4] N.Ay,N.Bertschinger,R.Der,F.Gu¨ttler,andE.Olbrich,“Predic-
p.211,1987.
tiveinformationandexplorativebehaviorofautonomousrobots,”
[36] M. Lewicki, “Efficient coding of natural sounds,” nature neuro-
The European Physical Journal B-Condensed Matter and Complex
science,vol.5,no.4,pp.356–363,2002.
Systems,vol.63,no.3,pp.329–339,2008.
[37] G. Loewenstein, “The psychology of curiosity: A review and
[5] P. F. Baldi and L. Itti, “Of bits and wows: A bayesian theory of
reinterpretation.”psychologicalBulletin,vol.116,no.1,p.75,1994.
surprisewithapplicationstoattention,”NeuralNetworks,vol.23,
[38] D. MacKay and L. Peto, “A hierarchical dirichlet language
no.5,pp.649–666,Jun2010.
model,”Naturallanguageengineering,vol.1,no.3,pp.1–19,1995.
[6] J.Baron,“Rationalityandintelligence.”1985.
[39] C. Manning, P. Raghavan, and H. Schu¨tze, “Introduction to
[7] A. Barto and S. Singh, “On the computational economics of
informationretrieval,”2008.
reinforcementlearning,”inConnectionistModels:Proceedingsofthe
[40] T. Matsuzawa and M. Tanaka, Cognitive development in chim-
1990SummerSchool.MorganKaufmann. Citeseer,1990.
panzees. SpringerVerlag,2006.
[8] R. E. Bellman, Dynamic Programming. Princeton, NJ: Princeton
[41] V. Mikheev and O. Andreev, “Two-phase exploration of a novel
UniversityPress,1957.
environment in the guppy, poecilia reticulata,” Journal of fish
[9] D.Berlyne,“Noveltyandcuriosityasdeterminantsofexploratory
biology,vol.42,no.3,pp.375–383,1993.
behaviour1,”BritishJournalofPsychology.GeneralSection,vol.41,
[42] K.Montgomery,“Exploratorybehaviorasafunctionof”similar-
no.1-2,pp.68–80,1950.
ity”ofstimulussituation.”JournalofComparativeandPhysiological
[10] ——,“Conflict,arousal,andcuriosity.”1960.
Psychology,vol.46,no.2,p.129,1953.
[11] ——,“Curiosityandexploration,”Science,vol.153,no.3731,p.25,
[43] A. Myers and N. Miller, “Failure to find a learned drive based
1966.
on hunger; evidence for learning motivated by” exploration.”.”
[12] J. Byers, Animal play: evolutionary, comparative, and ecological per-
Journal of Comparative and Physiological Psychology, vol. 47, no. 6,
spectives. CambridgeUnivPr,1998.
p.428,1954.
[13] T. Cover and J. Thomas, Elements of information theory. Wiley
[44] J. Nelson, “Finding useful questions: on bayesian diagnosticity,
OnlineLibrary,1991,vol.6.
probability, impact, and information gain.” Psychological Review,
[14] J.CrutchfieldandD.Feldman,“Regularitiesunseen,randomness
vol.112,no.4,p.979,2005.
observed:levelsofentropyconvergence.”Chaos(Woodbury,NY),
[45] A.Noe¨,Actioninperception. theMITPress,2004.
vol.13,no.1,p.25,2003.
[46] M.OaksfordandN.Chater,“Arationalanalysisoftheselection
[15] J. Crutchfield and B. McNamara, “Equations of motion from a
dataseries,”Complexsystems,vol.1,no.3,pp.417–452,1987. task as optimal data selection.” Psychological Review, vol. 101,
no.4,p.608,1994.
[16] S.Dura-Bernal,T.Wennekers,andS.Denham,“Modellingobject
perception in cortex: Hierarchical bayesian networks and belief [47] B. Olshausen et al., “Emergence of simple-cell receptive field
propagation,”inInformationSciencesandSystems(CISS),201145th propertiesbylearningasparsecodefornaturalimages,”Nature,
AnnualConferenceon,march2011,pp.1–6. vol.381,no.6583,pp.607–609,1996.
[17] K. Friston, “The free-energy principle: a rough guide to the [48] J. O’Regan and A. Noe¨, “A sensorimotor account of vision and
brain?”Trendsincognitivesciences,vol.13,no.7,pp.293–301,2009. visualconsciousness,”Behavioralandbrainsciences,vol.24,no.5,
[18] D. George and J. Hawkins, “A hierarchical bayesian model of pp.939–972,2001.
invariant pattern recognition in the visual cortex,” in Neural [49] E. Pfaffelhuber, “Learning and information theory,” International
Networks,2005.IJCNN’05.Proceedings.2005IEEEInternationalJoint JournalofNeuroscience,vol.3,no.2,pp.83–88,1972.
Conferenceon,vol.3,july-4aug.2005,pp.1812–1817vol.3. [50] W.Pisula,“Costsandbenefitsofcuriosity:Theadaptivevalueof
[19] H. Gimbert, “Pure stationary optimal strategies in markov deci- exploratorybehavior.”PolishPsychologicalBulletin,2003.
sionprocesses,”STACS2007,pp.200–211,2007. [51] ——, “Play and exploration in animalsa comparative analysis,”
[20] M.Glanzer,“Curiosity,exploratorydrive,andstimulussatiation.” PolishPsychologicalBulletin,vol.39,no.2,pp.104–107,2008.
PsychologicalBulletin,vol.55,no.5,p.302,1958. [52] ——,Curiosityandinformationseekinginanimalandhumanbehavior.
[21] S.GlickmanandR.Sroges,“Curiosityinzooanimals,”Behaviour, BrownWalkerPr,2009.
pp.151–188,1966. [53] P.Poupart,N.Vlassis,J.Hoey,andK.Regan,“Ananalyticsolu-
[22] G.Gordon,D.Kaplan,B.Lankow,D.Little,J.Sherwin,B.Suter, tiontodiscretebayesianreinforcementlearning,”inProceedingsof
and L. Thaler, “Toward an integrated approach to perception the23rdinternationalconferenceonMachinelearning. ACM,2006,
andaction:Conferencereportandfuturedirections,”Frontiersin pp.697–704.
SystemsNeuroscience,vol.5,2011. [54] M. Rehn and F. Sommer, “A network that uses few active neu-
[23] R. W. Guillery, “Anatomical pathways that link perception and ronestocodevisualinputpredictsthediverseshapesofcortical
action,”Prog.BrainRes.,vol.149,pp.235–256,2005. receptive fields,” Journal of Computational Neuroscience, vol. 22,
[24] R.W.GuilleryandS.M.Sherman,“Branchedthalamicafferents: no.2,pp.135–146,2007.
what are the messages that they relay to the cortex?” Brain Res [55] M. J. Renner, “Learning during exploration: the role of behav-
Rev,vol.66,pp.205–219,Jan2011. ioral topography during exploration in determining subsequent
[25] Y. Hochberg and A. Tamhane, Multiple comparison procedures. adaptivebehavior,”IntJCompPsychol,vol.2,no.1,p.4356,1988.
WileyOnlineLibrary,1987,vol.82. [56] M. Renner, “Neglected aspects of exploratory and investigatory
[26] S. Holmes, “The selection of random movements as a factor behavior.”Psychobiology,1990.
in phototaxis,” Journal of Comparative Neurology and Psychology, [57] P.Rochat,“Objectmanipulationandexplorationin2-to5-month-
vol.15,no.2,pp.98–112,1905. oldinfants.”DevelopmentalPsychology,vol.25,no.6,p.871,1989.
[27] C. Hutt, “Specific and diversive exploration1,” Advances in child [58] M. Sato, K. Abe, and H. Takeda, “Learning control of finite
developmentandbehavior,vol.5,p.119,1970. markov chains with an explicit trade-off between estimation
[28] C.HuttandR.Bhavnani,“Predictionsfromplay.”Nature,1972. and control,” Systems, Man and Cybernetics, IEEE Transactions on,
[29] L.IttiandP.Baldi,“Bayesiansurpriseattractshumanattention,” vol.18,no.5,pp.677–684,1988.
Advances in neural information processing systems, vol. 18, p. 547, [59] T. Serre, L. Wolf, S. Bileschi, M. Riesenhuber, and T. Poggio,
2006. “Robust object recognition with cortex-like mechanisms,” IEEE
[30] L. Itti and P. F. Baldi, “Bayesian surprise attracts human atten- transactionsonpatternanalysisandmachineintelligence,pp.411–426,
tion,”VisionResearch,vol.49,no.10,pp.1295–1306,May2009. 2007.
16
[60] J.ShinskeyandY.Munakata,“Somethingold,somethingnew:a
developmentaltransitionfromfamiliaritytonoveltypreferences
withhiddenobjects,”Developmentalscience,vol.13,no.2,pp.378–
384,2010.
[61] B. Si, J. Herrmann, and K. Pawelzik, “Gain-based exploration:
Frommulti-armedbanditstopartiallyobservableenvironments,”
2007.
[62] P. Silvia, “What is interesting? exploring the appraisal structure
ofinterest.”Emotion,vol.5,no.1,p.89,2005.
[63] S. Singh, R. Lewis, A. Barto, and J. Sorg, “Intrinsically moti-
vatedreinforcementlearning:Anevolutionaryperspective,”Au-
tonomous Mental Development, IEEE Transactions on, vol. 2, no. 2,
pp.70–82,2010.
[64] A. Stewart, J. Cachat, K. Wong, N. Wu, L. Grossman, C. Suciu,
J.Goodspeed,M.Elegante,B.Bartels,S.Elkhayatetal.,“Pheno-
typing of zebrafish homebase behaviors in novelty-based tests,”
Neuromethods,vol.51,pp.143–156,2011.
[65] A.Stewart,S.Gaikwad,E.Kyzar,J.Green,A.Roth,andA.Kalu-
eff, “Modeling anxiety using adult zebrafish: A conceptual re-
view,”Neuropharmacology,2011.
[66] S. Still, “Information-theoretic approach to interactive learning,”
EPL(EurophysicsLetters),vol.85,p.28005,2009.
[67] J. Storck, S. Hochreiter, and J. Schmidhuber, “Reinforcement
driven information acquisition in non-deterministic environ-
ments,”inICANN’95. Citeseer,1995.
[68] M. Stowe, T. Bugnyar, M. Loretto, C. Schloegl, F. Range, and
K.Kotrschal,“Novelobjectexplorationinravens(corvuscorax):
effectsofsocialrelationships,”Behaviouralprocesses,vol.73,no.1,
pp.68–75,2006.
[69] R.SuttonandA.Barto,Reinforcementlearning. MITPress,1998,
vol.9.
[70] R.SuttonandB.Pinette,“Thelearningofworldmodelsbycon-
nectionistnetworks,”inProceedingsoftheseventhannualconference
ofthecognitivesciencesociety. Citeseer,1985,pp.54–64.
[71] S.Thrun,“Efficientexplorationinreinforcementlearning,”Techni-
calreport,SchoolofComputerScience,Carnegie-MellonUniversity,1992.
[72] N.Tishby,F.Pereira,andW.Bialek,“Theinformationbottleneck
method,” Proceedings of the 37th Allerton Conference on Communi-
cation,ControlandComputation,1999.
[73] N. Tishby and D. Polani, “Information theory of decisions and
actions,”Perception-ActionCycle,pp.601–636,2011.
[74] H.Wa¨ssle,B.Boycottetal.,“Functionalarchitectureofthemam-
malianretina.”Physiologicalreviews,vol.71,no.2,p.447,1991.
[75] A.Wu¨nschmann,“Quantitativeuntersuchungenzumneugierver-
halten von wirbeltieren,” Zeitschrift fu¨r Tierpsychologie, vol. 20,
no.1,pp.80–109,1963.
17
a = 1 a = 2
a = 3 a = 4
0.5
0.4
0.3
0.2
0.1
0
Fig. S1. Example Dense World. Dense Worlds consist of 4 actions
(separately depicted) and 10 states (depicted as nodes of the graphs). The
transition probabilities associated with taking a particular action are depicted
as arrows pointing from the current state to each of the possible resultant
states. Arrow color depicts the likelihood of each transition.
18
a = 1 a = 2
a = 3
1
0.8
0.6
0.4
0.2
0
Fig. S2. Example 1-2-3 World. 1-2-3 Worlds consist of 3 actions
(separately depicted) and 20 states (depicted as nodes of the
graphs). The transition probabilities associated with taking a particu-
lar action are depicted as arrows pointing from the current state to
each of the possible resultant states. Arrow color depicts the likeli-
hood of each transition. The absorbing state is depicted in gray.
19
Dense Worlds Mazes 1-2-3 Worlds
140 200
20 PIG(VI)
120
PEIG(VI)
150
PIG(Q) 100
15
PEIG(Q)
80
Unemb. 100
10
Random 60
40
5 50
20
0 0 0
0 1000 2000 3000 0 1000 2000 3000 0 500 1000
Time (steps)
)stib(
noitamrofnI
gnissiM
Time (steps) Time (steps)
)stib(
noitamrofnI
gnissiM
)stib(
noitamrofnI
gnissiM
Fig. S3. Comparison between different features of current and previous exploration strategies. The Average missing infor-
mation is plotted over time for agents that apply either VI (circles) or Q-learning (triangles) towards maximization of either
PIG (green) or PEIG (magenta). Standard control strategies are also shown. Standard errors are plotted as dotted lines
above and below learning curves. (n=200)
Dense Worlds Mazes 1-2-3 Worlds
3 6
40
2.5 5
2 30 4
1.5 3
20
1 2
10
0.5 1
0 0 0
0 1000 2000 3000 0 1000 2000 3000 0 500 1000
Time (steps)
ecnatsiD
1L
PIG(VI)
PMC(VI)
PLC(VI)
Unembodied
Random
Time (steps) Time (steps)
ecnatsiD
1L
ecnatsiD
1L
Fig. S4. Comparison between utility functions under L1 objective. The average L1 distance is plotted over time for agents
that coordinate actions using VI to maximize long-term gains in PIG, PMC, or PLC. Standard control strategies are also
shown. Standard errors are plotted as dotted lines above and below learning curves. (n=200)
20
Dense Worlds
20 20
15 15
10 10
5 5
0 0
0 1000 2000 3000 0 1000 2000 3000
Dense Worlds
30
25 150
20
100
15
10
50
5
0 0
0 1000 2000 3000 0 1000 2000 3000
)eurt(α
-
)derrefni(α
Time (steps)
Time (steps)
)stib(
noitamrofnI
gnissiM
Mazes
Time (steps)
(a)
Mazes
PIG(VI) (α-given)
PIG(VI) (α-inferred)
Unemb.
Random
Time (steps)
(c)
)stib(
noitamrofnI
gnissiM
)eurt(α
-
)derrefni(α
(b)
(d)
Fig. S5. Inferring the concentration parameter during learning. Maximum Likelihood estimatation
of the concentration parameter, α, is performed from data collected during exploration. A maxi-
mum concentration of α=20 is imposed. (a,b) The mean error in inferred concentration parameter
over time is plotted for Dense Worlds and Mazes. (c,d) The missing information for a PIG(VI)
explorer updating its internal model using the true (green with circles) or inferred (purple with
stars) over time is plotted for Dense Worlds and Mazes. Standard control explorers (with α given)
have been included. Dotted lines above and below learning curves depict standard errors. Notice,
even when required to infer the appropriate concentration parameter, the explorer is still able to
quickly learn an accurate internal model.