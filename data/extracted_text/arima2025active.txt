Active Inference Modeling of Socially Shared
Cognition in Virtual Reality
Yoshiko ARIMA
Center for Social and Psychological Research of Metaverse, Kyoto University of Advanced Science
Kyoto, Japan https://orcid.org/0000-0002-9243-2606
Mahiro OKADA
Kyoto University of Advanced Science Kyoto, Japan https://orcid.org/0009-0000-5832-8410
Research Article
Keywords: active inference, virtual reality, eye move- ment synchrony, collaborative learning, human-robot
interaction, shared social cognition
Posted Date: November 14th, 2025
DOI: https://doi.org/10.21203/rs.3.rs-8081630/v1
License:   This work is licensed under a Creative Commons Attribution 4.0 International License.
Read Full License
Additional Declarations: The authors declare no competing interests.
Active Inference Modeling of Socially Shared
Cognition in Virtual Reality
Yoshiko ARIMA Mahiro OKADA
Center for Social and Psychological Research of Metaverse Center for Social and Psychological Research of Metaverse
Kyoto University of Advanced Science Kyoto University of Advanced Science
Kyoto, Japan Kyoto, Japan
arima.yoshiko@kuas.ac.jp ORCID: 0009-0000-5832-8410
ORCID: 0000-0002-9243-2606
Abstract—This study proposes a process model for sharing is achieved. From an engineering perspective, the goal is to
ambiguous category concepts in virtual reality (VR) using an provide insights into the design of socially intelligent agents
active inference framework. The model executes a dual-layer
by modeling how symbol sharing, grounded in the body
Bayesian update after observing both self and partner actions
and objects, can be established. To achieve these aims, we
and predicts actions that minimize free energy.
A disagreement in category judgment was added to the free attemptedtomodeltheprocessofconceptsharingusingactive
energy as a risk term (corresponding to expected surprise in inference modeling, which provides a unified framework in
active inference). As the weight of this term, gaze synchrony cognitive neuroscience.
measured by Dynamic Time Warping (DTW), assumed to re-
flect joint attention, was used. The hypothesis was that higher
Inthisstudy,toexaminehowcategoryrecognitionisshared,
weightingofgazesynchronywouldimprovepredictionaccuracy.
weimplementedahandleeffecttask[1]inaVRenvironment.
To validate the model, an object classification task in VR
including ambiguous items was created. The experiment was The handle effect is a psychological experimental task where
conducted first under a bot avatar condition, in which gaze response times change depending on handle direction, similar
was not synchronized and ambiguous category judgments were to the Simon effect. Since this effect will be reported sepa-
alwaysincorrect,andthenunderahuman–humanpaircondition.
rately, we do not report it in this paper. Regarding previous
This design allowed verification of the collaborative learning
studies relevant to this report, when such tasks are performed
process by which human pairs reached agreement.
Analysis of experimental data from 14 participants showed ascollaborativeworkwithdividedroles,ithasbeensuggested
that the model achieved high prediction accuracy for observed that joint action is influenced by whether participants share
values as learning progressed. Introducing DTW as a model representations with others and whether they perceive their
parameter further improved prediction accuracy, with optimal
partner as an intentional agent [2], [3].
performanceatsynchronyweightsofγ0 =0.5-0.9.Thisapproach
provides a new framework for modeling shared social cognition
using active inference. However, such representation sharing hypotheses tend not
tobesupported.Thisisbecauseindividualcognitiveprocesses
Index Terms—active inference, virtual reality, eye move- are closed systems, and there is a lack of methods to demon-
mentsynchrony,collaborativelearning,human-robotinteraction, strate having the same representations as others. Therefore, in
shared social cognition
this study, while assuming that cognitive processes are inter-
nally closed, we examine them using active inference models
I. INTRODUCTION
that adapt to the environment. Active inference based on the
How can people share ambiguous concepts with others? free energy principle provides the following computational
This is a fundamental question in social cognition research framework for modeling predictive learning processes [4]–
and is directly related to applications in education and hu- [6].Biologicalagentsobserveenvironmentalchangesresulting
man–robotinteraction(HRI).Collaborativeconceptlearningin from active behaviors and update internal models about them-
virtual reality (VR), adopted as the experimental environment selves and their environment through Bayesian inference [7].
in this study, has potential applications in education, training, The probability distributions of responses to possible actions
HRI, and the development of embodied artificial intelligence are calculated as Kullback-Leibler (KL) divergence, and it
in robots. Therefore, this research is positioned at the inter- is assumed that actions are selected to minimize prediction
section of cognitive modeling, applied VR/HRI studies, and errors.Insocialcontexts,activeinferenceextendstomodeling
embodied AI, aiming to serve as a bridge between theoretical others’ mental states through mutual prediction, where syn-
explanationsofconceptsharingandpracticalimplementations chronization emerges when agents share common generative
in collaborative technologies. models [8], [9].
The aim of this study is, from a psychological perspective,
to deepen the understanding of how socially shared cognition In human face-to-face communication, there is an uncon-
scious process by which attention and behavior become spon-
taneously synchronized with those of others. A meta-analysis
ofstudiesonsynchronyhasshownthatsensoryandbehavioral
synchrony enhances prosocial attitudes and behaviors [10],
[11].Studieshavealsodemonstratedthatbehavioralsynchrony
influences conformity and social decision-making processes
[12], while negative emotions can reduce interpersonal syn-
chrony [13]. Interpersonal synchrony can also serve as an
indicator of perceiving the other person as part of the self
[14], and manipulating gaze direction in a categorization task Fig. 1: An example of task setting of the Joint Handle Task
hasbeenreportedtoaltertrust-relatedbehaviors[15].Evenin in VR space.
virtual environments, avatars can produce synchrony effects Note:Duetoimagecopyrightissues,theavatarsinthisphoto-
similar to those observed in face-to-face interactions [16]. graphhavebeenreplacedwithboxes.Intheactualexperiment,
neutral humanoid avatars were used, with pupil positions pro-
A previous study [17] analyzed gaze and motor synchrony grammed to move according to either pre-configured bot eye
between human or bot avatar pairs as indicators of interper- movements or real experimental participants’ eye movements.
sonal coordination in VR handle tasks. The results suggested
that the synchrony index, measured using Dynamic Time
Warping (DTW), was associated with changes in behavior. touch it only if it did. Each trial began with a pre-object
waiting phase (Phase 1 + Phase 2, total 1.5 s), followed by a
Thisstudyexaminescollaborativelearningprocesseswhere fixation cross (”+”), and then object presentation for up to 3
pairs attempt to share ambiguous category concepts through s. Participants responded by moving their virtual hand toward
bodymovementsandgazeinvirtualenvironments.Weapplya the object to ”touch” it. Only the first touch was recorded;
dual-layer active inference model framework to capture these simultaneous touches were not possible.
learningprocesses.Specifically,weestablishedamodelwhere In the Bot condition, the bot traced the human partic-
lower-level interactions such as gaze synchrony influence ipant’s movements but touched the opposite category; for
higher-level decision-making regarding category recognition High-difficultyobjects, thebotalwaysperformed theopposite
judgments. We calculated the prediction accuracy of this action.Thebot’sgazebehaviorincludedfixatingontheobject
modelfromactualmeasuredvaluesateachtrialandexamined during Phase 3, alternating gaze between the object and
the time-series changes as the learning process of the model. the partner’s face, making small tremors, and occasionally
executing large, abrupt gaze shifts.
II. METHOD
A. Participants
C. Apparatus and Stimuli
Fourteen undergraduate and graduate students participated
The experiment was conducted using an HTC VIVE Pro
in the experiment. Each participant took part in two sessions:
Eye head-mounted display (HMD) and VIVE controllers in
theBotcondition(session3)andtheHumancondition(session
a networked VR environment built with Unity (HandleAI). In
4).Theorderofsessionswasfixed,withtheBotconditional-
theVRspace,avatarsfacedeachother,andanobjectappeared
waysconductedfirst.IntheHumancondition,twoparticipants
at the center between them (Fig. 1). Each object belonged to
with consecutive IDs (e.g., IDs 1 and 2, 3 and 4) were paired.
oneoftwocategories:”Kitchen”or”Garage,”withsixvariants
In the Bot condition, each participant’s partner was a bot.
including left/right-handle versions.
Role assignments were as follows: the host (odd-numbered
Based on a preliminary survey [17], difficulty levels were
IDs) was normally assigned to the ”Kitchen” category and
classified as Low (difficulty = 0) or High (difficulty = 1);
switched to the ”Garage” category only in the Human condi-
each block of 24 trials contained 12 High-difficulty objects.
tion;theclient(even-numberedIDs)wasnormallyassignedto
These high-difficulty objects included ambiguous items such
the”Garage”categoryandswitchedtothe”Kitchen”category
ascampingequipmentthatrequiredcollaborativejudgmentfor
only in the Human condition.
accurate categorization.
TheexperimentprocedurewasapprovedbytheEthicsCom-
mittee of the Kyoto University of Advanced Science (project
D. Experimental Design
no. 22H07). All methods were carried out in accordance with
relevant guidelines and regulations. Full informed consent The experiment consisted of four types of sessions. The
was obtained from all participants who participated in the practice session contained eight trials; all other sessions
experiment. contained 24 trials. Session types included: (1) Practice:
Task familiarization with no category restriction, (2) Individ-
B. Task and Procedure
ual: Respond only to assigned category objects with partner
The VR Handle task required participants to judge whether present but no interaction, (3) Bot Pair: Paired with bot mak-
the presented object belonged to their assigned category and ing opposite-category responses (always incorrect for High-
velocity components (x, y, z axes) for both Phase 3 (object
presentation) and Phase 1+2 (pre-object waiting period).
F. Synchrony Analysis
In this study, gaze synchrony was analyzed based on pupil
position data obtained from the HTC VIVE Pro Eye head-
mounted display. Although the device also provides an esti-
mated eye direction vector, this output frequently contained
missing values and was therefore not suitable for reliable
trial-level analysis. Instead, we directly treated the three-
dimensionalpupilpositionsignalsasgazedata.Thisapproach
(a) Easy categorization targets allowed us to capture participants’ visual dynamics without
depending on device-specific estimation procedures.
From the raw pupil position time series, we computed
gaze orientation velocity components along the x, y, and z
axes. These values represent instantaneous changes in eye
orientation and were used as the basis for synchrony analysis.
For each trial, paired time-series data from the two partici-
pants were compared using two complementary methods: (1)
cross-correlation coefficients, which measure linear temporal
alignment, and (2) Dynamic Time Warping (DTW) distances,
which capture flexible similarity by allowing temporal shifts
in gaze trajectories.
Because DTW distance is inversely related to similarity,
(b) Hard categorization targets
the values were inverted and normalized to the [0, 1] range,
Fig. 2: Categorization targets used in the experiment.
resulting in a synchrony index where higher values indicate
Note: (a) Easy targets with accuracy rates of .90 or higher in
stronger gaze coordination. This normalized synchrony index
preliminary testing. (b) Hard targets with accuracy rates be-
was subsequently used as a parameter in the active inference
low .80, including camping equipment requiring collaborative
model to examine its influence on collaborative decision-
judgment. Objects were created using assets from Frogbytes
making.
[18],Yagunov[19],andHighQualityAssetsforVideoGames
Figure 4 (leftpanel) shows the time-series averagesof gaze
and Film [20].
trajectories for Human–Human pairs, while the right panel
shows those for Human–Bot pairs. The Human–Human pairs
exhibit stronger synchrony overall, although it is also evident
difficulty objects), and (4) Human Pair: Paired with a human
partner with category assignments swapped between partners.
E. Measures
Behavioral data included: touch responses (1 = participant
touch, -1 = partner touch, 0 = no touch), correctness (1
= correct, 0 = incorrect), reaction time (maximum 3 s),
and difficulty level (High/Low). Eye movement synchrony
data were collected using DTW analysis of eye orientation
TABLE I: Experimental Session Types and Descriptions
Session Name Description
1 Practice Taskfamiliarization,nocategory
restriction.
2 Individual Respondonlytoassignedcategory
objects(righthandonly).Partner Fig. 3: Experimental session flow diagram showing the pro-
presentbutnointeraction. gression from individual practice through collaborative ses-
3 BotPair SamecategoryasSession2,paired
sions.
withbotmakingopposite-category
responses(alwaysincorrectfor Note: Participants move through four distinct phases: Prac-
High-difficultyobjects). tice (no category restriction), Individual (category-specific
4 HumanPair Pairedwithahumanpartner;
responses), Bot Pair (collaboration with artificial agent), and
categoryassignmentsswapped
betweenpartners. Human Pair (human-human collaboration with role reversal).
that the human partner’s pupil dynamics are modulated when framework’s gaze control mechanisms while enabling precise
interacting with a Bot, producing patterns distinct from Hu- experimental control over social gaze interactions in virtual
man–Human interactions. It should be noted that these graphs environments.
represent averaged trajectories; actual DTW-based synchrony
III. RESULTS
values vary across individual pairs.
A. Manipulation Check
G. Model–Data Loop (Overview of Figure 5)
As a manipulation check, we tested whether task difficulty
Figure 5 illustrates the procedure linking empirical data influenced response accuracy (measured as the deviation from
with the active inference model. At each trial, observed thegeneralpopulation’saccuracy,giventhatthetaskincluded
values—selfandpartneractionstogetherwithgazesynchrony ambiguous categories such as camping gear). No significant
indices—update the lower-level Bayesian layer. These beliefs effect of difficulty was found on the mean response accuracy,
are then passed to the higher-level Bayesian layer, which whereasasignificanteffectwasobservedonthevariance(Fig.
refines predictions of partner behavior. In parallel, the syn- 6).
chrony index isconverted into the γ parameter thatmodulates Tofurtherexaminevariancedifferences,atwo-wayBrown–
collaborative weighting. Forsythe ANOVA on absolute deviations from group medians
Based on these updated beliefs, the model evaluates candi- showed a significant main effect of difficulty on response
dateself-actionsusingthefree-energyfunction,andasoftmax variance (F(1,76) = 61.06, p ¡ .001), no significant main
policy determines the next action probability. From this pro- effect of period (F(2,76) = 1.12, p = .331), and a marginally
cess, both partner-action predictions and self-action policies significant difficulty × period interaction (F(2,76) = 2.58, p =
arederivedandcomparedwiththesubsequenttrial’sobserved .082).
data. Accuracy and free-energy trajectories are then plotted These manipulation check results confirmed that responses
across trials, focusing on the 12 high-difficulty objects. to difficult tasks showed high variance, validating that our
The mathematical details of Bayesian updating, synchrony experimental conditions successfully induced the ambiguous
weighting, and free-energy minimization are provided in the category concepts assumed in this study. There was a trend
Results section. for this variance to converge toward the later trials, although
the interaction effect was not statistically significant.
H. Bot Avatar Gaze Control System Implementation
We implemented a real-time gaze control system utilizing B. Eye Movement Synchrony Analysis
UniVRM version 0.127.2 (com.vrmc.univrm) for avatar gaze Weanalyzedeyemovementsynchronybetweenparticipants
control in Unity 6000.0.2f1. VRM (Virtual Reality Model) usingsensordatafromtheVRheadsetsduringPhase3(object
is an open-source 3D avatar format developed by the VRM presentation phase). Two measures were employed: cross-
Consortiumspecificallyforvirtualrealityapplications,provid- correlation coefficients and DTW distances.
ing standardized components including VRMLookAtHead for Statistical analysis revealed significant differences between
automated gaze direction control. For human participants, we conditions. For cross-correlation synchrony, Human-Human
developed HumanGazeTracker to interface with the UniVRM pairs showed higher synchrony (M = 0.159, SD = 0.137)
system, retrieving eye-tracking data from the Tobii system at compared to Human-Bot pairs (M = 0.106, SD = 0.094), with
10Hz and feeding calculated 3D gaze directions to VRM’s asignificantdifference(Mann-WhitneyU=33070.0,p¡.001,
built-in LookAt components. The VRM framework automat- Cohen’s d = 0.48). These results demonstrate that participants
ically projected gaze vectors to target positions 2.0 meters exhibit more coordinated eye movement patterns when inter-
from the avatar’s head with integrated smoothing (factor acting with human partners compared to bot partners.
5.0)fornaturaltransitions.AllVRMLookAtHeadcomponents
C. Active Inference Model Results
were configured without pre-assigned targets, allowing our
custom scripts to maintain full programmatic control through To evaluate participants’ partner behavior prediction accu-
dynamically created target GameObjects. racy and self-behavior prediction accuracy in the pair task,
For Bot agents, we developed VRMUnifiedGazeSync to we implemented and compared Bayesian learning with active
interface with the UniVRM gaze system, simulating human- inference models.
like behavior patterns while leveraging VRM’s automatic 1) Active Inference Model Architecture: The model ex-
head-eyecoordinationmechanisms.Duringobjectpresentation plored in this study aims to capture the process of sharing
phases, Bots directed gaze toward experimental objects with ambiguous category concepts by predicting others’ internal
80% probability, with remaining attention allocated to partner models. Therefore, we attempted to construct an active infer-
avatars or random environmental points within a 30-degree ence model that learns sequentially from observed values and
cone. Random gaze movements occurred with 90% proba- can predict partner behavior more accurately.
bility during non-task periods, updated at 100Hz intervals. We calculated KL divergence from the difference between
Natural tremor simulation (0.01-unit magnitude at 0.5-second the ”probability distribution from the previous trial (prior
intervals) was added to mimic human fixational movements. distribution)” and the ”updated probability distribution of the
This approach leveraged the established UniVRM 0.127.2 current trial (posterior distribution)”, and compared a model
Fig. 4: Time-series data of average eye position during Phase 3 for Human-Human (left panel) and Human-Bot (right panel)
conditions.
Note:Bluesolidlinesrepresenthostparticipants,orangedottedlinesrepresentclientparticipantsorbot.Thefiguredemonstrates
more coordinated eye movement patterns in Human-Human pairs compared to Human-Bot conditions, with greater temporal
synchrony evident in the left panel.
Fig. 5: Active inference model with eye-gaze synchrony.
Note: The model integrates a two-layer Bayesian update (Single Bayes, Double Bayes) with free energy minimization. Partner
predictionandself-actionselectionaremodulatedbyasynchronyparameter(γ),derivedfromeye-movementsynchrony(DTW),
to enhance collaborative agreement.
that weights the synchrony parameter gamma to the KL term tionaccuracyofPrecision=1.000,Recall=0.202,F1=0.336,
with a model that weights it to the reward (risk) term. As a AUC=0.652,MCC=0.378,whilethelattershowedPrecision
result, both models showed roughly equivalent partner predic- = 0.958, Recall = 0.404, F1 = 0.568, AUC = 0.769, MCC =
0.534,but, a tendency for accuracy to remain unchanged as Ourmodelimplementsanactiveinferenceframeworkbased
trials progressed, particularly decreasing in the former, was on dual-layer Bayesian inference that integrates eye-gaze
confirmed. Next, we redefined the KL definition based on synchrony for collaborative decision-making. The framework
equation(7)asthedifferencebetween”probabilitydistribution employs two hierarchical layers: (1) Single Bayes provides
after simple Bayes update” and ”probability distribution after direct partner behavior prediction P(A = 1|A ,O), and
p s
doubleBayesupdate”,andcomparedmodelswheregammais (2) Double Bayes applies metacognitive corrections yielding
appliedtotheKLterm,appliedtotherewardterm,andmixed Q(A =1|A ,O).
p s
models. As a result, the model applying gamma to the KL BayesianUpdateProcess:Attrialt,thefirstlayermaintains
term did not improve the learning curve, and only the model Beta distributions for partner predictions:
applying gamma to the reward term showed the most stable
P (A =1|A =1,O)∼Beta(α ,β ) (1)
accuracyimprovementthroughouttrials.Thelattershowedthe t p s 11 11
highest performance with model accuracy throughout trials of P (A =1|A =0,O)∼Beta(α ,β ) (2)
t p s 01 01
Precision = 0.964, Recall = 0.465, F1 = 0.627, AUC = 0.831,
whereA ∈{0,1}representstheselfaction(1=touchobject,
MCC = 0.583, so this model was finally adopted. s
0 = no touch), A ∈{0,1} represents the partner action with
The adopted model differs from general active inference p
the same encoding, and O represents the object.
models in two points. First, while KL in the free energy
Uponobservingactions(at−1,at−1),parametersupdateas:
principle is originally based on ”the difference between prior s p
and posterior distributions”, in this study it is operationally α ←α +δ(at−1 =i,at−1 =j) (3)
ij ij s p
defined as ”the belief difference between simple Bayes and
β ←β
+δ(at−1 =i,at−1
̸=j) (4)
double Bayes”. Second, the parameter gamma representing ij ij s p
confidence(degreeofattentiontowardtargets:gazesynchrony where at−1,at−1 are the observed actions from the previous
s p
in this study) is applied to the reward term rather than the KL
trial (lowercase denotes observed values, superscript t − 1
term.
indicates the previous trial), and δ(·) is the Kronecker delta
function (equals 1 when the condition is true, 0 otherwise).
The Beta distribution parameters α ,β are maintained for
ij ij
each combination of i=A ,j =A .
s p
The second layer employs exponential moving averages for
double Bayesian updating:
Q t (A p =1|A s =k)=α·at p −1+(1−α)·Q t−1 (A p =1|A s =k)
(5)
whenat−1 =k,whereP(A =1|A ,O)representsthesingle
s p s
Bayes prior distribution for predicting partner behavior based
on self action and object, while Q(A = 1|A ,O) represents
p s
thedoubleBayesposteriordistributionincorporatingmetacog-
nitive corrections.
Fig. 6: Changes in response accuracy across periods by task
difficulty (Easy/Hard) in the Human condition.
Note:Eachperiodconsistsof8trialsoutofatotalof24trials,
and the plots are based on per-ID means. Boxplots show the
”difference from population mean” (diff from expected) with
95% confidence intervals of the means. Results demonstrate
category concept convergence in Hard trials and stability in
Easy trials.
TABLE II: Model Accuracy Comparison Across Synchrony
Weight Conditions
SyncWeight Condition PartnerAcc SelfAcc Fig. 7: Changes in Free Energy Across Session Progression.
0.0 Human 0.6429 0.7143 Note: G values based on active inference updated with ob-
Bot 0.4345 0.5952
served values from each trial. Free energy decreases over
Human 0.6429 0.7143
0.1
Bot 0.4345 0.5952 trials,indicatingprogressininternalmodelupdating.Thegaze
Human 0.6845 0.7440 synchrony parameter of 0.5 shows overall lower values than
0.5
Bot 0.5179 0.6548
0.1.Botconditionshowsmorelinearchangesduetoconsistent
Human 0.6845 0.7440
0.9 Bot 0.5417 0.6786 bot behavior.
Free Energy Calculation: For each candidate action a ∈ Collaboration: The synchrony parameter modulates collabora-
{0,1}, free energy is defined as: tive decision-making:
D (Bern(q(a))∥Bern(p(a))) γ =γ 0 ·DTWn sy o n rm c alized (11)
G(a) =κ KL t t +β γ (2q(a)−1)
t 1+D (Bern(q(a))∥Bern(p(a))) 0 t t where γ is the synchrony weight acting as a weighting
KL t t
(6) factor for social coordination in self action selection, and
where G(a) is the free energy for self action a ∈ {0,1} γ ∈ {0,0.1,0.5,0.9} represents the base synchrony weight
t 0
at trial t (lower values indicate preferred actions), κ is the parameter (baseline values for setting coordination impor-
scaling parameter for the KL divergence term (set to 0.1), tance). Higher synchrony values enhance collaborative agree-
D (Bern(q)∥Bern(p)) is the Kullback-Leibler divergence ment weighting in the γ ·R(a) term of Equation (6). Model
KL
between Bernoulli distributions with parameters q and p, Evaluation:Modelperformanceisassessedthroughprediction
q(a) = Q (A = 1|A = a) is the predicted probability accuracy measures:
t t p s
of partner action given self action a from the double Bayes Acc =E[1(a ∗ =atrue)] (12)
s s s
layer, p(a) =P (A =1|A =a) is the predicted probability
t t p s
from the single Bayes layer, β 0 is the base reward weighting Acc p =E[1(1(Pˆ(A p =1)≥0.5)=at p rue)] (13)
parameter, γ is the gaze synchrony weight at trial t (derived
t where Acc is the self-action prediction accuracy (proportion
from DTW analysis), and R(a)≡(2q(a)−1) is the collabo- of correctly s predicted self actions), Acc is the partner action
t p
rative agreement reward term. The KL divergence for binary prediction accuracy (proportion of correctly predicted partner
distributions is computed as: actions), and FE is the minimum free energy value calculated
as FE=min(G(0),G(1)) to evaluate action preferences.
q 1−q
D KL (Bern(q)∥Bern(p))=qlog +(1−q)log (7) 3) Comparative Performance Analysis: Free energy de-
p 1−p
creases across trials, indicating that internal model updating
where numerical stability is ensured through epsilon clipping is progressing. The gaze synchrony parameter of 0.5 shows
(ϵ=10 −12)topreventlogarithmicsingularities.Thecompres- overall lower values than 0.1. The Bot condition shows more
sion term DKL maps the KL divergence to the range [0,1) linear changes because bot behavior is consistent (Figure 7).
1+DKL
forbalancedweightingwiththerewardterm.Thesecondterm Figures8and9showthecomparativeperformancebetween
(2q t (a) −1) represents a risk component that increases when conditionswithandwithoutsynchronyweightingintheactive
disagreementwiththepartner’scategoryjudgmentisexpected. inferencemodelacrosstrialsforambiguouscategoryjudgment
This serves as a target term that encourages action selection objects.
toward anticipated agreement. The results demonstrate that synchrony weighting signif-
Action Selection: Action selection follows the softmax icantly enhances prediction accuracy, particularly for Human
function: pairs.IntheHumancondition,synchronyweightingimproved
exp{−G(a)/τ} both partner prediction accuracy and self-behavior prediction
a t =argm
a
inG t (a), π t (a)=
P exp{−
t G(a′)/τ} (8) accuracy. Bot pairs showed more modest improvements with
a′ t synchrony weighting, consistent with reduced interpersonal
wherea istheselectedactionattrialt(deterministicselection coordination in human-bot interactions.
t
via free energy minimization), π (a) is the probability of Table II presents the comprehensive accuracy comparison
t
selecting action a at trial t (computed for analysis purposes), across synchrony weight conditions. When comparing overall
and τ is the softmax temperature parameter controlling the accuracy across all trials, synchrony weighting at γ 0 = 0.1,
randomness in action selection (lower values increase deter- γ = 0.5, and γ = 0.9 showed notable performance
0 0
ministic behavior). The actual action selection uses argmin differences compared to the baseline condition (γ = 0).
0
deterministically, while π (a) provides the selection probabil- The results indicate that synchrony contributes significantly
t
ities for model evaluation. to collaborative agreement as a reward mechanism. While
2) Synchrony Parameter Integration: DTW Synchrony γ = 0.5 and γ = 0.9 yielded optimal and equivalent
0 0
Calculation: Eye movement synchrony is calculated using performance, this suggests that synchrony weighting reaches
DynamicTimeWarping(DTW)distancebetweenparticipants’ saturation around γ =0.5.
0
eye orientation velocity components. The DTW distance val- Figures8and9showtheaccuracyofthedifferencebetween
ues are normalized to create a synchrony index: thenexttrialbehaviorpredictedbytheactiveinferencemodel
DTWraw =−DTW (9) and the observed values for partner and self, respectively.
sync distance
Figure8showstime-serieschangesintheaccuracywithwhich
DTWraw −min(DTWraw)
DTWnormalized = sync sync (10) the active inference model’s predicted values could accurately
sync max(DTWraw)−min(DTWraw) predict the partner’s next action, showing the progression
sync sync
of learning. Human pairs (blue lines) demonstrate superior
where DTW is the raw DTW distance (negative values),
raw
DTW is the normalized DTW synchrony index scaled to learning compared to Bot pairs (orange lines) across different
norm
[0,1] range (higher values indicate greater synchrony), and synchrony weighting conditions (γ=0.1 and γ=0.5). Figure
highervaluesindicategreatersynchrony.Synchrony-Weighted 9 shows time-series changes in how well the experimental
indexDTWsetasaparameterrepresentingattentiontopartner
attention, higher gaze synchrony resulted in improved model
accuracy.
The handle task situation in this study, namely the task of
reachingouttoobjectsascollaborativeworkwithothersfacing
eachother,hasalsobeenusedinstudiesofimitationbehavior
[21].The underlyingbodymovementand attentionsynchrony
for concept sharing is also related to triadic relationships
known in developmental research of interpersonal communi-
cation. The results of this study, obtained from task situations
withsuchpsychologicalbreadth,areexpectedtoprovideclues
for approaching the symbol grounding problem in concept
Fig. 8: Partner Prediction Accuracy across trials.
acquisition in AI research by utilizing body movements and
attention synchrony.
A methodological contribution of this study lies in the
integration of dynamic time warping (DTW) synchrony in-
dices into the active inference framework as a γ–weighting
parameter. While DTW has been widely used in cognitive
science to quantify temporal alignment of behavioral signals,
its application as a modulator of the free energy functional
is novel. By treating gaze synchrony as a factor influencing
the balance between risk and information gain, the present
approach extends active inference beyond purely internal pre-
dictive mechanisms toward an embodied, interaction-sensitive
formulation. This methodological innovation provides a new
Fig. 9: Self Prediction Accuracy across trials.
tool for modeling mutual adaptation in joint action and could
be generalized to other multimodal synchrony measures such
as gesture or speech rhythm.
participants’behaviormatchedwhatthemodelpredictedasthe
Despitethesepromisingfindings,severallimitationsremain.
nextactionoftheexperimentalparticipants,withHumancon-
The Bot agents in this experiment exhibited relatively simple
dition (blue lines) showing enhanced self-prediction accuracy
and predetermined gaze behaviors, which limited the ecolog-
compared to Bot condition (orange lines). Both approaches
ical validity of human–Bot interactions. More sophisticated
1.0 as they near the final trial, indicating that learning is
bidirectional models, where both partners engage in active
progressing smoothly. For self-behavior (difference between
inference processes simultaneously, would allow for richer
actual participant behavior and model predictions), accuracy
simulationsofmutualadaptationandnegotiationinjointtasks.
changes are not as rapid as for partners, but accuracy still
Extending the present framework to such reciprocal active
improves as approaching the final trial. For self-behavior, the
inference would not only improve the realism of the task
gaze synchrony parameter of 0.5 also tends to show better
setting but also open pathways toward designing interactive
accuracy than 0.1. For both measures, until the middle of the
AI systems that learn to align their beliefs and actions with
trials,thehumanpairconditiontendstoshowhigheraccuracy
human partners in real time.
than the Bot pair condition.
ACKNOWLEDGMENT
IV. DISCUSSION
The authors would like to thank all participants who took
Thisstudyattemptedtoapplyactiveinferencemodelstothe part in this study. This research was supported by the Center
process of collaborative learning between pairs, specifically for Social and Psychological Research of Metaverse at Kyoto
the agreement process for ambiguous category concepts. We University of Advanced Science.
performed beta distribution updates from observed values,
followedbydoubleBayesianupdatingtopredictpartnerinter- REFERENCES
nal models, and calculated free energy from active inference
[1] M. Tucker and R. Ellis, “On the relations between seen objects and
models based on these results, attempting to match model componentsofpotentialactions,”JournalofExperimentalPsychology:
predictions with experimental observations. HumanPerceptionandPerformance,vol.24,no.3,pp.830–846,1998.
[2] C.-C. Tsai and M. Brass, “Does the human motor system simulate
Experimental results showed that our model demonstrated
pinocchio’s actions?” Cognitive Neuropsychology, vol. 24, no. 2, pp.
progressive learning across trials, ultimately achieving high 175–194,2007.
accuracyinpredictingbothpartnerandselfnextactions.Addi- [3] A.Stenzel,T.Dolk,L.S.Colzato,R.Sellaro,R.Liepelt,andB.Hom-
mel, “The joint simon effect depends on perceived agency, but not
tionally, when partner category concept agreement was added
intentionality,”JournalofExperimentalPsychology:HumanPerception
as a reward term to the G equation, with the gaze synchrony andPerformance,vol.38,no.5,pp.1132–1141,2012.
[4] T.ParrandK.J.Friston,“Generalisedfreeenergyandactiveinference,”
BiologicalCybernetics,vol.113,pp.495–513,2019.
[5] R. Smith, K. J. Friston, and C. J. Whyte, “A step-by-step tutorial
on active inference and its application to empirical data,” Journal of
MathematicalPsychology,vol.107,p.102632,2022.
[6] Z. Zhang and F. Xu, “An overview of the free energy principle and
related research,” Neural Computation, vol. 36, no. 5, pp. 963–1021,
2024.
[7] T.Inui,Activeinference:Thefreeenergyprincipleinmind,brain,and
behavior. Kyoto,Japan:MinervaShobo,2022,translationofworkby
T.Parr,G.Pezzulo,andK.J.Friston.
[8] W. Yoshida, R. J. Dolan, and K. J. Friston, “Game theory of mind,”
PLoSComputationalBiology,vol.4,no.12,p.e1000254,2008.
[9] K.FristonandC.Frith,“Aduetforone,”ConsciousnessandCognition,
vol.36,pp.390–405,2015.
[10] M.RennungandA.S.Go¨ritz,“Prosocialconsequencesofinterpersonal
synchrony:Ameta-analysis,”Zeitschriftfu¨rPsychologie,vol.224,no.3,
pp.168–189,2016.
[11] G.FrondaandM.Balconi,“Eeginter-brainconnectivityduringprosocial
decision-making: The role of giving behavior,” Neuropsychologia, vol.
164,p.108091,2022.
[12] P. Dong, X. Dai, and R. S. Wyer Jr, “Actors conform, observers
react: The effects of behavioral synchrony on conformity,” Journal of
PersonalityandSocialPsychology,vol.108,no.1,pp.60–75,2015.
[13] Y.Smykovskyietal.,“Negativeaffectreducesinterpersonalsynchrony,”
Emotion, 2024, advance online publication. [Online]. Available:
https://doi.org/10.1037/emo0001322
[14] J. Hao et al., “Interpersonal synchrony promotes self-other merging:
Evidencefrommovementandgazecoordination,”Cognition,vol.245,
p.105614,2024.
[15] T. B. Rogers, T. L. Rogers, and P. L. Rogers, “Gaze cues and trust in
human–computerinteraction,”ComputersinHumanBehavior,vol.39,
pp.56–65,2014.
[16] J.N.BailensonandN.Yee,“Digitalchameleons:Automaticassimilation
ofnonverbalgesturesinimmersivevirtualenvironments,”Psychological
Science,vol.16,no.10,pp.814–819,2005.
[17] Y. Arima, Y. Harada, M. Okada et al., “Interpersonal synchrony in
human-robot interaction: Sensor analysis of interpersonal interaction
usingvrdata,”June2025,researchSquarePreprint.[Online].Available:
https://doi.org/10.21203/rs.3.rs-5846894/v1
[18] Frogbytes, “Realistic kitchen pack!” 2017,
unity Asset Store. [Online]. Available:
https://assetstore.unity.com/packages/3d/props/interior/realistic-kitchen-
pack-80277
[19] A.Yagunov,“Garagepbrtoolspack,”2023,unityAssetStore.
[20] High Quality Assets for Video Games and Film, “Mega
camping bundle,” 2022, unity Asset Store. [Online].
Available: https://assetstore.unity.com/packages/3d/props/tools/mega-
camping-bundle-206287
[21] E.J.Coleetal.,“Actionobservationandimitationinjointaction:The
roleofsharedgoals,”ExperimentalBrainResearch,vol.236,no.9,pp.
2465–2476,2018.