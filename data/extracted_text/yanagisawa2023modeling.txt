Yanagisawa, H. & Honda, S. 
1 
 
Modeling arousal potential of epistemic emotions using Bayesian 
information gain: Inquiry cycle driven by free energy fluctuations 
 
Hideyoshi Yanagisawa*a, Shimon Hondab 
 
a The University of Tokyo, 7-3-1 Bunkyo, Hongo, Tokyo, 113-8656, JAPAN. hide@mech.t.u-tokyo.ac.jp 
b The University of Tokyo, 7-3-1 Bunkyo, Hongo, Tokyo, 113-8656, JAPAN. hondar8@g.ecc.u-tokyo.ac.jp 
*Corresponding author 
 
Abstract 
Epistemic emotions, such as curiosity and interest, drive the inquiry process. This study proposes a novel 
formulation of epistemic emotions such as curiosity and interest using two types of information gain 
generated by the principle of free energy minimization: Kullbackâ€“Leibler divergence (KLD) from 
Bayesian posterior to prior, which represents free energy reduction in recognition, and Bayesian surprise 
(BS), which represents the expected information gain by Bayesian prior update. By applying a Gaussian 
generative model with an additional uniform likelihood, we found that KLD and BS form an upward-
convex function of surprise (minimized free energy and prediction error), similar to Berlyneâ€™s arousal 
potential functions, or the Wundt curve. We consider that the alternate maximization of BS and KLD 
generates an ideal inquiry cycle to approach the optimal arousal level with fluctuations in surprise, and 
that curiosity and interest drive to facilitate the cyclic process. We exhaustively analyzed the effects of 
prediction uncertainty (prior variance) and observation uncertainty (likelihood variance) on the peaks of 
the information gain function as optimal surprises. The results show that greater prediction uncertainty, 
meaning an open-minded attitude, and less observational uncertainty, meaning precise observation with 
attention, are expected to provide greater information gains through a greater range of exploration. The 
proposed mathematical framework unifies the free energy principle of the brain and the arousal potential 
theory to explain the Wundt curve as an information gain function and suggests an ideal inquiry process 
driven by epistemic emotions. 
Keywords: Emotion, free energy, Bayes, arousal, curiosity, inquiry. 
 
1. Introduction 
Inquiry is an essential cognitive process in human activities such as scientific research, creation, and 
education. American philosopher Charles Sanders Peirce defines inquiry as a cycle of three inferences: 
abduction, deduction, and induction (Peirce, 1974). In the observation of surprising phenomena, 
abduction infers a possible cause of the observation, deduction predicts unknown effects based on the 
inferred cause, and induction tests the prediction and updates the causal knowledge. A voluntary inquiry 
Yanagisawa, H. & Honda, S. 
2 
 
process is facilitated by epistemic emotions such as surprise, curiosity, interest, and confusion (Kashdan 
& Silvia, 2009; V ogl, Pekrun, Murayama, & Loderer, 2020). Psychologist Berlyne defined two types of 
epistemic curiosity: diversive and specific (Berlyne, 1966; Silvia, 2012). Diversive curiosity seeks 
novelty, and thus, in this type of curiosity, surprise triggers abductive reasoning. On the other hand, 
specific curiosity drives induction, which seeks evidence of deductive reasoning to resolve confusion. 
Emotions are generally mapped to a dimensional space (Lang, 1995; Russell, 1980). The most 
commonly used dimensions are arousal and valence, termed the core affect (Russell, 2003). Arousal is the 
intensity of emotions, whereas valence is the dimension of the positive and negative poles. A recent 
functional magnetic resonance imaging (fMRI) study showed that arousal and valence are correlated with 
neural activity in the orbitofrontal cortex and amygdala, respectively (Wilson-Mendenhall, Barrett, & 
Barsalou, 2013). The emotional dimensions are not independent, and arousal affects valence. Berlyneâ€™s 
arousal potential theory suggests that an appropriate level of arousal potential induces a positive hedonic 
response, whereas extreme arousal induces a negative response (Berlyne, 1960). Thus, valence forms an 
inverse-U-shaped function of the arousal potential, termed the Wundt curve (Fig. 1). Berlyne suggests that 
epistemic curiosity approaches the optimal arousal potential, where the hedonic response (or positive 
valence) is maximized (Berlyne, 1960, 1966; Silvia, 2012).  
Berlyne also illustrated a number of arousal potential factors such as novelty, complexity, and 
uncertainty (Berlyne, 1960). Yanagisawa mathematically explains that the free energy, which is 
information on the brainâ€™s prediction error or surprise (Friston, Kilner, & Harrison, 2006), represents the 
arousal potential because free energy is decomposed into information quantity terms representing 
perceived novelty, complexity, and uncertainty (Yanagisawa, 2021). This free-energy arousal model 
suggests that an appropriate level of free energy or surprise induces a positive emotional valence based on 
Berlyneâ€™s Wundt curve, which is supported by experimental evidence (Honda, Yanagisawa, & Kato, 
2022; Sasaki, Kato, & Yanagisawa, 2023).  
By contrast, the free energy principle (FEP) (Friston et al., 2006), known as the unified brain 
theory (Friston, 2010), suggests that the brain must minimize its free energy during perception and action. 
Previous studies have proposed that decreasing and increasing free energy (or expected free energy) 
correspond to positive and negative valence, respectively (Clark, Watson, & Friston, 2018; Hesp et al., 
2021; Joffily & Coricelli, 2013; Seth & Friston, 2016; Wager et al., 2015; Yanagisawa, Wu, Ueda, & 
Kato, 2023), and that high and low free energies indicate uncertain and certain states, respectively. 
Reducing free energy resolves uncertainty and produces positive emotions.  
The FEP argument that minimizing free energy corresponds to a positive valence seems to 
contradict the argument of arousal potential theory that an appropriate level of arousal potential 
(represented by free energy (Yanagisawa, 2021)) maximizes positive valence. To resolve this 
contradiction and integrate the FEP-based valence and arousal potential theories, we propose a novel 
valence framework based on the theory that a decrement in free energy and its expectation explain the 
Yanagisawa, H. & Honda, S. 
3 
 
valence of epistemic emotions. A decrease in free energy represents information gain and an epistemic 
value (Friston et al., 2017; Parr, Pezzulo, & Friston, 2022). The more information gain (epistemic value) 
one obtains or expects, the more positive the valence one experiences.  
Based on this framework, we formulated emotion valence functions of the arousal potential 
using decrements in free energy (or information gains). By applying a Gaussian generative model with an 
additional uniform likelihood, we demonstrated that the epistemic valence function forms an inverse-U 
shape and analyzed the effects of prediction error and uncertainties on the peaks of the valence functions. 
We associated epistemic emotions such as curiosity and interest with the free-energy-based valence 
model. Furthermore, we proposed an inquiry cycle model based on free-energy-based epistemic emotions. 
 
 
Fig. 1 Arousal potential function, or Wundt curve. Appropriate level of arousal maximizes positive 
emotion valence (optimal arousal level).  
 
2. Method 
2.1 Free energy formulations 
FEP suggests that the brain must minimize its free energy through recognition, action, and learning 
(Friston et al., 2006). Assume an agent recognizes a hidden state ğ‘  as a cause of an observation ğ‘œ given 
by an action based on a policy ğœ‹. We assume that the agent has a generative model ğ‘áˆºğ‘ , ğ‘œ|ğœ‹áˆ» as its 
knowledge about the probabilistic relationship between hidden states and observation and a recognition 
density ğ‘áˆºğ‘ |ğœ‹áˆ» of hidden states for a given policy. The free energy of a policy ğœ‹ is defined as a function 
of an observation representing the difference between a recognition density and a generative model 
averaged by the recognition density in terms of their energies (negative log probability). 
 ğ¹à°— àµŒ âŸ¨ln ğ‘áˆºğ‘ |ğœ‹áˆ»àµ†ln ğ‘áˆºğ‘ , ğ‘œ|ğœ‹áˆ»âŸ©à¯¤áˆºğ‘ |ğœ‹áˆ» (1) 
The free energy represents the prediction error of recognition from the knowledge, i.e., the generative 
model. It refers to uncertainty and the prediction error of signals in a Bayesian brain theory (Knill & 
Pouget, 2004). The first and second terms on the right-hand side denote the negative-state entropy and 
Arousal
(Surprise)
Positive
Negative Valence
WundtÂ curve
OptimalÂ arousalÂ level
Novel,Â complexFamiliar,Â simple
Yanagisawa, H. & Honda, S. 
4 
 
internal energy, respectively. Thus, the definition corresponds to the Helmholtz free energy when the 
temperature is one.  
With the definition of conditional probability, the generative model is factorized into true 
posterior and evidence: ğ‘áˆºğ‘ , ğ‘œ|ğœ‹áˆ» àµŒğ‘ áˆºğ‘ |ğ‘œ, ğœ‹áˆ»ğ‘áˆºğ‘œ|ğœ‹áˆ». With this factorization, the free energy is expanded 
to the summation of a Kullbackâ€“Leibler (KL) divergence and Shannon surprise (hereafter referred to as 
surprise). 
 ğ¹à°— àµŒğ·à¯„à¯…áˆ¾ğ‘áˆºğ‘ |ğœ‹áˆ»||ğ‘áˆºğ‘ |ğ‘œ, ğœ‹áˆ»áˆ¿àµ† ln ğ‘áˆºğ‘œ|ğœ‹áˆ» (2) 
The first-term KL divergence forms the true posterior to the recognition density, which represents a 
statistical difference between the two distributions: ğ·à¯„à¯…áˆ¾ğ‘áˆºğ‘ |ğœ‹áˆ»||ğ‘áˆºğ‘ |ğ‘œ, ğœ‹áˆ»áˆ¿ àµŒ âŒ©ln ğ‘áˆºğ‘ |ğœ‹áˆ»àµ†
ln ğ‘áˆºğ‘ |ğ‘œ, ğœ‹áˆ»âŒªà¯¤áˆºğ‘ |ğœ‹áˆ». When the recognition approximates the true posterior to minimize free energy, the KL 
divergence becomes zero, and the free energy is approximated to the second term, i.e., surprise. Thus, the 
lower bound of free energy is surprise. Surprise is a negative log of the model evidence, ğ‘áˆºğ‘œ|ğœ‹áˆ», and 
refers to the information content used to process given observations, representing cognitive load 
(Yanagisawa, 2021).  
The generative model is decomposed to a state prior ğ‘áˆºğ‘ |ğœ‹áˆ» for a given policy and a likelihood 
function ğ‘áˆºğ‘œ|ğ‘ áˆ».  
 ğ‘áˆºğ‘ , ğ‘œ|ğœ‹áˆ» àµŒğ‘ áˆºğ‘ |ğœ‹áˆ»ğ‘áˆºğ‘œ|ğ‘ áˆ» (2) 
With this decomposition, the free energy is expanded to another two terms.  
 ğ¹à°— àµŒğ·à¯„à¯…áˆ¾ğ‘áˆºğ‘ |ğœ‹áˆ»||ğ‘áˆºğ‘ |ğœ‹áˆ»áˆ¿àµ† âŒ©ln ğ‘áˆºğ‘œ|ğ‘ áˆ»âŒªà¯¤áˆºğ‘ |ğœ‹áˆ» (3) 
The first term is a KL divergence of state prior from recognition. This term represents the complexity of 
the generative model. The second term is the difference between likelihood and recognition. This term 
indicates negative model accuracy. Thus, minimizing the free energy signifies minimizing the complexity 
and maximizing the accuracy of the model. 
 
2.2 Information gain in recognition  
Assume that an initial recognition density before an action based on a policy ğœ‹ is approximated to the 
state prior. The initial free energy ğ¹à°—à¬´ is a summation of KL divergence and surprise. 
 ğ¹à°—à¬´ àµŒ âŒ©ln ğ‘áˆºğ‘ |ğœ‹áˆ»àµ†ğ‘áˆºğ‘ , ğ‘œ|ğœ‹áˆ»âŒªà¯£áˆºğ‘ |ğœ‹áˆ» àµŒğ·à¯„à¯…áˆ¾ğ‘áˆºğ‘ |ğœ‹áˆ»||ğ‘áˆºğ‘ |ğ‘œ, ğœ‹áˆ»áˆ¿àµ† ln ğ‘áˆºğ‘œ|ğœ‹áˆ» (4) 
An agent receives an observation ğ‘œ by the action based on the policy ğœ‹. The recognition density 
approximates the true posterior by minimizing the free energy. The KL divergence becomes zero, and the 
free energy decreases to the lower bound ğ¹à°—à¯‹, corresponding to surprise. 
 ğ‘áˆºğ‘ |ğœ‹áˆ»: ğ‘áˆºğ‘ |ğœ‹áˆ» â†’ğ‘ áˆºğ‘ |ğ‘œ, ğœ‹áˆ», ğ¹à°—à¯‹ àµŒàµ† ln ğ‘áˆºğ‘œ|ğœ‹áˆ» (5) 
The decrease in free energy in the recognition process is equivalent to the KL divergence from the true 
posterior to the initial recognition, ğ¾ğ¿ğ·à°—. Herein, ğ¾ğ¿ğ·à°— denotes the information gain from recognizing 
the causal state of observations given by an action based on a policy ğœ‹. 
 ğ›¥ğ¹à¯‹ àµŒğ¹à°—à¬´ àµ†ğ¹à°—à¯‹ àµŒğ¾ ğ¿ ğ·à°— àµŒğ·à¯„à¯…áˆ¾ğ‘áˆºğ‘ |ğœ‹áˆ»||ğ‘áˆºğ‘ |ğ‘œ, ğœ‹áˆ»áˆ¿ (6) 
Yanagisawa, H. & Honda, S. 
5 
 
A greater KLD indicates that the recognition of an observation under a policy provides greater 
information gain. Thus, KLD represents the epistemic value of recognizing an observation under a policy. 
This suggests that an agent prefers to recognize observations with a greater KLD and is motivated to act 
based on a policy that likely obtains such observations. Therefore, we infer that KLD increases positive 
valence by increasing information gain (epistemic value) in recognition. 
 
2.3 Information gain expected from Bayesian updating prior belief: Bayesian surprise 
The free energy minimized by a recognition, ğ¹à°—à¯‹, approximates surprise. The minimized free energy 
equals a summation of complexity and inverse accuracy with a recognition approximated to the true 
posterior, ğ‘áˆºğ‘ |ğœ‹áˆ» àµğ‘ áˆºğ‘ |ğ‘œ, ğœ‹áˆ».  
 ğ¹à°—à¯‹ àµŒàµ† ln ğ‘áˆºğ‘œ|ğœ‹áˆ» àµŒğµ ğ‘†à°— àµ…ğ‘ˆà°— (7) 
 ğµğ‘†à°— àµŒğ·à¯„à¯…áˆ¾ğ‘áˆºğ‘ |ğ‘œ, ğœ‹áˆ»||ğ‘áˆºğ‘ |ğœ‹áˆ»áˆ¿ (8) 
 ğ‘ˆà°— àµŒàµ† âŒ©ln ğ‘áˆºğ‘œ|ğ‘ áˆ»âŒªà¯£áˆºğ‘ |ğ‘œ, ğœ‹áˆ» (9) 
The complexity and inverse accuracy terms represent the Bayesian surprise ğµğ‘†à°— and perceived 
uncertainty U, respectively, and their summation (surprise) denotes the arousal potential (Yanagisawa, 
2021). The Bayesian surprise, ğµğ‘†à°—, is a KL divergence from posterior to prior, i.e., the deviation of 
recognition from prior expectation. It represents the novelty of the recognized observation and is 
correlated with the surprise response to novel stimuli (Yanagisawa, Kawamata, & Ueda, 2019). The 
surprise response decreases with repeated exposure to the same novel stimuli. Such habituation is 
formulated as a decrease in BS in the Bayesian update of the prior (Ueda, Sekoguchi, & Yanagisawa, 
2021).  
By repeatedly observing the same observation ğ‘œ by an action under the same policy, the prior 
is updated by Bayesian updating such that the prior comes close to the true posterior, i.e., 
ğ‘áˆºğ‘ |ğœ‹áˆ»: ğ‘áˆºğ‘ |ğœ‹áˆ» â†’ğ‘ áˆºğ‘ |ğ‘œ, ğœ‹áˆ». When the prior is updated to the posterior, ğµğ‘†à°— is zero, and the free energy 
decreases to the inverse accuracy term. We refer to this term as uncertainty because it refers to the 
perceived uncertainty (Yanagisawa, 2021). Thus, the lower bound of free energy after the prior updating 
is the uncertainty, ğ¹à°—à¯… àµ‘ğ‘ˆà°—, whereas the upper bound of the free energy decrease is the Bayesian 
surprise, ğµğ‘†à°—. 
 ğ›¥ğ¹à¯… àµŒğ¹à°—à¯‹ àµ†ğ¹à°—à¯… àµ‘ğµ ğ‘†à°—  (10) 
Herein, ğµğ‘†à°— is equivalent to the maximum information gain expected from the prior update based on 
observing a sufficient number of the same observations given under the same policy. A greater ğµğ‘†à°— 
denotes a greater information gain expected from the update with the action under the policy ğœ‹. Thus, BS 
represents the expected epistemic value given by the model (prior) update or learning. This suggests that 
an agent prefers novel observations with a greater BS, which is expected to provide a chance to learn new 
information (update its own generative model), and that the agent is motivated to approach such novel 
observations. Therefore, we infer that BS increases emotional valence in anticipation of information gain 
Yanagisawa, H. & Honda, S. 
6 
 
from updating prior beliefs.  
 
2.4 Linking free energy reduction, information gain, and arousal potential 
Fig. 2 summarizes the two-step free energy reduction and information gain. The free energy given an 
observation ğ‘œ decreases by ğ¾ğ¿ğ· as the first information gain when one succeeds in recognizing the state 
as a cause of the observation. The minimized free energy approximates surprise. The surprise is a 
summation of ğµğ‘† and ğ‘ˆ. When oneâ€™s prior is updated to approximate the true posterior, the free energy 
is decreased by ğµğ‘†, which is the expected second information gain.  
 
Fig. 2 Two-step free energy reduction and information gain. Decreases in free energy in recognition and 
belief update correspond to KL divergence (KLD) and Bayesian surprise, respectively.  
 
The upper bound of the total free energy reduction (or information gain) from recognizing and updating 
state beliefs, given an observation, is a summation of the two KL divergences, i.e., information gain.  
 ğ›¥ğ¹
à¯‹ àµ…ğ›¥ğ¹à¯… àµ‘  ğ¾ğ¿ğ·à°— àµ…ğµğ‘†à°— àµŒğ·à¯„à¯…áˆ¾ğ‘áˆºğ‘ |ğœ‹áˆ»||ğ‘áˆºğ‘ |ğ‘œ, ğœ‹áˆ»áˆ¿àµ…ğ·à¯„à¯…áˆ¾ğ‘áˆºğ‘ |ğ‘œ, ğœ‹áˆ»||ğ‘áˆºğ‘ |ğœ‹áˆ»áˆ¿ â‰¡ğ¼ ğº  
  (11) 
We consider that the total information gain represents the epistemic values that explain the emotional 
valence of the arousal potential.  
The two types of KL divergence denote the difference between the prior and posterior. When 
the posterior given an observation is the same as the prior, the KL divergences are zero, and the 
observation provides minimum free energy and minimum surprise (or maximum evidence). Hence, an 
observation that provides minimal free energy does not provide any KL divergence or information gain. 
To provide epistemic value with an emotional valence, given information gain, a certain level of surprise 
representing arousal potential (Yanagisawa, 2021) is required by observing unexpected outcomes that 
give certain KL divergences. However, if the likelihood of an observation is far from the prior 
Recognition
Belief update
Surprise
Uncertainty
Bayesian surprise
KL divergence
time
Free energy
Total information gain
Yanagisawa, H. & Honda, S. 
7 
 
distribution, where the likelihood does not provide any information, the posterior is not updated from the 
prior. In this case, the KL divergences are zero, and the observation does not provide any information. 
Therefore, we consider that an appropriate level of surprise maximizes the KL divergences (information 
gains) and that an appropriate level represents the optimal arousal potential that maximizes the positive 
valence for its epistemic value.  
KL divergence is an asymmetric operation. Hence, although both KL divergences, ğ¾ğ¿ğ·
à°— and 
ğµğ‘†à°—, denote differences between the prior and posterior, they are different from each other. This suggests 
that the two KL divergences as functions of surprise are different. KLD signifies information gain due to 
recognition, whereas BS signifies information gain expected from updating prior beliefs. Namely, KLD is 
the current, whereas BS is the future, information gain. This suggests that maximizing KLD and BS are 
different strategies for approaching the optimal arousal level that maximizes the total epistemic value 
with a positive valence. 
2.5 Analytical methodology  
We modeled the two information gains, KLD and BS, as functions of surprise using a Gaussian-like 
generative model with a flat likelihood of uniform noise and demonstrated that the two functions, KLD 
and BS, form an inverse-U shape and have different peaks. Using the function model, we analyzed the 
effect of Gaussian parameters, the difference between the prior mean and likelihood peak as prediction 
error (Yanagisawa, 2016), variance of prior as prediction uncertainty, and variance of likelihood as 
observation uncertainty on the peaks of the information gain functions. From the analysis, we elucidated 
the conditions for optimal prediction errors and uncertainties of prediction and observation to maximize 
the information gains in an ideal inquiry process.  
 
3. Results 
3.1 Gaussian model of information gains 
The Gaussian Bayesian model has been used in past research studies to analyze the characteristics of free 
energy and Bayesian surprise (Buckley, Kim, McGregor, & Seth, 2017; Yanagisawa et al., 2023; 
Yanagisawa, 2021). The Laplace approximation suggests that a Gaussian distribution is applied around 
the mode of unknown distributions. The Gaussian form is useful for analyzing the effect of interpretable 
and independently manipulatable parameters on free energy and KL divergence. The distance between the 
prior mean and likelihood peak, ğ›¿, represents prediction error; the variance of prior, ğ‘ 
à¯£, represents prior 
uncertainty; and the variance of likelihood, ğ‘ à¯…, represents observation uncertainty. The likelihood 
function of n data randomly sampled from a source following a Gaussian distribution is 
 ğ‘áˆºğ‘œà¯¡|ğ‘ áˆ» àµŒàµ¬
à¬µ
à¶¥à¬¶à°—à¯¦à²½
àµ°
à¯¡
exp á‰‚
à¬¿áˆºà¯¦à¬¿à¯¢à´¤áˆ»à°®à¬¾à¯¡à¯
à¬¶à¯¦à²½
á‰ƒ,  (12) 
where ğ‘œÌ… and ğ‘‰ denote the mean and variance of the observed data, respectively. With a Gaussian prior 
distribution ğ‘áˆºğ‘ áˆ» àµŒğ‘ àµ« ğœ‚, ğ‘ à¯£àµ¯â‰¡ğ‘ à¯£à¯¥à¯œ, the posterior distribution is also of a Gaussian form. 
Yanagisawa, H. & Honda, S. 
8 
 
 ğ‘áˆºğ‘ |ğ‘œà¯¡áˆ» àµŒ
à¯£àµ«ğ‘œà¯¡à¸«ğ‘ àµ¯à¯£áˆºà¯¦áˆ»
à¯£áˆºà¯¢à³™áˆ» àµŒğ‘ àµ« ğœ‚à¯£à¯¢à¯¦à¯§, ğ‘ à¯£à¯¢à¯¦à¯§àµ¯â‰¡ğ‘ à¯£à¯¢à¯¦à¯§,  (13) 
where ğœ‚à¯£à¯¢à¯¦à¯§ àµŒ
à¯¡à¯¦à³›à¯¢à´¤à¬¾à¯¦à²½ à°
à¯¡à¯¦à³›à¬¾à¯¦à²½
 and ğœ‚à¯£à¯¢à¯¦à¯§ àµŒ
à¯¡à¯¦à³›à¯¢à´¤à¬¾à¯¦à²½ à°
à¯¡à¯¦à³›à¬¾à¯¦à²½
. The evidence ğ‘áˆºğ‘œáˆ» is a marginal likelihood: 
 ğ‘áˆºğ‘œà¯¡|ğ‘ áˆ»ğ‘áˆºğ‘ áˆ»ğ‘‘ğ‘ 
à®¶
à¬¿à®¶  àµŒ à¶§
à¯¦à²½
à¯¡à¯¦à³›à¬¾à¯¦à²½
àµ¬
à¬µ
à¶¥à¬¶à°—à¯¦à²½
àµ°
à¯¡
exp àµ¤àµ†
à¯¡
à¬¶àµ«à¯¦à³›à¬¾à¯¦à²½àµ¯ğ›¿à¬¶ àµ†
à¯¡àµ«à¯¦à³›à¬¾à¯¦à²½àµ¯
à¬¶à¯¦à²½àµ«à¯¡à¯¦à³›à¬¾à¯¦à²½àµ¯ğ‘‰àµ¨ â‰¡ğ‘’ áˆºğ›¿áˆ»   
  (14) 
where ğ›¿àµŒ  ğœ‚àµ†ğ‘œ Ì… is a prediction error. The evidence is an inverse exponential function of the square of 
the prediction error. Hence, we describe the Gaussian evidence as ğ‘’áˆºğ›¿áˆ». The evidence exponentially 
decreases as the prediction error increases. Surprise, the lower bound of free energy, is a negative log 
function of evidence, i.e., àµ†log ğ‘áˆºğ‘œà¯¡áˆ» àµŒàµ† log ğ‘’áˆºğ›¿áˆ». Thus, the surprise is a quadratic function of a 
prediction error.  
The free energy of n observations randomly obtained from a stimulus source following a 
Gaussian distribution of variance ğ‘ à¯… is formed as a quadratic function of the prediction error with 
coefficients of variance (for the derivation, see (Yanagisawa, 2021)): 
 ğ¹àµŒğ´ à®¿ğ›¿à¬¶ àµ…ğµà®¿, (15) 
where the coefficients are functions of uncertainties, i.e., ğ´à®¿ àµŒ
à¬µ
à¬¶
à¯¡
à¯¡à¯¦à³›à¬¾à¯¦à³—
 and ğµà®¿ àµŒ
à¬µ
à¬¶àµ›lnàµ«ğ‘›ğ‘ à¯£ àµ…ğ‘ à¯Ÿàµ¯ àµ…
áˆºğ‘›àµ† 1áˆ»ln ğ‘ à¯Ÿ àµ…ğ‘› ln 2ğœ‹àµ…ğ‘› ğ‘‰/ğ‘ à¯ŸàµŸ. To simplify further analysis, we consider the case of a single data 
observation, n=1. When n=1, the coefficients AF and BF are simplified as ğ´à®¿ àµŒ
à¬µ
à¬¶
à¬µ
à¯¦à³›à¬¾à¯¦à³—
 and ğµà®¿ àµŒ
à¬µ
à¬¶àµ›lnàµ«ğ‘ à¯£ àµ…ğ‘ à¯Ÿàµ¯ àµ…ln 2ğœ‹àµŸ. 
The gradient ğ´à®¿ is the inverse of the sum of two variances. Thus, both uncertainties increase 
the sensitivity of the free energy to prediction error. 
Using the same Gaussian model with a single data observation, we derive the information 
gains, KLD and BS, as quadratic functions of the prediction error with the coefficients of variance: 
 ğ¾ğ¿ğ· àµŒ ğ·à¯„à¯…áˆºğ‘áˆºğ‘ áˆ»||ğ‘áˆºğ‘ |ğ‘œáˆ»áˆ» àµŒğ´ à¯„à¯…à®½ğ›¿à¬¶ àµ…ğµà¯„à¯…à®½, (16) 
where the coefficients are ğ´à¯„à¯…à®½ àµŒ
à¯¦à³›
à¬¶à¯¦à²½àµ«à¯¦à³›à¬¾à¯¦à²½àµ¯ and ğµà¯„à¯…à®½ àµŒàµ† ln
à¯¦à³›à¬¾à¯¦à³—
à¯¦à²½
àµ…
à¯¦à³›
à¯¦à²½
 ; and  
 ğµğ‘† àµŒ ğ·à¯„à¯…áˆºğ‘áˆºğ‘ |ğ‘œáˆ»||ğ‘áˆºğ‘ áˆ»áˆ» àµŒğ´ à®»à¯Œğ›¿à¬¶ àµ…ğµà®»à¯Œ, (17) 
where the coefficients are ğ´à®»à¯Œ àµŒ
à¯¦à³›
à¬¶àµ«à¯¦à³›à¬¾à¯¦à²½àµ¯à°® and ğµà®»à¯Œ àµŒ ln
à¯¦à³›à¬¾à¯¦à³—
à¯¦à²½
àµ†
à¯¦à³›
à¯¦à³›à¬¾à¯¦à³—
. 
The prediction error always increases both KLD and BS. We found that the observation variation ğ‘ à¯… 
always increases the gradient of both KLD and BS because the partial derivatives of the gradients are 
always negative, i.e., 
à°¡à®ºà²¼à²½à²µ
à°¡à¯¦à²½
àµ 0 and 
à°¡à®ºà²³à³„
à°¡à¯¦à²½
àµ 0. This signifies that the lower the observation uncertainty 
Yanagisawa, H. & Honda, S. 
9 
 
(i.e., the more precise the observation), the more susceptible the information gains (KLD and BS) are to 
prediction errors.  
However, the effects of prediction uncertainty on the sensitivity of prediction errors are 
inversed between KLD and BS. We found that the prediction uncertainty increases sensitivity for KLD 
but decreases it for BS because the partial derivatives are 
à°¡à®ºà²¼à²½à²µ
à°¡à¯¦à³›
àµ 0 and 
à°¡à®ºà²³à³„
à°¡à¯¦à³›
àµ 0. Thus, the lower the 
prediction uncertainty, the more susceptible the information gains in recognition, i.e., KLD, and the more 
susceptible the information gains expected from Bayesian updating prior beliefs, i.e., BS.  
To compare KLD and BS in the gradient of functions of prediction error, we derived the 
difference and found that KLD is always greater than BS because the coefficients are always positive.  
 ğ¾ğ¿ğ· àµ†ğµğ‘† àµŒ ğ´à¯„à¯…à®½à¬¿à®»à¯Œğ›¿à¬¶ àµ…ğµà¯„à¯…à®½à¬¿à®»à¯Œ àµ 0, (18) 
where ğ´à¯„à¯…à®½à¬¿à®»à¯Œ àµŒ
à¯¦à³›à°®
à¬¶à¯¦à²½àµ«à¯¦à³›à¬¾à¯¦à²½àµ¯ àµ 0 and ğµà¯„à¯…à®½à¬¿à®»à¯Œ àµŒ
à¯¦à³›à°®
à¯¦à²½àµ«à¯¦à³›à¬¾à¯¦à²½àµ¯ àµ 0. Therefore, for any prediction error, the 
information gain in recognition is greater than that expected from updating the prior.  
 
3.2 Convexity of information gain function by considering uniform noise 
The Gaussian model suggests that prediction error always increases information gain. This is because the 
likelihood function is distributed over an infinite band and deviates from the prior distribution as the 
prediction error increases. However, the Laplace approximation is valid only around the mode, and there 
is no guarantee that the tail of the likelihood is infinitely distributed following a Gaussian distribution. 
The rate-coding hypothesis suggests that the likelihood function is coded by the distribution of the firing 
rates of neurons. A single stimulation fires specific neural populations but not all neurons. Neurons that 
do not receive external stimulation fire spontaneously (Raichle, 2006). The frequency of the spontaneous 
firing activity of neurons is lower than the millisecond-order frequency of stimuli-driven neural activity 
(Destexhe, Rudolph, & ParÃ©, 2003). We infer that such spontaneous firing activity is independent of the 
neural activity evoked by sensory observations and provides no information about the cause of sensory 
stimuli (observation). To represent the influence of such independent and spontaneous neural activity, we 
added an independent uniformed likelihood with very small constant probability ğœ€ to the observation-
based likelihood (Jones, 2016). 
 ğ‘
à°Œáˆºğ‘œ|ğ‘ áˆ» àµŒğ‘ áˆºğ‘œ|ğ‘ áˆ»àµ…ğœ€  (19) 
 
Yanagisawa, H. & Honda, S. 
10 
 
 
Fig. 3 Gaussian Bayesian model with uniform likelihood. ğ‘ à¯£: prior variance, ğ‘ à¯Ÿ: Gaussian likelihood 
variance, ğ›¿: prediction error, and ğœ€: probability of uniform likelihood. 
 
This uniform likelihood addition flattens the tail of the Gaussian likelihood function, as shown in Fig. 3. 
The effect of the Gaussian tail becomes negligible as the prediction error increases. Therefore, we infer 
that adding a uniform likelihood is the simplest modeling method to represent the likelihood of 
spontaneous neural activity and to ignore the effect of the Gaussian likelihood tail. 
The evidence with the likelihood function is the Gaussian evidence and the constant 
probability.  
 ğ‘
ğ‘à°Œáˆºğ‘œ|ğ‘ áˆ»ğ‘áˆºğ‘ áˆ»ğ‘‘ğ‘ 
à®¶
à¬¿à®¶ àµŒğ‘’ áˆºğ›¿áˆ»àµ…ğœ€ (20) 
Note that surprise increases monotonically with respect to the prediction error. We find that the posterior 
distributions with the likelihood function form a weighted linear model of the Gaussian posterior and 
prior.  
 ğ‘
à°Œáˆºğ‘ |ğ‘œáˆ» àµŒ
à¯£áˆºà¯¦áˆ»à¯£à´„áˆºğ‘œ|ğ‘ áˆ»
à¯£à´„áˆºà¯¢áˆ» àµŒ
à¯˜áˆºà°‹áˆ»à¯‡à³›à³šà³à³Ÿà¬¾à°Œà¯‡à³›à³à³”
à¯˜áˆºà°‹áˆ»à¬¾à°Œ àµŒğ‘¤à¯£à¯¢à¯¦à¯§ğ‘à¯£à¯¢à¯¦à¯§ àµ…ğ‘¤à¯£à¯¥à¯œğ‘à¯£à¯¥à¯œ, (21) 
where ğ‘¤à¯£à¯¢à¯¦à¯§ àµŒ
à¯˜áˆºà°‹áˆ»
à¯˜áˆºà°‹áˆ»à¬¾à°Œ and ğ‘¤à¯£à¯¥à¯œ àµŒ
à°Œ
à¯˜áˆºà°‹áˆ»à¬¾à°Œ are the standardized linear weight s. When the prediction error is 
small, the term ğœ€ğ‘à¯£à¯¥à¯œ is negligible because ğœ€ is very small compared to ğ‘’áˆºğ›¿áˆ». In this case, the posterior 
is approximated to the Gaussian posterior, ğ‘à°Œáˆºğ‘ |ğ‘œáˆ» àµğ‘à¯£à¯¢à¯¦à¯§. Thus, the prediction error increases both 
information gains, KLD and BS. By contrast, when the prediction error increases toward infinity, the 
evidence converges to zero, lim
à°‹â†’à®¶ 
ğ‘’áˆºğ›¿áˆ» àµŒ 0, where ğ‘‰àµŒ 0, because the evidence is the inverse exponential 
function of the prediction error. In this case, the Gaussian posterior is negligible, and thus, the posterior is 
approximated to the Gaussian prior, lim
à°‹â†’à®¶ 
ğ‘à°Œáˆºğ‘ |ğ‘œáˆ» àµŒğ‘à¯£à¯¥à¯œ. When the posterior is equal to the prior, both 
information gains, KLD and BS, are zero. Thus, in the case of a large prediction error, where ğ‘’áˆºğ›¿áˆ» is 
very small compared to ğœ€, and ğœ€ğ‘à¯£à¯¥à¯œ is dominant in the posterior, the information gains decrease to zero 
as prediction error increases toward infinity. We use ğœ€àµŒ 10à¬¿à¬· for the following analysis.  
The standardized linear weights ğ‘¤à¯£à¯¢à¯¦à¯§ and ğ‘¤à¯£à¯¥à¯œ represent the dominances of the Gaussian 
Prior likelihoodPosterior
ğ‘ 
ğ‘ à¯£
ğ‘ à¯Ÿ
ğœ€
Prediction error ğ›¿
ğ‘áˆºğ‘ áˆ» ğ‘áˆºğ‘ áˆ»àµğ‘áˆºğ‘  |ğ‘œáˆ» ğ‘áˆºğ‘œ|ğ‘ áˆ»
Yanagisawa, H. & Honda, S. 
11 
 
posterior and prior, respectively, in the mixed posterior distribution. Fig. 4 shows the dominances as 
functions of prediction error ğ›¿. When the prediction error is zero or small, the Gaussian posterior is 
dominant. For a certain prediction error, the prior becomes dominant as the prediction error increases. 
Fig. 5 shows an example of posterior distributions switching over from Gaussian posterior dominance to 
prior dominance. In the switching over area of prediction errors, the Gaussian posterior and prior are 
mixed with certain weights, ğ‘¤
à¯£à¯¢à¯¦à¯§ and ğ‘¤à¯£à¯¥à¯œ.  
Using the posterior function, we derived KLD and BS:  
 ğ¾ğ¿ğ· àµŒ ğ·à¯„à¯…áˆ¾ğ‘áˆºğ‘ áˆ»||ğ‘áˆºğ‘ |ğ‘œáˆ»áˆ¿ àµŒğ¾ ğ¿ ğ·à¯‡ àµ…ln á‰€1 àµ…
à°Œ
à¯˜áˆºà°‹áˆ»á‰àµ†ğ¼, (22) 
where ğ¾ğ¿ğ·à¯‡ is a KLD using only the Gaussian likelihood, and ğ¼ is an improper integral: 
 ğ‘à¯£à¯¥à¯œ ln àµ¬1 àµ…
à°Œà¯‡à³›à³à³”
à¯˜áˆºà°‹áˆ»à¯‡à³›à³šà³à³Ÿ
àµ°ğ‘‘ğ‘ 
à®¶
ğ‘à¯‰à¯¥à¯œ
à®¶
à¬¿à®¶ ln á‰‚1 àµ…ğœ€ áˆº2ğœ‹ğ‘ à¯Ÿáˆ»
à³™
à°®exp á‰„
à¯¡áˆºà¯¦à¬¿à¯¢à´¤áˆ»à°®à¬¾à¯¡à¯
à¬¶à¯¦à³—
á‰…á‰ƒğ‘‘ğ‘ .  (23) 
Using the KLD, we derived BS as 
 ğµğ‘† àµŒ ğ·à¯„à¯…áˆ¾ğ‘áˆºğ‘ |ğ‘œáˆ»||ğ‘áˆºğ‘ áˆ»áˆ¿ àµŒ
à¬µ
à¯˜áˆºà°‹áˆ»à¬¾à°Œ á‰‚ğ‘’áˆºğ›¿áˆ»á‰„ğµğ‘†à¯‡ àµ†ln á‰€1 àµ…
à°Œ
à¯˜áˆºà°‹áˆ»á‰àµ…ğ½á‰…àµ†ğœ€ğ¾ğ¿ğ·á‰ƒ , (24) 
where ğµğ‘†à¯‡ is the ğµğ‘† of using only the Gaussian likelihood, and ğ½ is an improper integral: 
 ğ‘à¯£à¯¢à¯¦à¯§ ln àµ¬1 àµ…
à°Œà¯‡à³›à³à³”
à¯˜áˆºà°‹áˆ»à¯‡à³›à³šà³à³Ÿ
àµ°ğ‘‘ğ‘ 
à®¶
ğ‘à¯£à¯¢à¯¦à¯§ ln á‰‚1 àµ…ğœ€áˆº2ğœ‹ğ‘ à¯Ÿáˆ»
à³™
à°® exp á‰„
à¯¡áˆºà¯¦à¬¿à¯¢à´¤áˆ»à°®à¬¾à¯¡à¯
à¬¶à¯¦à³—
á‰…á‰ƒ
à®¶
à¬¿à®¶ ğ‘‘ğ‘ .  (25) 
Because improper integrals ğ¼ and ğ½ could not be solved analytically, we used a computational approach 
for further analysis.  
 
Fig. 4 Dominances of Gaussian posterior and prior in posterior distribution as functions of prediction 
error. The dominances swatch over at a certain prediction error level. (Variances: ğ‘ à¯£ àµŒ 10.0, ğ‘ à¯Ÿ=1.0.) 
Dominance in posterior

Yanagisawa, H. & Honda, S. 
12 
 
 
 
Fig. 5 Posterior distributions for different prediction errors (ğ›¿=2.0, 4.0, 5.0, and 6.0). The dominances in 
the posterior distributions switch over from Gaussian posterior to prior. (Variances: ğ‘ à¯£ àµŒ 10.0, ğ‘ à¯Ÿ=1.0.) 
 
Fig. 6 (a) shows the information gains and their total value, ğ¼ğº àµŒ ğ¾ğ¿ğ· àµ…ğµğ‘†, as functions of the 
prediction errors. All information gains are upward-convex functions of the prediction errors. This 
convexity is general because when the prediction error is small, the Gaussian posterior is dominant in the 
posterior, and information gains increase as the prediction error increases; whereas when the prediction 
error is larger than a certain level, the prior becomes dominant, and the information gains decrease to zero 
as the prediction error increases. 
Fig. 7 shows surprise as a function of the prediction error. Surprise increases monotonically 
with respect to the prediction error. Thus, the information gains are also upward-convex functions of 
surprise, and the total information gain ğ¼ğº that induces positive emotions by reducing free energy is an 
upward-convex function of surprise (and prediction error). We infer that the upward-convex function of 
the total information gain represents the arousal potential function (i.e., the Wundt curve). Fig. 6 (b) 
shows an example of information gain as a function of surprise. 
 
-10 -5 0 5 10 15
0
0.1
0.2
0.3
0.4
0.5
0.6Probability
Î´=2.0
Prior
Posterior
Likelihood
-10 -5 0 5 10 15
0
0.1
0.2
0.3
0.4
0.5
0.6Probability
Î´=4.0
Prior
Posterior
Likelihood
-10 -5 0 5 10 15
0
0.1
0.2
0.3
0.4
0.5
0.6Probability
Î´=5.0
Prior
Posterior
Likelihood
-10 -5 0 5 10 15
0
0.1
0.2
0.3
0.4
0.5
0.6Probability
Î´=6.0
Prior
Posterior
Likelihood
Yanagisawa, H. & Honda, S. 
13 
 
   
Fig. 6 Example of information gain functions of (a) prediction error and (b) surprise using Gaussian 
model with uniform noise. KLD and BS represent free energy reduction in recognition and prior updating 
(learning), respectively. Total information gain ğ¼ğº is a summation of KLD and BS. (Uncertainties: ğ‘ à¯£ àµŒ
10.0, ğ‘ à¯Ÿ=1.0.) 
 
 
 
Fig. 7 Surprise as a function of prediction error. (variances: ğ‘ à¯£ àµŒ 10.0, ğ‘ à¯Ÿ=1.0.) 
 
Information gain functions are upward-convex and have a peak. We define the prediction errors that 
maximize information gains ğ¾ğ¿ğ·, ğµğ‘†, and ğ¼ğº as optimal prediction errors ğ›¿à¯„à¯…à®½, ğ›¿à®»à¯Œ and ğ›¿à¯‚à¯€, 
respectively. Similarly, we define the surprises that maximize information gains ğ¾ğ¿ğ·, ğµğ‘†, and ğ¼ğº as 
optimal surprises ğ‘†à¯„à¯…à®½, ğ‘†à®»à¯Œ, and ğ‘†à¯‚à¯€, respectively. We use the term â€œoptimalâ€ because it represents the 
optimal arousal level that maximizes information gain (epistemic value) that evokes emotional valence. 
When the prediction errors are greater than ğ›¿à¯„à¯…à®½ and smaller than ğ›¿à®»à¯Œ, ğ¾ğ¿ğ· and ğµğ‘† have a negative 
relationship, where ğ¾ğ¿ğ· decreases as ğµğ‘† increases, and vice versa. The prediction error that maximizes 
the total information gain ğ›¿à¯‚à¯€ always falls into this area. Alternate maximizations of ğ¾ğ¿ğ· and ğµğ‘† by 
decreasing and increasing the prediction error and surprise in this area iteratively reach the optimal 
Information Gain
Information Gain
02468
Prediction Error 
1
2
3
4
5
6
7Surprise
Yanagisawa, H. & Honda, S. 
14 
 
surprise ğ‘†à¯‚à¯€. This alternation generates fluctuations of surprise. The magnitude of fluctuation is 
determined by the difference between KLD and BS in the optimal prediction error ğ·à°‹ àµŒğ›¿à®»à¯Œ àµ†ğ›¿à¯„à¯…à®½ and 
surprise ğ·à¯Œ àµŒğ‘†à®»à¯Œ àµ†ğ‘†à¯„à¯…à®½. In the next section, we analyze the effects of uncertainties on the optimal 
prediction errors and surprise, together with their differences.  
 
3.3 Effects of uncertainties on information gains 
The optimal prediction error and surprise change depending on uncertainties. We found the optimal 
prediction error and optimal surprise for all combinations of likelihood variances ğ‘ à¯Ÿ [1.0, 50] and prior 
variance ğ‘ à¯£ [1.0, 50] in steps of 0.1 using the MATLAB fminbnd.m function, which is based on golden 
section search and parabolic interpolation. 
Fig. 8 shows the maximum information gain as a function of the two uncertainties, ğ‘ à¯Ÿ and ğ‘ à¯£. 
All maximum information gains decrease as ğ‘ à¯Ÿ increases, and increase as ğ‘ à¯£ decreases. While ğ‘ à¯£ 
approaches zero, the sensitivity of ğ‘ à¯Ÿ to the maximum information gains are low. The sensitivity of ğ‘ à¯Ÿ 
increases as ğ‘ à¯£ increases. The peak of the maximum information gain is observed when ğ‘ à¯Ÿ is small, and 
ğ‘ à¯£ is large. The maximum information gains of a large ğ‘ à¯Ÿ and large ğ‘ à¯£ are greater than those of a small 
ğ‘ à¯Ÿ and small ğ‘ à¯£. Fig. 9 shows examples of the maximum information gain as a function of ğ‘ à¯Ÿ and ğ‘ à¯£. The 
maximum information gains increase exponentially as ğ‘ à¯Ÿ decreases. Thus, the sensitivity of ğ‘ à¯Ÿ to the 
maximum information gain increases as ğ‘ à¯Ÿ decreases. By contrast, the sensitivity of ğ‘ à¯£ to information 
gain is significant when ğ‘ à¯£ is small (e.g., from 1.0 to 10.0 in this example).  
 
 
Fig. 8 Maximum information gains as function of uncertainties ğ‘ à¯Ÿ and ğ‘ à¯£. (a) Max KLD, (b) Max BS, 
and (c) Max ğ¼ğº. 

Yanagisawa, H. & Honda, S. 
15 
 
 
Fig. 9 Maximum information gains as functions of (a) likelihood variance when ğ‘ à¯£ àµŒ 10 and (b) prior 
variance when ğ‘ à¯Ÿ àµŒ 1.0.  
 
Fig. 10 shows the optimal prediction errors, ğ›¿à¯„à¯…à®½, ğ›¿à®»à¯Œ and ğ›¿à¯‚à¯€, as functions of likelihood variance ğ‘ à¯Ÿ 
and prediction variance ğ‘ à¯£. These two variances increase the optimal prediction errors. The sensitivity of 
ğ‘ à¯Ÿ is greater than that of ğ‘ à¯£ in KLD. Fig. 11 shows an example of the optimal prediction error as a 
function of each uncertainty. All functions are monotonically increasing convex. ğ›¿à¯„à¯…à®½ is more sensitive 
to ğ‘ à¯Ÿ than ğ›¿à®»à¯Œ. Thus, the difference ğ›¿à¯„à¯…à®½ and ğ›¿à®»à¯Œ decreases as ğ‘ à¯Ÿ increases. By contrast, ğ›¿à¯„à¯…à®½ is less 
sensitive to ğ‘ à¯£ than ğ›¿à®»à¯Œ. Thus, the difference increases as ğ‘ à¯£ increases. 
 
 
Fig. 10 Optimal prediction errors as functions of observation and prediction uncertainties for (a) ğ¾ğ¿ğ·, (b) 
ğµğ‘†, and (c) ğ¼ğº.  
Maximum Information Gain
Maximum Information Gain

Yanagisawa, H. & Honda, S. 
16 
 
 
Fig. 11 Optimal prediction errors as functions of uncertainties, (a) likelihood variance when ğ‘ à¯£=10.0 and 
(b) prediction variance when ğ‘ à¯Ÿ=1.0.  
 
 
 
Fig. 12 Optimal surprises as functions of observation and prediction uncertainties for (a) KLD, (b) BS, 
and (c) ğ¼ğº. 
 
Fig. 12 shows the optimal surprises ğ‘†à¯„à¯…à®½, ğ‘†à®»à¯Œ, and ğ‘†à¯‚à¯€ as functions of the two uncertainties. ğ‘ à¯Ÿ 
monotonically increases all optimal surprises. However, the effects of ğ‘ à¯£ are different. ğ‘ à¯£ decreases 
ğ‘†à¯„à¯…à®½ and increases ğ‘†à®»à¯Œ. Fig. 13 shows examples of optimal surprises as functions of each uncertainty. 
ğ‘†à¯„à¯…à®½ is more sensitive to ğ‘ à¯Ÿ than ğ‘†à®»à¯Œ, and thus, ğ‘†à®»à¯Œ approaches ğ‘†à¯„à¯…à®½ as ğ‘ à¯Ÿ increases. Consequently, the 
difference between ğ‘†à¯„à¯…à®½ and ğ‘†à®»à¯Œ decreases as ğ‘ à¯Ÿ increases. By contrast, ğ‘ à¯£ decreases ğ‘†à¯„à¯…à®½ and 
increases ğ‘†à®»à¯Œ. Thus, the difference between ğ‘†à¯„à¯…à®½ and ğ‘†à®»à¯Œ increases as ğ‘ à¯£ increases.  
 
 
Optimal Prediction Error
Optimal Prediction Error

Yanagisawa, H. & Honda, S. 
17 
 
 
Fig. 13 Optimal surprises as functions of (a) likelihood variance when ğ‘ à¯£ àµŒ 10.0 and (b) prediction 
variance when ğ‘ à¯Ÿ àµŒ 1.0. 
 
Fig. 14 shows the differences in the optimal prediction error and optimal surprise. Both differences are 
always positive, and thus, ğ›¿à®»à¯Œ àµğ›¿à¯„à¯…à®½ and ğ‘†à®»à¯Œ àµğ‘†à¯„à¯…à®½. Both differences increase as ğ‘ à¯Ÿ decreases and ğ‘ à¯£ 
increases. Thus, the larger the ğ‘ à¯£, and the smaller the ğ‘ à¯Ÿ, the larger the differences in both the optimal 
prediction errors and surprises. ğ‘ à¯Ÿ has the greatest sensitivity to increase the difference when ğ‘ à¯£ is large.  
For the optimal prediction errors, ğ‘ à¯£ has the greatest sensitivity to increase the difference when 
ğ‘ à¯Ÿ is small. The difference in the optimal prediction error is larger when both ğ‘ à¯Ÿ and ğ‘ à¯£ are large than 
when both ğ‘ à¯Ÿ and ğ‘ à¯£ are small. By contrast, the difference in the optimal surprise is larger when both ğ‘ à¯Ÿ 
and ğ‘ à¯£ are small than when both ğ‘ à¯Ÿ and ğ‘ à¯£ are large. 
 
 
Fig. 14 Difference in optimal prediction error ğ·à°‹ and difference in optimal surprise ğ·à¯Œ. 
 
4. Discussions 
4.1 Arousal potential functions and curiosities 
The results of the analysis using a Gaussian generative model with an additional uniform likelihood 
suggest that the two information gains, KLD and BS, form upward-convex functions of surprise and 
Optimal Surprise
Optimal Surprise

Yanagisawa, H. & Honda, S. 
18 
 
prediction errors (i.e., the distance between the prior mean and likelihood peak). The prediction error 
monotonically increases surprise, as shown in Fig. 7. Fig. 15 shows a schematic of the information gain 
functions that conceptualize the analytical results, as shown in Fig. 6 and the related emotions. Surprise, 
àµ†ln ğ‘áˆºğ‘œáˆ», corresponds to free energy minimized in recognition. A previous study argued that surprise 
represents arousal potential because minimized free energy consists of the summation of information 
content provided by novelty and perceived complexity, which are collative variables as dominant factors 
of arousal potential (Yanagisawa, 2021).  
Berlyne suggested that an appropriate level of arousal potential induces a positive hedonic 
response, termed the optimal arousal level (Berlyne, 1960). Extreme arousal level caused by novel and 
complex stimuli may cause confusion. By contrast, a low arousal level with familiar and simple stimuli 
results in boredom. Thus, emotional valence shapes the upward-convex function of the arousal potential, 
termed the Wundt curve.  
Berlyne also suggested that two epistemic curiosities, diversive and specific, drive the 
approach to the optimal arousal level (Berlyne, 1966). Diversive curiosity drives the pursuit of novelty, 
whereas specific curiosity drives the search for evidence of oneâ€™s model predictions. Consequently, 
diversive curiosity increases the arousal potential to climb the Wundt curve on the left, from a low level 
of arousal (boredom). By contrast, specific curiosity motivates a decrease in the arousal potential to climb 
the Wundt curve on the right side from a high arousal level (confusion). The alternation between the two 
curiosity-driven activities approaches the optimal arousal level.  
ğ¾ğ¿ğ· is a free energy reduction in recognition of a state ğ‘  given an observation ğ‘œ that 
increases model evidence, ğ‘áˆºğ‘œáˆ» àµŒ âŒ©ğ‘áˆºğ‘œ|ğ‘ áˆ»âŒª
à¯¤áˆºà¯¦áˆ», where recognition ğ‘áˆºğ‘ áˆ» is updated from a prior ğ‘áˆºğ‘ áˆ» to 
true posterior ğ‘áˆºğ‘ |ğ‘œáˆ». ğµğ‘† is the expected information gain given by novel stimuli that corresponds to 
human surprise response to novelty (Itti & Baldi, 2009; Sekoguchi, Sakai, & Yanagisawa, 2019; Ueda et 
al., 2021; Yanagisawa et al., 2019). Therefore, we consider that specific curiosity drives an increase in 
KLD, whereas diversive curiosity drives an increase in BS.  
 
Yanagisawa, H. & Honda, S. 
19 
 
 
Fig. 15 Schematic of arousal potential functions and related emotions. Valence of epistemic emotions 
represented by information gains forms upward-function of arousal potential represented by free energy 
or surprise. Diversive and specific curiosity drive to maximize KLD and BS, respectively. These alternate 
maximizations achieve optimal arousal level with fluctuation of surprise. Emotions such as boredom, 
pleasure, interest, and confusion are induced by free energy and its fluctuations (see main text for detailed 
discussion) 
 
4.2 Inquiry process and epistemic emotions 
The analytical result shown in Fig. 14 demonstrate that the optimal surprise and optimal prediction error 
of BS is always greater than that of KLD, i.e., ğ‘†à®»à¯Œ àµğ‘†à¯„à¯…à®½ and ğ›¿à®»à¯Œ àµğ›¿ à¯„à¯…à®½, respectively. This result 
suggests that maximizing information gain through novelty seeking (driven by diversive curiosity) 
requires a greater prediction error, causing greater surprise than that from maximizing information gain 
through evidence seeking (driven by specific curiosity).  
When surprise is less than ğ‘†à¯„à¯…à®½, both KLD and BS monotonically increase as surprise 
increases. By contrast, when surprise is greater than ğ‘†à®»à¯Œ, both KLD and BS monotonically decrease as 
surprise increases. Thus, the two curiosities increase and decrease prediction errors in the former and 
latter areas of surprise, respectively. However, when surprise is greater than ğ‘†à¯„à¯…à®½ and less than ğ‘†à®»à¯Œ, KLD 
decreases, and BS increases as surprise increases. Thus, in this area of surprise, maximizing both the 
KLD and BS at same time is impossible. We infer that the two types of curiosity alternately maximize 
KLD and BS. This alternating maximization of information gains generates fluctuations of surprise. The 
optimal arousal level, as a maximum summation of KLD and BS, falls into this area. Therefore, the 
optimum arousal level, ğ‘†à¯‚à¯€, involves fluctuations in surprise by alternately seeking novelty and evidence, 
driven by the two types of curiosity.  
BS
KLD
IG
Boredom ConfusionInterestPleasure
Arousal potential: Surprise F
Epistemic valence: Information gain Î”F
Optimal
arousal
Yanagisawa, H. & Honda, S. 
20 
 
We consider that alternating the two kinds of curiosity by increasing and decreasing prediction 
errors represents an ideal inquiry process that achieves optimal arousal. This process provides continuous 
positive emotions through the continuous acquisition of maximum information gain (i.e., epistemic 
value). For example, â€œinterestâ€ is defined as disfluency reduction in fluencyâ€“disfluency theory (Graf & 
Landwehr, 2015). We previously formalized disfluency reduction as free energy reduction in recognition 
(i.e., KLD) from increased free energy (Yanagisawa et al., 2023). This corresponds to an increase in KLD 
from the high-surprise state shown in Fig. 15. Thus, â€œinterestâ€ is achieved by specific curiosity (i.e., 
climbing a hill of KLD from the right side in Fig. 15). By contrast, increasing KLD from the low-surprise 
state (i.e., climbing a hill of KLD from the left side in Fig. 15) may explain â€œpleasureâ€ defined as increase 
in fluency (Graf & Landwehr, 2015). We have previously formalized fluency as KLD in recognition 
(Yanagisawa et al., 2023).  
BS denotes the expected information gain, as discussed in the Methods section. Active 
inference suggests that an agent infers an optimal policy of action that minimizes expected free energy. 
The expected free energy includes the negative expected information gain as an epistemic value. This 
epistemic value drives curious behavior (Friston et al., 2017). Thus, diversive curiosity, formalized as 
maximizing the BS, corresponds to curiosity in active inference. We discuss the mathematical 
interpretations of KLD and BS in terms of the expected free energy in a later section. 
 
4.3 Effect of uncertainties on optimal arousal level and epistemic values 
We analyzed the effects of prediction and observation uncertainties, manipulated using prior and 
likelihood variances, on optimal information gains. Table 1 summarizes the effects of the two 
uncertainties in four quadrants for combinations of small and large uncertainties. A small prediction 
uncertainty ğ‘ à¯£ indicates that the prior belie f is certain because of, for example, prior experience and 
knowledge. However, prior beliefs are not always correct. The prediction error represents the error of 
prior belief from reality. Thus, a case with small ğ‘ à¯£ and large prediction error indicates a preconceived 
notion. By contrast, a large ğ‘ à¯£ denotes that the prior belief is uncertain, owing to, for example, a lack of 
prior knowledge and experience. Thus, observation uncertainty ğ‘ à¯Ÿ indicates precision of observations.  
We evaluate the condition of uncertainties using four indices: maximum information gain 
(max ğ¼ğº), optimal prediction errors (ğ›¿à¯„à¯…à®½, ğ›¿à®»à¯Œ), optimal surprises (ğ‘†à¯„à¯…à®½, ğ‘†à®»à¯Œ), difference in optimal 
prediction errors (ğ·à°‹), and difference in optimal surprises (ğ·à¯¦). As shown in Fig. 8, the condition 
combining a small ğ‘ à¯Ÿ and large ğ‘ à¯£ provides the largest max ğ¼ğº with the largest ğ·à°‹ between small ğ›¿à¯„à¯…à®½ 
and moderate ğ›¿à®»à¯Œ. A larger ğ·à°‹ signifies a wider exploration range through alternations of diversive and 
specific curiosities. Smaller ğ‘†à¯„à¯…à®½ and ğ‘†à®»à¯Œ indicate less surprise as a cognitive load in the inquiry 
process. Therefore, the condition combining a small ğ‘ à¯Ÿ and large ğ‘ à¯£ is the best solution to achieve the 
ideal inquiry process with the largest epistemic value (information gain; max ğ¼ğº) and the largest range of 
exploration (ğ·à°‹) under less cognitive load (ğ‘†à¯„à¯…à®½ and ğ‘†à®»à¯Œ).  
Yanagisawa, H. & Honda, S. 
21 
 
The condition combining a small ğ‘ à¯Ÿ and small ğ‘ à¯£ is expected to yield the second largest 
epistemic value (information gain) under less cognitive load (ğ‘†à¯„à¯…à®½, ğ‘†à®»à¯Œ); however, the range of 
exploration (ğ·à°‹) is small. The condition combining a large ğ‘ à¯Ÿ and large ğ‘ à¯£ is expected to result in a small 
information gain with a moderate range of exploration at the largest prediction error level. The condition 
combining a large ğ‘ à¯Ÿ and small ğ‘ à¯£ is the worst case, corresponding to the smallest information gain and 
the smallest exploration range. 
As overall trends, prediction uncertainty ğ‘ à¯£ increases the range of exploration ( ğ·à°‹). This 
suggests that an extremely certain prior brief, such as a preconceived notion and strong assumption, 
suppresses the range of exploration, whereas an open mind involving a flat prior belief widens the range 
of exploration. The observation uncertainty ğ‘ à¯Ÿ decreases the expected maxi mum information gain (max 
ğ¼ğº). This suggests that precise observation increases expected information gains (epistemic value) with 
positive emotions. ğ‘ à¯Ÿ can be decreased in different ways; for example, by increasing the precision of 
stimuli, paying attention to stimuli, and improving the accuracy of the observation models. 
 
Table 1 Summary of the effects of likelihood variance (observation uncertainty) ğ‘ à¯Ÿ and prior variance 
(prediction uncertainty) ğ‘ à¯£ on maximum information gain, max ğ¼ğº, optimal prediction errors, ğ›¿à¯„à¯…à®½, ğ›¿à®»à¯Œ, 
optimal surprises, ğ‘†à¯„à¯…à®½, ğ‘†à®»à¯Œ, difference in optimal prediction errors, ğ·à°‹, and difference in optimal 
surprises, ğ·à¯¦. ğ‘‹: â‡’ğ‘Œ  signifies that X dominantly affects Y. Solid and broken underlines denote positive 
and negative effects on epistemic emotions, respectively. 
ğ‘ à¯£: â‡’ ğ·à°‹ 
ğ‘ à¯Ÿ: â‡’ max ğ¼ğº, ğ·à¯¦ 
Small ğ‘ à¯£: 
small ğ·à°‹ 
Large ğ‘ à¯£: 
 
Small ğ‘ à¯Ÿ: 
large max ğ¼ğº 
Small ğ›¿à¯„à¯…à®½ 
Small ğ‘†à¯„à¯…à®½, ğ‘†à®»à¯Œ 
large ğ·à¯¦. 
Small ğ‘ à¯Ÿ and small ğ‘ à¯£: 
Large max ğ¼ğº, 
smallest ğ›¿à¯„à¯…à®½, ğ›¿à®»à¯Œ, 
smallest ğ‘†à¯„à¯…à®½, ğ‘†à®»à¯Œ, 
small ğ·à°‹, large ğ·à¯¦. 
Small ğ‘ à¯Ÿ and large ğ‘ à¯£: 
Largest max ğ¼ğº, 
small ğ›¿à¯„à¯…à®½, moderate ğ›¿à®»à¯Œ,  
small ğ‘†à¯„à¯…à®½, ğ‘†à®»à¯Œ, 
largest ğ·à°‹, ğ·à¯¦. 
Large ğ‘ à¯Ÿ: 
small max ğ¼ğº 
 
large ğ‘†à¯„à¯…à®½ 
 
Large ğ‘ à¯Ÿ and small ğ‘ à¯£: 
smallest max ğ¼ğº, 
moderate ğ›¿à¯„à¯…à®½, ğ›¿à®»à¯Œ,  
largest ğ‘†à¯„à¯…à®½, moderate ğ‘†à®»à¯Œ, 
smallest ğ·à°‹, ğ·à¯¦. 
Large ğ‘ à¯Ÿ and large ğ‘ à¯£: 
small max ğ¼ğº, 
largest ğ›¿à¯„à¯…à®½, ğ›¿à®»à¯Œ,  
large ğ‘†à¯„à¯…à®½, largest ğ‘†à®»à¯Œ, 
moderate ğ·à°‹, ğ·à¯¦. 
 
4.4 Expected free energy and information gains 
An active inference framework suggests that an agentâ€™s action policy is selected to minimize expected 
free energy (Friston et al., 2017; Parr et al., 2022; Smith, Friston, & Whyte, 2022). Here, we discuss the 
relationship between the expected free energy and the two types of information gains, KLD and BS, as 
Yanagisawa, H. & Honda, S. 
22 
 
drivers of specific and diversive curiosity, respectively.  
Before giving observations by action, an agent calculates expected free energy under a policy ğœ‹.  
 ğºà°— àµŒ âŒ©ln ğ‘áˆºğ‘ |ğœ‹áˆ»àµ†ğ‘áˆºğ‘ , ğ‘œ|ğœ‹áˆ»âŒªà¯¤áˆºğ‘ , ğ‘œ|ğœ‹áˆ», (26) 
where ğ‘áˆºğ‘ , ğ‘œ|ğœ‹áˆ» àµŒğ‘ áˆºğ‘ |ğœ‹áˆ»ğ‘áˆºğ‘œ|ğ‘ áˆ». This definition implies that the expected free energy is the free energy 
averaged by likelihood ğ‘áˆºğ‘œ|ğ‘ áˆ» of observations expected by  future action under a policy ğœ‹. The expected 
free energy forms the prior distribution of the policies. A policy is randomly selected based on the policy 
prior ğ‘áˆºğœ‹áˆ» àµŒğœ áˆºàµ†ğ›¾ğºà°—áˆ» such that the expected fr ee energy is minimized, where ğœáˆºğ¸áˆ» is a softmax 
function that transforms from energy ğ¸ to probability, and ğ›¾ is the precision of policy representing the 
confidence of policy selection.  
The expected free energy is expanded in two terms using the decomposition of a generative 
model ğ‘áˆºğ‘ , ğ‘œ|ğœ‹áˆ» àµŒğ‘ áˆºğ‘ |ğœ‹áˆ»ğ‘áˆºğ‘œ|ğ‘ áˆ».  
 ğºà°— àµŒ âŒ©ln ğ‘áˆºğ‘ |ğœ‹áˆ»àµ†ğ‘áˆºğ‘ |ğœ‹áˆ»âŒªà¯¤áˆºğ‘ , ğ‘œ|ğœ‹áˆ» àµ†âŒ©ln ğ‘áˆºğ‘œ|ğ‘ áˆ»âŒªà¯¤áˆºğ‘ , ğ‘œ|ğœ‹áˆ»  
 àµŒğ·à¯„à¯…áˆ¾ğ‘áˆºğ‘ |ğœ‹áˆ»||ğ‘áˆºğ‘ |ğœ‹áˆ»áˆ¿àµ†ğ”¼ à¯¤áˆºğ‘œ|ğœ‹áˆ»áˆ¾âŒ©ln ğ‘áˆºğ‘œ|ğ‘ áˆ»âŒªà¯¤áˆºğ‘ |ğ‘œ, ğœ‹áˆ»áˆ¿  
 àµŒğ·à¯„à¯…áˆ¾ğ‘áˆºğ‘ |ğœ‹áˆ»||ğ‘áˆºğ‘ |ğ¶áˆ»áˆ¿àµ…ğ”¼à¯¤áˆºğ‘œ|ğœ‹áˆ»áˆ¾ğ‘ˆà°—áˆ¿ (27) 
The first term is a KL divergence from a state prior to a recognition density under a policy, ğ‘áˆºğ‘ |ğœ‹áˆ». We 
assume that the state prior is given from the agentâ€™s preference ğ¶, ğ‘áˆºğ‘ |ğœ‹áˆ» àµŒğ‘ áˆºğ‘ |ğ¶áˆ». A preference refers 
to a desired state expected to be achieved through actions based on policy selection. The KL divergence, 
termed risk in state, refers to the difference between the preferred state and the state expected by acting 
with a policy. A lower KL divergence indicates that the desired state is more likely to be achieved. The 
second term is expected uncertainty, which represents uncertainty averaged over expected observations. 
This term is called ambiguity because it is equivalent to the entropy of likelihood, 
àµ†âŒ©ln ğ‘áˆºğ‘œ|ğ‘ áˆ»âŒª
à¯¤áˆºğ‘ , ğ‘œ|ğœ‹áˆ» àµŒ âŒ©àµ†ln ğ‘áˆºğ‘œ|ğ‘ áˆ»âŒªà¯£áˆºğ‘œ|ğ‘ áˆ»à¯¤áˆºğ‘ |ğœ‹áˆ» àµŒ âŒ©ğ»áˆºğ‘œ|ğ‘ áˆ»âŒªà¯¤áˆºğ‘ |ğœ‹áˆ».  
The expected uncertainty is decomposed into two terms using a conditional probability definition: 
ğ‘áˆºğ‘œ|ğ‘ áˆ» àµŒ
à¯¤áˆºğ‘œ|ğœ‹áˆ»à¯¤áˆºğ‘ |ğ‘œ, ğœ‹áˆ»
à¯¤áˆºğ‘ |ğœ‹áˆ» . 
 ğ”¼à¯¤áˆºğ‘œ|ğœ‹áˆ»áˆ¾ğ‘ˆà°—áˆ¿ àµŒàµ† ğ”¼à¯¤áˆºğ‘œ|ğœ‹áˆ»àµ£âŒ©ln ğ‘áˆºğ‘œ|ğ‘ áˆ»âŒªà¯¤áˆºğ‘ |ğ‘œ, ğœ‹áˆ»àµ§  
 àµŒàµ† ğ”¼à¯¤áˆºğ‘œ|ğœ‹áˆ»àµ£âŒ©ln ğ‘áˆºğ‘ |ğ‘œ, ğœ‹áˆ»àµ†ln ğ‘áˆºğ‘ |ğœ‹áˆ»àµ…ln ğ‘áˆºğ‘œ|ğœ‹áˆ»âŒªà¯¤áˆºğ‘ |ğ‘œ, ğœ‹áˆ»àµ§  
 àµŒàµ† ğ”¼à¯¤áˆºğ‘œ|ğœ‹áˆ»áˆ¾ğ·à¯„à¯…áˆ¾ğ‘áˆºğ‘ |ğ‘œ, ğœ‹áˆ»||ğ‘áˆºğ‘ |ğœ‹áˆ»áˆ¿áˆ¿àµ†ğ”¼ à¯¤áˆºğ‘œ|ğœ‹áˆ»áˆ¾ln ğ‘áˆºğ‘œ|ğœ‹áˆ»áˆ¿ (28) 
The first term of the expected uncertainty is a negative KL divergence from approximate posterior to prior 
averaged by expected observations with a policy ğœ‹. This KL divergence corresponds to the Bayesian 
surprise, ğµğ‘†à°—. Thus, this term signifies the expected information gain by prior updating using predicted 
observations under policy ğœ‹. Note that the observation is not yet given, and ğµğ‘†à°— is averaged based on 
the predicted distribution under a policy, ğ‘áˆºğ‘œ|ğœ‹áˆ».  
The second term is entropy under a policy. By definition, surprise is the sum of the negative 
KLD and free energy. 
 àµ†ln ğ‘áˆºğ‘œ|ğœ‹áˆ» àµŒàµ† ğ·à¯„à¯…áˆ¾ğ‘áˆºğ‘ |ğœ‹áˆ»||ğ‘áˆºğ‘ |ğ‘œ, ğœ‹áˆ»áˆ¿àµ…âŒ©ln ğ‘áˆºğ‘ |ğœ‹áˆ»àµ†ln ğ‘áˆºğ‘ , ğ‘œ|ğœ‹áˆ»âŒªà¯¤áˆºğ‘ |ğœ‹áˆ» (29) 
Yanagisawa, H. & Honda, S. 
23 
 
Thus, the entropy is a summation of the negative predictive KLD and the predicted free energy. 
 àµ†ğ”¼à¯¤áˆºğ‘œ|ğœ‹áˆ»áˆ¾ln ğ‘áˆºğ‘œ|ğœ‹áˆ»áˆ¿ àµŒàµ† ğ”¼à¯¤áˆºğ‘œ|ğœ‹áˆ»áˆ¾ğ·à¯„à¯…àµ£ğ‘áˆºğ‘ |ğœ‹áˆ»à¸«|ğ‘áˆºğ‘ |ğ‘œ, ğœ‹áˆ»áˆ¿àµ§àµ…  ğ”¼à¯¤áˆºğ‘œ|ğœ‹áˆ»áˆ¾ğ¹à°—áˆ¿  (30) 
In summary, the expected free energy under a policy is the sum of the risk, predicted free energy, and 
negative predicted information gains.  
 ğºà°— àµŒğ‘… ğ‘– ğ‘  ğ‘˜àµ…ğ‘ ğ¹à°— àµ†áˆºğ‘ğ¾ğ¿ğ·à°— àµ…ğ‘ğµğ‘†à°—áˆ» (31) 
where 
risk in state: ğ‘…ğ‘–ğ‘ ğ‘˜ àµŒ ğ·à¯„à¯…áˆ¾ğ‘áˆºğ‘ |ğœ‹áˆ»||ğ‘áˆºğ‘ |ğ¶áˆ»áˆ¿,  (32) 
predicted free energy: ğ‘ğ¹à°— àµŒğ”¼ à¯¤áˆºğ‘œ|ğœ‹áˆ»àµ£âŒ©ln ğ‘áˆºğ‘ |ğœ‹áˆ»àµ†ln ğ‘áˆºğ‘ , ğ‘œ|ğœ‹áˆ»âŒªà¯¤áˆºğ‘ |ğœ‹áˆ»àµ§, (33) 
predicted KLD: ğ‘ğµğ‘†à°— àµŒğ”¼ à¯¤áˆºğ‘œ|ğœ‹áˆ»áˆ¾ğ·à¯„à¯…áˆ¾ğ‘áˆºğ‘ |ğ‘œ, ğœ‹áˆ»||ğ‘áˆºğ‘ |ğœ‹áˆ»áˆ¿áˆ¿,  (34) 
and predicted Bayesian surprise: ğ‘ğ¾ğ¿ğ·à°— àµŒğ”¼ à¯¤áˆºğ‘œ|ğœ‹áˆ»áˆ¾ğ·à¯„à¯…áˆ¾ğ‘áˆºğ‘ |ğœ‹áˆ»||ğ‘áˆºğ‘ |ğ‘œ, ğœ‹áˆ»áˆ¿áˆ¿.  (35) 
When preference is expected to be fully satisfied by a policy, the predicted state equals the state 
prior, ğ‘áˆºğ‘ |ğœ‹áˆ» àµğ‘ áˆºğ‘ |ğ¶áˆ». In this case, the risk term becomes zero. However, the expected free energy still 
remains. The remaining expected free energy is the predicted free energy minus the two predicted 
information gains.  
 ğºà°— àµğ‘ ğ¹à°— àµ†áˆºğ‘ğ¾ğ¿ğ·à°— àµ…ğ‘ğµğ‘†à°—áˆ» (36) 
The remaining expected free energy is minimized by maximizing the predicted information gains, 
ğ‘ğ¾ğ¿ğ·à°— àµ…ğ‘ğµğ‘†à°—, and minimizing the predicted free energy. Therefore, the two types of expected 
information gains, ğ‘ğ¾ğ¿ğ·à°—, ğ‘ğµğ‘†à°—, drive the agentâ€™s action based on the active inference framework. This 
corresponds to the expected drives of the two types of curiosity.  
 
4.5 Limitations and further discussions 
The analytical results are based on a Gaussian generative model. A Gaussian model was used to 
independently manipulate the prediction errors and uncertainties and analyze their effects on information 
gains. Although Laplace approximation and the principle of maximum entropy reasonably support the 
Gaussian assumption, true distributions can be more complex than Gaussian distributions. For specific 
applications with complex distributions, further analysis based on the method proposed in this study 
paper for specific applications with complex distributions.  
This study focusses on emotions induced by epistemic values (epistemic emotions) such as curiosity and 
interest. However, emotions are affected by individual preference and appraisal of the situation against 
objectives(Ellsworth & Scherer, 2003). We may expand the emotion model to include such preference-
based emotions by introducing the pragmatic value formalized as risk term in expected free energy(Parr et 
al., 2022). The model does not consider individual capacity to process information. Surprise (free energy) 
exceeding the capacity may affect negative emotions.  
This study was limited to analyzing two types of information gain linked to epistemic emotions as 
functions of surprise in a context-independent manner. Epistemic emotions based on epistemic values, 
such as curiosity, can be observed based on the agentâ€™s behavior. Active inference, where an action policy 
Yanagisawa, H. & Honda, S. 
24 
 
is inferred to minimize the expected free energy, can be used to simulate agent behavior based on 
epistemic emotions in a specific context (Friston et al., 2017). As discussed, the expected free energy 
includes two types of information gain. In future studies, it will be necessary to accumulate evidence of 
the model predictions based on correspondence between agent simulations and actual human behavior in 
a variety of specific contexts.  
 
5. Conclusion 
This study mathematically formulated arousal potential functions of epistemic emotions, such as curiosity 
and interest, that drive inquiry processes, based on information gains. Decrements in free energy in 
Bayesian recognition and prior belief update correspond to two types of information gain, i.e., KLD and 
BS, respectively. Free energy reduction induces positive emotions by reducing surprise caused by 
prediction errors and uncertainty, which provide information gains (i.e., epistemic value). We 
demonstrated that the two types of information gain form upward-convex curve functions of surprise 
using a Gaussian generative model with a uniform noise likelihood, and defined epistemic emotions as 
information gains (or decrements of free energy). An analysis using the model exhaustively revealed the 
effects of prediction and observation uncertainties on the peak of information gain functions as the 
optimal arousal level. Specifically, the analytical results suggest that the greater the prediction uncertainty 
and the lower the observation uncertainty, the greater the information gained through a larger exploration 
range.  
These results provide general and fundamental knowledge to increase the valence of epistemic 
emotions that facilitate the inquiry process because the model is deduced from the synthesis of free 
energy minimization as the first principle of the brain and the well-established arousal potential theory. 
Therefore, this model framework is applicable to diverse areas that deal with epistemic emotions and 
motivations, such as education, creativity, aesthetics, affective computing, and related cognitive sciences. 
Further studies are needed to accumulate empirical evidence for the principle-based model and 
understand the relationship between the inquiry process and emotions in diverse complex situations. 
 
 
Acknowledgements 
This research was supported by Japan Society for the Promotion of Science (KAKENHI Grant Number 
21H03528, Mathematical model development of emotion dimensions based on variation of uncertainty and 
its application to inverse problems). 
 
 
 
Yanagisawa, H. & Honda, S. 
25 
 
References 
Berlyne, D. E. (1960), Conflict, arousal, and curiosity, 350. New York: McGraw-Hill Book Company. 
Berlyne, D. E. (1966). Curiosity and exploration. Science, 153(3731), 25â€“33. 
doi:10.1126/science.153.3731.25. 
Buckley, C. L., Kim, C. S., McGregor, S., & Seth, A. K. (2017). The free energy principle for action and 
perception: A mathematical review. Journal of Mathematical Psychology, 81, 55â€“79. 
doi:10.1016/j.jmp.2017.09.004. 
Clark, J. E., Watson, S., & Friston, K. J. (2018). What is mood? A computational perspective. 
Psychological Medicine, 48(14), 2277â€“2284. doi:10.1017/S0033291718000430. 
Destexhe, A., Rudolph, M., & ParÃ©, D. (2003). The high-conductance state of neocortical neurons in vivo. 
Nature Reviews. Neuroscience, 4(9), 739â€“751. doi:10.1038/nrn1198. 
Ellsworth, P. C., & Scherer, K. R. (2003). Appraisal processes in emotion. Handbook of affective sciences, 
1199, 572â€“595. 
Friston, K. (2010). The free-energy principle: A unified brain theory? Nature Reviews. Neuroscience, 
11(2), 127â€“138. doi:10.1038/nrn2787. 
Friston, K. J., Lin, M., Frith, C. D., Pezzulo, G., Hobson, J. A., & Ondobaka, S. (2017). Active inference, 
curiosity and insight. Neural Computation, 29(10), 2633â€“2683. doi:10.1162/neco_a_00999. 
Friston, K., Kilner, J., & Harrison, L. (2006). A free energy principle for the brain. Journal of Physiology, 
Paris, 100(1â€“3), 70â€“87. doi:10.1016/j.jphysparis.2006.10.001. 
Graf, L. K. M., & Landwehr, J. R. (2015). A dual-process perspective on fluency-based aesthetics: The 
pleasure-interest model of aesthetic liking. Personality and Social Psychology Review, 19(4), 
395â€“410. doi:10.1177/1088868315574978. 
Hesp, C., Smith, R., Parr, T., Allen, M., Friston, K. J., & Ramstead, M. J. D. (2021). Deeply felt affect: 
The emergence of valence in deep active inference. Neural Computation, 33(2), 398â€“446. 
doi:10.1162/neco_a_01341. 
Honda, S., Yanagisawa, H., & Kato, T. (2022). Aesthetic shape generation system based on novelty and 
complexity. Journal of Engineering Design, 33(12), 1016â€“1035. 
doi:10.1080/09544828.2022.2155343. 
Itti, L., & Baldi, P. (2009). Bayesian surprise attracts human attention. Vision Research, 49(10), 1295â€“
1306. doi:10.1016/j.visres.2008.09.007. 
Joffily, M., & Coricelli, G. (2013). Emotional valence and the free-energy principle. PLOS Computational 
Biology, 9(6), e1003094. doi:10.1371/journal.pcbi.1003094. 
Jones, P. R. (2016). A tutorial on cue combination and Signal Detection Theory: Using changes in 
sensitivity to evaluate how observers integrate sensory information. Journal of Mathematical 
Psychology, 73, 117â€“139. doi:10.1016/j.jmp.2016.04.006. 
Kashdan, T. B., & Silvia, P. J. (2009). Curiosity and interest: The benefits of thriving on novelty and 
Yanagisawa, H. & Honda, S. 
26 
 
challenge. Oxford handbook of positive psychology, 2, 367â€“374. 
Knill, D. C., & Pouget, A. (2004). The Bayesian brain: The role of uncertainty in neural coding and 
computation. Trends in Neurosciences, 27(12), 712â€“719. doi:10.1016/j.tins.2004.10.007. 
Lang, P. J. (1995). The emotion probe. Studies of motivation and attention. American Psychologist, 50(5), 
372â€“385. doi:10.1037//0003-066x.50.5.372. 
Parr, T., Pezzulo, G., & Friston, K. J. (2022), Active inference: The free energy principle in mind, brain, 
and behavior. Cambridge, MA: MIT Press. 
Peirce, C. S. (1974), Collected papers of Charles Sanders Peirce. Cambridge, MA: Harvard University 
Press. 
Raichle, M. E. (2006). Neuroscience. The brainâ€™s dark energy. Science, 314(5803), 1249â€“1250. 
doi:10.1126/science. 1134405. 
Russell, J. A. (1980). A circumplex model of affect. Journal of Personality and Social Psychology, 39(6), 
1161â€“1178. doi:10.1037/h0077714. 
Russell, J. A. (2003). Core affect and the psychological construction of emotion. Psychological Review, 
110(1), 145â€“172. doi:10.1037/0033-295x.110.1.145. 
Sasaki, H., Kato, T., & Yanagisawa, H. (2023). Quantification of â€œnoveltyâ€ based on free-energy principle 
and its application for â€œaesthetic likingâ€ for industrial products. Research in Engineering Design. 
doi:10.1007/s00163-023-00422-6. 
Sekoguchi, T., Sakai, Y ., & Yanagisawa, H. (2019). Mathematical model of emotional habituation to 
novelty: Modeling with Bayesian update and information theory IEEE International Conference 
on Systems, Man and Cybernetics (SMC), 2019 (pp. 1115â€“1120). 
doi:10.1109/SMC.2019.8914626. 
Seth, A. K., & Friston, K. J. (2016). Active interoceptive inference and the emotional brain. Philosophical 
Transactions of the Royal Society of London. Series B, Biological Sciences, 371(1708), 
20160007. doi:10.1098/rstb.2016.0007. 
Silvia, P. J. (2012). Curiosity and motivation. The Oxford handbook of human motivation, 157â€“166. 
Smith, R., Friston, K. J., & Whyte, C. J. (2022). A step-by-step tutorial on active inference and its 
application to empirical data. Journal of Mathematical Psychology, 107. 
doi:10.1016/j.jmp.2021.102632. 
Ueda, K., Sekoguchi, T., & Yanagisawa, H. (2021). How predictability affects habituation to novelty. 
PLOS ONE, 16(6), e0237278. doi:10.1371/journal.pone.0237278. 
V ogl, E., Pekrun, R., Murayama, K., & Loderer, K. (2020). Surprisedâ€“curiousâ€“confused: Epistemic 
emotions and knowledge exploration. Emotion, 20(4), 625â€“641. doi:10.1037/emo0000578. 
Wager, T. D., Kang, J., Johnson, T. D., Nichols, T. E., Satpute, A. B., & Barrett, L. F. (2015). A Bayesian 
model of category-specific emotional brain responses. PLOS Computational Biology, 11(4), 
e1004066. doi:10.1371/journal.pcbi.1004066. 
Yanagisawa, H. & Honda, S. 
27 
 
Wilson-Mendenhall, C. D., Barrett, L. F., & Barsalou, L. W. (2013). Neural evidence that human 
emotions share core affective properties. Psychological Science, 24(6), 947â€“956. 
doi:10.1177/0956797612464242. 
Yanagisawa, H. (2016). A computational model of perceptual expectation effect based on neural coding 
principles. Journal of Sensory Studies, 31(5), 430â€“439. doi:10.1111/joss.12233. 
Yanagisawa, H. (2021). Free-energy model of emotion potential: Modeling arousal potential as 
information content induced by complexity and novelty. Frontiers in Computational 
Neuroscience, 15, 698252. doi:10.3389/fncom.2021.698252. 
Yanagisawa, H., Kawamata, O., & Ueda, K. (2019). Modeling emotions associated with novelty at 
variable uncertainty levels: A Bayesian approach. Frontiers in Computational Neuroscience, 
13(2), 2. doi:10.3389/fncom.2019.00002. 
Yanagisawa, H., Wu, X., Ueda, K., & Kato, T. (2023). Free energy model of emotional valence in dual-
process perceptions. Neural Networks: The Official Journal of the International Neural Network 
Society, 157, 422â€“436. doi:10.1016/j.neunet.2022.10.027. 
 