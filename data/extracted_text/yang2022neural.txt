A NEURAL ACTIVE INFERENCE MODEL OF
PERCEPTUAL -MOTOR LEARNING
Zhizhuo Yang
Computer Science Department
Rochester Institute of Technology
Rochester, NY 14623
zy8981@rit.edu
Gabriel J. Diaz
Chester F. Carlson Center for Imaging Science
Rochester Institute of Technology
Rochester, NY 14623
gabriel.diaz@rit.edu
Brett R. Fajen
Cognitive Science Department
Rensselaer Polytechnic Institute
Troy, NY , USA
fajenb@rpi.edu
Reynold Bailey
Computer Science Department
Rochester Institute of Technology
Rochester, NY 14623
rjb@cs.rit.edu
Alexander Ororbia
Computer Science Department
Rochester Institute of Technology
Rochester, NY 14623
ago@cs.rit.edu
ABSTRACT
The active inference framework (AIF) is a promising new computational framework grounded in
contemporary neuroscience that can produce human-like behavior through reward-based learning.
In this study, we test the ability for the AIF to capture the role of anticipation in the visual
guidance of action in humans through the systematic investigation of a visual-motor task that
has been well-explored – that of intercepting a target moving over a ground plane. Previous
research demonstrated that humans performing this task resorted to anticipatory changes in speed
intended to compensate for semi-predictable changes in target speed later in the approach. To
capture this behavior, our proposed “neural” AIF agent uses artiﬁcial neural networks to select
actions on the basis of a very short term prediction of the information about the task environment
that these actions would reveal along with a long-term estimate of the resulting cumulative
expected free energy. Systematic variation revealed that anticipatory behavior emerged only
when required by limitations on the agent’s movement capabilities, and only when the agent
was able to estimate accumulated free energy over sufﬁciently long durations into the future. In
addition, we present a novel formulation of the prior function that maps a multi-dimensional
world-state to a uni-dimensional distribution of free-energy. Together, these results demonstrate
the use of AIF as a plausible model of anticipatory visually guided behavior in humans.
Keywords Active inference ·Interception ·Locomotion ·Anticipation
1 Introduction
The active inference framework (AIF) [23] is an emerging theory of neural encoding and processing that captures
a wide range of cognitive, perceptual, and motor phenomena, while also offering a neurobiologically plausible
means of conducting reward-based learning through the capacity to predict sensory information. The behavior of
an AIF agent involves the selection of action-plans that span into the near future and centers around the learning of
arXiv:2211.10419v1  [q-bio.NC]  16 Nov 2022
Preprint, Work in Progress
Figure 1: A top-down view of the interception problem. The agent (triangle) and target (circle) approach the
invisible interception point (square) by going straight ahead. Ψ denotes the exocentric direction of the target
(bearing angle) and αdenotes the target’s approach angle. Image adapted from [11].
a probabilistic generative model of the world through interaction with the environment. Ultimately, the agent must
take action such that it is making progress towards its goals (goal-seeking behavior) while also balancing the drive
to explore and understand its environment (information maximizing behavior), adjusting the internal states of its
world to better account for the evidence that it acquires over time. As a result, AIF uniﬁes perception, action, and
learning by framing them as processes that result from approximate Bayesian inference.
The AIF framework has been used to study a variety of reinforcement learning (RL) tasks, including the inverted
pendulum problem (CartPole) [35, 43], the mountain car problem (MountainCar) [23, 47, 45, 5, 43] and the frozen
lake problem (Frozen Lake) [41]. Each task places different demands on motor and cognitive abilities. For instance,
CartPole requires online control of a paddle to balance a pole upright, whereas MountainCar requires intelligent
exploration of the task environment; a simple “greedy” policy (typical of many modern-day RL approaches) would
fail to solve the problem. The popular Frozen Lake requires skills related to spatial navigation and planning if the
agent is to ﬁnd the goal while avoiding unsafe states.
One fundamental aspect of human and animal behavior that has so far not been sufﬁciently studied from an active
inference perspective is the on-line visual guidance of locomotion. On-line visual guidance comprises a class of
ecologically important behaviors for which movements of the body are continuously regulated based on currently
available visual information seen from the ﬁrst-person perspective. Some of the most extensively studied tasks
include steering toward a goal [50], negotiating complex terrain on foot [10, 34], intercepting moving targets [14],
braking to avoid a collision [13, 52], and intercepting a ﬂy ball [6, 12]. For each of these tasks, researchers have
formulated control strategies that capture the coupling of visual information and action.
One aspect of on-line visual guidance that AIF might be particularly well-suited to capture is anticipation. To
successfully perform these kinds of tasks, actors must be able to regulate their actions in anticipation of future
events. One approach to capturing anticipation in visual guidance is to identify sources of visual information that
specify how the actor should move at the current instant in order to reach the goal in the future. For example, when
running to intercept a moving target, the sufﬁciency of the interceptor’s current speed is speciﬁed by the rate of
change in the exocentric visual direction of the target, or bearing angle (Figure 1). If the interceptor is able to move
so as to maintain a constant bearing angle (CBA), then an interception is guaranteed. Such accounts of anticipation
are appealing because they avoid the need for planning on the basis of predictions or extrapolations of the agent’s
or target’s motion, thereby presumably requiring fewer cognitive resources for task execution. Similar accounts of
anticipation in the context of locomotor control have been developed for ﬂy ball catching [6] and braking [33].
However, there are other aspects of anticipatory control that are more difﬁcult to capture based on currently
available information alone. For example, moving targets sometimes change speeds and directions in ways that are
somewhat predictable, allowing actors to alter their movement in advance in anticipation of the most likely change
2
Preprint, Work in Progress
in target motion. This was demonstrated in a previously published study in which subjects were instructed to adjust
their self-motion speed while moving along a linear path in order to intercept a moving target that changed speed
partway through each trial [11]. The ﬁnal target speed randomly varied between trials such that the target usually
accelerated but occasionally decelerated. In response, subjects quickly learned to adjust their speed during the ﬁrst
part of the trial in anticipation of the change in target speed that was most likely given past experience and the
initial conditions of that trial.
Active inference offers a potentially useful framework for understanding and modeling this kind of anticipatory
behavior. The behavior of an AIF agent involves the selection of action plans (or policies) that span into the near
future. These plans are selected based on expected free energy (EFE), i.e., a signal that takes into account both the
action’s contribution to reaching a desired goal state (i.e., aninstrumental component), and the new information
gained by the action (i.e., an epistemic component). This method of action selection is ideal for the study of
predictive and anticipatory behavior in that it allows for the selection of action plans that do not immediately
contribute to task completion, but that reveal to the agent something previously unknown about how the agent’s
action affects the environment. Similarly, in the task presented in [11], the human participants learned that success
required increasing speed early in the trial in order to increase the likelihood of an interception after the target’s
semi-predictable change in speed. Critically, this early change in speed was not motivated by currently available
visual information, but rather by the positive reinforcement of actions selected in the process of task exploration.
In contrast to reinforcement learning methods, active inference (AIF) formulates action-driven learning and
inference from a Bayesian, belief-based perspective [41, 40]. Generally, AIF offers: 1) ﬂexibility to deﬁne a prior
preference (or preferred outcome) over the observation space (which pushes the agent to uncover goal-orienting
policies), which provides an alternative to designing a reward function, 2) a principled treatment for epistemic
exploration as a means of uncertainty reduction, information gain, and intrinsic motivation [38, 42, 40], and 3) an
encompassing uncertainty or precision over the beliefs that the generative model of the AIF agent computes as a
natural part of then agent’s belief updating [38]. Despite being a popular and powerful framework of perception,
action [17, 18, 20, 4], decision-making and planning [29, 39] with biological plausibility, AIF has been mostly
applied to problems with a low-dimensionality and often discrete state space and actions [21, 20, 24, 25, 22]. We
refer to [7] for a comprehensive review on AIF.
The present study makes several contributions to the understanding of visually guided action and active inference:
• We present a novel model for locomotor interception of a target that changes speeds semi-predictably,
as in [11]. This model is a generalization of AIF where EFE is treated as a negative value function in
reinforcement learning (RL) [43] and deep RL methodology is utilized to scale AIF to solve tasks such as
locomotor interception with continuous state spaces. Speciﬁcally, our method predicts action-conditioned
EFE values with a recognition network (see 2.4.2) and by bootstrapping on the continuous observation
space over a long time horizon. This allows the agent to account for the long-term effects of its current
chosen action(s).
• To calculate the instrumental value, we designed a problem-speciﬁc prior function to convert the original
observations into a one-dimensional prior space where a prior preference can be (more easily) speciﬁed.
This allows us to inject domain knowledge into theinstrumental reward. The instrumental measurements in
prior space simultaneously promote interpretability as well as computationally efﬁcient task performance.
• We present a comparison of task performance of a baseline deep-Q network (DQN) agent, or an AIF agent
in which EFE is computed using only the instrumental signal/component, with a full AIF agent in which
EFE is computed using both instrumental and epistemic signals/components.
• We demonstrate behavioral differences among our full AIF agent under the inﬂuence of two varying
parameters: the discount factor γ, which describes the weight on future accumulated quantities when
calculating EFE value at each time step, and pedal lag coefﬁcient K, which speciﬁes how responsive
3
Preprint, Work in Progress
changes in pedal position is reﬂected on agent’s speed (or the amount of inertia that is associated with the
agent’s vehicle).
• We interpret our ﬁndings in as a model for anticipation in the context of visually guided action as well as
in terms of speciﬁc contributions to the active inference and machine learning communities.
2 Materials and Methods
Our aim in this study was to develop an agent that selects from a set of discrete actions in order to perform the
task of interception. In this section, we describe the task that we aim to solve as well as formally describe the AIF
model designed to tackle it. We start with the problem formulation and brief notation and deﬁnitions, then move on
to describe our proposed agent’s inference and learning dynamics.
2.1 The Perceptual-Motor Problem: Intercepting a Moving Target
We designed and simulated a perception-motor problem based on the human interception task used by [11]. In
the original study, subjects sat in front of a large rear-projection screen depicting an open ﬁeld with a heavily
textured ground plane. The subject’s task was to intercept a moving spherical target by controlling the speed of
self-movement along a linear trajectory with a foot pedal, the position of which was mapped onto speed according
to a ﬁrst-order lag. Subjects began each trial from a stationary position at an initial distance sampled uniformly
from between 25 and 30 meters (m) from the interception point. The spherical target approached the subject’s
path at one of three initial speeds (11.25, 9.47, 8.18 meters/second or m/s). Between 2.5 and 3.25 s after the trial
began, the target changed speeds linearly by an amount that was sampled from a normal distribution of possible
ﬁnal speeds. The mean of the distribution was 15 m/s such that target speed usually increased, but occasionally
decreased (standard deviation was 5 m/s, ﬁnal speed is truncated by one standard deviation from the mean). The
change of target speed takes exactly 500 ms.
In [11], subjects were found to increase their speed during the early part of the trial in order to anticipate the most
likely change in target speed, which helped them perform at near optimal levels. Differences between the behavior
of subjects and the ideal pursuer were also found under some conditions. Findings in the original study further
yielded insight into the strategies that humans adopt when dealing with uncertainty in realistic interception tasks.
2.2 Notation
We next deﬁne the notation and mathematical operators that we will use throughout the rest of this paper. ⊙
indicates a Hadamard product, ·indicates a matrix/vector multiplication (or dot product if the two objects it is
applied to are vectors of the same shape), and (v)T denotes the transpose. ||v||p is used to represent the p-norm
where p= 2 results in the 2-norm or Euclidean (L2) distance.
2.3 Action and Input Space Speciﬁcation
To simplify the problem for this work, we assume that the mapping between environmental (latent) states and
observations is the identity matrix. Furthermore, we formulate the problem as a Markov Decision Process
(MDP) with a discrete action space. The action space at (action vector at time t) is deﬁned as a one-hot vector
a ∈{0,1}6×1, where each dimension corresponds to a unique action and the actions are mutually exclusive. Each
dimension corresponds to one of the pedal speeds (m/s) in {2,4,8,10,12,14}respectively. Once a pedal speed is
selected, the agent will change its own speed by the amount of ∆V = K ∗(Vp −Vs) in one time step where Vp
is pedal speed, Vs is current subject speed and K is the lag coefﬁcient. Similar to Tschantz et.al [46], we assume
that the control state vector (which, in AIF, control states are originally treated separately from action states) lines
up one-to-one with the action vector, meaning that it too is a vector of the form u ∈{0,1}6×1. We deﬁne the
4
Preprint, Work in Progress
Figure 2: Our neural AIF architecture for the interception task. The recognition model is a two-headed artiﬁcial
neural network which consists of shared hidden layers, an EFE (estimation) head, and a transition dynamics
(prediction) head. The EFE head estimates EFE values for all possible actions given the current (latent/hidden)
state. An action which is associated with maximum EFE value is selected and executed in the environment and the
resulting observation is fed into the prior function where the instrumental value Rt,i is calculated in prior space.
Meanwhile, the transition dynamics head predicts the resulting observation given the current (latent/hidden) state.
The error between the predicted and actual observation at t+ 1 forms the epistemic value Rt,e. The summation of
Rt,i and Rt,e results in the ﬁnal EFE (target) value.
observation/state space (o ∈R4×1) to be a 4-dimensional vector ot = ⟨xt,vt,xs,vs⟩T, which corresponds to
target distance, target speed, subject distance and subject speed. All distances aforementioned are with respect to
the invisible interception point.
2.4 Neural Active Inference
Active inference (AIF) is a (Bayesian) computational framework that brings together perception and action under
one single imperative: minimizing free energy. It accounts for how self-organizing agents operate in dynamic, non-
stationary environments [19], offering an alternative to standard, reward function-centric reinforcement learning
(RL). In this study, we craft a simple AIF agent that resembles Q-learning [ 43] where the expected free energy
(EFE) serves the role of a negative action-value function in RL. We frame the deﬁnition of EFE in the context of a
stochastic policy and cast action-conditioned EFE as a negative action-value using a policyφ= φ(at|st) (where
st = ot as per our assumption earlier). The same policy φis used for each future time step τ, and the probability
distribution over the ﬁrst-step action is separated from φresulting in a substitution distribution q(at) for φ(at).
Therefore, the one-step substituted EFE can be interpreted as the EFE of a policy of (φ(at),φ(at+1),...,φ (aT))).
As in [43], we simplify and approximate the search for optimal EFE values by adapting an estimation approach
based on the Bellman equation, arriving at a Q-learning bootstrap scheme. Note that the Q-learning style framing of
negative EFE estimation is referred to as G-learning. We utilize the AIF framework within the G-learning framing
for the interception task and modify the framework to ﬁt the interception task, see Figure 2. Spatial variables, i.e.
distance and speed, will serve as the inputs to our framework and, as mentioned before, an identity mapping is
assumed to connect the observation directly to the state variables (allowing us to avoid having to learn additional
5
Preprint, Work in Progress
Figure 3: The prior preference speciﬁed in the prior space where each action corresponds to a differentinstrumental
value. Circles correspond to pedal positions to choose from.
parameterized encoder/decoder functions). As a result, the AIF agent we designed for this paper’s experiments
consists of two major components: a prior function and a multi-headed recognition neural model.
Notably, our particular proposed recognition model works jointly as a function approximator of EFE values as well
as a forward dynamics predictor. It takes in the current observation ot as input and then conducts, jointly, action
selection and next-state prediction (as well as epistemic value estimation). The selected action is executed and the
resulting observation is returned by the environment. The prior function itself takes in as input the next observation
ot+1, the consequence/result of the agent’s currently selected action, and calculates the log likelihood of the
preferred/prior distribution (set according to expert knowledge related to the problem), or theinstrumental term
Rt,i. The difference between the outcome of the selected action ot+1 and its estimation ˆot+1 (as per the generative
transition component of our model) forms the epistemic term Rt,e. The summation of the instrumental and
epistemic terms forms the G-value (or negative EFE value) which is ultimately used to train/adapt the recognition
model. We explain each component in detail below.
2.4.1 The Prior Preference Function and Prior Space
With the ability and freedom of designing a prior preference (or distribution over problem goal states) afforded
by AIF, we integrate domain knowledge of the interception task into the design of a prior function. In essence,
our designed prior function transforms the original observation vector ot to a lower-dimensional space where
a semantically meaningful variable is calculated (the prior space) and prior preference distribution is speciﬁed
over this new variable – in our case, this is set to be the speed difference, as shown in Figure 3. The speed
difference represents the difference between the agent’s speed after taking the selected action and the speed required
for successful interception, i.e. speed difference = speedagent −speedrequired. Given the current observation,
the required speed is calculated as the agent’s distance to the interception point divided by the ﬁrst-order target
time-to-contact (TTC). We refer to this prior function as the ﬁrst-order prior. In our neural AIF framework, the
instrumental values are calculated given all possible actions (blue circles in Figure 3) and a prior distribution over
speed difference. The smaller the speed difference associated with a particular action, the higher the instrumental
value our function assigns.
Note that the agent might not have enough time to adjust its speed later in the interception task if it only follows
the guidance of this prior function without anticipating the likely future speed change of target, since this prior
function only accounts/embodies ﬁrst-order information. To overcome this limitation, we investigated the effects
of discounted long-term EFE value on the behavior of the agent in Section 3.4.
2.4.2 Recognition model
Our proposed recognition model embodies two key functionalities: EFE estimation and transition dynamics
prediction, which are typically implemented as separate artiﬁcial neural networks (ANNs) in earlier AIF studies [43]
6
Preprint, Work in Progress
(in contrast, we found that, during preliminary experimentation, that a joint, fused architecture improved both the
agent’s overall generalization ability as well as its training stability). Concretely, we implement the recognition
model as a multi-headed ANN with an EFE head and a transition head (see Figure 2). The system takes in the
current observation ot and predicts: 1) the EFE values for all possible actions, and 2) a future observation at a
distance ot+D (in this work, we ﬁx the temporal distance to be one step, i.e. D = 1). Within the recognition
model, the current observation ot is taken as input and a latent hidden activity vector zt is produced, which is then
provided to both output heads as input. The transition head p(ot+D|zt) serves as a generative model (or a forward
dynamics model) and the EFE head Gφ(zt,a) represents a variational density over the EFE values. As a result,
EFE module and transition modules are wired together such that the prediction of the future observation ot+D
and the estimation of EFE values Gt+D are driven by the shared encoding from the topmost (hidden) layer of the
recognition model. This enables the sharing of underlying knowledge between the module selecting actions and
the module predicting the outcome(s) of an action. Our intuition is that we humans tend to evaluate the “value” of
an action by the consequences that it produces.
We next formally describe the dynamics of our recognition model, speciﬁcally its inference and learning processes.
Inference: In general, our agent is meant to produce an action conditioned on observations (or states) sampled
from the environment at particular time-steps. Speciﬁcally, within any given T-step episode, our agent receives
as input the observation ot ∈RD×1, where Dis the dimensionality of the observation space ot (D= 4 for the
problem investigated in this study). The agent then produces a set of approximate free energy values, one for each
action (similar in spirit to Q-values) as well as a prediction of the next observation that it is to receive from its
environment (i.e., the perceptual consequence of its selected action).
Formally, in this work, the outputs described above are ultimately produced by a multi-output functionz3
a,z3
o =
fΘ(ot), implemented as a multi-layer perceptron (MLP), where z3
a contains estimated expected free energy values
(one per discrete action) while z3
o is the generative component’s estimation of the next incoming observation
ot+1. Note that we denote only outputting an action value set from this model as z3
a = fa
Θ(ot) (using only the
action output head) and only outputting an observation prediction as z3
o = fo
Θ(ot) (using only the state prediction
head). This MLP is parameterized by a set of synaptic weight matrices Θ = {W1,W2,W3
a,W3
o}, that operates
according to the following:
z1 = φz(W1 ·z0), z2 = φz(W2 ·z1) (1)
z3
a = φa(W3
a ·z2)), z3
o = φa(W3
o ·z2)) (2)
where z0 = ot (the input layer to our model is the observation at t). Note that a single discrete action is
read out/chosen from our agent function’s action output head as: a = arg maxafa
Θ(ot). The linear rectiﬁer
φz(v) = max(0,v) was chosen to be the activation function applied to the internal layers of our model while
φa(v) = v (the identity) is the function speciﬁcally applied to the action neural activity layer z3
a and φo(v) = v is
the function applied to predicted observation layer neurons. Note that the ﬁrst hidden layer z1 ∈RJ1×1 contains
J1 neurons and z2 ∈RJ2×1 contains J2 neurons, respectively. The action output layer z3
a ∈RA×1 contains A
neurons (A= 6 for the problem investigated in this study), one neuron per discrete action (out of Atotal possible
actions as deﬁned by the environment/problem), while the observation prediction layerz3
o ∈RD×1 contains D= 6
neurons, making it the same dimensionality/shape as the observation space.
Learning: While there are many possible ways to adjust the values inside of Θ, we opted to design a cost function
and calculate the gradients of this objective with respect to the synaptic weight matrices of our model for the sake
of simulation speed. The cost function that we designed to train our full agent was multi-objective in nature and is
7
Preprint, Work in Progress
deﬁned in the following manner:
L(ot+1,t; Θ) = La(ot+1; Θ) +Lo(tt+1; Θ) (3)
La(t; Θ) = 1
2σ2a
||t −z3
a||2
2 (4)
Lo(ot+1; Θ) = 1
2σ2o
||ot+1 −z3
o||2
2 (5)
where the target value for the action output head is calculated as tj = rj + γmaxafa
Θ(ot) while the target action
vector is computed as tj = tjaj + (1 −aj) ⊗fa
Θ(ot). In the above set of equations, we see that the MLP model’s
weights are adjusted so as to minimize the linear combination of two terms, the cost associated with the difference
between a target vector t, which contains the bootstrap-estimated of the EFE values, and the agent’s original
estimate z3
a as well as the cost associated with how far off the agent’s prediction/expectationz3
o of its environment
is from the actual observation ot+1. In this study, the standard deviation coefﬁcients associated with both output
layers are set to one, i.e., σa = σo = 1 (highlighting that we assume unit variance for our model’s free energy
estimates and its environmental state predictions – note that a dynamic variance could be modeled by adding an
additional output head responsible for computing the aleatoric uncertainty associated with ot+1).
Updating the parameters Θ of the neural system then consists of computing the gradient ∂L(ot+1,t;Θ)
∂Θ using reverse-
mode differentiation and adjusting their values using a method such as stochastic gradient descent or variants,
e.g., Adam [30], RMSprop [44]. Speciﬁcally, at each time step of any simulated episode, our agent ﬁrst stores
the current transition of the form (ot,at,rt,ot+1) into an episodic memory replay buffer and then immediately
calculates ∂L(ot+1,t;Θ)
∂Θ from a batch of observation/transition data (uniformly) sampled from the replay buffer,
which stores up to 105 transitions. We will demonstrate the beneﬁt of this design empirically in the results section.
3 Results
3.1 Hypotheses for Interception Strategies
Given the fact that the target changes its speed during a trial in our interception task, the agent / human subject
could gain advantage by anticipating the target speed change prior to the change of target speed. To select an
optimal action early within the trail, the agent needs to take into consideration the initial target speed in the current
trial and make adjustments based on the experience acquired from previous trials. So, the question becomes: how
does the agent adapt its behavior on the basis of current trials’ observation of target speed / distance from the
interception point and the learned statistics across trials?
3.2 Experimental Setup
We implemented the interception task as an environment in Python based on the OpenAI gym [3] library. This
integration provides the full functionality and usability of the gym environment, which means that the environment
can work / be used with any RL algorithm and is made accessible to the machine learning community as well. Our
AIF agents and baseline algorithm DQN are implemented with the Tensorﬂow2 [1] library. Experimental data and
code will be made publicly available upon acceptance.
3.3 Task Performance
We compare AIF agents with and without the epistemic component and a baseline algorithm, i.e. a deep-Q network
(DQN). Experiments are conducted for 20 trials where each trial contains 3000 episodes. The task performance of
agents is shown as curves plotting window-averaged rewards (with a window size of 100 episodes) in Figure 4,
where the solid line depicts the mean value across trials and the shaded area represents standard deviation. We
conducted a set of experiments where the discount factor γ of the models and the pedal lag coefﬁcient K were
8
Preprint, Work in Progress
Figure 4: Window-averaged reward measurements of agent performance on the interception task. DQN_Reward
represents a DQN agent that utilized the sparse reward signal and ϵ−greedy exploration; AIF_InstOnly
represents our AIF agent with onlyinstrumental component which is deﬁned by the prior function;AIF_InstEpst
represents an AIF agent that consists of both instrumental and epistemic components. Discount factor is denoted
by γ, pedal lag coefﬁcient is denoted by K.
varied (note that, in AIF and RL research, γis typically ﬁxed to a value between 0.9 and 1 to enable the model to
account for long term returns). In order to compare the performance of our agents to that of human subjects, we
apply the original pedal lag coefﬁcient in one set of our experiments (speciﬁcally shown in Figure 4C).
Observe that our AIF agents are able to reach around a 90% success rate stably with very low variance. This
beats human performance with 47% (std=11.31) on average and 54.9% in the ﬁnal block of experiments reported
in [11]. The baseline DQN agent, which learns from the problem’s sparse reward signal at the end of each episode,
yields an average success rate of 22% at test time. Similarly, the AIF agent with both instrumental and epistemic
components achieves a 90% mean success rate.
Note that the DQN agent is outperformed by the AIF agents trained with our customized prior preference function
by a large margin. This reveals that the ﬂexibility of injecting prior knowledge is crucial for solving complex tasks
more efﬁciently and validates our motivation of applying AIF to cognitive tasks. In our preliminary experiments,
we tested an AIF agent which consists of an EFE network and a transition network separately. This AIF agent
is out-performed by the AIF agent with recognition model in terms of windowed mean rewards and stability.
Furthermore, the AIF agent with recognition model has lower model complexity. Speciﬁcally, AIF agent with
recognition model has only 66.8% of the parameter counts of that of AIF agent with separate models. This supports
our intuition that combining the EFE model with the transition model yields an overall better model agent.
Interestingly, the AIF agent with only instrumental component was able to nearly reach the same level of perfor-
mance as the full AIF agent. However, success rate of this agent exhibited a larger variance than the full AIF agent.
9
Preprint, Work in Progress
Based on comparison between agents with and without epistemic component, we argue that epistemic component
serves, at least in the context of the interception task we investigate, as a regularizer for the AIF models, providing
improved robustness. Since we apply experience replay and bootstrapping to train the AIF models, it is possible
that a local minimum is reached in the optimization process because the replay buffer is ﬁlled up with samples
which come from the same subspace as the state space. Therefore, with the help of epistemic component, the agent
is encouraged to explore the environment more often and adjusts its prediction of future observations such that it
has a higher chance of escaping poorer local optima. Our proposed AIF agent reaches a plateau in performance
after about 1000 episodes and stabilizes more after 1500 episodes. Note that, in contrast, human subjects were able
to perform the task at an average success rate after 9 episodes of initial practice [11].
3.4 Anticipatory Behavior of AIF Agents
Do the AIF agents exhibit a similar capacity for anticipatory behavior as humans do? To answer this question and
to compare the strategy used by our AIF agents to that of human subjects, we record the Time-To-Contact (TTC)
from trained AIF agents at the onset of the target’s speed change in each episode. We then calculate, at the same
time: 1) the target’s TTC using ﬁrst-order information, and 2) target’s TTC with the assumption that the target
would change its speed at the most likely time and reach an averaged ﬁnal speed. Finally, we compose these three
types of TTC data grouped by target initial speed into a single boxplot in Figure 5.
Figure 5: TTC values taken at the onset of target’s speed change. In each subplot, the target’s ﬁrst-order TTC, the
target’s actual mean TTC, and the agent’s TTC are shown in different colors, with data grouped by target initial
speed. The discount factor is denoted by γwhile the pedal lag coefﬁcient is denoted by K.
In our experimental analysis, we found that the discount factor γplays a big role in forming different behavior
patterns within AIF agents. All variants of AIF agents were trained with the instrumental value computed using
our ﬁrst-order prior function. Intuitively, the agent’s behavior should conform to a reactive agent who uses only
10
Preprint, Work in Progress
the ﬁrst-order information and acts to match its own TTC to the target’s ﬁrst-order TTC, just like what has been
observed in Figure 5A (please see that the green box is nearly identical to the blue box under all target initial
conditions). The AIF agent depicted in Figure 5A is set to use a discount factor of 0, which means that the agent
only seeks to maximize its immediate reward without considering the long-term impact of the action(s) that it
selects. Such an agent converges to a reactive behavior. However, when we increase the discount factor to0.99
(which is a common practice in RL literature), the AIF agent starts to behave more interestingly. In Figure 5C, the
agent’s TTC (green box) lies in between target’s ﬁrst-order TTC (blue box) and target’s actual mean TTC (orange
box), which suggests that the AIF agent tends to move faster than a pure-reactive, ﬁrst-order agent would in the
early phase of interception. In other words, the agent tends to anticipate the likely target speed change in the future
and adjusts its action selection policy. This behavioral pattern can be explained as exploiting the beneﬁts provided
by estimating long-term accumulated instrumental reward signal (when the discount factor value is increased).
Given a higher discount factor, in this caseγ = 0.99, the AIF agent estimates the summation of instrumental values
from its current (time) step in the task until the end of the interception using discounting. This leads to an agent
who seeks to maximize long-term beneﬁts in terms of reaching the goal when selecting actions.
3.5 Effect of Vehicle Dynamics on Agent Behavior
To test how anticipatory behavior is affected when simple reactive behavior is no longer sufﬁcient, we increased
the inertia on the agent’s vehicle by changing the pedal lag coefﬁcientK. Given the same discount factor γ = 0.99,
we compare two different pedal lag coefﬁcients K = 1.0 in Figure 5C and K = 0.5 in Figure 5D, where lower
K indicates less responsive vehicle dynamics. With the same discount factor, the AIF agent performing the task
under a lower pedal lag coefﬁcient in Figure 5D has a lower success rate in intercepting the target. This is due to
the fact that the agent’s ability to manipulate its own speed is limited, therefore there is less room left for error.
However, the AIF agent in this condition yields TTC values that are closer to the target’s actual mean TTC. Note
that, when the target initial speed is 11.25 m/s(Figure 5D), the median of agent’s TTC value is actually smaller
than target’s actual mean TTC. This supports our hypothesis that purely reactive behavior is not sufﬁcient for
successful interception and anticipatory behavior is emergent when the vehicle becomes less responsive.
4 Discussion
Variations of an AIF agent were trained to manipulate the speed of movement so as to intercept a target moving
across the ground plane, and eventually across the agent’s linear path of travel. On each trial, the target would
change in speed on most trials to a value that was selected from a Gaussian distribution of ﬁnal speeds. The
results demonstrate that the AIF framework is able to model both on-line visual and anticipatory control strategies
in an interception task, as was previously demonstrated by humans performing the same task [11]. The agent’s
anticipatory behavior aimed to maximize the cumulative expected free energy in the duration that follows action
selection. Variation of the agent’s discount factor modiﬁed the length of this duration. At lower discount factors, the
agent behaved in a reactive manner throughout the approach, consistent with the constant bearing angle strategy of
interception. At higher values, actions that were selected before the predictable change in speed took into account
the most likely change in target speed that would occur later in the trial. Anticipatory behavior was also inﬂuenced
by the agent’s capabilities for action.This anticipatory behavior was most apparent when the pedal lag coefﬁcient
was set to lower values, which had the effect of changing the agent’s movement dynamics so that purely reactive
control was insufﬁcient for interception behavior.
Despite the agent’s demonstration of qualitatively human-like prediction, careful comparison of the agent’s
behavior to the human performance and learning rates demonstrated in [11] reveals notable differences. Analysis
of participant behavior in the fourth and ﬁnal block of Experiment 1 in [11] reveals that subject TTC at the onset of
the target’s change in speed was well matched to the most likely time and magnitude of the target’s likely change
11
Preprint, Work in Progress
Figure 6: Human subject data from Exp 1. of [ 11]. TTCs were taken at the onset of target’s speed change. Dotted
line represents the mean of target’s ﬁrst-order TTC, solid line represents the mean of target’s actual TTC, black
disk represents the mean of subject’s TTC with a bar indicating95% conﬁdence interval of the mean.
in speed (i.e., the mean actual target TTC in Figure 6). In contrast, the AIF agent with an equivalent pedal lag (K=
1.0; i.e. the matched agent) demonstrated only partial matching of its TTC to the likely change in target speed
(the target’s mean actual TTC in Figure 5C). Although one might attribute this to under-training of the agent, it is
notable that the agent achieved a hit rate exceeding 80% by the end of training, while human participants in the
original study consistently improved in performance until reaching 55% hit rate at the end of the experiment.
To better understand the potential causes of these differences between agent and human performance, it is helpful
to consider how the agent’s mechanism for anticipation differs from that of humans. The agent chooses actions on
the basis of a weighted combination of reward-based reinforcement (instrumental reward) and short model-based
prediction (epistemic reward), both of which are computed within the two-headed recognition model. Instrumental
reward is computed in the EFE head, which is responsible for selecting the action (i.e., pedal position) that it
estimates would produce the greatest expected free energy later in the agent’s approach. The estimate of EFE
associated with each pedal position does not involve an explicit process of model-based prediction, but is learned
retrospectively, through the use of an experience replay buffer. Following action selection, visual feedback provides
an indication of the cumulative EFE over the duration of the replay buffer. The values of EFE within this buffer
are weighted by their temporal distance from the selected action in accordance with the parameter of discount
factor. This is similar to both reward-based learning and is often compared to the dopaminergic reward system
in humans [26, 32, 28, 36]. The epistemic component of the EFE reward signal is thought to drive exploration
towards uncertain world states, and it relies on predictions made in the transition head. This component of the
model relies on the hidden states provided by the shared neural layers in the recognition model and predicts an
observation at next time step ˆot+1. The estimated observation at next time step is then compared to the ground truth
observation ot+1 and the difference between them generates the epistemic signal Rt,e. The role of the transition
head is in many ways consistent with a “strong model-based” form of prediction [53], whereby predictive behaviors
are planned on the basis of an internal model of world states and dynamics that facilitate continuous extrapolation.
In summary, whereas the EFE head is consistent with reward based learning, the transition head is consistent with
relatively short-term model based prediction.
How does this account of anticipation demonstrated by our agent compare with what we know about anticipation
in humans? As discussed in the introduction, empirical data on the quality of model-based prediction suggests
that it degrades sufﬁciently quickly that it cannot explain behaviors of the sort demonstrated here, by our agent,
or by the humans in [11]. In contrast, a common theory in motor control and learning relies upon a comparison
12
Preprint, Work in Progress
of a very short-term prediction (e.g. milliseconds) of self-generated action with immediate sensory feedback
[2, 51, 27, 48]. However, this similarity is weakened by the observation that, in the context of motor-learning,
short-term prediction is thought to rely upon access to an efferent copy of the motor signal used to generate the
action. For this reason, it is problematic that the AIF agent is predicting both its own future state ( xs,vs) and
the future state of the target ( xt,vt), for which there is no efferent copy or analogous information concerning
movement dynamics. Although research on eye movements has revealed evidence for the short-term prediction
of future object position and trajectory [ 9, 8, 15], it remains unclear whether these behaviors are the result of
predictive models of object dynamics or representation-minimal heuristics.
Another possible contribution to the observed differences between agent and human performance is the perceptual
input. When considering potential causes for the difference between agent and human anticipatory behavior, it
is notable that the agent relies upon an observation vector deﬁned by agent’s and target’s position and velocity
measured in meters, and meters per second, respectively. However, in the natural context, these spatial variables
must be recovered or estimated on the basis of perceptual sources of information, such as the rate of global optic
ﬂow due to translation over the ground plane, the exocentric direction of the target, the instantaneous angular size
of the target, or the looming rate of the target during the agent’s approach. It is possible that by depriving the agent
of these optical variables, we are also depriving the agent of opportunities to exploit task-relevant relationships
between the agent and environment, such as the bearing angle. It is also notable that some perceptual variables
may provide redundant information about a particular spatial variable (e.g. both change in bearing angle and rate
of change in angular size may be informative about an objects approach speed). However, redundant variables
will differ in reliability by virtue of sensory thresholds and resolutions. For these reasons, a more complete and
comprehensive model of human visually guided action and anticipation would take as input potential sources of
information and learn to weight them according to context-dependent reliability and variability.
Another potential contributor to differences between human and agent performance is the notable lack of visuo-
motor delays within the agent’s architecture. In contrast, human visuo-motor delay has been estimated to be on
the order of 100-200 ms between the arrival of new visual information and the modiﬁcation or execution of an
action [37, 31]. Because uncompensated delays would have devastating consequences on human visual and motor
control, they are often cited as evidence that humans must have some form of predictive mechanism that acts in
compensation [51]. Future attempts to make this model’s anticipatory behavior more human-like in nature may
do so by imposing similar length delay between the agent’s choice of motor plan on the basis of the observed
world-state and the time that this motor plan is executed [49].
4.1 Conclusion
We present a novel generalized active inference framework (AIF) model for studying online visually guided
locomotion using an interception task where a moving target changes its speeds in a semi-predictable manner. In
order to drive the agent towards the goal more effectively, we devised a problem-speciﬁc prior function, improving
the agent’s computational efﬁciency and interpretability. Notably, we found that our proposed AIF agent exhibits
better task performance when compared to a commonly used RL agent, i.e. the deep-Q network (DQN). The full
AIF agent, containing both instrumental and epistemic components, exhibited slightly better task performance and
lower variance compared to the AIF agent with only an instrumental component. Furthermore, we demonstrated
behavioral differences among our full AIF agents given different discount factorγvalues as well as levels of the
agent’s action-to-speed responsiveness. Finally, we analyzed the anticipatory behavior demonstrated by our agent
and examined the differences between the agent’s behavior and human behavior. While our results are promising,
future work should address the following limitations - ﬁrst, inputs to our agent are deﬁned in a simpliﬁed vector
space whereas sensory inputs to the humans that actually perform the interception task are visual in nature (i.e., the
model should work directly with unstructured sensory data such as pixel values). We remark that a vision-based
approach could facilitate the extraction of additional information and features that are useful for solving the
13
Preprint, Work in Progress
interception task more reliably. Second, our simulations do not account for visuo-motor delays inherent to the
human visual and motor systems, and that might be modeled using techniques like delayed Markov decision
process formulations [49, 16].
References
[1] ABADI , M., A GARWAL , A., B ARHAM , P., B REVDO , E., C HEN , Z., C ITRO , C., C ORRADO , G. S., D AVIS,
A., D EAN , J., D EVIN , M., G HEMAWAT, S., G OODFELLOW , I., H ARP, A., I RVING , G., I SARD , M., J IA,
Y., JOZEFOWICZ , R., K AISER , L., K UDLUR , M., L EVENBERG , J., M ANÉ , D., M ONGA , R., M OORE ,
S., M URRAY, D., O LAH , C., S CHUSTER , M., S HLENS , J., S TEINER , B., S UTSKEVER , I., T ALWAR, K.,
TUCKER , P., VANHOUCKE , V., VASUDEVAN , V., V IÉGAS , F., V INYALS , O., W ARDEN , P., WATTENBERG ,
M., W ICKE , M., Y U, Y., AND ZHENG , X. TensorFlow: Large-scale machine learning on heterogeneous
systems, 2015. Software available from tensorﬂow.org.
[2] BLAKEMORE , S.-J., W OLPERT , D. M., AND FRITH , C. D. Central cancellation of self-produced tickle
sensation. Nature neuroscience 1, 7 (1998), 635–640.
[3] BROCKMAN , G., C HEUNG , V., P ETTERSSON , L., S CHNEIDER , J., S CHULMAN , J., T ANG , J., AND
ZAREMBA , W. Openai gym (2016). arXiv preprint arXiv:1606.01540 (2016).
[4] BUCKLEY , C. L., K IM, C. S., M CGREGOR , S., AND SETH , A. K. The free energy principle for action and
perception: A mathematical review. Journal of Mathematical Psychology 81 (2017), 55–79.
[5] ÇATAL, O., W AUTHIER , S., D E BOOM , C., V ERBELEN , T., AND DHOEDT , B. Learning generative state
space models for active inference. Frontiers in Computational Neuroscience 14(2020), 574372.
[6] C HAPMAN , S. Catching a baseball. American Journal of Physics 36, 10 (1968), 868–870.
[7] DA COSTA , L., P ARR , T., S AJID , N., V ESELIC , S., N EACSU , V., AND FRISTON , K. Active inference on
discrete state-spaces: A synthesis. Journal of Mathematical Psychology 99 (2020), 102447.
[8] DIAZ , G., C OOPER , J., AND HAYHOE , M. Memory and prediction in natural gaze control. Philosophical
Transactions of the Royal Society B: Biological Sciences 368, 1628 (Oct. 2013), 20130064.
[9] DIAZ , G., C OOPER , J., R OTHKOPF , C., AND HAYHOE , M. Saccades to future ball location reveal memory-
based prediction in a virtual-reality interception task. Journal of vision 13, 1 (2013), 20–20.
[10] D IAZ , G. J., P ARADE , M. S., B ARTON , S. L., AND FAJEN , B. R. The pickup of visual information about
size and location during approach to an obstacle. PLOS ONE 13, 2 (Feb. 2018), e0192044.
[11] DIAZ , G. J., P HILLIPS , F., AND FAJEN , B. R. Intercepting moving targets: a little foresight helps a lot.
Experimental brain research 195, 3 (2009), 345–360.
[12] FAJEN , B., D IAZ , G., AND CRAMER , C. Reconsidering the role of action in perceiving the catchability of
ﬂy balls. Journal of Vision 8, 6 (2008), 621–621.
[13] FAJEN , B. R., AND DEVANEY, M. C. Learning to control collisions: The role of perceptual attunement and
action boundaries. Journal of Experimental Psychology: Human Perception and Performance 32, 2 (2006),
300–313.
[14] FAJEN , B. R., AND WARREN , W. H. Behavioral dynamics of intercepting a moving target. Experimental
Brain Research 180, 2 (June 2007), 303–319.
[15] FERRERA , V. P., AND BARBORICA , A. Internally Generated Error Signals in Monkey Frontal Eye Field
during an Inferred Motion Task. Journal of Neuroscience 30, 35 (Sept. 2010), 11612–11623.
[16] FIROIU , V., JU, T., AND TENENBAUM , J. At human speed: Deep reinforcement learning with action delay.
arXiv preprint arXiv:1810.07286 (2018).
14
Preprint, Work in Progress
[17] FRISTON , K. The free-energy principle: a rough guide to the brain? Trends in cognitive sciences 13, 7
(2009), 293–301.
[18] FRISTON , K. The free-energy principle: a uniﬁed brain theory? Nature reviews neuroscience 11, 2 (2010),
127–138.
[19] F RISTON , K. A free energy principle for a particular physics. arXiv preprint arXiv:1906.10184 (2019).
[20] FRISTON , K., F ITZ GERALD , T., R IGOLI , F., S CHWARTENBECK , P., AND PEZZULO , G. Active inference:
a process theory. Neural computation 29, 1 (2017), 1–49.
[21] FRISTON , K., R IGOLI , F., O GNIBENE , D., M ATHYS , C., F ITZGERALD , T., AND PEZZULO , G. Active
inference and epistemic value. Cognitive neuroscience 6, 4 (2015), 187–214.
[22] FRISTON , K., S AMOTHRAKIS , S., AND MONTAGUE , R. Active inference and agency: optimal control
without cost functions. Biological cybernetics 106, 8 (2012), 523–541.
[23] FRISTON , K. J., D AUNIZEAU , J., AND KIEBEL , S. J. Reinforcement learning or active inference? PloS one
4, 7 (2009), e6421.
[24] FRISTON , K. J., L IN, M., F RITH , C. D., P EZZULO , G., H OBSON , J. A., AND ONDOBAKA , S. Active
Inference, Curiosity and Insight. Neural Computation 29, 10 (10 2017), 2633–2683.
[25] FRISTON , K. J., R OSCH , R., P ARR , T., P RICE , C., AND BOWMAN , H. Deep temporal models and active
inference. Neuroscience & Biobehavioral Reviews 90 (2018), 486–501.
[26] HARUNO , M. A Neural Correlate of Reward-Based Behavioral Learning in Caudate Nucleus: A Functional
Magnetic Resonance Imaging Study of a Stochastic Decision Task. Journal of Neuroscience 24, 7 (Feb.
2004), 1660–1665.
[27] HOIST , E. V., M ITTELSTAEDT , H., AND MARTIN , R. Das reafferenzprìnzip. wechselwirkung zwischen
zentralnervensystem und peripherie. Die Naturwissenschaften 37 (1950), 464.
[28] HOLROYD , C. B., AND COLES , M. G. H. The neural basis of human error processing: Reinforcement
learning, dopamine, and the error-related negativity. Psychological Review 109, 4 (Oct. 2002), 679–709.
[29] KAPLAN , R., AND FRISTON , K. J. Planning and navigation as active inference. Biological cybernetics 112,
4 (2018), 323–343.
[30] KINGMA , D., AND BA, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980
(2014).
[31] LE RUNIGO , C., B ENGUIGUI , N., AND BARDY, B. G. Visuo-motor delay, information–movement coupling,
and expertise in ball sports. Journal of Sports Sciences 28, 3 (Feb. 2010), 327–337.
[32] LEE, D., S EO, H., AND JUNG , M. W. Neural Basis of Reinforcement Learning and Decision Making.
Annual Review of Neuroscience 35, 1 (July 2012), 287–308.
[33] LEE, D. N. A Theory of Visual Control of Braking Based on Information about Time-to-Collision.Perception
5, 4 (Dec. 1976), 437–459.
[34] MATTHIS , J. S., AND FAJEN , B. R. Humans exploit the biomechanics of bipedal gait during visually guided
walking over complex terrain. Proceedings of the Royal Society B: Biological Sciences 280, 1762 (July 2013),
20130700.
[35] MILLIDGE , B. Deep active inference as variational policy gradients. Journal of Mathematical Psychology 96
(2020), 102348.
[36] MOMENNEJAD , I., R USSEK , E. M., C HEONG , J. H., B OTVINICK , M. M., D AW, N. D., AND GERSHMAN ,
S. J. The successor representation in human reinforcement learning. Nature Human Behaviour 1, 9 (Sept.
2017), 680–692.
15
Preprint, Work in Progress
[37] NIJHAWAN , R. Visual prediction: Psychophysics and neurophysiology of compensation for time delays.
Behavioral and Brain Sciences 31, 2 (Apr. 2008), 179–198.
[38] P ARR , T., AND FRISTON , K. J. Uncertainty, epistemics and active inference. Journal of The Royal Society
Interface 14, 136 (2017), 20170376.
[39] PARR , T., AND FRISTON , K. J. The anatomy of inference: generative models and brain structure. Frontiers
in computational neuroscience 12 (2018), 90.
[40] PARR , T., AND FRISTON , K. J. Generalised free energy and active inference. Biological cybernetics 113, 5
(2019), 495–513.
[41] SAJID , N., B ALL , P. J., P ARR , T., AND FRISTON , K. J. Active inference: demystiﬁed and compared.
Neural computation 33, 3 (2021), 674–712.
[42] SCHWARTENBECK , P., PASSECKER , J., H AUSER , T. U., F ITZ GERALD , T. H., K RONBICHLER , M., AND
FRISTON , K. J. Computational mechanisms of curiosity and goal-directed exploration. Elife 8 (2019),
e41703.
[43] SHIN , J., K IM, C., AND HWANG , H. J. Prior preference learning from experts: Designing a reward with
active inference. arXiv preprint arXiv:2101.08937 (2021).
[44] TIELEMAN , T., H INTON , G., ET AL . Lecture 6.5-rmsprop: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural networks for machine learning 4, 2 (2012), 26–31.
[45] TSCHANTZ , A., B ALTIERI , M., S ETH , A. K., AND BUCKLEY , C. L. Scaling active inference. In 2020
international joint conference on neural networks (ijcnn) (2020), IEEE, pp. 1–8.
[46] TSCHANTZ , A., S ETH , A. K., AND BUCKLEY , C. L. Learning action-oriented models through active
inference. PLoS computational biology 16, 4 (2020), e1007805.
[47] U ELTZHÖFFER , K. Deep active inference. Biological cybernetics 112, 6 (2018), 547–573.
[48] W ADE , N. J. Hermann von helmholtz (1821–1894), 1994.
[49] WALSH , T. J., N OURI , A., L I, L., AND LITTMAN , M. L. Learning and planning in environments with
delayed feedback. Autonomous Agents and Multi-Agent Systems 18, 1 (2009), 83–105.
[50] WARREN , W., FAJEN , B., AND BELCHER , D. Behavioral dynamics of steering, obstacle avoidance, and
route selection. Journal of Vision 1, 3 (Mar. 2010), 184–184.
[51] WOLPERT , D. M., G HAHRAMANI , Z., AND JORDAN , M. I. An internal model for sensorimotor integration.
Science 269, 5232 (1995), 1880–1882.
[52] YILMAZ , E. H., AND WARREN , W. H. Visual control of braking: A test of the ˙τ hypothesis. Journal of
Experimental Psychology: Human Perception and Performance 21, 5 (1995), 996.
[53] ZHAO , H., AND WARREN , W. H. On-line and model-based approaches to the visual control of action. Vision
Research 110 (2015), 190–202. On-line Visual Control of Action.
16