A Model of Agential Learning Using
Active Inference
Riddhi J. Pitliya1,2 and Robin A. Murphy2
1 VERSES Research Lab, Los Angeles, California, 90016, USA
2 Department of Experimental Psychology, University of Oxford, Oxford, UK
Abstract. Agential learning refers to the process of forming beliefs re-
garding one’s degree of control over actions and outcomes in their en-
vironment. We first provide an overview and evaluation of associative,
statistical,andBayesianmodelsofagentiallearning.Wethenarguethat
theexistingmodelshavelimitationsinexplainingtheprocessofagential
learning. Finally, we introduce an active inference account of agential
learning, and present results from simulations. We propose that the ac-
tive inference framework may provide a comprehensive model of agen-
tial learning describing three fundamental processes: (i) perception, (ii)
learning, and (iii) action.
Keywords: Agency · Agential Learning · Active Inference · Computa-
tional Psychology
1 Introduction
Anagentissomeone orsomething thatactstocontroltheiractionsandeventsin
the environment. Agency, then, refers to having control over one’s own actions,
andleveragingthatsensetocontrolthemselvesoreventsintheenvironment[1],
[2]. Agential learning is the process of tracking and forming relevant beliefs [3]
regarding one’s degree of agency. Having an ongoing registration of the degree
of control agents (self and others) have over the states in their environment
facilitates individual- and group-level goal-directed behaviours [4].
Rather than a binary concept, degree of agency refers to the amount of con-
trol the agent has to generate or prevent the occurrence of the event. When
based purely on objective experience, agency can be formalised as a statisti-
cal relationship, or contingency, between the action produced by an agent and
its consequence/outcome (discrete variables), each with a dichotomous state of
being present or absent. Contingencies, and correlations, vary on a scale from
-1 to +1: a positive contingency is when an action predicts the outcome (e.g.,
pressing a button and the light being turned on), negative contingency is when
an action signals the absence of the outcome (e.g., pressing a button and the
light being turned off), and zero contingency is when the action has no relation
tothepresenceorabsenceoftheoutcome(e.g.thepressingofabuttondoesnot
have an impact on the light).
One experimental task widely used to assess action-outcome contingency
learning involves an action that the participant can freely perform (a so-called
2 Pitliya & Murphy
free-operant procedure [5]), such as pressing a button, and depending on the
objective contingency set by the experimenter, an outcome is present or absent,
such as a light being on or off. Subsequently, participants report the degree
of control they perceive they have on a visual analog or numeric rating scale
varyingfrom-1to+1.Ithasbeenwell-demonstratedthatperceivedcontingency
as reported on the rating scale is aligned with the action-outcome objective
contingency [6]–[12]. In this paper, we examine agential learning in the context
of a simple scenario involving a single action and single outcome, though more
complex versions could be entertained.
2 Previous Models of Agential Learning
Philosophersandthenpsychologistshavebeenchallengedtoexplainhowagents
learnthatoneeventpredicts(orcauses)thepresenceorabsenceofanotherevent
[13]–[17]. Models that were originally used to explain cue-outcome contingency
learning in non-human animals have been employed to explain human perfor-
mance with some success. Several models based on associative learning theory,
statistical accounts, inferential reasoning, and Bayesian learning have been pro-
posed.However,noneaccountforthecomplexityoflearning[18]–[20]astheyfail
to capture the relations between perception, learning, and action in informing a
sense of agency.
2.1 Associative Models
Associativemodels[21]–[25]adoptabottom-upapproachandareprocess-driven.
One model first applied to Pavlovian learning and then extended to explain
instrumental learning is the Rescorla-Wagner model. Based on reinforcement
principles and successfully applied to human statistical learning, the learning
rule is as follows:
∆V =αβ(λ −V ) (1)
n n total
∆V represents the change in associative strength of an action in that trial
(n). The learning rate parameters, α and β, represent the associability of the
actionandoutcomerespectively,representinghowfastaparticularactioncanbe
learnt. The subtraction in the parenthesis represents the prediction error, which
is the discrepancy between the expected and actual occurrence of the outcome
given an action. λ represents the absolute value of the outcome on a trial (n).
V is the total current associative prediction of all stimuli presented at that
total
trial,thereforeitcomprisesV +V +...+V .Insum,theRescorla-Wagnermodel
1 2 n
proposes that learning involves forming associations between all actions present
in the environment, and those associations compete with one another as there
is a limit to the amount of associative strength the outcome can support. The
agent’s knowledge regarding associations is represented as a single weight value
on each action.
Associative learning explanations of action-outcome contingency learning
suggest that agents integrate information online as each action’s associative
strength gets updated, requiring few cognitive resources and being computa-
tionally cheap. It is often described as a form of model-free associative learning.
A Model of Agential Learning Using Active Inference 3
However, because the values get updated with each trial, explaining phenomena
suchasretrospectiverevaluation,whichhasbeendemonstratedinhumans[26]–
[28], require additional assumptions.
Thestatisticalaccountofaction-outcomecontingencylearning[13],[29]sug-
gests that the perceived contingency by the agent is related to an estimate of
thedifferencebetweentheprobabilityofoutcomeoccurringgivenanaction and
probability of outcome occurring given a lack of action. Models based on such
statistical metrics alone, however, are unable to account for learning curves be-
causeprobabilitiesarenotaffectedbytheamountofevidenceonwhichtheyare
based [30].
The associative learning and statistical models explain agential learning as
the perception of a punctate value reflecting the action-outcome contingency,
overlooking other processes that may be involved. For example, the selection of
actions by the agent are not accounted for, except in the obvious cases where
an experimenter impels or instructs action. An agent’s action produces data for
the agent, which they use to form beliefs about their agency [31]. Indeed, a link
between probability of acting and objective contingency has been established
in free-operant tasks [32]–[34]. Moreover, agency may emerge from a form of
inferentialreasoning[18],[35],[36],whereinagentsnotonlyrelyondirectsensory
input and statistical metrics, but also engage in processes that involve learning
aboutthedynamicsandcausesofthelatentstatesoftheworld.Bayesianmodels
of contingency learning provide an alternate account and address some of the
limitations of previous models [37], [38].
2.2 Bayesian Associative Models
Inferring a Causal Structure
Researchers have proposed that agents may conduct Bayesian inference to infer
the causal structure of the environment [39], [40]. The agent may do this by
using bottom-up sensory information (observations) to infer the causal hidden
states of their environment using an internal model of the world: a generative
model that captures the agent’s beliefs about how (potentially dynamic) latent
states of the world relate to observable sensory data.
A generative model of agential learning would comprise: (i) a prior proba-
bility distribution which represents the agent’s current beliefs about the hidden
states, and (ii) a likelihood probability distribution which captures the agent’s
knowledgeofhowobservations(theactionandoutcome)aregeneratedfromhid-
den states by encoding the likelihood of observations given states. Using Bayes’
rule, one can compute a posterior probability distribution over hidden states,
givenobservations.Thiscanbeinterpretedastheagent’sbeliefsregardingwhich
hidden states best explain its sensory data, i.e., beliefs regarding their degree
of agency. In the context of Bayesian cognitive neuroscience, this updating of
beliefs via Bayesian inference has been analogised to perception [41].
The discrepancy between the agent’s predictions (from the priors) and be-
liefs about hidden states after receiving observations (posterior) is quantified by
Bayesiansurprise,asimilarmetrictopredictionerrorasintheRescorla-Wagner
4 Pitliya & Murphy
model. This is a measure of the degree to which the internal model and poste-
riorbeliefsgetupdatedtoreducefuturesurprise,whichwouldensureaninternal
modelofthecausalstructureoftheworldtobeasclosetotherealcausalstruc-
ture of the world as possible. Bayesian inference can therefore be framed as an
alternative problem of maximising marginal (log) likelihood, or, in other words,
minimising surprise.
In traditional models of contingency learning, punctate values represent all
of the agent’s knowledge. Bayesian approaches assume a different knowledge
representation in the generative model as the agent entertains a probabilistic
representation of its world, allowing a spectrum of alternative hypotheses to be
represented via their posterior beliefs. The probability distributions allow the
agent to express uncertainty, where the more spread out the beliefs are (repre-
sented by a flatter probability distribution), the greater the uncertainty. Such a
representation of knowledge allows the model to keep track of multiple combi-
nationsofhypotheticalbeliefs,makingtheperceivedcausalstructuremalleable.
Therefore,whenbeliefregardinganassociationishighlyuncertain,observational
datahasarapidinfluenceonchangingthatbelief.ThesepropertiesofaBayesian
approachaccountforhowanagentperceivesanaction-outcomecontingency[37].
Explaining Actions
Themodelsdescribedsofarconsidertheagentasapassiveobserver,andpredict
action based on the action that is strongest associated with the outcome to
produce the most favoured outcomes [7]. However, in reality, when agents are
learningthedegreeofagencytheyhave,theagenthastheopportunitytoexplore
or manipulate the world in order to extract information. In other words, the
agent actively samples the environment, creating observations for itself to infer
and perceive (a degree of) agency and test its beliefs in order to attain the
preferred outcome state.
The representation of uncertainty in Bayesian models used to explain ob-
servational learning can be leveraged to guide active learning. Here, the agent’s
actions are explained as the agent actively engaging with the environment to
maximise expected information gain based on the generative model to reduce
uncertainty [42]. While this explains exploratory behaviour, exploitation is ex-
plained by a separate function, based on Bayesian decision theory (or expected
utility theory), wherein a value function of states is computed, which represents
howrewardingthestateisfortheagenttobein.Thevalueofthestatesdepend
on the agent’s learning history of state-action pairs, i.e., tracking how many
times the agent attains the outcome by conducting that action from that state.
The agent would thereby select the action that yields the outcomes it values.
However, while exploitation and exploratory behaviour can be explained by
differentfunctions,thebalancebetweenthetwooftenmustbeadjustedbyintro-
ducingtrade-offparameters,anddifferentstrategieshavebeenemployedaccord-
ing to task constraints [43]. This calls for a universal model of active learning
instead of selecting a model from a class of models to optimally conduct the
trade-offbetweenexplorationandexploitationdependentonthe context.Inthe
A Model of Agential Learning Using Active Inference 5
next section, we introduce an active inference model of agential learning, where
atrade-offbetweeninformationgainandrewardsinherentlyarisesasperception
and action are not treated as processes optimising two different functions but
ratherasinglefunction.Itisarguedthattheactiveinferenceframeworkprovides
a comprehensive model of agential learning.
3 Active Inference
3.1 Perception, Action, and Learning in Active Inference
Active inference is aprocess theory, basedon the free energyprinciple [44], that
provides a unified account of perception, action, and learning in agents. Active
inference extends the (variational) Bayesian inferential process described earlier
for perception to action, stemming from the notion that the agent minimises
surprise.Inactiveinference,however,aproxyforBayesiansurprise,(variational)
free energy, is minimised [31], [45]. It is argued that while Bayesian frameworks
considersurprisetobedependentontheagent’sgenerativemodel,surpriseisalso
dependent on observations [45]. Active inference leverages this dependence to
predictactions,whereintheagentinferstheconsequencesofitsownactionsand
the hidden states of the world, to exhibit behaviour that attains its preferences
and actively reduces uncertainty in the agent’s world model [46], [47].
Under active inference, action selection is not only a function of past and
presentobservations(asinBayesianaccounts),butalsoafunctionofprospective
formsofinferencebasedonanticipatedfutureobservations.Theagentinfersthe
best action sequence (policies) on the basis of future observations the actions
wouldengender,whichisbasedonbeliefsaboutlikelihoodsofobservationsgiven
the anticipated states in addition to the transitions of states across time as a
function of the policy. This formulation of action selection in active inference
casts action trajectories as a functional of beliefs (i.e., beliefs of beliefs, with
probability distributions) inevitably encompassing the notions of uncertainty
and preferences.
Accordingtoactiveinference,actionselectionoccursbyexpectedfreeenergy
(EFE) being calculated for each policy and a policy is selected according to its
negative EFE as policies that afford the lowest EFE are the most likely. EFE
can be seen as the combination of (i) the anticipated information gain afforded
byexpectedobservationsunderapolicy(exploration)and(ii)howwellexpected
observations align with preferences (exploitation). Maximising the exploration
term is equivalent to maximising the expected divergence between the expected
posterior distribution, with and without observations expected under a policy
- maximising this leads to behaviour that actively seeks out observations that
resolve the most (posterior) uncertainty. Maximising the exploitation term is
equivalenttochangingpoliciestoproducethoseobservationsthatbestmatchthe
agent’s prior beliefs about observations (i.e., its preferences), which is specified
intheagent’sgenerativemodel.Hence,activeinferencebalancesexplorationand
exploitation, ensuring that an optimal agent pursues both. Often, in situations
where the agent is uncertain about hidden states that are relevant to preferred
6 Pitliya & Murphy
observations,activeinferenceagentswillfirstperformmoreepistemicallydriven
actions to resolve uncertainty, before opting for a more pragmatic action that
maximises utility, i.e., exploit the resolved structure of the environment.
Learning occurs in active inference by updating model parameters, such as
the likelihood distributions and state transition beliefs. In the discrete state-
space models commonly used in active inference, these likelihood and transition
distributionsaredescribedascategoricaldistributionswithmatricesofparame-
ters.ThesedistributionsareoftenequippedwithconjugateDirichletpriors[48],
whose parameters take the form of pseudocounts or positive real numbers that
parameterise prior beliefs about the corresponding categorical parameters. The
values of these Dirichlet hyper parameters can be interpreted as pseudocounts
that are proportional to the prior probability of seeing particular state-outcome
contingencies or coincidences between states and actions over time. Learning is
thuscastasposteriorinferenceovertheseDirichlethyperparameters[48].Hence,
when a new observation is received by the agent, a posterior distribution over
the model parameter is acquired to be used as the prior distribution in the next
time step, equipping the agent to sequentially update beliefs about the model
parameters.Alearningrateparametercanalsobespecifiedtocontrolhowmuch
thevaluesintheDirichletdistributionchangeaftereachtimestep,representing
how quickly the agent can get stuck in its ways during learning [48].
Tosummarise,whenthereisamismatchbetweentheagent’spredictionsand
sensoryinputs,theagent(i)updatesitsinternalmodeltoreducefuturesurprise
by updating its beliefs about the states that caused the observation, and/or
(ii) updating its beliefs about the dynamics of the world (updating model pa-
rameters), and/or (iii) actively engages with the environment to generate and
maximise model evidence, thereby reducing future surprise. These processes of
minimising surprise respectively map onto three fundamental processes: (i) per-
ception, (ii) learning, and (iii) action.
3.2 Generative Model of Agential Learning
The Agential Learning Task
Inthissection,adiscrete-timegenerativemodeloftheclassicfree-operantagen-
tial learning task is presented as in Figure 1), along with a set of simulations
presented in Figures 3 and 4). In the learning task, the agent produces an ac-
tion by pressing a button or not, and according to the objective contingency, an
outcome is present or absent. In some of our experimental conditions, the agent
has 100% or 80% (positive or negative) control over the outcome, referred to as
the deterministic and probabilistic conditions, respectively. In other conditions,
the agent has no control, i.e, the outcome is produced at random, independent
of the agent’s actions.
Generative Model of Agential Learning Task
The generative model an active inference agent is equipped with is in the form
of a Partially Observable Markov Decision Process (POMDP; [45]). POMDPs
expressthegenerativemodelwithasequenceofhiddenstates(s)thatevolveover
time. The hidden states inferred by the agent in this agential learning task are
A Model of Agential Learning Using Active Inference 7
theobjectivecontingency(positive,negative,orzero)betweentheself-produced
action and outcome, which is the context (or experimental condition) the agent
is in (scontext).
t
Ateachtimestep(t),thecurrentstateisconditionallydependentonthestate
at the previous time step and on the actions (u; aka control states) currently
beingexecuted.Theactionsaredependentonthepolicy(π;akaactionsequence)
currently being executed. Each time step is associated with an observation (o)
thatdependsonlyonthestateatthattime.Theobservationstheagentreceives
are of the outcome (ooutcome), wherein the outcome can be present or absent,
t
and the observations of the action the agent conducted (oaction), wherein the
t
agent observes that it pressed the button or not.
The hidden and control states are classified into state factors, and observa-
tions are classified into observation modalities. This means that at any given
time, observations will be evinced from each modality, and hidden states will be
inferred from each state factor, and an action (control state) is selected accord-
ingly.Thes,u,andoarediscreterandomvariables,soallmodelparametersare
categorical distributions too.
Theagent’sgenerativemodelisequippedwithmodelparametersdenotedas
A, B, C, and D tensors that allow the agent to perform active inference. The
likelihood tensor (A), represents the beliefs of probability of some observation
given the states in the agent’s environment, P(o |s ). The top-left matrix in A
t t
tensor panel in Figure 2 illustrates that the agent believes the probability it
will observe the outcome being present given it is in the positive control state
andhaspressedthebuttonis0.6.Thisvalueisnot1.0astheagentcannothave
alreadylearnedthepreciselikelihoodmappingsasitdoesnotknowtheobjective
contingency in all of the possible contexts in which it could be operating, so
it conducts learning by updating the likelihood tensor regarding outcomes via
Dirichlet counts (Dir(Aoutcome),). The degree of control the agent perceives is
indicated by the posterior probability of the state.
The state transition tensor (B), represents the beliefs of the dynamics of
the environment as how hidden states and actions determine subsequent hidden
states, P(s |s ,u ). The objective contingency does not change over a block
t t−1 t
of trials, and we assume the agent knows this fact veridically, and thus their
generative model has an identity matrix in the left matrix presented in the B
tensor panel in Figure 2.
Samplingtheenvironmentoccursasafunctionofpreferringeachobservation,
representedinthepreferencetensor(C)inFigure2,reducinguncertainty.There
is a slight preference for not producing an action as producing an action costs
resource. To introduce evidence variance, periods of sub-optimal action would
be intentionally conducted by the agent to create variation in the observations
and assess the agent’s generative model. The D tensor represents the agent’s
beliefs of the prior probability of being in each state, which is a flat distribution
to reflect the agent’s lack of a bias towards being in a positive, negative, or
zero-contingency state.
8 Pitliya & Murphy
Simulation Results
Allsimulationsdescribedinthispaperwereconductedusingthesparse likelihoods 111
branch of pymdp, a freely available Python package for performing active infer-
enceindiscretestatespaces[49].Thecodeusedforthesimulationsdescribedin
thispapercanbefoundhere:https://github.com/riddhipits/iwai_agency_
oneagent.
Figures 3 and 4 illustrate the results of simulations of an agent conduct-
ing agential learning in the deterministic and probabilistic learning task across
three experimental conditions. Panels correspond to each experimental condi-
tion:positivecontrol,negativecontrol,andzerocontrol.Thethreesub-panelsin
each panel illustrate the agent’s beliefs over time (x-axis) regarding the experi-
mentalcondition(orcontext),theactionsittook,andtheoutcomesitobserved.
The strength of the belief is reflected in the grayscale cells, with black cells in-
dicating a value of 0.0 and white cells indicating a value of 1.0. The agent had
50 trials to learn about the degree of control.
Inthedeterministic(100%)agencysimulation(Figure3),inthepositiveand
negative control condition, the agent quickly learned that it had full positive
and negative control, respectively; this is illustrated by the gradual transition
fromblacktowhiteonthetop-mostsub-panels.Inthefirstfewtrials,theagent
tracks(viaDirichletcounts)theoutcomeobservationgiventhestatesitobserves
(actions) and infers (context) and reflects its learning of the environment being
deterministicbyupdatingthelikelihoodtensorsinitsgenerativemodel.Accord-
ingly, the agent then infers, with certainty that it is in the positive or negative
condition. As predicted, the agent introduces evidence variance by occasionally
acting sub-optimally to increase certainty regarding its beliefs. In the proba-
bilistic(80%)agencysimulation(Figure4),theagentlearnssimilarly,albeitless
quickly and with more uncertainty as illustrated with more grey cells.
In the zero-control condition, the agents in both simulations (Figure 3 and
Figure4)takeslongerlearnthatitsactionshavenocontrolovertheoutcome.To
elaborate, in Figure 3, during the first few trials (Box A), the agent’s actions of
pressing the button were coincidentally paired with the outcome being present,
whichiswhyithadahigherbeliefofbeinginthenegativecontrolstate.Andin
themiddleoftheblockoftrials(BoxB),theagentsactionsalignedwithwhatit
wouldpredicttoperceiveinapositivecontrolcondition,whichiswhyitsbeliefs
shift towards the positive control condition until it receives evidence against
that belief. Finally, the agent’s beliefs increase for the zero-control condition.
Throughouttheblockoftrials,theagenttestsitshypothesesbyvariablypressing
the button or not.
4 Discussion and Concluding Remarks
These simulations reveal that the active inference framework has potential to
provide a comprehensive model of agential learning tying perception, actions,
and learning processes, resulting from the minimisation of a single metric: free
A Model of Agential Learning Using Active Inference 9
energy. Previous models have treated these processes as optimising disparate
functions.
ComparedtoBayesianagents,activeinferenceagentspossessadeeperrepre-
sentationofthecausalstructureanddynamicsoftheenvironmentasanactivein-
ferenceagent’sgenerativemodelisequippedwithbeliefsaboutstatetransitions
acrosstime.Thisisleveragedbytheactiveinferenceagentasitallowstheagent
to consider future states and observations based on future actions to optimally
selectanaction.Theactionsmaximiseevidencefortheagent’sgenerativemodel
oftheirenvironmentbyexploringtheenvironmentwhenuncertaintyishighand
thenexploitingtheenvironmenttoattainpreferredobservations/outcomes,and
introduce evidence variance to continually assess the agent’s generative model.
The active inference model of agential learning may allow us to explain indi-
vidual differences in agential learning. For example, agents experiencing learned
helplessness (a key symptom of depression) may have a higher learning rate for
the zero-control state due to generalisation from trauma, resulting in them hav-
ing a bias and getting stuck when the belief of being in a zero-control state is
higher. Over time, this may result in them developing a habit of not producing
an action (due to deep temporal active inference models; see [50]), resulting in
reduced variance in sampling the environment.
The simulation results in this paper emphasise that observations of differ-
ent action-outcome combinations make a big difference to the perceived contin-
gencyinazero-controlcondition.Thispredictsthatagentswhoproduceactions
wouldexperiencemore(butaccidental)action-outcome-presentobservationsand
thereby perceive an illusion of (positive) control, whereas agents who withhold
actions would perceive more no-action-outcome-present observations, resulting
in perceiving zero control. The predictions are in line with data from humans
as experimenters showed that non-depressed individuals produced more actions
in the zero-control condition, perceiving an illusion of (positive) control, and in-
dividuals experiencing depression withheld actions, perceiving a lack a control,
potentially explaining their lack of sense of agency [51].
Nonetheless,furtherexaminationoftheactiveinferenceformulationofagen-
tial learning is warranted. In future research studies, we intend to: (i) conduct
statisticalmodelcomparisonsbetweenthedifferentaccountsofagentiallearning
via model fitting to human behavioural data, (ii) examine if active inference ex-
plainsindividualdifferencesinagentiallearningacrossthedepressionspectrum,
and (iii) explore more complex scenarios of agential learning such as one with
multiple agents and outcomes.
5 Figures
10 Pitliya & Murphy
Fig.1.Agraphicalrepresentation[52]oftheactiveinferencebasedgenerativemodelof
theagentiallearningtask.Thevariablesofthemodelareillustratedascirclesandmodel
parameters as squares and rectangles. The arrows indicate the direction of influence.
Please see the main text for a description of the variables and parameters.
Fig.2. The details of the model parameters of the generative model of the agential
learning task.
A Model of Agential Learning Using Active Inference 11
Fig.3.Simulationresultsfordeterministic(100%control)agentiallearningtask.The
threepanelsillustratethreeseparatesimulations,oneforeachexperimentalcondition:
positive control, negative control, and zero control. Within each panel of simulation
result, there are three sub-panels, where the x axis is the timestep. The black cells
represent the value of 0.0 and white cells represent the value of 1.0, so the grayscale
cells are values within that range. The top sub-panel illustrates the beliefs the agent
hasregardingthecontextstates,themiddlesub-panelillustratestheactionstheagent
selected over time, and the bottom sub-panel illustrates the outcomes the agent ob-
served over time.
12 Pitliya & Murphy
Fig.4. Simulation results for probabilistic (80% control) agential learning task. The
threepanelsillustratethreeseparatesimulations,oneforeachexperimentalcondition:
positive control, negative control, and zero control. Within each panel of simulation
result, there are three sub-panels, where the x axis is the timestep. The black cells
represent the value of 0.0 and white cells represent the value of 1.0, so the grayscale
cells are values within that range. The top sub-panel illustrates the beliefs the agent
hasregardingthecontextstates,themiddlesub-panelillustratestheactionstheagent
selected over time, and the bottom sub-panel illustrates the outcomes the agent ob-
served over time.
A Model of Agential Learning Using Active Inference 13
References
[1] S. Gallagher, “Philosophical conceptions of the self: Implications for cog-
nitive science,” Trends in cognitive sciences, vol. 4, no. 1, pp. 14–21, 2000.
[2] P. Haggard, “Sense of agency in the human brain,” Nature Reviews Neu-
roscience, vol. 18, no. 4, pp. 196–207, 2017.
[3] M. Albarracin and R. J. Pitliya, “The nature of beliefs and believing,”
Frontiers in Psychology, vol. 13, 2022.
[4] P. F. Verschure, C. M. Pennartz, and G. Pezzulo, “The why, what, where,
when and how of goal-directed choice: Neuronal and computational prin-
ciples,” Philosophical Transactions of the Royal Society B: Biological Sci-
ences, vol. 369, no. 1655, p. 20130483, 2014.
[5] C. B. Ferster, “The use of the free operant in the analysis of behavior.,”
Psychological Bulletin, vol. 50, no. 4, p. 263, 1953.
[6] L.G.AllanandH.M.Jenkins,“Thejudgmentofcontingencyandthena-
tureoftheresponsealternatives.,”CanadianJournalofPsychology/Revue
canadienne de psychologie, vol. 34, no. 1, p. 1, 1980.
[7] D.R.ShanksandA.Dickinson,“Instrumentaljudgmentandperformance
under variations in action-outcome contingency and contiguity,” Memory
& Cognition, vol. 19, pp. 353–360, 1991.
[8] E.A.Wasserman,D.Chatlosh,andD.Neunaber,“Perceptionofcausalre-
lationsinhumans:Factorsaffectingjudgmentsofresponse-outcomecontin-
gencies under free-operant procedures,” Learning and motivation, vol. 14,
no. 4, pp. 406–432, 1983.
[9] E. A. Wasserman, S. M. Elek, D. L. Chatlosh, and A. G. Baker, “Rating
causalrelations:Roleofprobabilityinjudgmentsofresponse-outcomecon-
tingency.,” Journal of Experimental Psychology: Learning, Memory, and
Cognition, vol. 19, no. 1, p. 174, 1993.
[10] F. Vall´ee-Tourangeau, R. A. Murphy, and A. Baker, “Contiguity and the
outcome density bias in action–outcome contingency judgements,” The
Quarterly Journal of Experimental Psychology Section B, vol. 58, no. 2b,
pp. 177–192, 2005.
[11] F. Vallee-Tourangeau and R. Murphy, “Action-effect contingency judg-
menttasksfosternormativecausalreasoning,”inProceedingsoftheTwenty
First Annual Conference of the Cognitive Science Society, 1999, pp. 820–
820.
[12] R.M.Msetfi,R.A.Murphy,J.Simpson,andD.E.Kornbrot,“Depressive
realism and outcome density bias in contingency judgments: The effect of
the context and intertrial interval.,” Journal of Experimental Psychology:
General, vol. 134, no. 1, p. 10, 2005.
[13] P. W. Cheng, “From covariation to causation: A causal power theory.,”
Psychological review, vol. 104, no. 2, p. 367, 1997.
[14] D. Hume, “A treatise of human nature: Volume 1: Texts,” 1739.
[15] I. Kant, “Critique of pure reason. 1781,” Modern Classical Philosophers,
Cambridge, MA: Houghton Mifflin, pp. 370–456, 1908.
[16] A. Michotte, The perception of causality. Routledge, 2017, vol. 21.
14 Pitliya & Murphy
[17] D.R.Shanks,F.J.Lopez,R.J.Darby,andA.Dickinson,“Distinguishing
associativeandprobabilisticcontrasttheoriesofhumancontingencyjudg-
ment,” in Psychology of learning and motivation, vol. 34, Elsevier, 1996,
pp. 265–311.
[18] J.DeHouwerandT.Beckers,“Areviewofrecentdevelopmentsinresearch
and theories on human contingency learning,” The Quarterly Journal of
Experimental Psychology: Section B, vol. 55, no. 4, pp. 289–310, 2002.
[19] O. Pinen˜o and R. R. Miller, “Comparing associative, statistical, and in-
ferential reasoning accounts of human contingency learning,” Quarterly
Journal of Experimental Psychology, vol. 60, no. 3, pp. 310–329, 2007.
[20] D. R. Shanks, “Associationism and cognition: Human contingency learn-
ing at 25,” Quarterly Journal of Experimental Psychology, vol. 60, no. 3,
pp. 291–309, 2007.
[21] N.J.Mackintosh,“Atheoryofattention:Variationsintheassociabilityof
stimuli with reinforcement.,” Psychological review, vol. 82, no. 4, p. 276,
1975.
[22] R. R. Miller and L. D. Matzel, “The comparator hypothesis: A response
rule for the expression of associations,” in Psychology of learning and mo-
tivation, vol. 22, Elsevier, 1988, pp. 51–92.
[23] J. M. Pearce and G. Hall, “A model for pavlovian learning: Variations in
the effectiveness of conditioned but not of unconditioned stimuli.,” Psy-
chological review, vol. 87, no. 6, p. 532, 1980.
[24] R.A.Rescorla,“Atheoryofpavlovianconditioning:Variationsintheeffec-
tiveness of reinforcement and non-reinforcement,” Classical conditioning,
Current research and theory, vol. 2, pp. 64–69, 1972.
[25] A. R. Wagner and R. A. Rescorla, “Inhibition in pavlovian conditioning:
Application of a theory,” Inhibition and learning, pp. 301–336, 1972.
[26] G. B. Chapman, “Trial order affects cue interaction in contingency judg-
ment.,”Journal of Experimental Psychology: Learning, Memory, and Cog-
nition, vol. 17, no. 5, p. 837, 1991.
[27] J. De Houwer and T. Beckers, “Higher-order retrospective revaluation in
human causal learning,” The Quarterly Journal of Experimental Psychol-
ogy Section B, vol. 55, no. 2b, pp. 137–151, 2002.
[28] A. Dickinson, “Within compound associations mediate the retrospective
revaluation of causality judgements,” The Quarterly Journal of Experi-
mental Psychology: Section B, vol. 49, no. 1, pp. 60–80, 1996.
[29] P.W.ChengandL.R.Novick,“Covariationinnaturalcausalinduction.,”
Psychological review, vol. 99, no. 2, p. 365, 1992.
[30] F. J. L´opez, J. Almaraz, P. Fern´andez, and D. Shanks, “Adquisici´on pro-
gresiva del conocimiento sobre relaciones predictivas: Curvas de apren-
dizaje en juicios de contingencia,” Psicothema, pp. 337–349, 1999.
[31] K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, and G. Pezzulo,
“Active inference: A process theory,” Neural computation, vol. 29, no. 1,
pp. 1–49, 2017.
A Model of Agential Learning Using Active Inference 15
[32] F. Blanco, H. Matute, and M. A. Vadillo, “Mediating role of activity level
in the depressive realism effect,” 2012.
[33] F.Blanco,H.Matute,andM.A.Vadillo,“Interactiveeffectsoftheproba-
bility of the cue and the probability of the outcome on the overestimation
of null contingency,” Learning & Behavior, vol. 41, pp. 333–340, 2013.
[34] N. Byrom, R. Msetfi, and R. Murphy, “Two pathways to causal control:
Use and availability of information in the environment in people with and
without signs of depression,” Acta psychologica, vol. 157, pp. 1–12, 2015.
[35] T. L. Griffiths and J. B. Tenenbaum, “Structure and strength in causal
induction,” Cognitive psychology, vol. 51, no. 4, pp. 334–384, 2005.
[36] M.R.Waldmann,“Competitionamongcausesbutnoteffectsinpredictive
and diagnostic learning.,” Journal of Experimental Psychology: Learning,
Memory, and Cognition, vol. 26, no. 1, p. 53, 2000.
[37] J.K.Kruschke,“Bayesianapproachestoassociativelearning:Frompassive
toactivelearning,”Learning & behavior,vol.36,no.3,pp.210–226,2008.
[38] J. B. Tenenbaum, T. L. Griffiths, and C. Kemp, “Theory-based bayesian
models of inductive learning and reasoning,” Trends in cognitive sciences,
vol. 10, no. 7, pp. 309–318, 2006.
[39] N. Chater, M. Oaksford, U. Hahn, and E. Heit, “Bayesian models of cog-
nition,” Wiley Interdisciplinary Reviews: Cognitive Science, vol. 1, no. 6,
pp. 811–823, 2010.
[40] K. Doya, S. Ishii, A. Pouget, and R. P. Rao, Bayesian brain: Probabilistic
approaches to neural coding. MIT press, 2007.
[41] H.VonHelmholtz,Handbuch der physiologischen Optik.Voss,1867,vol.9.
[42] J. D. Nelson, “Finding useful questions: On bayesian diagnosticity, proba-
bility,impact,andinformationgain.,”Psychologicalreview,vol.112,no.4,
p. 979, 2005.
[43] G. De Ath, R. M. Everson, A. A. Rahat, and J. E. Fieldsend, “Greed is
good: Exploration and exploitation trade-offs in bayesian optimisation,”
ACM Transactions on Evolutionary Learning and Optimization, vol. 1,
no. 1, pp. 1–22, 2021.
[44] K. Friston, “The free-energy principle: A unified brain theory?” Nature
reviews neuroscience, vol. 11, no. 2, pp. 127–138, 2010.
[45] T. Parr, G. Pezzulo, and K. J. Friston, Active inference: the free energy
principle in mind, brain, and behavior. MIT Press, 2022.
[46] K. J. Friston, J. Daunizeau, and S. J. Kiebel, “Reinforcement learning or
active inference?” PloS one, vol. 4, no. 7, e6421, 2009.
[47] K. Friston, F. Rigoli, D. Ognibene, C. Mathys, T. Fitzgerald, and G. Pez-
zulo,“Activeinferenceandepistemicvalue,”Cognitiveneuroscience,vol.6,
no. 4, pp. 187–214, 2015.
[48] R.Smith,K.J.Friston,andC.J.Whyte,“Astep-by-steptutorialonactive
inference and its application to empirical data,” Journal of mathematical
psychology, vol. 107, p. 102632, 2022.
16 Pitliya & Murphy
[49] C. Heins, B. Millidge, D. Demekas, et al., “Pymdp: A python library for
activeinferenceindiscretestatespaces,”arXivpreprintarXiv:2201.03904,
2022.
[50] K.J.Friston,R.Rosch,T.Parr,C.Price,andH.Bowman,“Deeptempo-
ral models and active inference,” Neuroscience & Biobehavioral Reviews,
vol. 90, pp. 486–501, 2018.
[51] F. Blanco, H. Matute, and M. A. Vadillo, “Depressive realism: Wiser or
quieter?” The Psychological Record, vol. 59, no. 4, pp. 551–562, 2009.
[52] K.J.Friston,T.Parr,andB.deVries,“Thegraphicalbrain:Beliefpropa-
gation and active inference,” Network neuroscience, vol. 1, no. 4, pp. 381–
414, 2017.