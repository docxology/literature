Exploring Action-Centric Representations
Through the Lens of Rate-Distortion Theory
Miguel De Llanza Varona1,2, Christopher Buckley1,2, Beren Millidge3
1 School of Engineering and Informatics, University of Sussex, Brighton, UK
2 VERSES Research Lab, Los Angeles, California, USA
M.De-Llanza-Varona@sussex.ac.uk, C.L.Buckley@sussex.ac.uk
3 MRC Brain Networks Dynamics Unit, University of Oxford, Oxford, UK
beren@millidge.name
Abstract. Organisms have to keep track of the information in the en-
vironment that is relevant for adaptive behaviour. Transmitting infor-
mation in an economical and efficient way becomes crucial for limited-
resourced agents living in high-dimensional environments. The efficient
coding hypothesis claims that organisms seek to maximize the informa-
tion about the sensory input in an efficient manner. Under Bayesian
inference, this means that the role of the brain is to efficiently allocate
resources in order to make predictions about the hidden states that cause
sensory data. However, neither of those frameworks accounts for how that
information is exploited downstream, leaving aside the action-oriented
role of the perceptual system. Rate-distortion theory, which defines op-
timal lossy compression under constraints, has gained attention as a for-
mal framework to explore goal-oriented efficient coding. In this work, we
explore action-centric representations in the context of rate-distortion
theory. We also provide a mathematical definition of abstractions and
we argue that, as a summary of the relevant details, they can be used to
fix the content of action-centric representations. We model action-centric
representations using VAEs and we find that such representations i) are
efficient lossy compressions of the data; ii) capture the task-dependent
invariances necessary to achieve successful behaviour; and iii) are not
in service of reconstructing the data. Thus, we conclude that full re-
construction of the data is rarely needed to achieve optimal behaviour,
consistent with a teleological approach to perception.
Keywords: Rate-distortion theory · Action-centric representations · Ef-
ficient coding · Bayesian Inference
1 Introduction
Embodied agents have to focus on the relevant information from their environ-
ment to achieve adaptive behaviour. Their resource-limited cognition and the
high-complexity structure inherent to the environment force them to economize
the transmission of information. Thus, the goal of the perceptual system is to
generate representations that are useful for successful behaviour while at the
same being encoded in the most efficient manner.
arXiv:2409.08892v1  [cs.AI]  13 Sep 2024
2 Miguel De Llanza Varona, Christopher Buckley, Beren Millidge
A well-known hypothesis in theoretical neuroscience called the efficient cod-
ing hypothesis proposes that the neural coding in the brain is optimized to max-
imize sensory information under metabolic and capacity constraints [3, 13, 23].
In particular, this hypothesis suggests that neurons are tuned to the statistical
properties of the environment, which allows them to efficiently allocate signaling
resources to generate compressed low-dimensional representations of the environ-
ment. In this theoretical framework, it is commonly assumed that the function
of neurons is to maximize their capacity to account for all the variability in
the sensory input. In information theory terms, this means that the brain seeks
to maximize the mutual information between stimuli and neurons’ response to
reduce as much as possible the uncertainty about the environment, which is
defined by its entropy. While this hypothesis answers the question about in-
formation processing under biological constraints, it leaves aside the utilitarian
aspect of perception [11,14,15,18,20].
Cognition can’t be fully understood without its ecological context, as agents
are coupled with their environment forming a perception-action feedback loop
[22]. In this sense, the functional role of perceptual processing has to be in service
of achieving behavioural objectives, and to do that, perceptual representations
must efficiently encode the relevant information needed by the motor system to
guide future actions. Thus, a key component of the perceptual system is to sum-
marize relevant sensory information to generate action-centric representations.
The teleological essence of the perceptual system imposes a normativity on
representations: a perceptual representation is accurate if it captures the relevant
information needed downstream and discards the irrelevant details. Thus, we
need an extra ingredient to account for the goodness of representations under
constraints. This is where the rate-distortion theory comes into play [19]. This
subfield of information theory defines the optimal trade-off between channel
capacity and expected communication error. When error-free communication is
not necessary to guide behaviour, the optimal encoding is a lossy compression
of the input.
Interestingly, rate-distortion theory can be seen as a way to perform Bayesian
inference under constraints. Under a Bayesian approach to cognition, the brain
performs inference to compute an optimal posterior distribution over hidden
environmental states given sensory data [8]. As computing the true posterior
is usually intractable, the brain approximates the true posterior by optimizing
the variational free energy [4, 9, 10]. The main conceptual contribution of rate-
distortion theory is to define the “goodness” of that approximation, as comput-
ing the true posterior is not always necessary to act optimally. In the context of
active inference, it has been shown that action-oriented models learn parsimo-
nious representations of the environment by capturing relevant information for
behaviour [21]. In the same spirit, we investigate the information-theoretic prop-
erties of action-centric representations and their relation to the formal definition
of abstractions we propose.
In this work, we explore action-centric representations under the lens of
rate-distortion theory to account for the teleological aspect of perception. To
Action-Centric Representations and Rate-Distortion Theory 3
do that, we provide a mathematical definition of abstraction that allows us to
specify the task-relevant information that should carry an action-centric rep-
resentation. Given the tight connection between Bayesian Inference and rate-
distortion theory, we use a Variational Autoencoder (VAE) framework to model
action-centric representations as optimal lossy compression. Our results show
that action-centric representations are optimal lossy compressions of the data;
can be successfully used in downstream tasks; and crucially, they achieve that
without being in service of reconstructing the data.
2 Efficient coding and rate-distortion theory
2.1 Efficient coding
The efficient coding hypothesis states that neurons are optimized to maximize
the information they carry about sensory states. In doing so, neurons have to
generate minimal redundancy codes to economically use limited resources. In
particular, neurons seek to maximize the ratio between information about sen-
sory inputs, defined by the mutual information I(X; Z) between sensory data
X and neural responses Z, and the channel capacity C: I(X;Z)
C . The maximum
mutual information is upper bounded by the channel capacity
C ≥ I(X; Z) (1)
so the best efficient coding satisfies
I(X; Z) = C (2)
where neuronal encoding exploits the whole bandwidth of the channel.
2.2 Rate-distortion theory as goal-oriented efficient coding
Under the classical conception of efficient coding, the exploitation of informa-
tion downstream is ignored. When not all sensory information is needed to guide
behaviour, error-free communication is not expected. This is precisely what is
addressed by the rate-distortion theory, which provides the theoretical founda-
tions for optimal lossy data compression. Formally, the rate-distortion function
defines an optimal lossy compression Z of some data X as the minimization of
their mutual information I(X; Z) given some expected distortion D associated
with reconstructing X from its lossy compression Z. It is defined as [6]
R(D) = min
q(z|x):Dq(x,z)≤D
I(X; Z) (3)
where q is the optimal distribution of z given x that satisfies the expected dis-
tortion constraint and the rate R is an upper bound on the mutual information:
R ≥ I(X; Z) (4)
4 Miguel De Llanza Varona, Christopher Buckley, Beren Millidge
The expected distortion D is defined by some arbitrary loss function (e.g., mean-
squared error) that quantifies the faithfulness of information transmission (i.e.,
how well can the data be recovered from its optimal lossy compression). Lossy
compression sacrifices the capacity to represent all the information in the input
in service of transmitting information that allows adaptive behaviour. Having
a faithfulness criterion allows the brain to efficiently represent the world by
allocating just the necessary amount of resources required to navigate the en-
vironment (Figure 1). Thus, rate-distortion adds a teleological perspective to
efficient coding that shifts the focus from efficient information maximization to
efficient transmission of action-oriented information.
In the lossy regime of the rate-distortion (i.e., all points such thatD >0), the
obtained representations can be understood as abstractions of the data, as their
function is to summarize the relevant properties of the data needed downstream.
In the next section, we provide a mathematical definition of abstractions based
on the intuition that are entities that convey the necessary information to answer
a set of queries about the data. The mathematical formulation of abstractions
is crucial to determine the content of action-centric representations.
Fig. 1: Rate-distortion function for a discrete random variable with four uni-
formly distributed states. Assuming that behavioral objectives are achievable
even when half of the information generated at the source is missing; that is,
when the expected distortion D does not exceed 0.5 (x-axis), an optimal agent
with bounded rationality can rely on a lossy compression scheme and transmit
information at a rate R of 0.20 bits (y-axis).
3 Abstractions and action-centric representations
3.1 Mathematical formalization of abstractions
An abstraction is the reduction of complexity by discarding certain features while
preserving others. As a relational concept, an abstraction involves two compo-
nents: its object (what is being abstracted) and its content (what the abstraction
Action-Centric Representations and Rate-Distortion Theory 5
is about). The content of the abstraction is a summary of the relevant proper-
ties of its object and the relevancy is fixed, ultimately, by the agent’s needs.
Thus, abstractions are intrinsically teleological entities; that is, their meaning or
content is fixed by their function or purpose, which is to transmit information
about the properties of interest for an agent.
Following [16], we address the content of abstractions as the information
necessary to answer a set of queries about the data. A query captures what the
agent wants to know about the data (i.e., what is relevant). Formally, given a
set of queries about the data Q = {Q1, Q2 . . . Q3}, each query is a mapping from
the data distribution to a probability distribution over a subset of elements of
the data Q : X → p(q|x). A good abstraction is one that fulfills its purpose;
namely, one that keeps track of those properties that make it possible to answer
a particular query. Thus, the ‘goodness’ of an abstraction z for a given query
can be defined as:
LQ(x, z) = D[Q(x)||Q(r(z))] (5)
where Q(x) is the query distribution over the true system or data, Q(r(z)) is
the query distribution over a lossy reconstruction of the data r(z) produced by
the abstraction z, and D is an arbitrary divergence function. Without loss of
generality, the ‘goodness’ of an abstraction given a set of queries can be defined
as the weighted loss over all the queries given the abstraction:
L(x, z) =
X
Qi∈Q
p(Qi)LQi(x, z) (6)
Ideally, the mutual information between the abstractions and the query
should be the same as the information transmitted between the data and the
query:
I(X; Q) = I(Q; Z) (7)
The intuition is that a good abstraction Z of the data should reduce the uncer-
tainty of the data X in the same way as the query Q does.
3.2 Abstractions as sufficient and non-superfluous representations
Following [7], an abstract representation Z that captures the relevant details
of the data X to answer a query Q should be sufficient ( I(X; Q|Z) = 0) and
non-superfluous (I(X; Z|Q) = 0):
I(X; Z|Q)| {z }
Superfluous
= I(X; Z) − I(X; Q; Z) (8)
= I(X; Z) − I(X; Q) + I(X; Q|Z)| {z }
Sufficient
(9)
= I(X; Z) − I(X; Q) (10)
6 Miguel De Llanza Varona, Christopher Buckley, Beren Millidge
therefore
 
I(X; Z|Q) = 0

∧
 
I(X; Q|Z) = 0

⇐⇒ I(X; Z) = I(X; Q) (11)
From an information theory perspective, a good abstraction Z only carries the
relevant information in the data; that is, the information necessary to answer a
query. Note that this is a continuum where at one extreme the optimal compres-
sion captures all the information in the data when the query contains the same
information as the data H(Q) = H(X) (an ideal scenario in the efficient coding
hypothesis, where the goal is to maximize mutual information):
I(X; Q) = H(X) − H(X|Q) = H(X) − H(X|X) = H(X) (12)
therefore
I(X; Z) = I(X; Q) (13)
I(X; Z) = H(X) (14)
which corresponds to the lossless compression regime of the rate-distortion func-
tion. On the contrary, when the communication channel is closed, then we recover
the other extreme of the rate-distortion curve, where the mutual information is
zero. This is the case when knowing the query does not reduce the uncertainty
of the data:
I(X; Q) = H(X) − H(X|Q) = H(X) − H(X) = 0 (15)
therefore
I(X; Z) = I(X; Q) (16)
I(X; Z) = 0 (17)
Any other stage in between is a case where the query carries partial information
about the data. Importantly, these information-theoretic entities are implicitly
optimized in rate-distortion theory. On the one hand, sufficient information is re-
lated to predictability and, therefore, to communication fidelity, which is satisfied
when the expected distortion allows for answering the query (i.e., successful be-
haviour). On the other hand, non-superfluous information is related to the min-
imization of the mutual information up to a point in which only query-relevant
information is encoded in the abstraction. Thus, optimal lossy representations,
whose function is to encode the relevant invariances and symmetries in the data,
lie in the rate-distortion curve.
4 Variational Free Energy and Rate-distortion theory
As computing the rate-distortion function is intractable in high-dimensional sys-
tems [6], variational inference can be used as a proxy of the amount of informa-
tion transmitted through a communication channel. In variational inference, a
Action-Centric Representations and Rate-Distortion Theory 7
quantity called variational free energy sets an upper bound on the sensory sur-
prisal (i.e., the entropy of sensory states), and by minimizing it is reduced the
uncertainty about the sensory data allowing for predictability of future states
and adaptive behaviour. One common variational free energy decomposition is
the ELBO, which involves two terms, accuracy and complexity, and is formally
defined as [17]
F =
Z
q(z|x) ln q(z|x)
p(x, z) (18)
F =
Z
q(z|x) ln q(z|x)
p(z) −
Z
q(z|x) lnp(x|z) (19)
F = DKL[q(z|x)||p(z)]| {z }
Complexity
− E
z∼q
[ln p(x|z)]
| {z }
Accuracy
(20)
To model lossy representations of the data, we use VAEs due to its close rela-
tion to variational inference and rate-distortion theory. VAEs is an unsupervised
learning framework that captures the underlying data distribution by using i)
an encoder that learns a latent representation of the data; and ii) a decoder
that generates data-like samples from the latent representation. The objective
function optimized by VAEs is the ELBO, where the complexity term can be
seen as a regularizer applied to the latent space, and the accuracy term as the
faithfulness of the decoder’s reconstruction.
As has been recently shown [1, 12], the ELBO is implicitly optimizing the
rate-distortion function. On the one hand, the expected complexity is an upper
bound on the mutual information I(X; Z) (see Appendix C for full derivation):
E
p(x)

DKL[q(z|x)||p(z)]

≥ I(X; Z) (21)
just as the rate R is in rate-distortion theory. On the other hand, the expected
distortion can be measured using any loss function that captures how faithful the
reconstruction of the decoder resembles the input data (e.g., hamming distance).
In this case, the negative log-likelihood used in VAEs can be used as a distortion
measure between the input and its reconstruction, so D can be defined as:
D = − E
z∼qϕ(z|x)

log pθ(x|z)

(22)
Thus, variational inference can be understood through the lens of rate-distortion
is characterized as
F = − E
z∼qϕ(z|x)

log pθ(x|z)

| {z }
Distortion
+DKL

qϕ(z|x)||p(z)

| {z }
Rate
(23)
8 Miguel De Llanza Varona, Christopher Buckley, Beren Millidge
5 Methods
5.1 Model
Inspired by the utilitarian perspective on the efficient coding hypothesis and
the mathematical foundations of abstractions, we present a modified VAEs to
model action-centric representations (Figure 2). The main novelty of the VAEs
presented here lies in the accuracy term of the free energy (Eq. (20)). Contrary
to vanilla VAEs, where the goal is to learn latent representations of the data to
reconstruct it as faithfully as possible, here we are interested in learning action-
centric representations that convey sufficient and non-superfluous information
about a query. In this model, full reconstruction of the data is not expected.
The final form of the objective function for our action-centric VAEs is:
F = −D[Q(x)||Q(r(z))] + βDKL

qϕ(z|x)||p(z)

(24)
where β is the gradient of the rate with respect to the distortion ∂R
∂D = β and
here it’s used to target specific regimes of the rate-distortion plane [5]. The
accuracy is modified to account for the goodness of the abstraction. The training
f
(a) Algorithmic details of the action-centric
VAEs
(b) Architecture of the action-
centric VAEs
Fig. 2: Action-centric VAEs. Left: algorithmic level of the action-centric VAEs.
Right: a schematic overview of the architecture. The novel component is in the
accuracy term of the ELBO. Instead of measuring the faithfulness of the recon-
struction, it is measured how good is the reconstruction for a specific task, which
in this case is an image classifier.
pipeline is as follows. First, we define a query to be the discrimination of the
ten classes of the FASHION-MNIST dataset. We first trained a classifier, using a
CNN, on the task specified by the query (i.e., multiclass classification). Once the
discriminator is trained we trained both the vanilla VAEs and our action-centric
VAEs. Importantly, both VAEs have the same channel capacity, as they share the
Action-Centric Representations and Rate-Distortion Theory 9
same architecture, so the maximum achievable rate in both models is the same.
The crucial difference is that our VAEs is not trained to fully reconstruct the data
but to generate reconstructions that can be well-classified by the discriminator.
By doing this divergence measure, we can evaluate the goodness of the abstract
representations for the given query. To compute the rate-distortion function, we
trained several VAEs using different β to study the rate-distortion trade-off in
different regimes and the potential differences between vanilla VAEs and our
model.
Using this model we can investigate whether the latent space can efficiently
encode just the relevant invariances and symmetries required for the downstream
task without the need to generate faithful reconstructions of the data. If that is
the case, full reconstruction no longer becomes a necessary condition for goal-
oriented representations. In the next section, we present the main results and
their connection to the theoretical framework presented previously.
5.2 Results
(a) Accuracy as a function
of the number of steps.
(b) Fully reconstruction
objective.
(c) Action-centric objec-
tive.
Fig. 3: Rate-distortion curve made up of different VAEs. Note that as a distortion
measure here we use the accuracy. Left figure: vanilla VAEs that try to maxi-
mize the mutual information given the channel constraints. Right figure: lossy
compression VAEs whose function is to maximize utility downstream given the
channel constraints.
The results regarding the transmission of information in the two different
VAEs are shown in Figure 3. In (Figure 3a) it can be seen how action-oriented
VAEs converges faster to an encoding-decoding scheme that is useful for the
downstream task (measured by the accuracy), compared to the VAEs. This in-
dicates that action-centric representations might require less exposure to data,
which makes them more efficient in terms of exploiting the available information.
Figure 3b and Figure 3c show the rate-distortion curve for both types of
VAEs. It is clear how action-oriented representations require significantly less
information from the data to achieve better results in the downstream task. In
10 Miguel De Llanza Varona, Christopher Buckley, Beren Millidge
particular, it can be seen that transmitting at a rate of around 10 bits the action-
centric VAEs reaches almost 85% of accuracy, compared to the 67% achieved
by the vanilla VAEs at approximately the same rate. This suggests that lossy
compression leads to efficient codings and, importantly, to better behaviour.
The results so far indicate that the main function of representations might
not be to fully reconstruct the data, but not capture the relevant invariances in
the data exploited by optimal behaviour. We explicitly show this by investigating
the reconstructions obtained by action-oriented representations. Figure 4 shows
a sample of the reconstructions obtained by the vanilla and action-centric VAEs,
respectively. While the vanilla VAEs generates relatively faithful reconstructions
of the data, the action-centric VAEs generates meaningless and uninterpretable
images. Interestingly, these action-oriented reconstructions are classified with
approximately 85% of accuracy, which suggests that the underlying structure of
these reconstructions is preserving some important invariances and symmetries
of the data. On the contrary, the full reconstruction might carry irrelevant infor-
mation that is non-task specific, which could explain why they are more difficult
to classify.
Fig. 4: Data reconstruction by VAEs and action-centric VAEs.
6 Discussion
Agents need to navigate complex environments with limited biological informa-
tion processing. Under this circumstance, an optimal perceptual system has to
efficiently allocate cognitive resources to transmit the relevant sensory informa-
tion to achieve successful behaviour. Thus, the goal of perception is not to gen-
erate faithful reconstructions of the sensory input, but abstract representations
that are useful downstream.
A common approach to representations in Artificial Intelligence and Neuro-
science is that they should be in service of fully reconstructing the data. However,
such representations will carry irrelevant information for downstream tasks that
only depend on the exploitation of specific invariances and symmetries of the
data.
Action-Centric Representations and Rate-Distortion Theory 11
In this work, we explore useful efficient coding within the framework of rate-
distortion to explore optimal information processing for task-dependent con-
texts. We have provided a formal definition of abstractions that can be used
to learn action-centric representations whose main function is to capture the
task-dependant invariances in the data. Such lossy compressions of the data lie
near optimal points of the rate-distortion curve. Crucially, we show that action-
centric representations i) are efficient lossy compressions of the data; ii) capture
the task-dependent invariances necessary to achieve adaptive behaviour; and iii)
are not in service of reconstructing the data. This could shed some light on how
organisms are not optimized to reconstruct their environment; instead, their
representational system is tuned to convey action-relevant information.
Interestingly, our work resonates with recent research on multimodal learning
such as the joint embedding predictive architecture and multiview systems [2,
7]. The main objective of these models is to obtain representations that are
useful downstream but from which it’s not possible to reconstruct the data.
These representations learn the relevant invariances by maximizing only the
information shared across different views or modalities of the data. We argue that
action-centric representations operate in a similar way, as shared information
across views is an implicit way to define a query (see Appendix D).
An interesting line of research is to explore faithful reconstruction in the
context of fine-grained queries such as pixel predictability. We hypothesize that,
as the number of pixel-specific queries approaches the pixel space of the image,
the abstract representation might allow for faithful reconstruction of the data.
Although that could be to the detriment of worse performance on downstream
tasks.
In conclusion, this work sets a promising line of research in the field of repre-
sentational theory by understanding representations not as faithful reconstruc-
tions of the data but as action-driven entities.
References
1. Alemi, A., Poole, B., Fischer, I., Dillon, J., Saurous, R.A., Murphy, K.: Fixing a
broken elbo. In International conference on machine learning pp. 159–168 (2018)
2. Bardes, A., Ponce, J., LeCun, Y.: Vicreg: Variance-invariance-covariance regular-
ization for self-supervised learning. arXiv preprint arXiv:2105.04906. (2021)
3. Barlow, H.B.: Possible principles underlying the transformation of sensory mes-
sages. Sensory communication 1(01), 217–233 (1961)
4. Buckley, C.L., Kim, C.S., McGregor, S., Seth, A.K.: The free energy principle for
action and perception: A mathematical review. Journal of Mathematical Psychol-
ogy 81, 55–79 (2017)
5. Burgess, C.P., Higgins, I., Pal, A., Matthey, L., Watters, N., Desjardins, G., Ler-
chner, A.: Understanding disentangling in β-vae. arXiv preprint arXiv:1804.03599
(2018)
6. Cover, T., Thomas, J.: Elements of Information Theory. New York: Wiley. (2006)
7. Federici, M., Dutta, A., Forr´ e, P., Kushman, N., Akata, Z.: Learning robust repre-
sentations via multi-view information bottleneck. arXiv preprint arXiv:2002.07017
(2020)
12 Miguel De Llanza Varona, Christopher Buckley, Beren Millidge
8. Friston, K.: The free-energy principle: a rough guide to the brain?.. Trends in
cognitive sciences 13(07), 293–301 (2009)
9. Friston, K.: The free-energy principle: a unified brain theory? Nature reviews neu-
roscience 11(2), 127–138 (2010)
10. Friston, K.: A free energy principle for biological systems. Entropy 14(11), 2100–
2121 (2012)
11. Genewein, T., Leibfried, F., Grau-Moya, J., Braun, D.A.: Bounded rationality,
abstraction, and hierarchical decision-making: An information-theoretic optimality
principle. Frontiers in Robotics and AI 2, 27 (2015)
12. Hoffman, M.D., Johnson, M.J.: Elbo surgery: yet another way to carve up the vari-
ational evidence lower bound. In Workshop in Advances in Approximate Bayesian
Inference, NIPS 1(2) (2016)
13. Laughlin, S.: A simple coding procedure enhances a neuron’s information capacity.
Zeitschrift f¨ ur Naturforschung c36(9-10), 910–912 (1981)
14. Lieder, F., Griffiths, T.L.: Resource rational analysis: Understanding human cog-
nition as the optimal use of limited computational resources. Behavioral and Brain
Sciences 47 (2020)
15. Manookin, M.B., Rieke, F.: Two sides of the same coin: Efficient and predictive
neural coding. Annual Review of Vision Science (9) (2023)
16. Millidge, B.: Towards a mathematical theory of abstraction. arXiv preprint
arXiv:2106.01826. (2021)
17. Millidge, B., Seth, A., Buckley, C.L.: Predictive coding: a theoretical and experi-
mental review. arXiv preprint arXiv:2107.12979. (2021)
18. Park, I.M., Pillow, J.W.: Bayesian efficient coding. BioRxiv 178418 (2017)
19. Shannon, C.E.: Coding theorems for a discrete source with a fidelity criterion. IRE
Nat. Conv. 4(142-163) (1959)
20. Sims, C.R.: Rate–distortion theory and human perception. Cognition 152(46),
181–198 (2016)
21. Tschantz, A., Seth, A.K., Buckley, C.L.: Learning action-oriented models through
active inference. PLoS computational biology 16(4) (2020)
22. de Wit, M.M., de Vries, S., van der Kamp, J., Withagen, R.: Affordances and
neuroscience: Steps towards a successful marriage. Neuroscience and Biobehavioral
Reviews 80, 622–629 (2017)
23. Zhou, D., et al.: Efficient coding in the economics of human brain connectomics.
Network Neuroscience 6(1), 234–274 (2022)
Action-Centric Representations and Rate-Distortion Theory 13
A Model details
The classifier used to implement the query is a deep convolutional network
(CNN) with three convolutional layers. The number of filters for the first layer is
16, and it is doubled in each layer. The kernel size is 3 in all layers, and padding
is set to 1, also in all layers. Stride is 1 in the first two layers, and 2 in the third
one. In addition, batch normalization is applied in each layer; 16 for the first one,
and doubled in each layer. The activation function in each layer is ReLU, and
max pooling is applied in the first two layers, both with a kernel size of 2, and
stride of 2 in the first and 1 in the second. Between the first two fully connected
layers it is used a dropout of 0.2. The number of neurons for the fully connected
layers is 512, 128, and 10. We use the Adam optimizer with a learning rate of
0.001. We trained the classifier for 15 epochs with a batch size of 64.
Regarding the VAEs, the encoder is a CNN of 4 layers with the same param-
eters as the CNN. Every VAEs trained has 8 latent dimensions and are trained
for 20 epochs using a batch size of 64. In the case of the vanilla VAEs, theβ used
to draw the rate-distortion curve are 100, 40, 20, 10, 5, 1, 0.5, 0.1, and 0.01. For
the custom VAEs, the β values are 6e-2, 3e-2, 1e-2, 6e-3, 3e-3, 1e-3, 5e-4, 1e-4,
1e-5, 1e-6.
B Latent space of VAEs
PCA to explore and show the latent space of the vanilla and action-centric VAEs
that achieve a good performance downstream:
(a) Vanilla VAEs (β = 0.5)
 (b) Action-centric VAEs (β = 1e − 4)
Fig. 5: Latent spaces of the two types of VAEs.
As can be seen, our VAEs achieves a compact meaningful encoding of the
data, with an apparent better separability among classes than the vanilla VAEs.
14 Miguel De Llanza Varona, Christopher Buckley, Beren Millidge
C ELBO and RDT
One way to derive the upper bound on mutual information from the complexity
term of the ELBO is:
E
p(x)

DKL[q(z|x)||p(z)]

= E
p(x)
Z
q(z|x) ln q(z|x)
p(z) dxdz

(25)
= E
p(x)
Z
q(z|x) ln q(z|x)q(z)
p(z)q(z) dxdz

(26)
= E
p(x)
Z
q(z|x) ln q(z|x)
q(z) dxdz +
Z
q(z|x) ln q(z)
p(z)dxdz

(27)
=
Z
q(z|x)q(x) ln q(z|x)
q(z) dxdz +
Z
q(z|x)q(x) ln q(z)
p(z)dxdz
(28)
=
Z
q(x, z) ln q(x, z)
q(x)q(z)dxdz +
Z
q(z) ln q(z)
p(z)dz (29)
= I(X; Z) + DKL[q(z)||p(z)] (30)
≥ I(X; Z) (31)
Another way to derive this upper bound is by splitting the expected com-
plexity into conditional entropy and entropy terms:
E
p(x)

DKL[q(z|x)||p(z)]

= E
p(x)
Z
q(z|x) ln q(z|x)
p(z) dxdz

(32)
=
Z
q(z|x)q(x) lnq(z|x)dxdz −
Z
q(z|x)q(x) lnp(z)dxdz
(33)
=
Z
q(x, z) lnq(z|x)dxdz −
Z
q(z) lnp(z)dz (34)
In the last equation, we can see that the first term is the negative conditional
entropy −H(Z|X) which is one of the two terms in which the mutual information
is decomposed: I(Z; X) = H(Z) − H(Z|X). To get the entropy H(Z) we need
to replace p(z) by an approximate distribution q(z). By Jensen’s inequality, we
know that DKL[q(z)||p(z)] ≥ 0, therefore, we know that:
Z
q(z) lnq(z) −
Z
q(z) lnp(z) ≥ 0 (35)
Z
q(z) lnq(z) ≥
Z
q(z) lnp(z) (36)
Replacing that term in the previous expression (34) we get:
Action-Centric Representations and Rate-Distortion Theory 15
Z
q(x, z) lnq(z|x)dxdz −
Z
q(z) lnp(z)dz ≥
Z
q(x, z) lnq(z|x)dxdz −
Z
q(z) lnq(z)dz
(37)
Z
q(x, z) lnq(z|x)dxdz −
Z
q(z, x) lnq(z)dxdz ≥
Z
q(x, z) ln q(x, z)
q(x)q(z)dxdz = I(X; Z)
(38)
D Multiview architecures and queries
Given a query Q(X) over X in a multiview scenario it can be understood as the
subset of information contained in the intersection of X and t(X) such that:
Q(X) ∈ X ∩ t(X) (39)
as the transformation t only preserves those symmetries relevant for the query
(i.e., relevant to solve a set of tasks that only depend on those invariances).
Therefore, the relevant query in a multiview scenario can be defined as:
Q(X) = p(X, t(X)) (40)
Mutual information between X and Z and between Q(X) and Z is (assuming
that X, X’ and Z form a dag where Z only depends on X):
I(X; Z) =
Z
p(x, z) ln p(x, z)
p(x)p(z)dxdz (41)
I(Q(X); Z) =
Z
p(q, z) ln p(q, z)
p(q)p(z)dqdz (42)
=
Z
p(x, x′, z) ln p(x, x′, z)
p(x, x′)p(z)dxdx′dz (43)
=
Z
p(x, x′, z) ln p(x′)p(x|x′)p(z|x)
p(x′)p(x|x′)p(z) dxdx′dz (44)
=
Z
p(x, x′, z) ln p(z|x)
p(z) dxdx′dz (45)
=
Z
p(x, x′)p(z|x) ln p(x, z)
p(x)p(z)dxdx′dz (46)
=
Z
p(x)p(x, z)
p(x) ln p(x, z)
p(x)p(z)dxdz (47)
=
Z
p(x, z) ln p(x, z)
p(x)p(z)dxdz (48)
= I(X; Z) = I(X; X′) (49)
16 Miguel De Llanza Varona, Christopher Buckley, Beren Millidge
The mutual information between the latentZ and one of the views X is equal
to the mutual information between the query distribution Q(X) and the latent
Z. As the mutual information between an optimal lossy representation and its
corresponding view is equal to the mutual information between views, then, the
information conveyed by the query is the one shared by the views. This shows
that the multiview architecture is essentially a query-oriented system where the
transformations applied to the data keep specific invariances with respect to a
set of implicit queries of interest.