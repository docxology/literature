This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
3
The High Road to Active Inference
Survival machines that can simulate the future are one jump ahead of survival
machines who can only learn on the basis of overt trial and error. The troub le
with overt trial is that it takes time and energy. The trou ble with overt error is that
it is often fatal. Simulation is both safer and faster.
— Richard Dawkins
3.1 Introduction
In chapter 2, we motivated the introduction of f ree energy as a means of
performing approximate Bayesian inference (i.e., the low road to Active
Inference). H ere, we introduce f ree energy from another perspective, that
of the high road, which inverts that reasoning: it starts from first princi ples
in statistical physics and the central imperative that organisms must main-
tain their existence—t hat is, avoid surprising states—a nd then introduces
the minimization of f ree energy as a computationally tractable solution to
this probl em. The chapter discloses the formal equivalence between the
minimization of variational f ree energy and the maximization of model
evidence (or self-e videncing) in approximate Bayesian inference, revealing
a connection between free energy and Bayesian perspectives on adaptive
systems. Fin ally, it discusses how Active Inference provides a novel first
princip le perspective to understand (optimal) beh avi or.
Active Inference is a theory of how living organisms maintain their exis-
tence by minimizing surprise—or a tractable proxy to surprise, variational
free energy—v ia perception and action. By starting from first princip les, it
advances a novel belief-b ased scheme to understand beh avi or and cogni-
tion, which has numerous empirical implications.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
42 Chapter 3
The high road to Active Inference starts from the premise that, to sur-
vive, any living organism has to maintain itself in a suitable set of preferred
states, while avoiding other, dis-p referred states of the environment. These
preferred states are first and foremost defined by niche-s pecific evolution-
ary adaptations. However, as we w ill see l ater, in advanced organisms t hese
can also extend to learned cognitive goals. For example, to survive, a fish
has to stay in a comfort zone that corresponds to a small subset of all the
poss ib le states of the universe: it has to stay in w ater. Similarly, a h uman
has to ensure that their internal states (e.g., physiological variables like
body temperature and heart rate) always remain within acceptable ranges—
other wise they w ill die (or more precisely will become something else, such
as a corpse). This acceptable range or comfort zone stipulatively defines the
characteristic states something has to be in to be that t hing.
Living organisms resolve this fundamental biological probl em by exert-
ing active control over their states (e.g., of body temperature) at many levels,
which range from automatic regulatory mechanisms such as sweating (physi-
ology) to cognitive mechanisms such as buying and consuming a drink (psy-
cholo gy) to cultural practices such as distributing air conditioning systems
(social sciences).
From a more formal perspective, Active Inference casts the biological
probl em of—or explanation for—s urvival as surprise minimization. This
formulation rests on a technical definition of surprising states from informa-
tion theory—e ssentially, surprising states index t hose outside the comfort
zone of living organisms. It then proposes f ree energy minimization as a
practical and biologically grounded way for organisms or adaptive systems
to minimize the surprise of sensory encounters.
3.2 Markov Blankets
An import ant precondition for any adaptive system is that it must enjoy
some separation and autonomy from the environment—w ithout which it
would simply dissipate, dissolve, and thereby succumb to environmental
dynamics. In the absence of this separation, t here would be no surprise to
minimize; t here must be something to be surprised and something to be
surprised about. In other words, t here are at least two t hings—s ystem and
environment—a nd t hese can be disambiguated from one another. A formal
way to express a separation between a system and the rest of the environ-
ment is the statistical construct of a Markov blanket (Pearl 1988); see box 3.1.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
The High Road to Active Inference 43
Box 3.1
Markov blankets
A Markov blanket is an import ant recurring concept in this book (Friston
2019a, Kirchhoff et al. 2018, Palacios et al. 2020). Technically, a blanket (b) is
defined as follows:
µ⊥x|b⇔p(µ,x|b)=p(µ|b)p(x|b)
This says (in two diff ere nt but equivalent ways) that a variable μ is condition-
ally ind ep end ent of a variable x if b is known. In other words, if we know
b, knowing x would give us no additional information about μ. A common
example of this is a Markov chain, where the past c auses the pres ent c auses the
future. In this scenario, the past may only influence the future via the pres ent.
This means no additional information about the f uture is gained by finding
out about the past (assuming we know the pres ent).
To identify a Markov blanket in a system wherein we know the conditional
dependencies, we can follow a s imple rule. The blanket for a given variable
comprises its parents (the variables it depends on), its children (the variables
that depend on it) and, in some settings, the other parents of its c hildren.
In brief, a Markov blanket is the set of variables that mediate all (statistical)
interactions between a system and its environment. Figure 3.1 illustrates an
interpretation of a Markov blanket in a dynamic setting. H ere the conditional
ind ep end ences have been supplemented with dynamical constraints, so that
the flows do not depend upon states on the opposite side of the blanket.
The Markov blanket in figure 3.1 distinguishes states internal to the adap-
tive system (i.e., brain activity) from external states of the environment.
Furthermore, it identifies two additional states, labeled sensory states and
active states, which form the blanket that (statistically) separates internal and
external states. Statistical separation means that if we knew about the active
and sensory states, the external states would offer no additional information
about internal states (and vice versa). In a dynamical setting, this is often
interpreted as saying internal states cannot directly change external states
but can do so vicariously by changing active states. Similarly, external states
cannot directly change internal states but can do so indirectly by changing
sensory states.
This is a restatement of the classical action-p erception cycle, wherein an
adaptive system and its environment can interact (only) through actions
and observations, respectively. This reformulation has two main benefits.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
44 Chapter 3
Active states
u· = fu (x, u, y) + ωu
External states Internal states
x· = fx (x, u, y) + ωx µ· = fµ (µ, u, y) + ωµ
Sensory states
y· = fy (µ, u, y) + ωy
b = (u, y)
Blanket states
Figure 3.1
A dynamic Markov blanket, which separates an adaptive system (h ere, the brain)
from the environment. The dynamics of each set of states are determined by a
deterministic flow specified as a function ( f ) giving the average rate of change and
additional stochastic (random) fluctuations (ω ). The arrows indicate the direction of
influence of each variable over the rates of change of other variables (technically, the
nonzero elem ents of the associated Jacobians). This is just one example; one can use
a Markov blanket to separate an entire organism from the environment or nest mul-
tiple Markov blankets within one another. For example, brains, organisms, dyads,
and communities can be conceived in terms of diff ere nt Markov blankets that are
nested within one another (see Friston 2019a; Parr, Da Costa, and Friston 2020 for a
formal treatment). Confusingly, diff ere nt fields use diff ere nt notations for the vari-
ables; sometimes, sensory states are denoted s, external states η, and active states a.
Here we have chosen variables for consistency with the other chapters in this book.
First, it formalizes the fact that an adaptive system’s internal states are
autonomous from environmental dynamics and can therefore resist their
influences. Second, it scaffolds the way in which adaptive systems mini-
mize their surprise: it highlights the internal, sensory, and active states they
have access to. Specifically, surprise is defined in relation to sensory states,
while internal and active state dynamics are the means by which the sur-
prise of sensory states may be minimized.
The key point to notice h ere is that the internal states of an adaptive
system bear a formal relation to external states. This is due to a kind of sym-
metry across the Markov blanket as both influence and are influenced by
blanket states. A consequence of this is that we can construct conditional
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
The High Road to Active Inference 45
probability distributions for the internal and external states, given the blan-
ket states. B ecause t hese are conditioned on the same blanket states, we can
associate pairs of expected internal and external states with one another. In
other words, on average, the internal and external states acquire a kind of
(generalized) synchrony—j ust as we might anticipate on attaching a pen-
dulum to each end of a wooden beam. Over time, as they synchronize, each
pendulum becomes predictive of the other through the vicarious influ-
ence of the beam (Huygens 1673). Figure 3.2 offers a graphical intuition
for this relationship. This means that if we can write down ind ep end ent
60
40
20
–20
–40
–10 –5 0
b
x
150
100
50
0
–50
–100
5 10 –10 –5 0
b
µ
x | b µ | b
0
5 10
60
40
20
–20
–40
–100 –50 0 –8
µ
ytilibaborP
–7 –6 –5 –4
x
x
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
x | [µ]
P(µ|b)
0
50 100
Figure 3.2
Association between average internal states of a Markov blanket and distributions
of external states. Top: Assuming a linear Gaussian form for the conditional prob-
abilities, t hese plots show samples from the conditional distribution over external
and internal states, respectively, given blanket states. The thick black lines indicate
the average of these variables given the associated blanket state. Bottom left: The
same data are plotted to illustrate the synchronization of internal and external states
afforded by sharing a Markov blanket—h ere, an inverse synchronization. The dashed
lines and black cross illustrate that if we knew the average internal state (vertical
line), we could identify the average external state (horizontal line) and the spread
around this point. Bottom right: We can associate the average internal state with a
distribution over the external state.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
46 Chapter 3
distributions over external and internal states given their Markov blanket,
the two states become informative about one another via this blanket.
This synchrony gives internal states the appearance of representing (or
modeling) external states—w hich links back to the idea of surprise mini-
mization introduced in chapter 2. This is b ecause surprise depends on an
internal model of how sensory data are generated. To recap, minimizing
the surprise (negative log probability) of sensory observations becomes
identical to maximizing the evidence (marginal likelihood) for the model,
which is just the probability of sensory observations under that model. This
notion of surprise minimization can be understood from two equivalent—
Bayesian and f ree energy—p erspectives, which we discuss next.
3.3 Surprise Minimization and Self-E videncing
Under a Bayesian perspective, an agent with a Markov blanket appears to
model the external environment in the sense that internal states correspond
(on average) to a probabilistic representation—an approximate posterior
belief—of external states of the system (figure 3.2). The dynamics of internal
states correspond to a form of (approximate) Bayesian inference of exter-
nal states, as their motion changes the associated probability distribution,
which is afforded by an implicit generative model of how sensations (or
sensory states in the Markov blanket jargon) are generated. If we reinstate
the notion of an agent as constituted by internal and blanket states, we can
talk about an agent’s generative model.
Importantly, the agent’s generative model cannot simply mimic exter-
nal dynamics (other wise the agent would simply follow external dissipative
dynamics). Rather, the model must also specify the preferred conditions
for the agent’s existence, or the regions of states that the agent has to visit
to maintain its existence, or satisfy the criteria for its existence in terms
of occupying characteristic states. T hese preferred states (or observations)
can be specified as the priors of the model—w hich implies that the model
implicitly assumes that its preferred (prior) sensations are more likely to
occur (i.e., are less surprising) if it satisfies the criteria for existence. This
means it has an implicit optimism bias. This optimism bias is necessary for
the agent to go beyond the mere duplication of external dynamics to pre-
scribe active states that underwrite its preferred or characteristic states.
Under this formulation, one can cast optimal beh avi or (with res pect to
prior preferences) as the maximization of model evidence by perception and
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
The High Road to Active Inference 47
action. Indeed, model evidence summarizes how well the generative model
fits or explains sensations. A good fit indicates that the model success-
fully accounts for its sensations (this is the descriptive side of inference); at
the same time, it realizes its preferred sensations, given that they are less
surprising (this is the prescriptive side of the inference). Such good fit is a
guarantee of surprise minimization, as maximizing model evidence P( y) is
mathematically equivalent to minimizing surprise: ℑ( y) = −ln P( y).
A way to reformulate the above arguments more succinctly consists in say-
ing that any adaptive system engages in “self-e videncing” (Hohwy 2016). Self-
evidencing h ere means acting to garner sensory data consistent with (i.e., that
affords evidence to) an internal model, hence maximizing model evidence.
3.3.1 Surprise Minimization as a Hamiltonian Princip le of Least Action
In the preceding sections, we have asserted that surprise must be minimized
but have not detailed why this is. Although the details of the underl ying phys-
ics of self-e videncing are outside the scope of this book (see Friston 2019b for
details), we h ere provide a brief overview of the princip les. T hese are under-
written by the idea that biological creatures—w ith Markov blankets—p ersist
over time, resisting the dispersive effects of environmental fluctuations. The
pers ist ence of a Markov blanket implies that the distribution of blanket
states remains constant over time. Simply put, this means that any deviation
of sensory (or active) states from regions that are highly probable u nder this
distribution must be corrected by the average flow of states (which is just
the deterministic part of the flow in figure 3.1). Expressing this as a physicist
might, stochastic (random) systems at steady state engage in dynamics that
(on average) descend an energy function (or Hamiltonian) that is interpre-
table as a negative log evidence or surprise. This is like a ball rolling down
a hill from high gravitational potential energy at the top of the hill to low
energy in a basin. See figure 3.3.
For the system shown on the left of figure 3.3, e very time a fluctua-
tion c auses a move to a less probable state, this is corrected by a move up
the probability gradient, such that the system occupies probability- dense
regions a greater proportion of the time. The key insight h ere is that this
system maintains sensory states within a narrow range by minimizing sur-
prise (on average)—in contrast to the system on the right, for which surprise
grows in def initely.
Surprise minimization permits living organisms to (temporarily) resist
the second law of thermodynamics, which states that entropy—or the
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
48 Chapter 3
y(t) y(t) (y)
10 20 10
5
5 10
0
0
0
–5
–5
–10 –10
–10 0 10 –20 0 20 –10 0 10
dispersion of systemic states—a lways grows. This is b ecause, on average,
entropy is the long-t erm average of surprise and, on average, the maximiza-
tion of a log probability of observations is equivalent to minimization of
(Shannon) entropy:2
H[P(y)]=EP(y) [ℑ(y)]=−EP(y) [lnP(y)] (3.1)
Ensuring that a small proportion of sensory states is occupied with
high probability is equivalent to maintaining a part icu l ar entropy. This is
a defining characteristic of self-o rganizing systems, as long recognized by
cybernetic theories.
From a physiologist’s perspective, surprise minimization formalizes the
idea of homeostasis. As a sensor value leaves its optimal range, negative
feedback mechanisms kick in that reverse t hese deviations. From a control
perspective, we can interpret optimal beh avi or in relation to some desired
steady state probability density. In other words, if we define a distribution
of preferred outcomes, optimal beh avi or w ill involve evolution of the sys-
tem t oward—a nd maintenance of—t hat distribution.
As we saw in chapter 2, f ree energy is an upper bound on surprise, sug-
gesting that optimal beh avi or can be obtained by minimizing free energy
y 2
y y y
1 1 1
y 2 y 2
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Figure 3.3
Left: Path taken by a 2-d imensional random dynamical system with a (nonequilib-
rium1) steady state. This can be interpreted as minimizing its surprise, which is shown
in the contour plot on the right. Right: The center is the least surprising region; the
circles moving away from the center represent progressively more surprising regions.
Middle: In contrast, this plot shows the trajectory of a system starting in the same place
(5, 5), with random fluctuations of the same amplitude, whose dynamics bear no rela-
tion to surprise. Not only does it enter more surprising regions of space; it also fails to
achieve any sort of steady state, dissipating in an unconstrained fashion over time. The
scope of Active Inference is restricted to systems like that on the left—w hich c ounter
random fluctuations with their average flow and thereby retain their form over time.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
The High Road to Active Inference 49
in the face of random fluctuations. Recall that the difference between free
energy and surprise is the divergence between an exact posterior proba-
bility (i.e., the distribution of external states given blanket states) and an
approximate posterior probability (i.e., the distribution over external states
given average internal states). As such, the motion of internal states can be
thought of as minimizing the divergence, which then enables active states,
on average, to minimize the surprise accompanying sensory states. In other
words, the optimal beh avi or resulting from f ree energy minimization is the
one that is least surprising and follows a path of least Action3 from the cur-
rent state to the desired state—t hat is, the Hamiltonian princip le of least
Action applied to beh avi or.
Figure 3.3 shows a very s imple example of a system equipped with a ran-
dom attractor. This is analogous to a thermostat, which (in cybernetic par-
lance) has a single set-p oint and cannot learn or plan. Active Inference aims
to use the same explanatory apparatus to cover much more complex and
adaptive systems. Here, the difference between simplest and more complex
systems can be reduced to the diff ere nt shapes of their attractors—f rom
fixed points to increasingly more complex and itinerant dynamics. From
this perspective, one can understand living organisms as constantly seeking
a compromise between excessive stability and excessive dispersion—a nd
Active Inference aims to explain how such compromise is achieved.
3.4 Relations between Inference, Cognition, and Stochastic Dynamics
The physicist E. T. Jaynes famously argued that inference, information
theory, and statistical physics are diff ere nt perspectives on the same thing
( Jaynes 1957). In the previous sections, we discussed how Bayesian and
statistical physics perspectives offer two equivalent ways to understand
surprise minimization and optimal beh avi or—e ffectively adding a form of
cognition to Jaynes’s triad. This equivalence between vario us schools of
thought is appealing but can be confusing to t hose who are not familiar
with the respective formalisms, where many diff ere nt words are used to
refer to the same quantities. To help demystify this, in this section we elab-
orate on the main equivalences between Bayesian and statistical physics
perspectives and their cognitive interpretations; see t able 3.1 for a summary
and box 3.2.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
50 Chapter 3
Table 3.1
Statistical physics, Bayesian inference, and information theory— and their cognitive
interpretations
Bayesian inference and
Statistical physics information theory Cognitive interpretation
Minimize variational f ree Maximize model evidence Perception and action
energy (or marginal likelihood);
minimize surprisal
(or self- information)
Minimize expected free Infer the most likely Planning as inference
energy; Hamiltonian (or less surprising) course
princi ple of least Action of action
Attain nonequilibrium Perform approximate Self- evidencing
steady- state Bayesian inference
Gradient flows on energy Gradient ascent on Neuronal dynamics
functions; gradient model evidence; gradient
descent on free energy descent on surprisal
Box 3.2
Free energy in statistical physics and Active Inference
The notion of free energy is widely used in statistical physics to character-
ize (for example) thermodynamic systems. Although Active Inference uses
exactly the same equations, it applies them to characterize the belief state
of an agent (in relation to a generative model). Hence, when we talk of an
Active Inference agent minimizing its (variational) free energy, we are refer-
ring to pro cesses that change its belief state, not (for example) the particles of
its body. To avoid misunderstandings, we use the term variational free energy,
hence adopting a terminology that is more common in machine learning.
Another more subtle point is that the concept of free energy is often used
in the context of equilibrium statistical thermodynamics. Active Inference
targets living organisms—or nonequilibrium steady state systems that are
open— that feature continuous, reciprocal exchanges with the environment.
This is an exciting novel field (Friston 2019a).
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
The High Road to Active Inference 51
3.4.1 Variational F ree Energy, Model Evidence, and Surprise
A first import ant equivalence is between the maximization of model evidence
(or marginal likelihood) in Bayesian inference and the minimization of
variational f ree energy—b oth of which minimize surprise. This equivalence
becomes evident when one appeals to a specific approximate solution to
intractable probl ems of inference—v ariational inference. Variational infer-
ence recasts the inference probl em as an optimization probl em by minimiz-
ing f ree energy. The minimum of the free energy is the point at which the
approximation of the exact solution is at its best. Expressing this formally
sheds light on the relations between the three quantities:
ℑ(y|m)=−lnP(y|m) ≤ D [Q(x)||P(x|y,m)]−lnP(y|m)
!#"#$ !#"#$ !K#L######"#######$ (3.2)
Surprise Model evidence Variational free energy
In equation 3.2, unlike in chapter 2, we have explici tly conditioned all
quantities on a model, m, to emphasize that t hese depend on the model we
have (or are) about how y are generated, and the quantities w ill vary if diff er-
ent models are used. The equivalence of t hese quantities raises the question
as to why it is useful to distinguish between them. The main reason is that,
unlike model evidence, variational f ree energy can be minimized efficiently.
Recall from chapter 2 that the variational f ree energy is only exactly equiv-
alent to the negative model evidence or surprise when the KL-D ivergence
term becomes zero. This is not always poss ib le, but this can be made close
to zero. Hence, in the proc ess of finding better and better values for Q (x),
variational free energy also approximates surprise more closely. We have said
this a few times already b ecause it is import ant to emphasize the central
relationship between f ree energy and surprise that is the foundation of this
book. Specifically, f ree energy is an upper bound on surprise. It can be the
same as or greater than surprise—w here what is greater than is quantified by
the KL- Divergence.
An int ere sti ng aspect of this is that any system minimizing its sur-
prise, including the very s imple system in figure 3.2, is also minimizing
a f ree energy, where the Q (x) is always set to be equal to the exact pos-
terior probability—t hat is, setting the KL-D ivergence to be zero. One per-
spective on the difference between cognitive and noncognitive systems is
that the latter always have a zero KL-D ivergence, while cognitive systems
must go through the (perceptual) proc ess of minimizing this term before
their actions are guaranteed to minimize surprise. Note that minimizing
the divergence is the only t hing that perception can do. This places a g reat
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
52 Chapter 3
deal of emphasis on the motion of internal states, such that the distribution
they par ame t erize (figure 3.2) is as close to the exact posterior as poss ib le.
However, perception cannot minimize the second (evidence) component
of variational f ree energy that corresponds to the a ctual surprise, b ecause it
cannot change the sensations that have been gathered. Only by acting in
ways that change sensations can an agent minimize the second (evidence)
component of variational f ree energy and resolve its surprise—or, equiva-
lently, maximize its model evidence. This places emphasis on the motion
of active states, given internal states, in self-e videncing.
An example helps in illustrating this point. Imagine that your generative
model predicts a distribution of glucose levels in your blood given levels of
hunger, with relatively high versus low glucose levels relating to satiation
and hunger, respectively. In addition, imagine this model ascribes a higher
prior probability to satiation and therefore to relatively high glucose levels—
making low glucose levels surprising. Imagine you are initially uncertain
about your hunger levels and sense low blood glucose. Perception leads to
the inference that you are hungry and the experience of hunger—c losing
the KL-D ivergence. However, perception cannot go further than that to
reduce your surprise—a nd the discrepancy between the high level of glucose
that you expect a priori and the low level of glucose that you sense—b ecause
it cannot act on your sensations (low glucose) or their c auses (physiology).
You can only minimize your surprise by acting to change (the hidden source
of ) the sensations you gather—f or example, by eating a dessert.
In sum, perception can minimize variational f ree energy by reducing
the discrepancy between approximate and true posterior but cannot go fur-
ther in minimizing surprise. The next step of surprise minimization entails
changing the sensations one gathers by acting, which is where inference
goes beyond perception and becomes active.
3.4.2 Expected F ree Energy and Inference of the Most Likely Trajectory
Another import ant equivalence is between the minimization of expected
free energy and inferring the most likely course of action, or policy. This goes
beyond specifying the least surprising part of state-s pace and deals with
how surprising alternative routes to that part or location may be. T hese
alternative paths are expressed in terms of policies, which are essentially
trajectories across states. Importantly, in Active Inference the log probabil-
ity of a policy is set proportional to the expected f ree energy if that policy
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
The High Road to Active Inference 53
was pursued. This implies that the most probable or least surprising path is
(set to be) the one that minimizes expected f ree energy. This formulation is
equivalent to the way Action is defined in physics, where it scores the proba-
bility of a path by an integral (or sum) of an energy. While a physical system
may pursue a space of hyp ot heti c al trajectories, the path it actually follows
is the one for which Action is minimized—t hat is, Hamilton’s princip le of
least Action. This analogy between Active Inference and Hamilton’s princi-
ple of least Action is unpacked in the next section.
3.5 Active Inference: A Novel Foundation to Understand
Be hav ior and Cognition
In fields like optimal control, reinforcement learning, and economics, the
optimization of beh avi or results from a value function of states, following
Bellman’s equation (Sutton and Barto 1998). Essentially, each state (or state-
action pair) is assigned a value, which represents how good a state is for an
agent to be in. The value of states (or state-a ction pairs) is usually learned
by trial and error, by counting how many times—a nd a fter how much
time—o ne obtains reward by starting from those states. Beh avi or consists
in optimizing reward acquisition by reaching high-v alued states, hence
capitalizing on learning history.
In contrast, in Active Inference, beh avi or is the result of inference and
its optimization is a function of beliefs. This formulation unites notions
of (prior) belief and preference. As discussed above, using the notion of
expected f ree energy amounts to endowing the agent with an implicit prior
belief that it w ill realize its preferences. Hence, the agent’s preference for a
course of action becomes simply a belief about what it expects to do, and to
encounter, in the f uture—or a belief about f uture trajectories of states that it
will visit. This replaces the notion of value with the notion of (prior) belief.
This is an apparently strange move, if one has a background in reinforce-
ment learning (where value and belief are separated) or Bayesian statistics
(where belief does not entail any value). However, it is a powerf ul move, for
at least three reasons.
First, it automatically entails a self-c onsistent proc ess model of purposive
(or teleological) beh avi or, which is akin to cybernetic formulations. If we
endow an Active Inference agent with some prior preference, then it w ill
act to realize such preferences—b ecause this is the only course of action
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
54 Chapter 3
consistent with its prior belief that it w ill act to fulfill its expectations. Note
that the resulting (preferred) course of action, or policy, is directly mea sur-
able in experimental settings, whereas a value function or prior belief needs
to be inferred and hence is a more indirect, if not tautological, meas ure.
Second, casting beh avi or as a functional of beliefs (probability distribu-
tions) automatically entails notions such as degree of belief and uncertainty.
These notions undergird import ant facets of adaptive action but are not
directly available in the Bellman formulation. By the same token, this for-
mulation gives more flexibility in modeling sequential dynamics and itin-
erant beh avi ors, which are harder to model in terms of a value function of
states (Friston, Daunizeau, and Kiebel 2009).
Third, in this formulation, optimal beh avi or comes to follow a Hamilto-
nian princip le of least Action in statistical physics. Indeed, Active Inference
goes one step further t oward the idea that beh avi or is a function of beliefs:
it also assumes that it becomes an energy function—a nd the most likely
course of action of an Active Inference agent is the one that minimizes f ree
energy. A profound consequence is that living organisms behave according
to Hamilton’s princip le of least Action: they follow a path of least res ist ance
until they reach a steady state (or a trajectory of states), as exemplified by
the beh avi or of a random dynamical system (shown in figure 3.3). This is a
fundamental assumption that distinguishes Active Inference from alterna-
tive theories of beh avi or and cognition based on the Bellman formulation.
It is worth briefly outlining what we mean by drawing analogies between
Hamiltonian physics and Active Inference. This is intended on three levels.
The first is that the advance offered by Active Inference to the behavioral
and life sciences is comparable to the advance Lagrangian4 and Hamiltonian
formulations offered to Newton’s accounts of mechanics. While Newtonian
mechanics w ere originally formulated in terms of differential equations—
including Newton’s famous third law expressing the proportionality
between acceleration and force—a complementary perspective on mechan-
ics was offered by considering what is conserved by dynamical systems.
Newtonian dynamics can then be derived from t hese conservation laws.
These offer a perspective on which to base further theoretical advances, and
they form the basis for parts of stochastic, relativistic, and quantum phys-
ics. Analogously, Active Inference reformulates the sorts of neuronal and
behavioral dynamics that might previously have been built up from a series
of differential equations by specifying the quantity— free energy— from
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
The High Road to Active Inference 55
which these dynamics may be derived. Just as dif fer ent sorts of Hamil-
tonians lead to diff ere nt types of physics, f ree energies based on diff ere nt
generative models lead to diff ere nt neuronal and behavioral dynamics.
The second point of connection between Hamiltonian physics and Active
Inference arises from a more direct association between a Hamiltonian and
probability meas ures. The idea here is to associate the conserved Hamilto-
nian with the energy of the system. Remember that the quantities we have
referred to as energies so far (h ere and in chapter 2) have all had the form of
a negative log probability. This reflects an interpretation of energy as simply
a meas ure of the improbability of any given configuration of a system. On
this view, conservation of energy and of probability are equivalent laws. As
dissipative systems—c oupled to external states via a Markov blanket—m ove
to states of low energy or high probability, we can directly associate the
energy or Hamiltonian with surprise. As such, Active Inference is Hamil -
tonian physics applied to a certain kind of system (systems that feature a
Markov blanket).
The third association between t hese formulations is the variational cal-
culus that underwrites the association between energies and dynamics.
This is most apparent when Hamiltonian physics is expressed as a princi-
ple of least Action, where Action refers to the integral of a Lagrangian over
a path. Crucially, this Action is a functional of a path. Here, a path is a
function of time whose output is the position and velocity of a particle
on that path at that time. The path followed by a (deterministic) particle
minimizes this Action. Similarly, Active Inference is predicated on the idea
that beliefs (themselves functions of hidden states) must minimize a free
energy functional. The key point of contact h ere is that in both cases, func-
tions (paths or beliefs) must be optimized in relation to functionals (Action
or f ree energy, respectively). This places both in the context of variational
calculus, which is a branch of mathem atics dedicated to finding extrema of
functionals. In physics, this leads to the Euler-L agrange equations. In Active
Inference, we arrive at variational inference procedures.
3.6 Models, Policies, and Trajectories
In section 3.2, we highlighted that the scope of Active Inference pertains to
those systems that enjoy some separation from their environment and saw
that this translates into the presence of a Markov blanket. In section 3.3,
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
56 Chapter 3
we highlighted that the pers ist ence of this blanket requires dynamics that
(on average) minimize the surprise of (sensory) states. As this may be inter-
preted as self-e videncing, we arrive at the conclusion that beh avi or is deter-
mined by a steady-s tate distribution that can be interpreted as a generative
model of how (sensory) data are generated.
This tells us something very import ant. Each generative model should be
associated with diff ere nt sorts of beh avi or. As such, diff ere nt sorts of be hav-
ior may be accounted for by specifying diff ere nt generative models—a nd
implicitly what that system would find surprising. Furthermore, diff ere nt
kinds of generative model may correspond to adaptive or cognitive creatures
having vario us levels of complexity (Corcoran et al. 2020). Very s imple gen-
erative models of the sort driving the dynamics in figure 3.3 offer a minimal
sort of cognition, as they cannot entertain the possibility of alternative (or
counterfactual) trajectories. Further, these models are shallow, in the sense
that they afford inference at just one timescale. In contrast, hierarchical gen-
erative models afford inference at multiple timescales. In hierarchical or
deep models, the dynamics at higher hierarchical levels generally encode
things that change more slowly (e.g., the sentence I am reading) and that
contextualize things that change faster (e.g., the word I am reading), which
are represented at lower hierarchical levels (Kiebel et al. 2008; Friston, Parr,
and de Vries 2017).
What do we need to include in a model to derive more complex beh av-
iors of the sort we would associate with agency and sentient systems? One
answer to this is the capacity to model alternative futures, or diff ere nt ways
in which events might play out—a nd to select among them. In turn, con-
sidering poss ib le f utures requires a generative model that has some tempo-
ral depth and explici tly represents the consequences of actions. Working
this into the model w ill ensure beh avi or that conforms to the most likely of
these f utures. The (counterfactual) capacity to entertain t hese alternatives
may be what separates the steady state associated with sentient systems
from simpler creatures. When alternative f utures pertain to t hings over
which we have control, we refer to t hese as policies or plans. As we saw in
chapter 2, one way of disambiguating between t hese plans is to incorporate
a prior belief into a model that says that those policies with the lowest
expected f ree energy are the most plausible. This offers a way of character-
izing a certain kind of system with a Markov blanket at steady state—w hich
seems to correspond well to systems like us.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
The High Road to Active Inference 57
3.7 Reconciliation of Enactive, Cybernetic, and Predictive Theories
under Active Inference
By emphasizing free energy minimization, Active Inference unites and
extends three apparently disconnected theoretical perspectives.
First, Active Inference is in keeping with enactive theories of life and
cognition, which emphasize the self-o rganization of beh avi or and autopoi-
etic interactions with the environment, which ensure that living organisms
remain within acceptable bounds (Maturana and Varela 1980). Active Infer-
ence provides a formal framework explaining how living organisms manage
to resist the dispersion of their states by self-o rganizing a statistical structure—
the Markov blanket—t hat affords reciprocal exchanges between organism
and environment while also separating (and in a sense protecting the integ-
rity of ) the organisms’ states from external, environmental dynamics.
Second, Active Inference is in keeping with cybernetic theories, which
describe beh avi or as purposive and teleological. Teleology means that beh av-
ior is internally regulated by a mechanism that continuously tests whether
a goal is achieved and, if not, steers corrective actions (Rosenblueth et al.
1943, Wiener 1948, Ashby 1952, G. Miller et al. 1960, Powers 1973). Simi-
larly, Active Inference agents use both perception and action to minimize
the discrepancy between preferred and sensed states. Active Inference pro-
vides a normative and v iable description of the minimization proc ess by
specifying that what is actually minimized is a statistical quantity that the
agent can measure—v ariational f ree energy—w hich u nder certain condi-
tions corresponds to a prediction error, or the difference between what is
expected and what is sensed. This implies a formulation of cybernetic con-
trol as a prospective process—w hich leads us to the next point.
Third, Active Inference is in keeping with theories that describe control
as a prospective proc ess that rests on a model of the environment—p ossibly
physically implemented in the brain (Craik 1943). Active Inference assumes
that agents use a (generative) model to construct predictions that guide per-
ception and action and to evaluate their f uture (and counterfactual) action
possibilities. This assumption is coherent with the good regulator theorem
(Conant and Ashby 1970), which says that any controller should have—or
be—a good model of the environment. Active Inference reconciles t hese
model-b ased perspectives on brain and be havi or u nder a rigorous charac-
terization in terms of (approximate) Bayesian inference and (variational
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
58 Chapter 3
and expected) free energy minimization. Furthermore, Active Inference is
largely coherent with ideomotor theory (Herbart 1825, James 1890, Hoff-
mann 1993, Hommel et al. 2001), which states that action starts with an
imaginative proc ess, and it is a predictive repres ent at ion (of action conse-
quences) that triggers actions—n ot a stimulus, like in stimulus-r esponse
theory (Skinner 1938). Active Inference casts this idea in an inferential
framework, in which an action stems from a belief (about the f uture); this
has a number of implications, such as the fact that in order to trigger action,
one has to temporarily attenuate sensory evidence (which would other wise
falsify the belief that triggers action) (H. Brown et al. 2013).
The reconciliation of t hese frameworks is int ere sti ng, as they are often
considered at odds. For example, self-o rganization and teleology are often
seen as incompatible in biology. Furthermore, enactive theories tend to
de-e mphasize repres ent at ion and control, which is instead a central con-
struct of most theories of model-b ased inference. Active Inference formal-
izes autopoietic dynamics of adaptive agents from an unusual a ngle, which
sim ult an eously considers self-o rganization and prediction. By connecting
diff ere nt perspectives, Active Inference can potentially help us understand
how they illuminate one another.
3.8 Active Inference, from the Emergence of Life to Agency
Active Inference starts from first princip les and unfolds them to explain
beh avi or and cognition expressed by the simplest to the most complex
forms of adaptive and living systems. In the continuum between sim-
pler and more complex creatures, Active Inference draws a line between
those that minimize variational free energy and those that also minimize
expected f ree energy.
Any adaptive system that actively samples sensations to minimize varia-
tional f ree energy is (equivalently) an agent that actively gathers evidence
for its generative model, aka a self- evidencing agent (Hohwy 2016). T hese
systems are able to avoid dissipation, self-r egulate, and survive by achieving
set-p oints provided by basic homeostatic proc esses. T hese systems can gen-
erate complex and diverse forms of beh avi or and can also have very high
fitness levels (as is already apparent in the case of viruses). Some may have
hierarchical generative models that permit inferring events that change
at diff ere nt timescales, from faster (at lower hierarchical levels) to slower
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
The High Road to Active Inference 59
(at higher levels)—a nd hence can develop sophisticated strategies to deal
with what they experience. However, t hese creatures are also fundamen-
tally l imited b ecause their generative models lack temporal depth—or the
capacity to plan and consider the f uture explici tly (although they can do
so implicitly, for example, as a result of ge net ic evolution)—a nd hence they
always live in the pres ent.
A generative model endowed with temporal depth opens the door to
the minimization of expected f ree energy—or in psychological terms, plan-
ning. In Active Inference, this entails much more than increased adaptivity:
it entails at least a primitive form of agency. For an adaptive system, mini-
mizing expected f ree energy is equivalent to having the (implicit) prior that
one is a f ree energy minimizing agent—b ut acts to minimize f ree energy in
the f uture. When this (prior) belief enters the generative model, the adap-
tive system becomes able to form beliefs about how it should behave in the
future and which trajectories it will pursue. In other words, it becomes able
to select among alternative f utures as opposed to simply selecting how to deal
with the sensed pres ent, as in the simplest agents described above. This tem-
poral depth therefore translates into a psychological depth. To ask about
the ways living creatures populate the continuum between the simplest
and most complex adaptive systems—a nd what forms of Active Inference
they can express—is an empirical question.
3.9 Summary
The main topics of this chapter can be summarized as follows: Living organ-
isms have to ensure that they only visit their characteristic or preferred
states. If one defines t hese preferred states as expected states, then one can
say that living organisms must minimize the surprise of their sensory obser-
vations (and maintain an optimal entropy; see box 3.3).
Doing this requires agents to exercise some autonomy from environ-
mental dynamics and to be equipped with a Markov blanket that separates
(i.e., expresses a conditional ind ep end ence between) their internal states
and the external states of the environment. Agents within the Markov blan-
ket can engage in reciprocal (action-p erception) exchanges with the envi-
ronment. T hese exchanges are formally described by the theory of Active
Inference, where both perception and action minimize surprise. They can
do so by being equipped with a probabilistic generative model of how their
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
60 Chapter 3
Box 3.3
Entropy minimization and open-e nded beh avi or
Active Inference is based on the premise that living organisms strive to main-
tain a relative order (or negative entropy), controllability and predictability,
despite being immersed in an environment whose natur al forces generate con-
tinuous fluctuations—a nd a never-e nding threat of entropic erosion. The most
basic manifestation of this active pursuance of order is physiological homeo-
stasis, with critical physiological para meters that need to be kept within v iable
regions. However, minimizing entropy should not be equated with a rigid
repertoire of responses (e.g., autonomic homeostatic responses) but rather the
opposite, especially in advanced organisms. We can develop open-e nded rep-
ertoires of novel beh avi ors to pursue our original homeostatic imperatives—
for example, to produce and buy good wine to satisfy thirst and other needs.
This is sometimes referred to as “allostasis” (Sterling 2012).
More broadly, we actively pursue some order and controllability per se,
without necessary reference to a specific homeostatic imperative—p erhaps
because preserving order facilitates many such imperatives. We actively carve
our ecological niches to render them more predictable and less surprising.
This is evident in the ways we construct our physical spaces (e.g., refuges and
cities that give shelter from uncontrolled natur al forces) and cultural spaces
(e.g., societ ies with laws and deontic norms that give shelter from anarchic
social forces). In all t hese examples, we usually need to accept some short- term
increase of entropy or surprise (e.g., when we build something new or shift
social stances) to ensure their long-t erm decrease. This helps us understand
how the basic requirement for surprise minimization is not at odds with but
rather promotes the epistemic imperatives and novelty-s eeking, curious, and
exploratory beh avi or that we recognize as central to many species.
A first way epistemic imperatives become apparent is during the minimiza-
tion of variational f ree energy. One of the ways to decompose f ree energy is to
express it as a Gibbs energy expected u nder the approximate posterior minus
the entropy of the approximate posterior. In other words, the agent is striving
to increase entropy. While this seems paradoxical, the paradox disa ppears if
one considers that this is the entropy of the agent’s (approximate posterior)
belief. This can be understood as the imperative to explain t hings as accurately
as poss ib le but also “keep options open” and avoid committing to any specific
explanation u nless this is necessary—t hat is, the maximum entropy princi ple
( Jaynes 1957).
A second way epistemic dynamics become apparent is during the mini-
mization of expected f ree energy, wherein—i nterestingly—t here are two entro-
pies with opposite signs. T hese include the posterior predictive entropy (how
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
The High Road to Active Inference 61
Box 3.3 (continued)
uncertain I am about what outcomes I would encounter given a choice) that
must be maximized—as for beliefs about states in the variational f ree energy—
and the conditional entropy of outcomes given states (the ambiguity entailed
by a policy) that must be minimized. While during the minimization of varia-
tional f ree energy the imperative is to maximize entropy of (pres ent) beliefs,
during the maximization of expected free energy the imperative is to select
actions that minimize the ambiguity of (f uture) beliefs. This gives rise to epis-
temic, curious, novelty-s eeking, and information-f oraging beh avi ors, which
support uncertainty resolution or improvement of the generative model—
which in turn minimizes surprise in the long run (Seth 2013; Friston, Rigoli
et al. 2015; Seth and Friston 2016; Schwartenbeck, Passecker et al. 2019).
sensory observations are generated. This model defines surprise—or better,
a tractable proxy, variational f ree energy, which can be meas ured and mini-
mized efficiently.
An Active Inference agent appears to perform (approximate) B ayesian in -
ference u nder a generative model and to maximize evidence for its model—
that is, it is a self-e videncing agent. The prospective bit of the inference
is realized by selecting courses of actions or policies that are expected to
minimize f ree energy in the f uture. This formalism leads to a novel view of
(optimal) beh avi or in terms of the Hamiltonian princip le of least Action—a
(first) princip le that connects Active Inference to the domains of statistical
physics, thermodynamics, and nonequilibrium steady states.
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a portion of the eBook at doi:10.7551/mitpress/12441.001.0001
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
This is a section of doi:10.7551/mitpress/12441.001.0001
Active Inference
The Free Energy Principle in Mind, Brain, and
Behavior
By: Thomas Parr, Giovanni Pezzulo, Karl J.
Friston
Citation:
ActiveInference:TheFreeEnergyPrincipleinMind,Brain,and
Behavior
By:ThomasParr,GiovanniPezzulo,KarlJ.Friston
DOI:10.7551/mitpress/12441.001.0001
ISBN(electronic):9780262369978
Publisher:TheMITPress
Published:2022
The open access edition of this book was made possible by
generous funding and support from MIT Press Direct to Open
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025
MIT Press Direct
© 2022 Massachusetts Institute of Technology
This work is subject to a Creative Commons CC BY-NC-ND license.
Subject to such license, all rights are reserved.
The MIT Press would like to thank the anonymous peer reviewers who provided
comments on drafts of this book. The generous work of academic experts is essential
for establishing the authority and quality of our publications. We acknowledge with
gratitude the contributions of these otherwise uncredited readers.
This book was set in Stone Serif and Stone Sans by Westchester Publishing Services.
Library of Congress Cataloging-in-Publication Data is available.
Names: Parr, Thomas, 1993– author. | Pezzulo, Giovanni, author. | Friston, K. J.
(Karl J.), author.
Title: Active inference : the free energy principle in mind, brain, and behavior /
Thomas Parr, Giovanni Pezzulo, and Karl J. Friston.
Description: Cambridge, Massachusetts : The MIT Press, [2022] | Includes
bibliographical references and index.
Identifiers: LCCN 2021023032 | ISBN 9780262045353 (hardcover)
Subjects: LCSH: Perception. | Inference. | Neurobiology. | Human behavior models. |
Knowledge, Theory of. | Bayesian statistical decision theory.
Classification: LCC BF311 .P31366 2022 | DDC 153—dc23
LC record available at https://lccn.loc.gov/2021023032
Downloaded from http://direct.mit.edu/books/oa-monograph/chapter-pdf/2246580/c002400_9780262369978.pdf by guest on 12 December 2025