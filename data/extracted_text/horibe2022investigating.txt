Investigating the impact of free energy based behavior on human
in human-agent interaction
Kazuya Horibe†∗, Yuanxiang Fan∗, Yutaka Nakamura and Hiroshi Ishiguro
Department of Systems Innovation, Osaka University, Osaka, Japan
(E-mail: horibe@irl.sys.es.osaka-u.ac.jp)
∗ These authors contributed equally to this work.
Abstract: Humans communicate non-verbally by sharing physical rhythms, such as nodding and gestures, to involve each other.
This sharing of physicality creates a sense of unity and makes humans feel involved with others. In this paper, we developed
a new body motion generation system based on the free-energy principle (FEP), which not only responds passively but also
prompts human actions. The proposed system consists of two modules, the sampling module, and the motion selection module.
We conducted a subjective experiment to evaluate the ”feeling of interacting with the agent” of the FEP based behavior. The
results suggested that FEP based behaviors show more ”feeling of interacting with the agent”. Furthermore, we conﬁrmed that
the agent’s gestures elicited subject gestures. This result not only reinforces the impression of feeling interaction but could also
realization of agents that encourage people to change their behavior.
Keywords: non-verbal communication, human-agent interaction, entrainment, free energy principal
1. INTRODUCTION
People interact with each other by drawing in each other’s
physical rhythms, such as non-verbal gestures[5, 10, 11]. The
synchronization of physical rhythms through attraction cre-
ates a sense of unity in people and plays an important role
in facilitating dialogue in terms of interpersonal impressions
and conversation satisfaction[7]. The sense of unity during
a dialogue has been quantiﬁed by measuring the feature val-
ues of the movements of two people during a dialogue. It
has been shown that the number of dimensions of the feature
values of the movements of two people decreases during a
dialogue, suggesting that it is easier to predict each other’s
movements during a dialogue[12].
Agents that interact with people daily have been studied
extensively for purposes such as guiding people at the re-
ception, serving customers, and recommending products in
stores, and household pets[1, 3, 4]. Since most agents use the
other person’s state as input for deciding their next action,
they do not recognize their movements and the other person’s
movements as a set, and it is difﬁcult to obtain the sense of
unity that can be achieved through the mutual directional at-
traction between people through agent-human interaction.
In this study, we designed an agent that generates the next
gesture using both its own and the other person’s movements.
To generate the gesture, we used Friston’s free energy princi-
ple[6]. The agent trained its generative model using the ges-
tures of two people who were facing each other and interact-
ing beforehand and generated gestures that took into account
both its own and the other person’s movements. As a result,
the subjects’ impressions of the life-likeness and human-like
nature of the agent were improved. Not only that, we showed
that the subjects paid attention to the agent’s gestures and
were drawn into the interaction. This proposed model is ex-
pected to open the door to a new type of human-agent inter-
action that actively engages people and encourages them to
†KH is the presenter of this paper.
change their behavior.
2. FREE ENERGY PRINCIPLE BASED AC-
TION SELECTION FOR INPUT-OUTPUT
HIDDEN MARKOV MODEL
Fig. 1: Schematic of the gesture generation model. (a) Ac-
tion selection module, where D is a “identiﬁer” function that
returns a decision from the current state s(t) as to whether
or not it is interacting with the person in front. (b) Action
alternatives are randomly sampled from a roadmap of action
sequences. The free energy of each sampled series is calcu-
lated and the smallest action series is selected.
arXiv:2201.10164v1  [cs.HC]  25 Jan 2022
Symbol Deﬁnition
Actions A(t) A(t) = [a(1), a(2), ..., a(t)], the input sequence from time step 1 to t
Latent state S(t) S(t) = [s(1), s(2), ..., s(t)], the latent state sequence from time step 1 to t.
Outputs O(t) O(t) = [o(1), o(2), ..., o(t)], the output sequence from time step 1 to t
Likelihood L(θ,O(t),A(t)) The likelihood of IO-HMM giving the output sequence O(t) and the input sequence A(t)
IO-HMM parameters θ θ = (θIN, θTR, θEM), which are the parameters of initial, transition, and emission model
Table 1: Symbol deﬁnition of IO-HMM
In this chapter, we describe the gesture generation model
of the proposed agent (Fig. 1). The gesture generation model
consists of two components. (1) Sampling module: a prob-
abilistic roadmap to generate gesture sequences (Fig. 1(a)).
(2) Action selection module: a time evolution model that cal-
culates the free energy of the series obtained from the gesture
roadmap and selects the gesture with the minimum free en-
ergy modeled by Partially Observable Markov Decision Pro-
cesses (POMDP) (Fig. 1(b)). The POMDP modeled the time
transition with IO-HMM (Table. 1) and used free energy as a
strategy for action selection.
2.1. Action selection module: Free Eenegy Principle
based action selection for IO-HMM
To inclement POMDP model for generating motion, we
proposed to combine the free energy principle (FEP) and
Input-output Hidden Markov Model (IO-HMM). For a set
of possible actions, the free energy F is calculated for each
action, and the policy is to select the action with minimal free
energy. The architecture of IO-HMM is shown in Fig. 1(b)
right side. The blue nodes represent the observed variables,
while the white nodes represent the latent variables. The top
row contains the input variables a(t); The middle row con-
tains the latent variables s(t); The bottom row contains the
output variables o(t). The output of IO-HMM is the free en-
ergy, i.e., the entropy of the predicted state at the next time,
given the belief state calculated from the observed series so
far and an action.
The dynamics is solved by maximizing parameter like-
lihood L(θ,O(T), A(T)) after input data sequence A(T) and
output data sequence O(T) has been given. The likelihood
can be calculated by
L(θ,O(T),A(T)) =
∑
s
Pr(s(1)|a(1); θIN)
T∏
t=2
Pr(s(t−1),a(t); θTR)
T∏
t=2
Pr(o(t)|s(t); θEM),
(1)
where θIN, θTR and θEM denotes the parameters of initial
model Pr(s(1)|a(1); θIN), transition model Pr(s(t)|s(t−1),
a(t); θTR) and emission model Pr( o(t)|s(t); θEM) respec-
tively.
The initial probability model is deﬁned as
Pr(s(1) =si|a(1); θIN) = eθi
IN a(1)
∑
keθk
IN a(1) . (2)
The parameter θIN for initial model is a matrix with the
ith row θi
IN being the coefﬁcients for the initial state being
in state si and is modeled by using multinomial logistic re-
gression. For a is deﬁned as a time sequence data, θi
INa(1)
represented the inner product of features transformed from
a(1) and vector θi
IN.
The transition probability model is deﬁned as
Pr(x(t) =xj|x(t−1) =xi,u(t); θTR) =
eθij
TR a(t)∑
k
eθik
TR u(t). (3)
The parameter θTR for transition model is a set of matrices
with the jth row of the ith matrix θij
TR being the coefﬁcients
for the next start being in statesj given the current state being
in si and modeled using multinomial logistic regression. For
a is deﬁned as a time sequence data,θij
TRa(t) represented the
inner product of features transformed from a(t) and vector
θij
TR.
The emission probability model is deﬁned as
Pr(o(t) = 1|s(t) =si; θEM) = 1
1 +e−θi
EM
. (4)
The parameter θEM for emission model is a set of array
where θi
EM denote the coefﬁcients when the hidden state is
si.
2.2. Sampling module:Probabilistic roadmap for motion
sampling
To obtain a choice of action sequences, we used a prob-
abilistic roadmap for motion sampling (Fig. 1(b) left side).
one node represents the posture of the upper body, and the
edges represent the probability of transitioning from one pos-
ture to another. A single node represents an upper body pos-
ture, and an edge represents the probability of transition from
one posture to another. The choices of action sequences are
randomly sampled action sequences of a ﬁxed time step.
3. DATA COLLECTION AND
MODULE TRAINING
3.1. Interaction motion data collection
We recorded by an omnidirectional camera(Insta 360 Air,
Shenzhen Arashi Vision, China). The participants are asked
to watch an interesting video then talk about it with the ex-
perimenters one by one. An omnidirectional camera between
them is used to record video data of participants and exper-
imenters. The video data is recorded in the 3d frame per
second. The resolution of the recorded image is 1536 × 768.
The total length of recorded video is around 80 minute (Fig.
2(a)).
Fig. 2: Data preparation. (a)Skeleton keypoints extracted by
OpenPose. Participants (left side) and experi- menter (right
side)(b)Result of 3d posture estimation(c)Process of initial-
izing Roadmap(d)Process of fusing nodes in Roadmap
3.2. Observation model: discriminator to calculate o(t)
Interaction motion is deﬁned as Ir
( τ) containing L coor-
dinates vectors from time point τ −Lto τ. The discrimina-
tor is implemented by convolutional neural network (CNN)
trained by semi-supervise learning method, where fake in-
teraction data If is generated from real interaction data Ir
by negative sampling. Then the real interaction and fake in-
teraction data is mixed for training. We used the network
structure of CNN, which is Fully-connected layer (On 2nd
axis 56 to 48), conv1(channel=16, kernel=[3,3], stride = 1,
padding = [0,0]), conv2(channel=32, kernel=[3,3], stride =
2, padding = [1,0]), conv3(channel=64, kernel=[3,3], stride
= 2, padding = [1,0]), Fully-connected layer (3200 to 128,
dropout =0.5), and FC(128 to 1).
Before training the network, We normalized the mixed
dataset to between 0 and 1, then augment data by adding
noise in the temporal axis and spatial axis. Fake interac-
tion data If can be generated from real interaction data Ir
by shifting the time of Ir
e(τ) or Ir
p(τ) randomly. The objec-
tive of training a Discriminator is to minimize binary cross
entropy.
The objective of training a Discriminator is to minimize
binary cross entropy.
Loss= −[ylogFd(I) + (1−y)log(1 −Fd(I))] (5)
where Fd(I) is the prediction on interaction motion I, y
is the ground truth of whether interaction motion I is real
data or not, y = 1 when I is real data, else y = 0. Fig. 3(a)
shows that the interaction motion discriminator can achieve
the accuracy of 60.5%, which is better than us since We can
not tell most the fake data when we watch them manually.
3.3. Probabilistic Roadmap for Motion Sampling
In order to apply the sampled motion to robots and vir-
tual agent, it is necessary to construct PRM by 3D skele-
ton keypoints. We use a 3D posture estimation method to
estimate 3D skeleton keypoints for each 2D skeleton key-
points[9]. The outcome is shown in Fig.2(b). Roadmap is
constructed by motion of experimenter-side in the dataset
Ce. Since robots are typically controlled by joint angles, We
construct dataset R by calculating the joint rotation for each
frame in the 3D skeleton keypoints sequence. Coordinates
vector R(τ) stands for the joint angles of dataset R at time
point τ.
In learning phase, graph G = {n, l}is initialized by con-
necting nodes according to the consecutiveness of time point
(e.g. from R(τ −1) to R(τ)) as shown in Fig.2(c). Prob-
abilistic connections is established based on joint angular
velocity V(τ) and joint angular acceleration W(τ). The
joint angular velocity V(τ) and joint angular acceleration
W(τ) is calculated by the difference of joint angles (e.g.
R(τ−1) −R(τ)) and the difference of joint angular veloci-
ties (e.g. V(τ) −V(τ −1))). Since the human motion must
be ﬂuent, the variation between two reachable nodes can be
considered lower than some limitations.The relationship be-
tween joint angular velocity and joint angular acceleration is
modeled by linear regression.
Openpose[2, 13, 14], is used to extract 2d skeleton key-
points sequences of human body from the videos. The out-
put of OpenPose is quite noisy, so We remove outlier and
resample data to 8 fps by linear interpolation. Since the hu-
man motion during interact can be considered always within
some speciﬁc frequency, We use a low pass ﬁlter with cutoff
frequency of 4 Hz to smooth data.
The dataset C is consists of skeleton keypoints sequence
of experimenters Cp and participants Ce. C(τ) stands for the
coordinates vector at time point τ, which contains the key-
points coordinates of participants Cp(τ) and experimenters
Ce(τ). They are also shown in the left side and right side of
Fig.2(a) respectively.
3.4. Training IO-HMM thorough EM algorithm
The input sequence A(T) and the output sequence O(T)
is necessary to estimate parameters in Input-output Hidden
Markov Model (IO-HMM) (Table. 1). Data used in training
IO-HMM is skeleton keypoints sequence of experimenterCe
and the prediction of interaction motion discriminatorFd(C)
given dataset C as input sequence A(T) and output sequence
O(T). The parameter θEM for emission model is a set of
array where θi
EM denote the coefﬁcients when the hidden
state is si. The parameters θ of this probabilistic model is
estimated to maximize likelihood L( θ,O(t), A(t)|C) under
the dataset C by Expectation-Maximization (EM) algorithm.
Fig. 3(b) shows the learning curve of training IO-HMM and
this ﬁgure suggests that this probabilistic model can ﬁt our
dataset.
4. EXPERIMENTS
4.1. Impression on the FEP based behavior
To the purpose of evaluating impression on the Free En-
ergy Principle (FEP) based behavior in human-perceptible
level, We gather some experiment subjects and the impres-
sion is measured by the score they gave to the generated mo-
tion. The experiment subjects are indicated to bring an ob-
ject back from the front of the projected human-size virtual
agent and evaluate the impression on the agent. An omni-
directional camera is used for capturing the sensory input of
the virtual agent. The snapshots of evaluation experiment for
each condition is shown in Fig. 4.
Fig. 3: Learning curve (a)Learning curve of discriminator.
Orange: Train accuracy, Blue: Test Accuracy (b)Learning
curve of IO-HMM
The impression is evaluated by three indexes, i) “lifelike-
ness”, ii) “human-likeness” and iii) “feeling of interacting
with agent”. Three motion generation methods are com-
pared in this experiment, i) “Minimizing free energy”, ii)
“Sampling based on PRM” and iii) “Perlin Noise”. Experi-
ment subject needs to answer three questions after each time
they bring the object back. The order of motion generation
method is determined randomly beforehand.
Motion generated by “Minimizing free energy” is imple-
mented as motion generation system explained in section 3.
At each time step, a set of possible motion is sampled from
Probabilistic Roadmap (PRM) and the one with minimal free
energy is selected. The motion generated by “Sampling
based on PRM” is implemented as sampling motion from
PRM randomly explained in chapter 3. The motion generated
from “Perlin Noise” is implemented as adding noise to idle
posture. In robotics, “Perlin Noise” has been used, added to
joint angles, to increase the lifelikeness of robot movements
and to generate idle motion. The experiment is conducted in
a laboratory room of the university. The experiment subjects
are bachelor students, master students and doctoral students
of robotics major. We gathered 8 experiment subjects and
repeat 5 times for each condition.
The subjective impression on three motion generation
methods is shown in Fig.5. As explained in the last section,
experiment subjects score the generated motion from 1 to 7.
To express the difference between three motion generation
method intuitively, the box plot on the left side of each ﬁg-
ures shows the minimum value, ﬁrst quartile, median value,
third quartile value and maximum value of score for each
motion generation method. On the right side of each ﬁgure,
histograms describe how many people gave a speciﬁc score
for motion generated by “Minimizing free energy”, “Sam-
pling based on PRM” and “Perlin Noise”.
By using Mann-Whitney’s U test [8], The results suggest
that motion generated by “Minimizing free energy” shows a
more “feeling of interacting with agent” than both “Sampling
based on PRM” and “Perlin Noise” (p < 0.01) (Fig. 5(a)).
Moreover, data indicate that motion generated by “Mini-
mizing free energy” shows more “lifelikeness” and “human-
likeness” than both “Sampling based on PRM” and “Perlin
Noise” (p <0.01) (Fig. 5(b)(c)). By watching the recorded
video of evaluation experiment, it can be found that the inten-
sity of motion may also affect the subjective impression. For
instance, the intensity of motion generated by “Perlin Noise”
and “Sampling based on PRM” is limited when comparing
to “Minimizing free energy”.
Fig. 4: Evaluation experiment for three conditions
4.2. Agents change humans behavior
Then, to investigate whether or not the agent causes the
pull-in, we gathered new subjects and experimented in the
same environment as Fig. 4. This time, we experimented
with two conditions: one with a recording agent and the other
with an agent interacting in real-time. For the recording con-
dition, we used a video of the author interacting with the
agent beforehand. Three subjects were used for each condi-
tion. Subjects entered a room where a human-size agent was
projected and were asked to record the names of 10 fruits into
a microphone in front of them. The subjects were instructed
to start recording at their timing and to leave the room when
they were ﬁnished and were not told that an agent was pro-
jected inside. We ﬁlmed the agent and the subject during
their stay in the room using a video camera.
The time spent by the subject in the room where the agent
was projected was measured. One of the subjects who in-
teracted with the agent in real-time showed interest in the
agent’s gestures and was listening to the agent’s gestures ev-
ery time the name of fruit was mentioned during the task.
After the task, the agent responded to the sound. After the
task was over, the subject was observed vocalizing into the
microphone and mimicking the agent’s gestures to see if the
agent responded to the sound.
5. DISCUSSION
We implemented a free-energy based agent that makes ac-
tion choices based on a set of self and opponent gestures.
This agent improved the impression of interaction for the
subject, as well as the impression of human-likeness and life-
likeness. In addition, we found that subjects who confronted
the agent paid more attention to the gestures generated by the
agent and spent more time observing them, and some sub-
jects were drawn into non-verbal interaction by imitating the
gestures themselves.
In human-agent interaction, there have been many stud-
ies of agents that take only the other person’s actions as in-
put. The proposed model generates gestures by taking into
account the set of actions of both the agent and the other
person, so that the mutual attraction that occurs during inter-
action between humans can be realized in human-agent in-
teraction. The agent’s gestures were able to induce a change
Fig. 5: Impression on the FEP based behavior (a) Feeling of interaction. (b) Human-lileness (c) Life-likeness
in the behavior of the person. It is expected to open the door
to the exciting challenge of agents that actively work with
people and encourage them to change their behavior.
6. ACKNOWLEDGMENTS
This work was supported by JST Moonshot R&D Grant
Number JPMJMS2011.
REFERENCES
[1] Timothy Bickmore, Laura Pfeifer, and Daniel Schul-
man. Relational agents improve engagement and learn-
ing in science museum visitors. In International
Workshop on Intelligent Virtual Agents, pages 55–67.
Springer, 2011.
[2] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser
Sheikh. Realtime multi-person 2d pose estimation us-
ing part afﬁnity ﬁelds. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pages 7291–7299, 2017.
[3] Ginevra Castellano, Iolanda Leite, Andr ´e Pereira, Car-
los Martinho, Ana Paiva, and Peter W Mcowan. Mul-
timodal affect modeling and recognition for empathic
robot companions. International Journal of Humanoid
Robotics, 10(01):1350010, 2013.
[4] Soumia Dermouche and Catherine Pelachaud. Genera-
tive model of agent’s behaviors in human-agent interac-
tion. In 2019 International Conference on Multimodal
Interaction, pages 375–384, 2019.
[5] David Efron. Gesture and environment. 1941.
[6] Karl Friston, Francesco Rigoli, Dimitri Ognibene,
Christoph Mathys, Thomas Fitzgerald, and Giovanni
Pezzulo. Active inference and epistemic value. Cog-
nitive neuroscience, 6(4):187–214, 2015.
[7] Spencer D Kelly, Dale J Barr, R Breckinridge Church,
and Katheryn Lynch. Offering a hand to pragmatic un-
derstanding: The role of speech and gesture in com-
prehension and memory. Journal of memory and Lan-
guage, 40(4):577–592, 1999.
[8] Henry B Mann and Donald R Whitney. On a test of
whether one of two random variables is stochastically
larger than the other.The annals of mathematical statis-
tics, pages 50–60, 1947.
[9] Julieta Martinez, Rayat Hossain, Javier Romero, and
James J Little. A simple yet effective baseline for 3d
human pose estimation. In Proceedings of the IEEE
International Conference on Computer Vision, pages
2640–2649, 2017.
[10] David McNeill. So you think gestures are nonverbal?
Psychological review, 92(3):350, 1985.
[11] David McNeill. Hand and mind. De Gruyter Mouton,
2011.
[12] Yusuke Nishimura, Yutaka Nakamura, and Hiroshi
Ishiguro. Human interaction behavior modeling us-
ing generative adversarial networks. Neural Networks,
132:521–531, 2020.
[13] Tomas Simon, Hanbyul Joo, Iain Matthews, and Yaser
Sheikh. Hand keypoint detection in single images using
multiview bootstrapping. In Proceedings of the IEEE
conference on Computer Vision and Pattern Recogni-
tion, pages 1145–1153, 2017.
[14] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and
Yaser Sheikh. Convolutional pose machines. In Pro-
ceedings of the IEEE conference on Computer Vision
and Pattern Recognition, pages 4724–4732, 2016.