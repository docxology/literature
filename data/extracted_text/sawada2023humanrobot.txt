HUMAN -ROBOT KINAESTHETIC INTERACTION BASED ON FREE
ENERGY PRINCIPLE
Hiroki Sawada, Wataru Ohata, Jun Tani∗
Cognitive Neurorobotics Research Unit
Okinawa Institute of Science and Technology Graduate University
Okinawa, Japan 904-0302.
{hiroki.sawada1, wataru.ohata, jun.tani}@oist.jp
ABSTRACT
The current study investigated possible human-robot kinaesthetic interaction using a variational
recurrent neural network model, called PV-RNN, which is based on the free energy principle.
Our prior robotic studies using PV-RNN showed that the nature of interactions between top-down
expectation and bottom-up inference is strongly affected by a parameter, called the meta-prior, which
regulates the complexity term in free energy. The current study examines how changing the meta-prior
w in the interaction phase affects the counter force generated when an experimenter attempts to
induce movement pattern transitions familiar to the robot through its prior training. The study also
compares the counter force generated when trained transitions are induced by a human experimenter
and when untrained transitions are induced. Our experimental results indicated that (1) the human
experimenter needs more/less force to induce trained transitions when wis set with larger/smaller
values, (2) the human experimenter needs more force to act on the robot when he attempts to induce
untrained as opposed to trained movement pattern transitions. Our analysis of time development
of essential variables and values in PV-RNN during bodily interaction clariﬁed the mechanism by
which gaps in actional intentions between the human experimenter and the robot can be manifested
as reaction forces between them.
Keywords Human-robot kinaesthetic interaction ·predictive coding ·active inference ·free energy principle.
1 Introduction
Studies on social human-robot interaction have attracted considerable attention recently because of their practical
applications, especially with using the linguistic modality [1, 2, 3]. However, investigation of more direct interaction
such as via kinaesthesia should be also indispensable when considering more embodied aspects or enactivism [ 4]
of human-robot interactions. Although there have been a reasonable number of practical studies on human-robot
interaction using kinaesthesia, which have contributed greatly to human-robot joint collaboration, human assistance,
and user interface [5, 6, 7], few studies have attempted to understand essential mechanisms underlying kinaesthetic
interaction in light of embodied cognition, social cognition, and system-level neuroscience.
In this regard, the current study investigated human-robot kinaesthetic interaction by applying system neuroscience
theory, the free energy principle, proposed by Friston [8] which is consonant with enactivism [9, 10]. Let us consider a
situation in which a robot and a human dance, holding each other with both hands, executing memorized dance patterns.
If the robot initiates a particular pattern from memory with strong intention, the human counterpart might follow it
without resisting because of the strong counter force. On the other hand, if the robot generates a pattern without strong
intention, the human counterpart might be able to shift to a different pattern without experiencing strong counter force.
Let us consider another situation. If the human counterpart attempts to induce a movement pattern that is familiar to the
robot, such guidance should proceed easily without strong counter force, since the robot can infer the intended pattern
immediately and can move as anticipated. On the other hand, if the human counterpart attempts to induce a movement
pattern unfamiliar to the robot, such guidance should experience strong counter force, since the movement is neither
inferential nor predictable for the robot.
arXiv:2303.15213v1  [cs.RO]  27 Mar 2023
Human-Robot Kinaesthetic Interaction Based on Free Energy Principle
We presume that these situations can be well explained by predictive coding [ 11] and active inference [12, 13, 14],
based on the free energy principle [8]. Predictive coding provides a theory in which perception is achieved by inferring
hidden causes for sensory observations that minimize the error between top-down prediction of sensation made by the
generative model and the actual sensation. On the other hand, active inference provides a theory for action generation
in which an optimal action is inferred to minimize the error between the preferred sensation and the actual sensation
resulting from the action. These two are not separate, but integrated through a sensorimotor loop in embodied cognition
in which a perceptual inference and action generation can be achieved simultaneously by minimising errors through
iterative interaction between the top-down predictive/generative process and the bottom-up inference. Also in social
embodied cognition, the dynamic interaction between the top-down pathway for predicting while acting on the others
and the bottom-up pathway for inferring the actional intention of the counterpart through sensory observation should
become a crucial element. Accordingly, we presume that different interactions in the dance appear, depending on the
balance between the strength of the top-down pathway and that of the bottom-up pathway.
Recently, the number of studies addressing application of free energy principle in cognitive robotics has increased
[15, 16, 17]. Maselli et al. [ 18] showed that the active inference model is able to characterise movements generated
by the agent’s intention to resolve multi-sensory conﬂict or to achieve an external goal, such as reaching with its arm
to a certain point with the agent having a VR-vision of its arm that was tilted from the actual position to confuse the
agent. Tschantz et al. [ 19] presented a working implementation of active inference to reinforcement learning that
demonstrated efﬁcient exploration and an order of magnitude higher sample efﬁciency in a high-dimensional task, such
as a mountain-car environment. Also, Pezzato et al. [20], showed that a robotic task, such as reactive action planning
can be formulated as a free energy minimisation problem by introducing a hybrid combination of active inference and
behaviour trees. However, studies investigating human-robot interaction using the free energy principle remain few in
number.
The author’s group has conducted neurorobotic studies related to frameworks of predictive coding and active inference
to develop various types of recurrent neural network (RNN) models [21]. However, an essential problem in these studies
is that RNN models have difﬁculty in dealing with probabilistic properties hidden in interactions between robots and
environments. To tackle this problem, our group developed a probabilistic variational RNN model, called PV-RNN
[22], based on the free energy principle.
An indispensable feature of PV-RNN is that free energy is computed as a sum of the negative accuracy term and the
complexity term, weighted by a parameter w, called the meta-prior. Here the complexity term represents the divergence
between the approximated posterior probability distribution and the prior probability distribution in probabilistic latent
variables allocated in PV-RNN. More intuitively, the complexity term can be understood as the internal gap between
top-down expectation and bottom-up sensory reality. The model can learn to extract probabilistic structures hidden in
data either by embedding them in nonlinear, deterministic dynamics of chaos by setting a large value of the meta-prior
wor into stochastic processes by setting a small w[22].
By following the above study, Chame et al. [ 23] conducted a human-robot bodily interaction experiment using a
PV-RNN model and attempted to show possible effects of w on the interaction dynamics. However, this study is
considered preliminary because only some snapshots of the experiments were shown and no rigorous analysis of the
experimental results was made. Wirkuttis et al. [24] performed simulation studies on synchronized imitative interaction
of dyadic robots using a PV-RNN model in which robots observed the sensation of simpliﬁed vision and proprioception,
but without kinaesthesis. Through statistical analysis of repeated simulation experiments, these studies show that a
robot with a larger win both the learning phase and the test phase tends to lead the counterpart robot set with smaller w
by projecting stronger actional intention to the counterpart.
The current study examined human-robot kinaesthetic interaction by implementing a PV-RNN model in a real humanoid
robot. In particular, the study attempted to translate top-down intention or prior belief, formulated using the free
energy principle to the force bodily exerted on the counterpart by conducting statistical analysis on data obtained from
repeated experiments. In this experiment, a humanoid robot controlled by a PV-RNN model is trained to generate
movement patterns with speciﬁc transition probability distributions among them. After the training phase, two types
of human-robot kinaesthetic interaction experiments were conducted. In the ﬁrst experiment, while the robot was
generating trained patterns, a human experimenter tried to physically induce a set of trained familiar movement pattern
transition in the robot. We examined how the robot’s rigidity in terms of the counter force in its response changed
depending on w, which was set during the interaction phase. The second experiment compared the counter force during
responses in these two cases, i.e., when the experimenter induced the robot to proceed with learned movement pattern
transitions and when the experimenter initiated previously unlearned pattern transitions.
Our intensive analysis of time-development of the latent variables, complexity term, and prediction error observed
in these experiments under different conditions clariﬁes how bodily interaction between an experimenter and a robot
2
Human-Robot Kinaesthetic Interaction Based on Free Energy Principle
proceeds and what sorts of neural activities are generated through top-down and bottom-up interactions during an
experimental task.
2 Model
2.1 Free Energy Principle
As mentioned in the Introduction, the current study used PV-RNN, based on the free energy principle, as the basic
model. The free energy principle assumes that brains learn as well as inferred for given observations, by minimizing
free energy, deﬁned in Eq. 1.
F= DKL[qφ(z|X)∥pθ(z)]  
complexity
−Eqφ(z|X)[log pθ(X|z)]
  
accuracy
,
(1)
pθ(X|z) is the likelihood of the sensory observationX, given the probabilistic latent variableszwhich is parameterized
by θ. qφ(z|X) is the inference model parameterised by φ. As shown in Eq. 1, free energy consists of two terms. The
ﬁrst term indicates the complexity, which is the divergence between the approximate posterior probability distribution
and prior probability distribution for the latent variable z, and the second term is the accuracyin predicting the sensory
observation.
2.2 PV-RNN implementation
PV-RNN is operated in two distinct phases, a training phase and an interaction phase. In the training phase, we prepared
a dataset (joint angle sequences of the robot) and trained the PV-RNN model using it. In the interaction phase, the robot
and the human experimenter interact physically, such that the PV-RNN attempts to drive the robot’s arms by predicting
next-time-step target joint angles while it infers latent variables using actual joint angle readings. Each operation phase
is explained in detail below.
2.2.1 Training Phase
First, we start with computation during the training phase. The evidence-free energy Ftrain as a loss function for
training the PV-RNN with the joint angle observation from time step 1 to T is shown in Eq.2 (the exact derivation
should refer to [25]).
Ftrain = β×DKL[qφ(z1|d0,X1)∥pθ(z1)]  
complexity
+ w×
T−1∑
t=1
Eqφ(z1:t|dt,Xt:T+1)
[
DKL[qφ(zt+1|dt,Xt+1:T+1)∥pθ(zt+1|dt)]
]
  
complexity
−
T∑
t=1
Eqφ(z1:t−1|dt−1,Xt:T)[ln pθ(Xt|dt)]
  
accuracy
,
(2)
where q(φ) and p(θ) are the inference model parameterised by φ and the generative model parameterised by θ,
respectively.
As shown in Eq.2, the meta-prior wweights the complexity term, except for the initial step. As described previously,
the setting of meta-prior in the training phase strongly affects behavioural characteristics of the trained network. Prior
studies [25, 24] showed that a network trained with a largerwdevelops higher precision, i.e., smaller standard deviation,
in the prior distribution, whereas a smaller wdevelops lower precision in the prior. More or less precision in the prior
means stronger or weaker top-down belief, respecyively. PV-RNN is composed of two variables d,z; in which the
former is a deterministic latent variable, and the latter is a random latent variable sampled from a Gaussian distribution.
During forward computation of PV-RNN, din layer l(l= 1 is the bottom layer, which is closest to the output layer) at
time step tis computed as,
hl
t =
(
1 − 1
τl
)
hl
t−1 + 1
τl
(
Wll
dddl
t−1 + Wll
zdzl
t + Wll+1
dd dl+1
t + bh
)
,
dl
t = tanh(hl
t).
(3)
3
Human-Robot Kinaesthetic Interaction Based on Free Energy Principle
Here, hl
t denotes the internal state of dl
t before the activation function tanh is applied, and bh denotes the bias term of
h. τl is a time constant unique to each layer, which encourages the network to process information by following an
intrinsic time scale of the layer [26, 27, 28, 29]. Matrices Wdd and Wzd express intra- and inter-layer connectivity
weights in the network, respectively. At t= 1, the input to h in the top layer is calculated only from zl
1 and bh.
Each dimension of zp
t is sampled over the prior distribution, which is a Gaussian distribution of mean µµµp
t and standard
deviation σσσp
t individually. At t= 1, zp
1 is sampled from a standard normal distribution, and zp
t for the following time
steps is computed by,
µµµp
t = tanh(Wll
dµdt−1 + bp
µ),
σσσp
t = exp(Wll
dσdt−1 + bp
σ),
(4)
zp
t = µµµp
t + σσσp
t ∗ϵϵϵt, with ϵϵϵt ∼N(0,I), (5)
where ∗indicates the element-wise product of the two vectors. This equation follows the idea of the conditional prior
[30]. Here, Wll
dµ,Wll
dσ,bp
µ and bp
σ are the weight matrices and the bias terms for µµµp
t and σσσp
t, respectively. ϵϵϵis a value
sampled from the standard normal distribution. In order to make model parameters differentiable through the random
latent variable, we use the reparametrization trick [31] to sample zp
t.
On the other hand, zq
t in inference model qφ is sampled from the approximate posterior probability distribution q(zt)
which is a Gaussian distribution with mean µq
t and standard deviation σq
t which are computed as,
µµµq
t = tanh(Aµ
t),
σσσq
t = exp(Aσ
t),
zq
t = µµµq
t + σσσq
t ∗ϵϵϵt, with ϵϵϵt ∼N(0,I)
(6)
Aµ
t,Aσ
t are adaptive variables which are optimised during training. During the training phase, zt in Eq.3 is sampled
from the approximate posterior. On the other hand, zp
t is sampled from the prior distribution to compute the Kullback-
Leibler divergence (KLD) between the approximate posterior and the prior, which is the complexity term of the loss
function Eq.2.
The obtained d1
t is used for computing the output layer, of which function is,
ot = Wod1
t + b0, (7)
where Wo,bo is the weight matrix and the bias of the output layer, respectively.
Finally, we compute the predicted output ¯ xt. Each dimension of the predicted output is represented in a probabilistic
distribution using the Softmax of Nsoft elements. Therefore, the j-th softmax element of the i-th dimension of the
predicted output ¯xi,j
t is computed by,
¯xt,i,j = exp(ot,i,j)
∑Nsoft
j=1 exp(ot,i,j)
. (8)
As shown in Eq.2, the evidence-free energy F, which is the loss function of this model, is the sum of the KLD between
the approximate posterior q(zt|xt) and prior p(zt|dt−1), and the negative log-likelihood calculated from the output ¯ xt
and the perception xt.
At time step t, the KLD between the approximate posterior and prior is deﬁned as,
DKL[q(zt|xt:T)||p(zt|dt−1)] =
Nz∑
i
q(zi
t|xt:T) ln p(zi
t|dt−1)
q(zi
t|xt:T) , (9)
where zi
t is the i-th dimension of zt, T is the length of the sequence and Nz is the dimension of zt of each layer.
Here, q(zt|xt) and p(zt|dt−1) are both assumed to follow multivariate Gaussian distributions with diagonal covariant
matrices. Hence, they are expressed with the mean µµµt (= [ µt,1,µt,2,...,µ t,Nz]) and the standard deviation σσσt (=
[σt,1,σt,2,...,σ t,Nz]) as below,
q(zi
t|xt:T) = 1√
2π(σq
t,i)2
exp

−1
2
(
zi
t −µq
t,i
σq
t,i
)2
, (10)
p(zi
t|dt−1) = 1√
2π(σp
t,i)2
exp

−1
2
(
zi
t −µp
t,i
σp
t,i
)2
. (11)
4
Human-Robot Kinaesthetic Interaction Based on Free Energy Principle
Therefore, the KLD loss rt between the approximate posterior and the prior is computed as,
rt =
{∑
iDKL[q(zi
t|xt)||p(zi
t|N(0,1))], if t= 1,∑
iDKL[q(zi
t|xt)||p(zi
t|dt−1)], otherwise. (12)
On the other hand, as an equivalent of the negative log-likelihood term of Eq.2, the KLD between the target and the
predicted output is computed. As mentioned above, the predicted output is represented as a probabilistic distribution
P(xi
t|θ) using Softmax. The target sequence is also transformed into a probabilistic distribution Q(xi
t) along the same
line [32]. Therefore, the KLD is computed as,
et =
Nx∑
i
DKL[Q(xt,i)||P(xt,i)] =
Nx∑
i
Nsoft∑
j
Q(xt,i,j) ln Q(xt,i,j)
P(xt,i|θ). (13)
The loss function of our model which is equivalent to the evidence-free energy Ftrain is computed as the weighted
sum of Eq.12 and Eq.13. As shown in Eq.2, the KLD between the approximate posterior and prior is regularised with
a meta-prior βfor t= 1 and wfor t̸= 1. Here, βis the weighting parameter speciﬁcally for t= 1, which regulates
the initial sensitivity of the PV-RNN model. Therefore, the loss function of our model during the training phase is
computed as,
F=



∑T
t=1
(
et + Nx
Nz
βrt
)
, if t= 1,
∑T
t=1
(
et + Nx
Nz
wtrt
)
, otherwise
(14)
where Nx and Nz are the ¯x dimension and z dimension in each layer, respectively. T is the length of the dataset
sequence. The negative log-likelihood, which we denote as prediction error, et is proportional to Nx and the KLD
between the approximate posterior and prior is proportional to Nz, under the assumption that each ¯x dimension and
z dimension is independent. Therefore, the meta-prior during the training phase wt is normalised by Nx and Nz
so that it can be compared among PV-RNN models with different ¯x dimension and z dimension. Through training,
weight matrices, biases and adaptive variables Aµ
t,Aσ
t are updated based on back-propagation through time (BPTT)
[33, 34, 35].
2.2.2 Interaction phase
We performed a human-robot real-time interaction in the interaction phase where the pre-trained PV-RNN drives the
robot’s movement by predicting joint angles of the next time step. The forward computation part remains the same
as that in the training phase. However, there are some differences in the loss function used and in ways of updating
variables. Unlike the training phase, weight matrices and biases are not updated. Only the adaptive variables Aµ
t and
Aσ
t are updated so that the approximate posterior can adapt to the ongoing observation of the joint angle sequence.
PV-RNN predicts the future sensation and infers past latent variables using the past window spanning from time-step
tc−tw to the current time-step tc with a window size tw as shown in Fig. 1. The evidence-free energy in the interaction
phase Fint, normalized in the same way as Eq.14, is computed inside the past window as:
Fint =



∑tc
t=tc−tw
(
et + Nx
Nz
βrt
)
, if t= 1,
∑tc
t=tc−tw
(
et + Nx
Nz
wirt
)
, otherwise,
(15)
The adaptive variables Aµ
t and Aσ
t at each time-step tfrom tc −tw to tc are modiﬁed so as to minimize Fint by
iterating the forward computation and the error back-propagation through time for a ﬁxed number of times, called
epochs. After modifying the approximate posterior by going through epochs of iteration, next-step joint angles are
predicted and fed into the PID controller of the robot in order to generate its movement. Then, the past window is
shifted one step ahead. wi, which is the meta-prior used in the interaction phase, could be set with different values from
wt used in the training phase. In [36], a simulation experiment using a PV-RNN model shows that when a PV-RNN
trained with a particular wt was reset with a smaller wi in the later interaction phase, the approximate posterior shifted
away from the prior and the PV-RNN tended to adapt to the observed sensory sequence. This means that top-down
intention in the PV-RNN became weaker. On the other hand, when reset with a larger wi, the PV-RNN tended to ignore
the observed sensory sequence by generating its own intended patterns, which means that top-down intention becomes
stronger. However, the precision structure in the prior does not change even aswi is changed, since the prior distribution
was developed in the learning phase.
5
Human-Robot Kinaesthetic Interaction Based on Free Energy Principle
Figure 1: Graphical representation of the interaction phase of PV-RNN. dl,t,zp
l,t,zq
l,t,xt and ¯xt indicates the determin-
istic latent variable, prior distribution, approximate posterior, output and target of layer land time-step t, respectively.
Blue and red arrows indicate forward propagation and backward propagation, respectively. The future, the past within
the window and the past outside the window are coloured red, blue, and green respectively. The past window size is 2
in this graphical model. a) shows the network at time-step t. b) shows the network at time-step t+ 1.
6
Human-Robot Kinaesthetic Interaction Based on Free Energy Principle
2.3 The employed robot controller
The current study uses a humanoid robot, Torobo 1 to conduct human-robot kinesthetic interaction experiments. Torobo
is equipped with a built-in force-feedback controller that enables humans to back-drive joint angles with subtle force. In
the Torobo control system, when a human exerts certain torque on joints by pushing or pulling the limbs of Torobo, the
exerted torque can be estimated by subtracting the torque inferred as necessary to account for the current static state as
well as a dynamic state of the robot from the actual torque measured in the joints. By computing the next time-step
joint target positions by adding the current positions with the estimated exerted torque multiplied by a constant gain and
feeding them in the PID controller, Torobo’s limbs move by following the force exerted on them by the human. This
force-feedback controller is integrated with the PV-RNN, which generates the next time-step target joint angles (Fig.2).
Figure 2: the human interacting with Torobo, the PV-RNN target joint angle generator, the inverse model, and the PID
joint controller.
The overall control diagram is shown in Fig.2, which includes a human experimenter interacting with Torobo, the
PV-RNN target joint angle generator, the inverse model, and the PID joint controller. The inverse model and PID
controller were developed by the manufacturer of Torobo. Details are as follows. Excess torque, τexc
t which is torque
exerted on joints by the human, can be estimated by extracting τdynm
t as the torque inferred for the current position,
velocity, and acceleration of the joints by using the inverse model of Torobo from the current measured torque,τt. Next,
excess torque, τexc
t is applied with a threshold control and et as excess torque after threshold control is obtained. Then,
et is time-ﬁltered with the decay parameter 0 <α< 1, which generates ˜et as the time-ﬁltered excess torque. These
operations are necessary to prevent unnecessary overreaction of the torque estimation against noise. These lines of
1Humanoid-robot Torobo from Tokyo Robotics: https://robotics.tokyo/products/torobo/
7
Human-Robot Kinaesthetic Interaction Based on Free Energy Principle
processes are described in the following mathematical form:
τexc
t = τt −τdynm
t ,
et =
{max(0,τexc
t −τth), if τext
t >0,
min(0,τexc
t + τth), otherwise,
˜et =
{
et, for t= 1,
max
(
α×˜et−1 + et,emax
)
, otherwise,
(16)
where ˜et has a ﬁxed upper limit emax.
The PV-RNN predicts ¯θt next time-step joint angles while performing the online inference, which is subtracted by θt as
the current joint angle to generate ∆θt, the target joint angle difference shown as:
∆θt = ¯θt −θt (17)
Finally, ∆θt representing the next move intended by the PV-RNN and ˜et representing the estimate of torque exerted by
the human are multiplied by each gain kr and kp, respectively and they are added to the current joint angles to generate
the ﬁnal target joint positions, ¯θt, which are fed into the PID controller as shown in the following:
∗
θt+1 = ∆θt ×kr + ˜et ×kp + θt, (18)
3 Experiment
3.1 Experiment Setup
3.1.1 Train Data-Set Preparation
The proposed PV-RNN model was trained in a supervised manner by preparing 4-dimensional joint angle teaching
trajectories. In preparation for teaching trajectories, we considered four types of cyclic movement patterns (20 time-
steps for each cycle) using Torobo’s shoulder and elbow joint angles in both arms. Then, it was assumed that cycling
movement patterns transit from one to another following a probabilistic ﬁnite state machine (Fig.3). For example, after
a movement pattern A is generated for one cycle with the state at S1, A can be generated for one more cycle with 90%
probability staying at the same state, or B or C can transit to S2 or S3 with a probability of 3% and 7%, respectively. We
S1
S2
S3
S4
95 %
5 %
90 %
7 %
3 %
90 %
10 %
15 %
85 %
A
B
B
D
D
DA
C
C
Figure 3: Schematic of the probabilistic ﬁnite state machine from which training data was generated.
prepared 10 sequences that each consisted of 200 cycles of movement patterns, extending 4000 time-steps.
3.1.2 The Network Conﬁguration and Training
To conduct human-robot dyadic interaction experiments, PV-RNN was trained 3 times with identical parameters
(Table.1). #d,#z,τ, wt,wi indicates the number of d neurons and z neurons, time constant, and meta-prior during
the training phase and interaction phase, respectively. The network was trained for 50,000 epochs to minimize the
evidence-free energy shown in Eq.2, starting with random weights generated with different seeds for each training.
Fig.4 shows one of the resultant training processes. It can be seen that prediction error and the KL divergence in both
the top and the bottom layers decreased throughout training. All three training processes converged in a similar way,
achieving prediction errors and KL-divergences shown in Table.2.
8
Human-Robot Kinaesthetic Interaction Based on Free Energy Principle
Table 1: PV-RNN Parameters
#d #z τ wt wi
Layer 1 (Top) 60 6 3 0.01 [0.01,0.05,0.1]
Layer 2 (Bottom) 30 3 9 0.01 [0.01,0.05,0.1]
Figure 4: Resultant time-development from the training phase of one of the PV-RNN models. Development of KLD
with respect to the number of training epochs is shown for (a) the Top layer (layer 2) and (b) the Bottom layer (layer 1).
Development of the average prediction error over time steps in each teaching sequence is shown in (c).
Table 2: Training processes of all three PV-RNN model
Epoch
Model 2,000 5,000 50,000
KL divergence
1 2.0 ×10−3 5.4 ×10−4 4.4 ×10−5
2 2.4 ×10−3 9.1 ×10−4 5.6 ×10−5
3 2.5 ×10−3 7.2 ×10−4 6.9 ×10−5
Pred. Err.
1 1.8 ×10−2 7.2 ×10−4 1.4 ×10−4
2 1.4 ×10−2 6.6 ×10−3 4.6 ×10−5
3 1.3 ×10−2 1.6 ×10−3 1.4 ×10−4
9
Human-Robot Kinaesthetic Interaction Based on Free Energy Principle
3.1.3 Evaluation of the trained networks
Trained PV-RNNs were evaluated on the basis of how closely the state transition probability wasreconstructed compared
with the target (Fig.3). For this purpose, we conducted Prior Generation in which the forward computation by
following Eq.3-7 was performed without any external observation for 40,000 time-steps for each trained network. Then,
probabilities for all possible movement pattern transitions were measured during prior generation and the resulting state
transition probability was inferred (Table.3). The state transition probability computed for all trained networks is quite
similar to the target one. Output and network dynamics of movement pattern transitions during prior generation are
shown in Appendix, Fig.8.
Table 3: Transition probabilities of the Prior Generation and data set
Model S1 →S2 S1 →S3 S2 →S4 S3 →S4 S4 →S1
1 2.8 % 3.5 % 15.6 % 12.5 % 3.1 %
2 2.7 % 9.3 % 14.1 % 11.5 % 7.2 %
3 3.1 % 4.1 % 9.8 % 12.3 % 5.1 %
Target 3 % 7 % 10 % 15 % 5 %
3.1.4 Human-Robot Interaction
After the training phase, we conducted two types of human-robot interaction experiments using trained PV-RNNs. In
these experiments, while Torobo was generating movement pattern transitions successively based on the training, the
human experimenter attempted to induce various movement pattern transitions by grasping both arms of Torobo and
exerting force on them. These movement pattern transitions included trained transitions (AB, AC, BD, CD, DA) and
untrained transitions (AD, DB, DC, BA, CA) in which AB, for example, dictates that A pattern is forced to transit to B
pattern. Each transition from one pattern to another requires some guiding force by a human experimenter, even for
trained transitions, since ongoing patterns tend to repeat another cycle with high probability, more than 85% for all
patterns. This means that there exist some conﬂicts between movement trajectories intended by the robot and those by
the human experimenter during both trained and untrained transitions.
Experiment-1 examined the effect of wi settings on the interactions. While Torobo was generating movement pattern
transitions successively for 2,200 time-steps, the human experimenter attempted to induce one of the trained transition
every 200 time-steps starting from t= 200 which resulted in 10 trained transitions. The duration of each attempt lasted
100 time-steps at the most. This experiment was conducted three times for all three trained PV-RNNs by changing wi
with 0.01, 0.05, and 0.1.
Experiment-2 examined the difference between trained and untrained transitions by setting wi with a ﬁxed value. The
experimental procedure was the same as in Experiment-1, although this time, wi was ﬁxed at 0.01 for both layers.
This experiment, using all three trained PV-RNNs, resulted in 30 untrained transition attempts. In these experiments,
we recorded the time development of essential values including latent variables, predicted and observed joint angles,
prediction error, and the KL-divergence in both layers for later analysis of experiment results.
3.2 Experiment Results
In this section, we show the results of the aforementioned experiments.
3.2.1 Experiment-1
First, we examined time-development of essential values during movement pattern transitions induced by the experi-
menter for each case with a different meta-prior setting. Fig.5 shows an example snapshot of future prediction and
past reﬂection, which shifted every 7 time-steps of the current time during the transition AB performed under different
settings of meta-prior, wi = 0.01,0.05,0.1. Each snapshot shows one of the observed joint angles θ (dotted blue)
and its prediction ¯θ(blue) in the top row, the KL-divergences between the approximate posterior and the prior in the
layer 1 (orange) and in the layer 2(dark orange) in the second row, the prediction error (green) in the third row, and the
excess torque (black) in the bottom row. The grey area represents the past window where the approximate posterior in
terms of adaptive variables Aµ
t,Aσ
t is updated. We provide two supplementary videos for experiment-1 showing the
interaction between the experimenter and Torobo, as well as network dynamics in the case with the meta-prior wi set to
0.01 (video-link1) and 0.1 (video-link2).
10
Human-Robot Kinaesthetic Interaction Based on Free Energy Principle
Figure 5: Time-development of excess torque, prediction error, and KL-divergence between the approximate posterior
and the prior in trained movement transitions in cases with three different meta-prior wi settings with 0.01, 0.05, and
0.1. The grey area represents the past window where the head of the window is the current time. The window is shifted
5 times during the movement transition AB.
The sequences of time-shifted snapshots in Fig.5, show that excess torque appears ﬁrst followed by rises in the prediction
error (negative log-likelihood) and the KL-divergence. Later, the predicted joint angle pattern shifts from pattern A to B
while the gap between the observed joint angle and the reconstructed joint angle remains in the past window. This is the
same for all three cases with different wi settings.
However, we can see some qualitative differences in the transition process depending on the wi setting. The prediction
error and the excess torque in the case with a small wi (wi = 0.01) are smaller than those in the case with large wi
(wi = 0.1). Also, the error and torque with a small wi are less persistent than those with a larger wi. However, the
KL-divergence in the case with a small wi is larger and persists longer than that in the case with large wi. In order to
conﬁrm these observations, we conducted statistical analysis on the excess torque, KL-divergence, and prediction error
time-averaged for each transition period (100 steps). This computation was repeated 10 times for each of three different
trained networks set with three different wi values.
The results are shown in Fig.6. Both the time-averaged prediction error and the excess torque measured in the cases
with small wi are signiﬁcantly smaller than those with large wi. On the other hand, the time-averaged KL-divergence
with small wi is signiﬁcantly larger than that with large wi.
By considering these statistical results, movement pattern transitions exerted by the experimenter require greater force
when wi is set larger, since the approximate posterior distribution strongly follows the prior distribution representing
the current movement intention of PV-RNN, minimizing the KL-divergence between them while the error∆θt between
the predicted joint angle ¯θt and the observed joint angle θt becomes larger. If the experimenter attempts to move the
trajectory of the robot’s joint angles in a direction different from that predicted by the PV-RNN, this requires a large
excess torque ˜et to counteract the large error ∆θt, as derived from Eq. 18.
On the other hand, when wi is set smaller, the approximate posterior follows the prior only weakly, allowing larger
KL-divergence between them while the prediction error becomes smaller. In this case, only a small amount of excess
torque is necessary to counteract the small error. The top-down actional intention of the robot became stronger in the
case of larger wi settings; therefore, the human experimenter was required to exert more force on the robot arms to
induce a transition, whereas less force was required with smaller wi settings, since the top-down actional intention of
the robot became weaker.
11
Human-Robot Kinaesthetic Interaction Based on Free Energy Principle
Figure 6: Time-averaged excess torque, prediction error, and the KL-divergence between the approximate posterior and
the prior in the trained movement transition in cases with three different meta-prior wi settings with 0.01, 0.05, and 0.1.
In the current experiment, the value of wi was set between 0.01 and 0.1. This is because our preliminary experiments
showed that the robot’s behaviour became noisy whenwi was set smaller than 0.01, since the approximate posterior
could easily deviate from the prior by noise sampling. On the other hand, it became difﬁcult for the experimenter to
initiate a transition when wi was set larger than 0.1, because of substantially increased resistance.
3.2.2 Experiment-2
Next, we looked at the difference in the excess torque, prediction error, and KL-divergence between trained and
untrained transitions while wi was ﬁxed at a given value. Both trained and untrained transitions were attempted 10
times for each of the three trained networks with wi set to 0.01 for both trained and untrained cases. However, for
the untrained case, among 30 attempts, only 24 succeeded in performing a transition. Our preliminary experiment
showed that the untrained transition became more difﬁcult when wi was set higher than 0.01 because of the strong
resistance when the robot attempted to lead trained movement transitions. Fig.7 shows the time-average of the excess
torque, prediction error, and KL-divergence for both trained and untrained transitions attempted by the experimenter.
The time-average of the prediction error, the excess torque, and the KL-divergence are larger in untrained than trained
transitions. This means that untrained transitions require the experimenter to exert more force than for trained transitions
because of the free energy, which is the sum of the prediction error and the KL-divergence, and which increases more in
untrained transitions. We provide two supplementary videos for experiment-2 showing the interaction between the
experimenter and Torobo as well as the network dynamics in the case with meta-prior wi set to 0.01, where trained
transitions (video-link1) and untrained transitions (video-link3) are performed.
4 Discussion
The current study investigated human-robot bodily interactions via kinaesthesia using a PV-RNN model that was
developed based on the free energy principle. Bodily interactions between a human experimenter and a robot were
conducted using Torobo, a humanoid robot equipped with a PV-RNN model that can sense excess torque exerted by a
human counterpart. We especially examined how the counter force between the robot and the human experimenter
12
Human-Robot Kinaesthetic Interaction Based on Free Energy Principle
Figure 7: The amount of excess torque, prediction error exerted and the KL-divergence between the approximate
posterior and the priorc during each attempt for trained and untrained transitions. The meta-prior wi was set to 0.01.
changed during movement pattern transitions physically guided by the human experimenter, depending on two different
condition changes.
In experiment 1, we examined how the setting of a parameter called the meta-prior wi, which regulates the KL-
divergence between the approximate posterior distribution and the prior distribution in the interaction phase, affects the
counter force generated in executed or attempted transitions. Results of this experiment showed that in the case of a
smaller wi, while the KL-divergence between the approximate posterior and the prior distribution becomes larger, the
prediction error (negative log-likelihood) becomes smaller. Since the prediction error diminishes, the excess torque
to counteract this error also decreases. On the other hand, in the case of a larger wi setting, while the KL-divergence
between the approximate posterior and the prior distribution becomes smaller, the prediction error becomes larger,
which requires more excess torque for the transition. The conﬂict that appeared between the movement intended by
the robot and that executed by the experimenter is distributed to the prediction error and the KL-divergence between
the approximate posterior and the prior in proportions determined by the meta-prior wi. With larger wi the top-down
movement intention of the robot becomes stronger, which results in a stronger counter force, whereas the top-down
intention as well as the counter force become weaker with smaller wi.
The above is consistent with past research from our group [ 37]. In [ 37], Tani conducted simulation studies on
synchronized imitative interaction by dyadic, vision-based robots using a PV-RNN model. That study showed that
a robot with smaller/larger wi tends to follow or lead the other robot set with a larger or smaller wi with weaker or
stronger actional intention in synchronized imitative interaction. Similarly in the current study, the experimenter easily
led the robot with a smaller wi with a smaller counter force because of the weaker top-down intention of the robot.
On the other hand, when wi is quite large, such as >0.1 for the robot, it was difﬁcult for the experimenter to lead the
robot because of the extremely strong counter force. In this situation, the experimenter just followed movement patterns
strongly led by the robot while grasping the robot’s hands, as shown in the preliminary experiment described previously.
In experiment 2, we examined the difference in the counter force required for the experimenter to execute trained and
untrained movement pattern transitions. These experimental results showed that untrained transitions require more force
since such transitions are accompanied by larger increases in free energy, the KL-divergence (between the approximate
posterior and the prior), and the prediction error. Trained transitions, on the other hand, require less force because of
smaller increases in free energy, the KL-divergence, and the prediction error.
13
Human-Robot Kinaesthetic Interaction Based on Free Energy Principle
As already mentioned, there have been few studies of human-robot interactions based on the free energy principle.
Although [23] showed that the setting of wt as meta-prior in the training phase could strongly affect characteristics of
human-robot kinaesthetic interactions, the description of their experiment results did not include rigorous analysis with
repeated experiments.
The major limitation of the present study is that presented human-robot interactions are not fully interactive since
experimenter-induced sequences of movement pattern transitions were determined a priori. In this regard, Ikegami
and his colleagues [38, 39] investigated underlying mechanisms for turn-taking that were generated by spontaneous
interaction between artiﬁcial agents as well as artiﬁcial agents and humans. Recently, Masumori et al. [ 40] developed a
humanoid robot platform, called Alter3, behaviour of which was controlled by sub-modules, including a self-simulator,
an automatic mimicry unit, and memory storage, which were perturbed by a speciﬁc neurodynamic model for the
purpose of conducting experiments on spontaneous human-robot interactions. They showed that spontaneous turn-taking
between imitator and imitated could be developed by autonomous switching of information ﬂow between the two sides.
In future studies, we will undertake human-robot kinaesthetic interaction experiments that assume less a priori. Such
experiments should be done not with experimenters as counterparts of the robots (as in the present study) but by inviting
an adequate number of human participants, since the human side also needs to be analyzed. Such studies will focus
on two research issues. One is to investigate how spontaneous turn-taking can occur in imitative interaction based
on kinaesthesis by using the active inference framework [12, 14, 41]. Spontaneous turn-taking in this setting means
that the role of the leader to initiate the next shared patterns switches autonomously between the two sides, such that
sometimes the robot may push hard with its own intended patterns and the human counterpart may do so at other times.
This study may require development of an autonomous wi adaptation scheme, since if wi on the robot side can shift
adaptively by sensing contextual ﬂow in the interaction, the leader-follower relationship should shift accordingly.
The other focus is to investigate how novel movement patterns can be developed through repeated kinaesthetic interaction
associated with continuous learning in both robots and human participants, based on the free energy principle. One
assumption is that novel patterns could develop in terms of false memory as the number of movement patterns memorized
distributively in the PV-RNN model increases. This phenomenon of false memory is due to potential non-linearity and
stochasticity in the PV-RNN model. A study on a deterministic RNN model demonstrated this property [42]. Novel
patterns generated by robots could enhance improvisation of new pattern generation from human counterparts through
iterative interaction.
Prior Generation
Here, in Fig.8, we show part of the result from the prior generation, including two transitions. As shown in Table 1,
since the numbers of d neurons in layers 2 and 1 were 30 and 60, respectively, we performed principle component
analysis and reduced both dimensions to 3.
References
[1] Sergio Guadarrama, Lorenzo Riano, Dave Golland, Daniel Go, Yangqing Jia, Dan Klein, Pieter Abbeel, Trevor
Darrell, et al. Grounding spatial relations for human-robot interaction. In2013 IEEE/RSJ International Conference
on Intelligent Robots and Systems, pages 1640–1647. IEEE, 2013.
[2] Takayuki Kanda, Takayuki Hirano, Daniel Eaton, and Hiroshi Ishiguro. Interactive robots as social partners and
peer tutors for children: A ﬁeld trial. Human–Computer Interaction, 19(1-2):61–84, 2004.
[3] Joshua Wainer, Ben Robins, Farshid Amirabdollahian, and Kerstin Dautenhahn. Using the humanoid robot
kaspar to autonomously play triadic games and facilitate collaborative play among children with autism. IEEE
Transactions on Autonomous Mental Development, 6(3):183–199, 2014.
[4] Francisco J Varela, Evan Thompson, and Eleanor Rosch. The embodied mind, revised edition: Cognitive science
and human experience. MIT press, 2017.
[5] Shikhar Bahl, Abhinav Gupta, and Deepak Pathak. Human-to-robot imitation in the wild. arXiv preprint
arXiv:2207.09450, 2022.
[6] Jörg Krüger, Terje K Lien, and Alexander Verl. Cooperation of human and machines in assembly lines. CIRP
annals, 58(2):628–646, 2009.
[7] Luka Peternel, Wansoo Kim, Jan Babi ˇc, and Arash Ajoudani. Towards ergonomic control of human-robot
co-manipulation and handover. In 2017 IEEE-RAS 17th International Conference on Humanoid Robotics
(Humanoids), pages 55–60. IEEE, 2017.
14
Human-Robot Kinaesthetic Interaction Based on Free Energy Principle
Figure 8: Neural activity in prior generation while performing a movement pattern transition. The two columns at the
left show neural activities in layers 2 and 1, respectively. The top row shows the 3 principal components of d values,
and the bottom row shows the mean value µp
t of the prior. The right column shows the joint angle at each time step.
The transition was performed at time step t= 40,340.
[8] Karl Friston. A theory of cortical responses. Philosophical transactions of the Royal Society B: Biological
sciences, 360(1456):815–836, 2005.
[9] Maxwell JD Ramstead, Karl J Friston, and Inês Hipólito. Is the free-energy principle a formal theory of semantics?
from variational density dynamics to neural and phenotypic representations. Entropy, 22(8):889, 2020.
[10] Jelle Bruineberg, Julian Kiverstein, and Erik Rietveld. The anticipating brain is not a scientist: the free-energy
principle from an ecological-enactive perspective. Synthese, 195(6):2417–2444, 2018.
[11] Karl Friston and Stefan Kiebel. Predictive coding under the free-energy principle. Philosophical transactions of
the Royal Society B: Biological sciences, 364(1521):1211–1221, 2009.
[12] Karl J Friston, Jean Daunizeau, James Kilner, and Stefan J Kiebel. Action and behavior: a free-energy formulation.
Biological cybernetics, 102(3):227–260, 2010.
[13] Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, Giovanni Pezzulo, et al. Active
inference and learning. Neuroscience & Biobehavioral Reviews, 68:862–879, 2016.
[14] Thomas Parr and Karl J Friston. Generalised free energy and active inference. Biological cybernetics, 113(5):495–
513, 2019.
[15] Alejandra Ciria, Guido Schillaci, Giovanni Pezzulo, Verena V Hafner, and Bruno Lara. Predictive processing in
cognitive robotics: a review. Neural Computation, 33(5):1402–1432, 2021.
[16] Pablo Lanillos, Cristian Meo, Corrado Pezzato, Ajith Anil Meera, Mohamed Baioumy, Wataru Ohata, Alexander
Tschantz, Beren Millidge, Martijn Wisse, Christopher L Buckley, et al. Active inference in robotics and artiﬁcial
agents: Survey and challenges. arXiv preprint arXiv:2112.01871, 2021.
[17] Tadahiro Taniguchi, Shingo Murata, Masahiro Suzuki, Dimitri Ognibene, Pablo Lanillos, Emre Ugur, Lorenzo
Jamone, Tomoaki Nakamura, Alejandra Ciria, Bruno Lara, et al. World models and predictive coding for cognitive
and developmental robotics: Frontiers and challenges. arXiv preprint arXiv:2301.05832, 2023.
[18] Antonella Maselli, Pablo Lanillos, and Giovanni Pezzulo. Active inference uniﬁes intentional and conﬂict-
resolution imperatives of motor control. PLoS computational biology, 18(6):e1010095, 2022.
15
Human-Robot Kinaesthetic Interaction Based on Free Energy Principle
[19] Alexander Tschantz, Manuel Baltieri, Anil K Seth, and Christopher L Buckley. Scaling active inference. In 2020
international joint conference on neural networks (ijcnn), pages 1–8. IEEE, 2020.
[20] Corrado Pezzato, Carlos Hernández Corbato, Stefan Bonhof, and Martijn Wisse. Active inference and behavior
trees for reactive action planning and execution in robotics. IEEE Transactions on Robotics, 2023.
[21] Jun Tani. Exploring robotic minds: actions, symbols, and consciousness as self-organizing dynamic phenomena.
Oxford University Press, 2016.
[22] Ahmadreza Ahmadi and Jun Tani. A novel predictive-coding-inspired variational rnn model for online prediction
and recognition. Neural computation, 31(11):2025–2074, 2019.
[23] Hendry Ferreira Chame and Jun Tani. Cognitive and motor compliance in intentional human-robot interaction.
arXiv:1911.01753, 2019. accepted for publication in IEEE ICRA2020.
[24] Nadine Wirkuttis and Jun Tani. Leading or following? dyadic robot imitative interaction using the active inference
framework. IEEE Robotics and Automation Letters, 6(3):6024–6031, 2021.
[25] Ahmadreza Ahmadi and Jun Tani. A novel predictive-coding-inspired variational rnn model for online prediction
and recognition. Neural computation, 31(11):2025–2074, 2019.
[26] Yuichi Yamashita and Jun Tani. Emergence of functional hierarchy in a multiple timescale neural network model:
a humanoid robot experiment. PLoS computational biology, 4(11), 2008.
[27] Léo Pio-Lopez, Ange Nizard, Karl Friston, and Giovanni Pezzulo. Active inference and robot control: a case
study. Journal of the Royal Society Interface, 16, 2016.
[28] Guido Schillaci, Alejandra Ciria, and Bruno Lara. Tracking emotions: Intrinsic motivation grounded on multi-level
prediction error dynamics. 10th Joint IEEE ICDL-EPIROB, pages 1–8, 2020.
[29] J. Hwang, J. Kim, A. Ahmadi, M. Choi, and J. Tani. Dealing with large-scale spatio-temporal patterns in imitative
interaction between a robot and a human by using the predictive coding framework.IEEE Transactions on Systems,
Man, and Cybernetics: Systems, 50(5):1918–1931, 2020.
[30] Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A recurrent
latent variable model for sequential data. In Advances in neural information processing systems, pages 2980–2988,
2015.
[31] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
[32] Ahmadreza Ahmadi and Jun Tani. How can a recurrent neurodynamic predictive coding model cope with
ﬂuctuation in temporal patterns? robotic experiments on imitative interaction. Neural Networks, 92:3–16, 2017.
[33] Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE ,
78(10):1550–1560, 1990.
[34] Timothy P Lillicrap and Adam Santoro. Backpropagation through time and the brain. Current opinion in
neurobiology, 55:82–89, 2019.
[35] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating
errors. nature, 323(6088):533–536, 1986.
[36] Wataru Ohata and Jun Tani. Investigation of the sense of agency in social cognition, based on frameworks of
predictive coding and active inference: A simulation study on multimodal imitative interaction. Frontiers in
Neurorobotics, 14:61, 2020.
[37] Nadine Wirkuttis, Wataru Ohata, and Jun Tani. Turn-taking mechanisms in imitative interaction: Robotic social
interaction based on the free energy principle. Entropy, 25(2):263, 2023.
[38] Takashi Ikegami and Hiroyuki Iizuka. Turn-taking interaction as a cooperative and co-creative process. Infant
Behavior and Development, 30(2):278–288, 2007.
[39] Hiroyuki Iizuka and Takashi Ikegami. Adaptability and diversity in simulated turn-taking behavior. Artiﬁcial Life,
10(4):361–378, 2004.
[40] Atsushi Masumori, Norihiro Maruyama, and Takashi Ikegami. Personogenesis through imitating human behavior
in a humanoid robot “alter3”. Frontiers in Robotics and AI, 7, 2021.
[41] Manuel Baltieri and Christopher L Buckley. Pid control as a process of active inference with linear generative
models. Entropy, 21(3):257, 2019.
[42] Jun Tani, Masato Ito, and Yuuya Sugita. Self-organization of distributedly represented multiple behavior schemata
in a mirror system: reviews of robot experiments using rnnpb. Neural Networks, 17(8-9):1273–1289, 2004.
16