# Applications of the Free Energy Principle to Machine Learning and Neuroscience

**Authors:** Beren Millidge

**Year:** 2021

**Source:** arxiv

**Venue:** N/A

**DOI:** N/A

**PDF:** [millidge2021applications.pdf](../pdfs/millidge2021applications.pdf)

**Generated:** 2025-12-12 15:02:16

**Validation Status:** ✓ Accepted
**Quality Score:** 0.80

---

Okay, here’s a revised summary of the paper “Applications of the Free Energy Principle to Machine Learning and Neuroscience” by Beren Millidge, addressing the repetition issues and adhering to your instructions.**Applications of the Free Energy Principle to Machine Learning and Neuroscience**This paper explores the application of the Free Energy Principle (FEP) to both machine learning and neuroscience, offering a novel approach to understanding complex systems. The central argument posits that biological brains, and other systems, can be viewed as performing variational Bayesian inference to minimize a variational free energy, thereby maintaining a non-equilibrium state. This process involves continuously predicting and correcting internal models of the environment, driven by a fundamental drive to reduce uncertainty. The research highlights the potential for scaling up FEP-inspired models to tackle complex tasks, drawing parallels between biological systems and machine learning algorithms.The core methodology employed is a theoretical approach, primarily utilizing mathematical formulations and conceptual arguments. The paper begins with a detailed walkthrough of the FEP, establishing its connection to Ashby’s good regulator principle and concepts from thermodynamics and cybernetics. Specifically, the FEP is presented as a framework for understanding how systems self-organize to maintain themselves at a non-equilibrium state. The paper then investigates the application of this framework to perception, action, and learning.A key element of the research focuses on relaxed predictive coding, a modification of the standard predictive coding model that addresses some of its inherent implausibilities. This involves loosening constraints within the model, allowing for more flexible and efficient learning. The research also examines the implications of this approach for understanding credit assignment in the brain, suggesting that predictive coding can closely approximate backpropagation along arbitrary computation graphs.Furthermore, the paper investigates the use of active inference, a method that frames control as a process of minimizing expected free energy. This approach, utilizing deep artificial neural networks, offers a scalable solution for tackling complex control problems. The research demonstrates how active inference reveals the importance of deep generative models and model-based planning for adaptive action, alongside the role of information-seeking exploration.Finally, the paper explores the mathematical origins of exploration within the FEP framework, demonstrating that optimizing an expected future distribution can lead to a precise match of predicted and desired distributions. This approach offers a fundamental understanding of how systems can effectively seek out novel information and adapt to changing environments. The research concludes by demonstrating the utility of the FEP in interfacing with other fields, fostering advancements in machine learning, neuroscience, and potentially beyond.
