### A deep active inference model of the rubber-hand illusionThis paper investigates the mechanisms underlying the rubber-hand illusion (RHI), a phenomenon where individuals experience a sense of ownership over an artificial limb. The authors propose a deep active inference model to account for the perceptual and motor aspects of this illusion.The authors state: "Understanding how perception and action deal with sensorimotor conflicts, such as the rubber-hand illusion (RHI), is essential to understand how the body adapts to uncertain situations." They note: "Recent experiments have shown that humans also generate meaningful force patterns towards the artificial hand during the RHI [1,16], adding the action dimension to this paradigm." The study demonstrates: "We show that our model, which deals with visual high-dimensional inputs, produces similar perceptual and force patterns to those found in humans."The methodology centers on a deep active inference agent operating within a virtual environment. The authors explain: “The complex mechanisms underlying perception and action that allow seamless interaction with the environment are largely occluded from our consciousness. To interact with the environment in a meaningful way, the brain must integrate noisy sensory information from multiple modalities into a coherent world model, from which to generate and continuously update an appropriate action [13].’ Specifically, the model utilizes a variational free-energy bound [7,17] to optimize the brain variables that encode the variational density approximating the body state distribution and defining a as the action exerted by the agent.The core of the model involves two sensory modalities: visual input (s) and proprioceptive information (p). The authors detail: “Computation of the body state is performed by optimizing the variational free-energy bound [7,17]. Under the mean-field and Laplace approximations and defining µ as the brain variables that encode the variational density that approximates the body state distribution and defining a as the action exerted by the agent, perception and action are driven by the following system of differential equations (see [6,4,19] for a derivation):” The model employs a convolutional decoder and a variational autoencoder (VAE, Fig.1c) to approximate the visual forward model g(µ) and the partial derivative of the error with respect to the brain variables ∂e by means of deep neural networks, inspired by [19].2.1 Generative model learningWe learn the forward and inverse generative process of the sensory input by exploiting the representational capacity of deep neural networks. Although in this work we only address the visual input, this method can be extended to any other modality. To learn the the visual forward model g (µ) we compare two different deep learning architectures, that is, a convolutional decoder (Fig.1b) and a variational autoencoder (VAE, Fig.1c).The convolutional decoder was designed in a similar fashion to the architecture used in [19]. After training the relation between the visual input and the body state, the visual prediction can be computed through the forward pass of the network and its inverse ∂g(µ)/∂µ by means of the backward pass. The VAE was designed using the same decoding structure as the convolutional decoder to allow a fair performance comparison. This means that these models mainly differed in the way they were trained. In the VAE approach we train using the full architecture and we just use the decoder to compute the predictions in the model.2.2 Modelling visuo-tactile stimulation synchronyTo synthetically replicate the RHI we need to model both synchronous and asynchronous visuo-tactile stimulation conditions. We define the timepoints at which a visual stimulation event and the corresponding tactile stimulation take place, denoted t and t respectively. Inspired by the Bayesian causal model [18], we distinguish between two causal explanations of the observed data. That is, C =c signifies that the observed (virtual) hand produced both the visual and the tactile events whereas C =c signifies that the observed hand produced the visual event and our real hand produced the tactile event (visual and tactile input come from two different sources). The causal impact of the visual information on the body state is represented byγ =p(c1 |t v ,t t)= p(t v ,t t |c1 p ) ( p t v (c ,1 t t ) | + c1 p ) ( p t v (c ,1 t t ) |c2)p(c2) (8)where p(t ,t | c ) is defined as a zero-mean Gaussian distribution over the difference between the timepoints (p(t −t |c )) and p(t ,t |c ) is defined as a uniform distribution since under c , no relation between t and t is assumed.