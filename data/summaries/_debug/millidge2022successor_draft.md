### OverviewThis paper investigates the successor representation (SR) as an active inference agent, offering a model-free approach to reinforcement learning. The authors argue that the SR, which represents value functions as a matrix of expected future state occupancies, provides a more efficient and flexible alternative to traditional model-based reinforcement learning. The research demonstrates that SR-AIF (successor-representation active inference) exhibits superior computational complexity and performance compared to standard active inference agents, particularly in terms of planning horizon and computational cost. The paper highlights the SR’s ability to dynamically adapt to changing reward functions, such as variants of the expected free energy, and its capacity to perform exploration and option discovery.### MethodologyThe authors derive the probabilistic interpretation of the SR in terms of Bayesian filtering, designing a novel active inference agent architecture utilizing successor representations instead of model-based planning. They establish a key duality between control and inference, as demonstrated by Todorov (2008,2009), where optimal control can be cast as a Bayesian filtering problem. Specifically, they define the expected free energy (EFE) as a combination of utility maximization and an information gain term, which promotes exploration. The SR is mathematically formulated as a matrix, Mπ, representing the discounted sum of expected state transitions, which can be computed analytically at initialization. The authors use a default policy, π, to assign a uniform probability across all actions, which allows for a direct computation of the SR. The SR-AIF algorithm involves iteratively computing the EFE value function using the SR, which is then used to select actions from the policy posterior. The authors utilize a discrete state-space active inference framework, employing a hybrid inference and control approach.### ResultsThe primary finding of the research is that SR-AIF demonstrates significant computational complexity benefits over standard active inference agents. The authors show that SR-AIF reduces the computational cost of planning, particularly in terms of planning horizon and computational cost. They demonstrate that SR-AIF can accurately represent the value function of the EFE and can be used to dynamically trade-off exploration and exploitation at runtime. The authors provide numerical results, stating that SR-AIF reduces the computational complexity of AIF from exponential to cubic. They show that SR-AIF can be used to perform exploration and option discovery, and that it can adapt to changing reward functions. The authors show that the SR matrix, Mπ, can be computed analytically at initialization, which eliminates the need for iterative computation. The authors state that SR-AIF reduces the computational cost of AIF from exponential to cubic.### DiscussionThe authors conclude that the SR offers a promising alternative to traditional model-based reinforcement learning. The SR’s ability to dynamically adapt to changing reward functions and its capacity to perform exploration and option discovery make it a valuable tool for solving complex reinforcement learning problems. The authors highlight the SR’s ability to represent the value function of the EFE and to dynamically trade-off exploration and exploitation at runtime. The authors state that SR-AIF reduces the computational cost of AIF from exponential to cubic. The research demonstrates that SR-AIF can be used to perform exploration and option discovery, and that it can adapt to changing reward functions. The authors show that the SR matrix, Mπ, can be computed analytically at initialization, which eliminates the need for iterative computation. The authors state that SR-AIF reduces the computational cost of AIF from exponential to cubic.