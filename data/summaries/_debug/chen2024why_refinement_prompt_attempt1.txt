=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: Why collective behaviours self-organise to criticality: A primer on information-theoretic and thermodynamic utility measures
Citation Key: chen2024why
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. Title mismatch: Paper title 'Why collective behaviours self-organise to criticality: A primer on information-theoretic and thermodynamic utility measures' not found in summary

Current draft (first 2000 chars):
Okay, here’s a draft summary of the paper, adhering to all the specified requirements. This is a first draft, and further refinement will be needed.### OverviewThis paper investigates the self-organisation of systems towards critical states, focusing on the role of information-theoretic and thermodynamic utility measures. The authors present a framework for understanding how systems transition to states of high complexity and non-equilibrium. The core concept is that systems, driven by internal forces and external influences, naturally evolve towards critical states, where the balance between order and disorder is maximised. This paper argues that this behaviour is governed by the interplay of information-theoretic and thermodynamic utility measures.### MethodologyThe authors employ a multi-faceted approach to investigate this phenomenon. They begin by defining key concepts, including critical phenomena, self-organisation, information-theoretic utility, and thermodynamic utility. They then present a theoretical framework based on the idea that systems seek to minimise free energy, which is a combination of thermodynamic and information-theoretic considerations. The authors utilise a mathematical model based on the Ising model, a statistical mechanics model that describes ferromagnetism. The model is used to simulate the behaviour of systems under different conditions, allowing for the exploration of the transition to critical states. The authors use a numerical simulation to explore the dynamics of the Ising model, demonstrating how the system evolves towards a critical state. The simulation parameters are set to mimic real-world scenarios, allowing for a robust assessment of the model’s predictive capabilities. The simulation results are presented in tables and figures, providing quantitative evidence of the observed behaviour. The simulation results are analysed and discussed in the context of the theoretical framework.### ResultsThe simulation results demonstrate...

Key terms: thermodynamic, collective, information, utility, systems, criticality, organise, behaviours

=== FULL PAPER TEXT ===
Why collective behaviours self-organise to criticality: A primer on
information-theoretic and thermodynamic utility measures
Qianyang Chen ∗ and Mikhail Prokopenko
Centre for Complex Systems, Faculty of Engineering,
The University of Sydney, Sydney, NSW 2006, Australia
Collective behaviours are frequently observed to self-organise to criticality. Existing proposals
to explain these phenomena are fragmented across disciplines and only partially answer the ques-
tion. This primer compares the underlying, intrinsic, utilities that may explain the self-organisation
of collective behaviours near criticality. We focus on information-driven approaches (predictive
information, empowerment, and active inference), as well as an approach incorporating both infor-
mation theory and thermodynamics (thermodynamic efficiency). By interpreting the Ising model
as a perception-action loop, we compare how different intrinsic utilities shape collective behaviour
and analyse the distinct characteristics that arise when each is optimised. In particular, we high-
light that thermodynamic efficiency — measuring the ratio of predictability gained by the system
to its energy costs — reaches its maximum at the critical regime. Finally, we propose the Principle
of Super-efficiency, suggesting that collective behaviours self-organise to the critical regime where
optimal efficiency is achieved with respect to the entropy reduction relative to the thermodynamic
costs.
I. INTRODUCTION
Self-organisation is a process where a system sponta-
neously develops new structured patterns or functions,
without being explicitly controlled by an external force.
This process is observed in a wide range of natural and
artificial systems, where local interactions among compo-
nents generate global order. As a fundamental concept in
complexity science, self-organisation is extensively stud-
ied in various disciplines, including systems theory, con-
densed matter physics, systems biology, as well as social
sciences.
From a physics perspective, self-organisation is gener-
ally viewed as entropy reduction or increase in order in an
open system “without specific interference from outside”
[1, 2]. From a biological perspective, self-organisation
is typically defined as a pattern-formation process that
relies entirely on interactions among the lower-level com-
ponents of the system [3]. There are three key aspects to
self-organisation [1–5]:
(i) Spontaneous order: the system evolves into a more
organised state without external control;
(ii) Emergence of coherent global behaviour: there is
an observable transition to a more coherent collec-
tive behaviour;
(iii) Local interactions and long-range correlations: sys-
tem components operate on local information but
exhibit long-range interaction and connectivity.
One of the underlying principles for the spontaneous
order created in self-organisation, as suggested by Kauff-
man [4], is the “constraint closure”, which means that the
∗ qianyang.chen@sydney.edu.au
system carries out some work to create constrains on the
release of energy, and those constraints, in turn, channel
the energy to perform more useful work. Thus, a suc-
cessful framework describing self-organisation needs to
account for thermodynamic characteristics of the spon-
taneous order, capturing the corresponding energy flows
and costs.
Typically, self-organised collective behaviours, such
as magnetisation, ant colony foraging, swarming, slime
mould aggregation, flocking of birds, and neural pro-
cessing in the brain, exhibit critical phenomena [6–14].
These phenomena occur at the critical point of a contin-
uous phase transition, and include scale-invariance [15],
divergence of correlation length, and divergence of the
response function [16]. These hallmarks are observed in
physical [17, 18], biological [7, 11, 19], social[20, 21] and
hybrid systems [22].
Scale-invariance means that the system near the criti-
cal regime does not exhibit a typical length scale, i.e.,
patterns appear similar on many magnification levels.
Consequently, the size of events at criticality follows a
power-law distribution. The correlation length measures
the scale on which fluctuations or changes at one point in
the system affect those at another point, and the diver-
gence of this quantity implies the long-range interaction
between constituent components of the system. In the
context of collective systems such as groups of biologi-
cal organisms, long-range interactions may generate more
coherent global behaviour for the group. The response
function characterises the system’s response to pertur-
bations. For example, magnetic susceptibility represents
the change of magnetisation of a material in response to
an applied magnetic field, and is known to diverge at crit-
icality, as even a small field can induce large changes in
magnetisation. At the critical regime, systems typically
become highly sensitive to small changes in parameters,
showing large responses to minor perturbations. Another
implication to collective behaviour in biological systems
arXiv:2409.15668v3  [nlin.AO]  3 Aug 2025
2
(a) Order-disorder phase
transition
(b) Power-law distribution of
event sizes
(c) Divergence of correlation
length
(d) Divergence of
susceptibility
FIG. 1: Schematic representations for second order phase transition and critical phenomena. The control parameter
is a variable that influences the state of the system, such as temperature or pressure, while the order parameter
quantifies the degree of order within the system, with non-zero value only in the ordered phase. (a) Order parameter
changes continuously in response to changes in the control parameter during a second-order phase transition. (b)
Scale-invariance observed at the critical regime indicates a power-law distribution of event sizes, e.g. avalanche or
earthquake magnitudes. (c) Correlation length diverges at the critical point, facilitating long-range correlation of
fluctuations between constituent parts of the system. (d) Susceptibility diverges at the critical point, reflecting the
system’s increased sensitivity to external perturbations.
is that the groups may become more sensitive to stim-
uli from the external environment, such as detection of
predators.
Physical systems, such as fluids or magnets, can be
driven to criticality by adjusting a control parameter,
e.g., temperature or pressure, that influences the state of
the system. As the control parameter reaches a critical
value, the order parameter, which measures the degree
of order or organisation within the system, undergoes
a transition, e.g., from zero (disordered phase) to non-
zero (ordered phase). However, for biological systems,
there are typically no well-defined protocols to adjust the
control parameters. Nevertheless, nature somehow finds
its way to poise the system at or near criticality.
A canonical framework describing the mechanism be-
hind such dynamics is the theory of Self-Organised
Criticality (SOC), initially introduced by Bak et al.
[23] based on a mathematical model known as the
Bak–Tang–Wiesenfeld (BTW) Sandpile Model. Central
to this theory is the interplay of two opposing forces that
push a system to criticality. The first force is the driv-
ing force, characterised by gradual, incremental changes
that increase the system’s energy, disorder, or stress (e.g.,
adding sand to a sand pile). When the accumulated
stress or energy reaches a certain threshold, a stabilising
force comes into play, triggering a response that dissi-
pates or redistributes the energy, typically in a sudden
and possibly widespread manner (e.g., sand avalanche).
Under the influence of these two opposing forces, the sys-
tem “evolves” to criticality and remains there. The con-
cept of SOC inspired a series of studies that applied it
to develop an understanding of the underlying mecha-
nism that generates critical phenomena in various com-
plex systems such as forest fires [24–26], earthquakes [27]
and brain activities [7, 10, 28, 29]. However, it can be
argued that SOC provides a possible explanation for how
criticality occurs, rather than why it benefits the system.
We are interested in exploring the intrinsic utility for
a self-organising system approaching and operating at
the critical regime. Here, an intrinsic utility is under-
stood broadly, as the inherent benefit or value gained
by the system from its own organisation, independently
of external rewards and objectives [30, 31]. Recent re-
search on intrinsic utilities shaping self-organising be-
haviours primarily examined how systems, especially au-
tonomous robots and biological entities, utilise task-
independent objectives in order to optimise and adapt
their behaviours. Notable strategies include predictive
information maximisation [32–38], empowerment max-
imisation [39–44], and free energy minimisation [45–49]
(which encompasses both intrinsic and extrinsic utili-
ties). A consistent feature of these approaches is their
employment of information theory in quantifying the in-
trinsic motivation for the spontaneous order and emer-
gence of collective behaviours. Informally, one identifies
a change in suitably defined entropic quantities with rel-
evant pattern formation at macro-level. Although these
approaches can sometimes induce critical behaviours [36–
38, 50], this outcome is not invariably guaranteed. Thus,
to understand the fundamental drivers of critical phe-
nomena in collective behaviours, it is essential to give a
thermodynamic account of the intrinsic motivation.
The three frameworks mentioned above predominantly
focus on the informational benefit (e.g., increase in pre-
dictability, order or potential influence, reduction in un-
certainty or surprise) without explicitly addressing the
associated energy costs. Although the free energy min-
imisation incorporates the term “energy” in its name,
the utilised concept is an information-theoretic construct
which does not align with the thermodynamic free en-
ergy (more details provided in Section III). As a result,
the trade-offs between informational benefits and ther-
3
modynamic costs are not captured explicitly. Further-
more, the lack of a common example that could be used
to directly compare these intrinsic utilities makes under-
standing these trade-offs more challenging.
In this work, we compare several intrinsic utilities ap-
plied to the same system and explore whether they attain
optimality near the critical regime. In doing so, we high-
light the salient features of these approaches, providing
a concise primer on information-theoretic and thermody-
namic utility measures in the context of self-organisation.
We then formulate a unifying principle connecting (i) the
intrinsic functional benefits of collective behaviour (mea-
sured as entropy reduction or gained predictability) and
(ii) the associated thermodynamic costs. Studies of var-
ious complex dynamical systems such as urban growth
[51], self-propelled particles [52], contagion network [53],
and the canonical Curie-Weiss model for magnetisation
[54], have shown that systems at critical points exhibit
maximum thermodynamic efficiency defined as a ratio of
the gained predictability (i.e., reduction in uncertainty,
or the increase in the internal order) to the amount of
work required to change the underlying control parame-
ter. These studies strongly suggested that the rate of en-
tropy reduction relative to the carried-out work diverges
(peaks in finite systems) at critical points. We further
strengthen the argument by analytically demonstrating
that the thermodynamic efficiency diverges at the critical
point of the canonical two-dimensional Ising model.
We argue that these studies exemplify a general prin-
ciple of super-efficiency: at the critical regime, a self-
organising system of interacting agents achieves the op-
timal thermodynamic efficiency by gaining maximal pre-
dictability of collective behaviour per unit of the ex-
pended work. Informally, one can say that a complex
system finds the regime where the cost of “keeping it
together” is justified. On one hand, given some avail-
able energy to change the control parameter, the system
identifies the control parameter value where the gain in
predictability maximises. On the other hand, given a re-
quired predictability gain, the system finds the point (the
value of the control parameter) where the energy cost as-
sociated with the change would be minimal. The princi-
ple of super-efficiency suggests that this point aligns with
the critical point.
This principle encapsulates the intrinsic utility of self-
organising collective behaviour, elucidating why some
systems gravitate towards criticality. Our discussion will
begin with a background on criticality and phase tran-
sitions provided in Section II. Section III provides an
overview of established intrinsic utility approaches, while
Section IV describes thermodynamic efficiency defined
at the intersection of information theory and thermody-
namics. A common example using the two-dimensional
Ising model is presented in Section V to compare the self-
organising behaviours driven by different intrinsic utili-
ties. Section VI offers a more in-depth discussion on the
principle of super-efficiency, and Section VII summarises
the findings and presents the final discussion. This study
offers insights into the distinct characteristics of collec-
tive behaviour derived within each framework, and em-
phasises how the principle of super-efficiency captures
the intrinsic utility for collective behaviour at the crit-
ical regime.
II. PHASE TRANSITIONS AND CRITICALITY
Generally, there are two types of phase transitions:
first-order phase transition, characterised by discontinu-
ity in the system’s order parameter during the transi-
tion, and second-order phase transition, where the order
parameter changes smoothly and continuously. Critical
phenomena are observed in second-order phase transi-
tions.
The percolation model serves as a canonical example
for understanding phase transition and criticality [15, 55].
Let us consider an infinite-size lattice where each site can
either be vacant or occupied. In the simplest version,
each site is independently occupied with a probability p,
leading to the formation of clusters of connected occu-
pied sites. A percolating cluster is a group of connected
occupied sites that span across the lattice from one side
to the opposite. The percolation phase transition is often
studied by tracing percolating cluster sizes with respect
to the control parameter p (the site occupancy probabil-
ity), and one can distinguish between:
• Largest cluster size (LCS): a low LCS value indi-
cates a more disordered state where most clusters
are small, while a high LCS value indicates a more
ordered state where majority of the sites belong to
the same cluster;
• Average cluster size (ACS) measures the average
cluster size excluding the percolating cluster. This
quantity corresponds to the initial susceptibility in
the scaling theory for magnetic systems [56, 57].
The divergence of ACS at the critical point can be un-
derstood as follows: at low p, predominantly small clus-
ters form, resulting in a small ACS. It increases with p
until the percolation threshold pc is reached. At p = pc, a
percolating cluster forms for the first time, spanning the
entire lattice, and the ACS diverges. Beyond pc, smaller
clusters are progressively absorbed by the giant perco-
lating cluster as more sites become occupied. The ab-
sorption reduces the average size of the remaining finite
clusters, causing the ACS to decrease.
Similarly, the correlation length ξ also diverges at the
critical point. The divergence of ξ indicates that fluctua-
tions at one single site can propagate infinitely far across
the system, reflecting extensive long-range interactions
throughout the lattice at the critical regime.
The relationship between average cluster size ⟨S⟩ or
correlation length ξ and the control parameter p at the
4
(a) Order-disorder phase transition (largest cluster size)
(b) Divergence of susceptibility (average cluster size)
FIG. 2: Site percolation model: largest cluster size and
average cluster size (excluding percolating cluster)
simulated for different lattice sizes L [58]. Vertical axis
in log-scale.
critical regime can be expressed as:
⟨S⟩ ∝ |p − pc|−γ (1)
ξ ∝ |p − pc|−ν (2)
where γ, νare critical exponents, positive numbers whose
value depends on the dimension of the system. Put sim-
ply, at criticality, the system is very sensitive to small
perturbations.
The critical phenomena observed in the lattice model
provide insights into the dynamics of collective behaviour
near the critical point. Structurally, a percolating cluster
spanning the entire lattice forms when the control pa-
rameter reaches its critical value. Functionally, the sys-
tem demonstrates a heightened responsiveness to changes
in control parameters, exhibiting long-range correlations
between its constituents, characterised by the divergence
of average cluster size and correlation length. The ability
to propagate information over long distances and max-
imise responsiveness enables more coherent global be-
haviour and enhances the collective group’s sensitivity to
external changes, thus offering advantages for the system
to operate at or near the critical regime.
The ubiquity of critical phenomena in nature led to vig-
orous research into potential mechanisms generating crit-
icality. In the 1980s, the theory of Self-Organised Crit-
icality (SOC) was proposed by Bak, Tang and Wiesen-
feld [23] as a possible explanation. Under SOC, specific
dissipative dynamical systems naturally evolve towards
criticality regardless of their initial states. At the core of
the SOC models is the presence of a slow driving force
that pushes a subcritical system towards critical or super-
critical state, and a fast regulating force that brings the
system back from supercritical states. The driving force
permits the build-up of energy that is later released by
the regulating force, propagating throughout the system
via localised interactions.
One of the classical SOC models is the Drossel and
Schwabl forest-fire model [24, 25], which enhances the
percolation model. Let the occupied sites represent trees
grown in the forest. Additionally, the model includes
another dynamics: lightning strikes a random site with a
probability of f. At each time step, the lattice sites are
updated based on four rules:
1. on an empty site, a tree grows with probability p;
2. lightning strikes a random site in the forest with
probability f, turning a tree into a burning tree;
3. a tree burns if at least one of its adjacent neighbours
is burning;
4. a burning tree turns into an empty site, and the
model runs indefinitely.
Following the above rules, as trees grow, they form
clusters that contribute to propagation of forest fires.
The tree growth probability at any given site mirrors
the site occupancy probability in the percolation model.
Lightning acts as an external factor that regulates tree
growth. While the percolation model requires a manual
adjustment of p to reach the critical point, the dynam-
ics of the DS forest-fire model self-regulates to criticality.
Thus, critical phenomena, such as long-range correlations
and scale-invariance, are considered to be self-organising
in response to the model’s inherent dynamics.
The system’s behaviour is governed by two opposing
forces: lightning strikes and tree growth. When trees
are sparse, the chance of them getting hit by a lightning
strike is low; therefore, trees continue to grow. When
tree density goes beyond the critical density, giant clus-
ters form, spanning the entire lattice. A lightning strike
on such a cluster results in a fire that consumes the entire
cluster, returning the system to a subcritical state. The
5
system approaches a steady state at the critical point un-
der the influence of these two forces, where the fire event
sizes follow a power-law distribution and the correlation
length ξ diverges (Figure 3).
(a) Power-law distribution of event sizes
(b) Divergence of correlation length
FIG. 3: DS forest-fire model: event size distribution and
correlation length plots (log-log scale), simulated on a
100×100 square lattice [58]. Critical point occurs in the
limit f/p → 0 [25].
The dynamic process that drives the system to the crit-
ical point hinges on the condition of “separation of time
scales”. This condition specifies that the time required
to burn down the whole cluster is much shorter than the
time it takes to grow a tree, which in turn is much shorter
than the time between two lightning strikes at the same
site. This condition translates to a slow dynamic of the
external driving force (the lightning) and a fast dynamic
of local interaction (fire spreading and tree burning). It
ensures that (1) no trees grow during a burn, and (2)
there is a substantial energy accumulation between light-
ning events, leading to the destruction of many trees by
a single lightning strike [24].
There is an ongoing debate regarding the characteris-
tics of the power-law distribution and criticality gener-
ated by the forest fire model or SOC models in general.
For example, numerical results indicate that the scaling
law observed in the forest-fire model is transient and does
not hold at larger scales [59]. Findings of [60] challenge
the view that SOC systems inherently exhibit exact criti-
cal scaling, but suggest that the forest-fire model demon-
strates weak criticality — a concept that is based only
on the way that correlation length diverges, without the
requirement for an exact power law distribution.
The theory of Self-Organised Criticality elucidates
the mechanism behind self-regulation of the dynam-
ics towards a critical point where scale-invariance and
long-range correlations may induce collective behaviours.
However, SOC does not explain what intrinsic utility is
attained by the collective behaviour of the system op-
erating at the critical regime. The subsequent sections
will review several approaches that explore the issue of
intrinsic motivations.
III. INFORMATION-DRIVEN
SELF-ORGANISATION
Information-driven self-organisation is an active area
of research that applies information theory to study
the behaviours of an agent or a group of agents. The
information-theoretic utility functions used to derive the
behaviours have the advantage of being universal and
domain-invariant. These measures are considered strong
candidates for capturing the informational benefit of in-
creased order in collective systems. Two broad categories
can be identified: purely intrinsic, and “hybrid” mea-
sures which incorporate, in addition, an extrinsic target
or preference.
In this work, we adopt the following notation:
• Information theoretic quantities (definitions pro-
vided in Appendix A):
– entropy H(.)
– conditional entropy H(.|.)
– mutual information I(.; .)
– Kullback-Leiber divergence D(.||.)
• Capital letters W, S, A, M... for random variables;
• Small letters w, s, a, m... for a realisation of the
corresponding random variable;
• Letters I, E, F for quantities computed using the
information-theoretic approaches, corresponding to
predictive information, empowerment and varia-
tional free energy, respectively;
6
• Blackboard bold font E, S, F, W, Q, I for thermo-
dynamic quantities or statistical quantities, corre-
sponding to energy, entropy, thermodynamic free
energy, work, heat and Fisher information respec-
tively. Technical preliminaries are provided in Ap-
pendix B.
Additionally, we adopt the notation where subscriptxt
denotes the state at time t in a time series, and super-
script x(i) indicates the ith instance in the population.
A. The perception-action loop
Approaches formalising information-driven self-
organisation typically assume an underlying model of
agent-world interaction. This interaction is generally
modelled with a perception-action (or sensorimotor)
loop, using random variables to reflect the probabilistic
nature of the dynamic. Figure 4 illustrates the causal
network of the perception-action loop traced over time,
where Wt, St, At, Mt represent the state of the world,
the sensor, the actuator and memory (the controller)
at time t. The perception-action loop captures the
following dynamics:
• At any given time t, the world state Wt leads to an
update of the agent’s sensory state St. The map-
ping from W to S is specified by kernel β : W → S,
representing the agent’s sensory mechanism;
• The agent’s memory (or controller)Mt is influenced
by both memory from the previous time step Mt−1
and the current sensory St, a relationship repre-
sented by kernel ϕ : M × S → M;
• Depending on the memory state, the agent updates
its action At according to the policy π : M → A.
Mt also carries through to the future Mt+1.
• The action At and the world state Wt jointly up-
date the next world state Wt+1. The mapping is
specified by kernel α : W × A → W, representing
the agent’s actuation mechanism.
It is worth noting that α and β are kernels that capture
the agent’s embodiment in terms of the agent’s sensor
and actuator capabilities. They set constraints to how
the agent may explore the environment, act and learn
[61, 62].
In the next subsections, we will elaborate on the vari-
ations of causal network representations within different
intrinsic utility frameworks. A common example will be
presented in section V.
B. Intrinsic utility approaches
The notion of “intrinsic utility” suggests that the util-
ity provided to the agent is internal and task-independent
FIG. 4: Causal structure of perception-action loop of
an agent with memory, traced over time.
[30, 63]. Predictive information maximisation [32–38]
and empowerment maximisation [39–44] are two impor-
tant approaches that utilise information-theoretic mea-
sures as intrinsic motivation for inducing self-organising
behaviours.
1. Predictive information
Predictive information [64], also known as effective
measure complexity [65] or excess entropy [66], mea-
sures how much the observed history reduces uncertainty
about the future. In the context of robotic behaviour
development, predictive information in the sensor space
may serve as an objective function for behaviour learn-
ing. Predictive information maximisation has been im-
plemented for memory-less agents [32–38] but potentially
can be adapted to incorporate external memory. Fig-
ure 5 illustrates a reduced causal network for a simple
memory-less agent (reactive control) on which predictive
information is applied.
FIG. 5: Causal structure of perception-action loop of a
memory-less agent traced over time. Coloured blocks
represent the two components that mutual information
is calculated for predictive information.
Predictive information (used as intrinsic utility) is de-
fined as the mutual information between past and future
sensory states [32]. This can be further decomposed into
components that represent the diversity and predictabil-
ity [5, 32]:
I := I(Spast; Sfuture ) = H(Sfuture )| {z }
Diversity of future states
− H(Sfuture |Spast)| {z }
Unpredictability of future
(3)
Considering only one time step into the future, predic-
tive information is defined as:
I := I(St; St+1) (4)
7
and measured in bits. Equation (3) indicates that pre-
dictive information is large when the entropy of the fu-
ture sensory states H(Sfuture ) is large, corresponding to
a rich future experience, and/or when conditional en-
tropy H(Sfuture |Spast) is small, representing a more pre-
dictable future. In both extremes, where there is com-
plete order (no diversity) or complete randomness (no
predictability), the predictive information will be zero.
The Venn diagram in Figure 6 illustrates the re-
lationship between the time series of past and fu-
ture sensory states. We note that conditional entropy
H(Spast|Sfuture ) represents the remaining entropy of his-
torical sensory states given the future states, which is the
part of history that we are unable to reconstruct using in-
formation from the future. For example, reconstructing
the question given the answer to that question.
FIG. 6: The Venn diagram of predictive information
shown as the mutual information (the overlap area)
between past and future sensory states: it represents
how useful the past is for predicting the future.
Behavioural rules can be derived using the predictive
information maximisation approach, forming a policy π
mapping sensory states to actions. Policy π can be ei-
ther deterministic, such as a simple mapping from sensor
values to actions, or stochastic, which is represented by
a conditional probability distribution π ≡ p(a|s). The
general form of the objective function for a one-step pre-
dictive information-driven agent is then expressed as:
π∗(at, st) = arg max
π(at,st)
{I} = arg max
π(at,st)
{I(St+1; St)} (5)
where π∗ denotes the optimal policy.
An agent motivated to maximise predictive informa-
tion chooses policies that result in more diverse and, at
the same time, predictable outcomes. In the context of
collective behaviour, maximising predictive information
has also been shown to induce cooperative behaviour un-
der decentralised control [34, 36, 67]. The increase of
predictive information differs from merely reducing ran-
domness in the system; it enhances the richness of struc-
ture in the collective system. As shown in Section V,
collective behaviour resulting from maximising predic-
tive information for each individual may appear random,
but locally, it maintains a high level of diversity, aligned
with the predictability of an individual’s sensory states.
2. Empowerment
Alternatively, we can focus on a specific segment of
the causal network that captures the influence of actions
on subsequent sensory states through the external world
(Figure 7). Empowerment measures this influence as
the maximum amount of information an agent can in-
ject from its actuators ( A) to its sensors ( S) at a future
time via the environment.
FIG. 7: The causal structure of the perception-action
loop of a memory-less agent traced over time. Coloured
blocks represent the components of mutual information
which determines empowerment.
The n-step empowerment is defined as the Shan-
non channel capacity C or maximum mutual informa-
tion between the current sequence of actions An
t =
{At, At+1, ..., At+n−1} and future sensor value St+n [39]:
E : = C(An
t → St+n)
≡ max
p(an
t )
I(St+n; An
t ) (6)
Empowerment is measured in bits. Considering only
the most immediate future, one-step empowerment can
be computed by:
E := max
p(at)
I(St+1; At) (7)
The definition (7) is referred to as general or context-free
empowerment since it measures only the agent’s general
ability to inject information into its future sensory states.
In order to use empowerment as the driver for an agent’s
action, one needs to distinguish between different states
of the environment, so that the agent can make decisions
accordingly. This is achieved by context-dependent em-
powerment. The context refers to the state of the envi-
ronment w that affects the perception-action loop charac-
teristic p(s|a). More specifically, the future sensory state
of the agent is affected by both its past actions and the
historical states of the world. In other words, the same
actions can lead to different distributions of future sen-
sory states when the external environment has changed.
Instead of considering a general action-perception char-
acteristic p(s|a), empowerment should be considered for
a specific world state or context [39, 40, 43, 68]:
E(wt) := max
p(an
t |wt)
I(St+n; An
t |wt) (8)
Given that an action at stochastically leads to a collec-
tion of possible future world states Γ, the resulting aver-
8
age context-dependent empowerment is computed as:
E(Wt) :=
X
wt∈Γ
p(wt)E(wt) (9)
This quantity can be used as an objective function for an
empowerment-driven agent to make decisions on which
action to take. More commonly, the state of the world W
would be replaced by some context K that approximates
it if the full world information is not available.
The general empowerment, as defined in equations (6)
and (7), is different from the average context-dependent
empowerment [43]. General empowerment does not
consider the varying influence of actions in different
states, since the channel capacity is computed using
only p(s|a) = P
w p(s|a, w)p(w). In contrast, the aver-
age context-dependent empowerment, as defined in equa-
tion (9), captures the nuanced ways in which different
states can affect the actuation-sensing channel by com-
puting max p(a|w) I(S; A|w), and then average over all
possible states.
The objective function for an n-step empowerment-
driven agent is:
a∗
t = arg max
at
{E(Wt+1)}
= arg max
at
| {z }
empw-driven
nX
w
p(wt+1) max
p(an
t+1|wt+1)
| {z }
free to act
I(St+n+1; An
t+1|wt+1)
| {z }
potential empowerment
o
(10)
where a∗ denotes the optimal action under which average
context-dependent empowerment is maximised.
Referring to the maximisation expression in equations
(6) – (8) and (10), we emphasise that p(a) is assumed to
be chosen without constraints, that is, an empowerment-
driven agent is free to act, so that the channel capacity
can potentially be achieved. This needs to be distin-
guished from predictive information maximisation, where
the agent’s action is mapped to the sensory input via a
policy π and hence, is constrained.
Furthermore, equation (10) indicates that the action
selected at time t is such that the potential empower-
ment is maximised at time t + 1. Therefore, the cho-
sen action a∗
t is different from the action distribution
p∗(an
t+1|wt+1) that maximises the mutual information
[43, 68]. As pointed out in [43], “Empowerment consid-
ers only the potential information flow, so the agent will
only calculate how it could affect the world, rather than
actually carry out its potential.” .
Equation (10) also implies dependence on the current
state of the world wt, as the context of the empowerment
(wt+1) is a combined result of the current world state wt
and the evaluated action at.
Similar to predictive information, a decomposition of
the mutual information in equation (6) is shown in Figure
8. To intuitively understand two conditional entropies,
we utilise the box-pushing example presented in [39]: a
grid world with a robot that can move anywhere except
where the box is. If the box is pushable but the robot’s
sensors cannot capture the box’s location, then the robot
cannot perceive its box-pushing actions. This is captured
in H(An
t |St+n), the unperceivable actions. On the other
hand, if the robot can see where the box is but cannot
move it, then this information is reflected inH(St+n|An
t ),
FIG. 8: The Venn diagram of mutual information
I(St+n; An
t ). Empowerment is the maximum of this
mutual information for a given action channel.
the unactionable sensory information. Only the amount
of information that is both actionable and perceivable
contributes to empowerment.
In summary, an empowerment-driven agent takes ac-
tions that maximise its ability to influence the ex-
ternal world in ways that are perceivable by its own
sensors. In multi-agent settings, it has been shown
that empowerment-maximisation for individual agents
leads to spontaneous coordination among the collective
[40, 69, 70]. This coordination arises because shared in-
formation enhances an individual’s empowerment, or in-
formally, its ability to make an influence.
Examples of predictive information and empowerment
in collective systems, along with their comparisons, are
presented in Section V.
9
C. Beyond intrinsic motivation
Another prominent approach to derive behaviours
based on fundamental principles is the free-energy princi-
ple which offers a formal account for the representational
capacities of physical systems [46]. The free-energy prin-
ciple was initially proposed by Friston et al.[45] as an
attempt to explain embodied perception-action loops in
neuroscience, thus providing an understanding of the dy-
namics of the brain and decision-making. Adoption of
this principle led to wide applications in the study of
learning [47, 48, 71], evolutionary dynamics [72], social
interactions [73] and collective intelligence [50, 74]. The
principle centres on the idea that self-organising biologi-
cal agents have a natural inclination to resist disorder. It
is argued that, as a result, the brain attempts to minimise
uncertainty or surprise.
The mechanism derived from the free-energy principle
is commonly referred to as active inference. Similar to
predictive information and empowerment, active infer-
ence can be conceptualised under the perception-action
loop representation, although based on different relation-
ships between state variables (Figure 9).
FIG. 9: The diagram illustrates interactions between
elements in the active inference framework. Solid lines
represent influences between components. Dash lines
represent directed influence from sensory to internal or
from action to external, which correspond to the two
stages of active inference. Figure is adapted from [49].
An underlying assumption in active inference is that
the brain makes Bayesian inference over the external
(world) states. Bayesian inference relies on some prior
probability distribution over the unknown world and up-
dates the distribution when more information is avail-
able. The main ingredients in the formulation of active
inference are the generative model p and the approxi-
mate posterior distribution q. The generative model p
maps causes (external states W) to consequences (sen-
sory S, action A and internal state M). It encodes the
dynamics of the external world and integrates the agent’s
prior preferences of behaviour [49]. While Bayesian in-
ference relies on updating the prior distribution p to the
posterior p(·|observations) given the observed data, the
posterior is notoriously costly to compute. To reduce
the computational difficulty, a parameterised distribu-
tion q is employed as an approximation to the true pos-
terior p(·|observations). The approximation distribution
q is parameterised by the internal states M, which sup-
plies the sufficient statistics of the conditional distribu-
tion. The Bayesian inference process with an approxi-
mated posterior distribution is referred to as variational
Bayesian inference. It is worth noting that the integra-
tion of external goals in the generative model sets active
inference apart from the other pure intrinsic motivation
approaches.
The variational free energy is defined as the Kull-
back–Leibler divergence between the approximate pos-
terior distribution q and the generative model p. The
expression can be expanded in terms of the difference
between a term that resembles expected energy and an
entropy term, hence the name “free energy”[49]:
F := Eq

log q(w)
p(s, a, m, w)

= Eq[−log p(s, a, m, w)]| {z }
Expected energy
−Eq[−log q(w)]| {z }
Entropy
(11)
However, the quantity called in this approach “free en-
ergy” is different from the thermodynamic free energy. In
active inference, it is instead the variational free energy
formulated in terms of information-theoretic quantities,
relating to the Bayesian inference process [45, 49]. Infor-
mally, anything that can be represented in the form:
free energy = energy ± const. × entropy (12)
can be interpreted as “free energy” [75].
Active inference involves two alternating stages: belief
update and action selection. During belief update, the
agent optimises the internal representation of the gener-
ative model p given the sensory samples; in action selec-
tion, the agent’s action ensures that it samples sensory
data that aligns with its current representation. The be-
lief update stage addresses uncertainty about the cur-
rent generative model, while the action selection stage
addresses uncertainty about the future (including future
hidden states and future observable outcomes) [48, 76].
The active inference approach has been shown to gen-
erate collective behaviour in a group of individuals, each
driven by the free-energy minimisation scheme [50]. Col-
lective dynamics are influenced by the individual’s belief
about uncertainty and can also be tuned to the chang-
ing environment by parameter learning over a slower
timescale.
Equation (11) can be rearranged in terms of the com-
plexity of the internal model and the accuracy of its rep-
resentation. In this configuration, minimising free energy
is equivalent to reducing complexity, consequently result-
ing in optimised energy consumption [49, 77]:
F = Eq[log q(w) − log p(w)] − Eq[log p(s, a, m|w)]
= KL[q(w)||p(w)]| {z }
Complexity
−Eq[log p(s, a, m|w)]| {z }
Accuracy
(13)
While equation (13) implies a connection between min-
imising free energy and reducing energy cost under Lan-
dauer’s principle [77], the relationship is not explicitly
formulated as a ratio of informational gain to energetic
costs.
10
A more detailed example of free-energy minimisation
and the comparison with other approaches is provided in
Section V.
IV. THERMODYNAMIC EFFICIENCY
At this stage we point out that the three information-
theoretic approaches reviewed in the previous section do
not explicitly account for the corresponding energy costs.
Thermodynamic efficiency, on the other hand, takes into
consideration both the benefits and the associated costs
of maintaining order within the system. Before present-
ing a formal definition of thermodynamic efficiency, it is
important to differentiate between thermal and thermo-
dynamic efficiency.
A. Thermal vs thermodynamic efficiency
Let us consider a system undergoing a non-ideal pro-
cess in which it receives energy and performs useful work.
Not all the received energy is converted into work; some
is inevitably lost as heat, which does not contribute to
work output (Figure 10). Thermal efficiency measures
the system’s efficiency of converting energy to work, and
is defined as the ratio of useful work output to total en-
ergy input, both measured in joules, rendering it a di-
mensionless quantity. In a non-ideal process, the second
law of thermodynamics implies that this ratio is less than
one.
In contrast, thermodynamic efficiency assesses the con-
version of work into the system order, measured during a
quasi-static change in the underlying control parameter.
It pertains to systems involving interactions among mul-
tiple components and considers the benefit of increasing
order within a collective system against the thermody-
namic cost incurred. A system may transition from a dis-
ordered to an ordered state by altering a control param-
eter according to a specific protocol. Thermodynamic
efficiency evaluates how efficiently the system converts
the carried out work into order, at each specific value
of the control parameter (Figure 11). It is quantified as
the ratio of the reduction in the system’s configuration
entropy (predictability gain) to the generalised work per-
formed during the control parameter adjustment (subject
to the unit of the Boltzmann constant kB, e.g., see the
expression for entropy, defined in the context of thermo-
dynamics in (B2)):
η(θ) = −dS/dθ
d⟨βWgen⟩/dθ (14)
where θ is the control parameter, S denotes the configu-
ration entropy of the system, and Wgen denotes the gen-
eralised work performed to change the control parameter.
FIG. 10: Thermal efficiency for a system undergoing a
specific process. It is generally defined as the
dimensionless ratio between the total work output and
the total energy input. Adapted from [78].
FIG. 11: Thermodynamic efficiency for a system. It is
defined as the ratio between the increase in order and
change in the generalized work carried out to generate
the order.
B. Perspectives on thermodynamic efficiency
The thermodynamic efficiency of interactions can be
expressed in two different ways [52, 54] (technical details
are provided in Appendix B):
η(θ) = −dS/dθ
d⟨βWgen⟩/dθ| {z }
In thermodynamic terms
= −dS/dθ
Rθ∗
θ I(θ′)dθ′
| {z }
In computational terms
(15)
where I denotes Fisher information.
Thermodynamic efficiency offers a dual perspective on
the energy dynamics within systems, encompassing both
thermodynamic and computational dimensions. From
the thermodynamic viewpoint, this quantity captures the
gain in internal order within a collective system of inter-
acting agents (e.g., a swarm) relative to the overall work
required to adjust the agent interactions. From a compu-
tational viewpoint, thermodynamic efficiency measures
the increase in predictability (reduction of uncertainty) of
collective action gained by accumulating additional sensi-
11
tivity to changes in the control parameter along the path
θ → θ∗. For example, a swarm may gain predictabil-
ity of a collective response by adjusting the individual’s
alignment strength or the number of effective neighbours
influencing an individual. This, however, may come at
the expense of additional sensitivity to changes in these
parameters, so that coherent motion may be disrupted
by a change of alignment strength or a reduced number
of effective neighbours.
V. A COMMON EXAMPLE
In previous sections, we explored different approaches
to quantifying the intrinsic utility of collective behaviour.
In this section, we compare the four considered ap-
proaches — predictive information, empowerment, ac-
tive inference, and thermodynamic efficiency — using the
canonical Ising model as a common example. This exam-
ple considers a system at equilibrium, in order to provide
a direct comparison of the collective behaviours resulting
from optimising utility functions in the absence of exter-
nal fluxes.
A. The 2D Ising model
The 2D Ising model offers a simplified representation
of ferromagnetism in statistical mechanics. It models a
collection of sites that can each exhibit either an up-spin
or down-spin configuration while interacting with their
neighbours to create a complex aggregate dynamic. The
2D Ising model is particularly relevant to our comparison
due to its characteristic phase transition in the collective
dynamics.
FIG. 12: A lattice of atoms with dipole magnetic
moments. Links in red represent higher energy bonds
(where two adjacent atoms have opposite spins), and
blue represents lower energy bonds (where two adjacent
atoms are aligned).
The 2D-Ising model considers a lattice of atoms with
magnetic spins oriented either up or down (Figure 12).
The vertices of the lattice are referred to as “sites” and
the edges as “links”. Assuming the absence of an exter-
nal magnetic field, the energy of a site is determined by
the total energies in the links with its neighbours. Each
site prefers to be in a lower energy state. For ferromag-
netic materials maintaining a link between two sites with
opposite spins requires additional energy, hence there is a
natural tendency for a site to align its spin with those of
its neighbours. The susceptibility of a site to neighbour-
ing influences depends on the coupling strengthJ. A high
value ofJ indicates strong coupling between sites, leading
to a greater tendency for spins to align with neighbouring
sites.
In this example, the dynamics of the Ising model are in-
terpreted from a perception-action loop perspective: each
site acts as an agent that senses the energy of its neigh-
bourhood and “decides” whether to flip its spin or main-
tain its current state. The agency of each site is deter-
mined by the choice of the coupling strength J. With
a high J value a site is more prone to align with its
neighbours, and vice versa. This parameter governs the
strength of the site’s response to the neighbourhood’s
energy landscape, influencing its decision to align with
neighbouring spins.
To draw a clearer connection between the Ising model
and the perception-action loop, we consider that each
of the four elements of the perception-action loop has a
corresponding representation in the Ising model:
• W (world): the magnetisation (average spin) of the
lattice;
• S (sensory): the energy state of a site (defined in
eq. (16));
• A (action): flipping the spin or remaining un-
changed, with flip = −1 and no-flip = +1;
To quantify the energy, we define:
• N: the number of sites in the lattice;
• J: the coupling strength between adjacent sites;
• σ(i): the spin of site i, with +1 representing up spin
and −1 is down;
• σ: the configuration of the lattice σ =
{σ(1), σ(2), ..., σ(N)}.
Let i,j be two sites connected by a link, then:
σ(i)σ(j) =
(
+1 if sites i, j aligned
−1 if sites i, j misaligned
Considering the interactions between a site and its near-
est four neighbours only, the total energy of this site is:
E(i) =
X
j∈ν(i)
−Jσ(i)σ(j) (16)
where ν(i) denotes the set of neighbouring sites of i.
We simulate the process for both Glauber dynamics
[79] and Metropolis [80, 81] dynamics. At each time step,
12
a site was selected uniformly at random. Under Glauber
dynamics, the site’s spin flips with the probability:
pG(flip) = 0.5
h
1 − tanh(0.5βdE(i))
i
(17)
Alternatively, using Metropolis dynamics, the proba-
bility to flip is:
pM (flip) = min
h
1, e−βdE(i) i
(18)
where β is the inverse of temperature and dE(i) is the
change in energy after a flip. We assume β = 1 for the
purpose of this experiment. The energy change dE(i) is
computed as:
dE(i) = E(i)(after flip) − E(i)(before flip)
=
X
j∈ν(i)
2Jσ(i)σ(j) (19)
The simulation setting is detailed in Appendix C, and
the source code is available in [58]. For the analysis, we
computed all four intrinsic utility measures for systems
at different values of of control parameter J. The goal
is to compare the range of control parameter J that op-
timises each utility and discuss the implications of the
different optimal ranges for each utility measure and the
associated characteristics of self-organised behaviours.
B. Computational results
During the simulations, we hold the coupling strength
J constant and run the simulation until the system
reaches equilibrium. We then calculate the corresponding
predictive information, empowerment, free energy (active
inference), and thermodynamic efficiency. To eliminate
the effects of initial conditions, we average these quanti-
ties across multiple simulations for each J. This process
is repeated for a range of J values. We aim to identify
the optimal range of J values under each approach in
order to answer the question: “If the coupling strength
J evolves independently using each of these quantities
as the fitness function, what behaviour should we expect
when fitness is optimised?”
We collect the following data for the selected site and
the lattice at time t:
• at: the action of flip (-1) or no-flip (+1) at time t;
• σt, σt+1: spin of the selected site before and after
the action is performed;
• wt, wt+1: the magnetisation of the lattice before
and after the action is performed;
• st, st+1: the selected site’s sensory state before and
after the action is performed;
The data form time series {at}, {σt}, {σt+1}, {wt},
{wt+1}, {st}, {st+1}, using which we compute the in-
trinsic utility measures. Leveraging the homogeneity of
the lattice sites, we can aggregate the random samples
from different sites to compute the measures. This ap-
proach ensures that the results represent the intrinsic
utility values corresponding to the coupling strength J
as experienced by an average site within the lattice.
1. Predictive information
For a given coupling strengthJ, the corresponding one-
step predictive information is the mutual information be-
tween the pre-action sensory state ( St) and post-action
sensory state (St+1):
I = I(St+1; St) [bits]
=
X
st
X
st+1
p(st, st+1) log p(st, st+1)
p(st)p(st+1)
(20)
where the probability distributions are parameterised by
J.
Figure 13 shows predictive information I for each cou-
pling strength J, with two main observations highlighted.
Firstly, predictive information is higher under weak cou-
pling and decreases to nearly zero under strong coupling;
this observation applies to both Glauber and Metropo-
lis dynamics. Recall that predictive information can be
decomposed into two terms:
I(St+1; St) = H(Sfuture )| {z }
Diversity of future states
− H(Sfuture |Spast)| {z }
Unpredictability of future
(21)
The overall trend is driven by the diversity component of
the equation. Weak coupling promotes more exploratory
behaviours, leading to diverse sensory states. In contrast,
strong coupling (as J → +∞) prevents sites from flipping
after settling into the lower energy state, resulting in a
predictable system with little sensory diversity.
Secondly, predictive information optimises at different
coupling strengths for Glauber and Metropolis dynam-
ics – a phenomenon driven by the unpredictability term
(Figure 14). Due to different behaviours of the condi-
tional entropies in the sub-critical regime, predictive in-
formation maximises close to the critical point in Glauber
dynamics, while peaks as J → 0 under Metropolis dy-
namics. A detailed comparison of the two dynamics is
provided in Appendix C.
2. Empowerment
To compute the average empowerment of a site at equi-
librium, we first analytically derive the channel capacity
of the action channel. In this model, the action channel
is defined by the conditional probability p(st+1|at). The
13
(a) Glauber dynamics
(b) Metropolis dynamics
FIG. 13: Average predictive information plotted
against different values of J, computed from 100
simulations, each of 20 million time steps on a 50 × 50
square lattice with periodic boundary conditions.
Predictive information maximises when J is small
(weak coupling), where an average site exhibits
explorative behaviour.
state of pre-action sensory st determines this conditional
probability, and thus, two cases must be considered sep-
arately: p(st+1|at, st ̸= 0) and p(st+1|at, st = 0).
When st ̸= 0, meaning that the up and down spins
of the neighbours are not perfectly balanced, a flipping
action will result in the next sensory state becoming the
opposite of what it was before the flip. The action chan-
nel, in this case, resembles the one shown in Figure 15.
This is a noiseless binary channel and, by definition, has
channel capacity C(st) = 1 bit. Full capacity is achieved
when the site follows action distribution p(a) = (1
2 , 1
2 ).
If st = 0, the channel simply reduces to the one shown
(a) Glauber dynamics
(b) Metropolis dynamics
FIG. 14: Decomposition of predictive information into
richness (dotted line) and unpredictability (dash line)
components; predictive information is the difference
between the two curves. Results obtained from the
average over 100 simulations, each of 20 million time
steps on a 50 × 50 square lattice with periodic boundary
conditions.
in Figure 16, that is, a channel that carries no informa-
tion as the output is always the same. This means that
the channel capacity is zero, C(st) = 0 bit.
Combining these two cases, we obtain:
C(st) =
(
1 if st ̸= 0
0 if st = 0 (22)
The average one-step empowerment, averaged over the
distribution of channels, is computed as follows:
¯E =
X
st
p(st)C(st) [bits] (23)
14
FIG. 15: A noiseless binary channel. Channel capacity
C = 1 bit.
FIG. 16: A single output channel. Channel capacity C
= 0 bit.
Empowerment optimises at a strong coupling (Figure
17), where the lattice stabilises with most atoms aligned
at the equilibrium. Empowerment measures an agent’s
ability to inject information into the environment via
current actions and later retrieve the information via its
sensors. A site’s action is most perceivable when all its
neighbours align in the same direction, in which case the
action of flip or no-flip leads to distinct sensory outputs
(s or −s, s ̸= 0). Conversely, if four neighbours have
an equal split between up and down spins, flipping the
spin of a site does not change its sensory state. That
is, the site will not be able to perceive the impact of its
action. A large positive J value increases the probability
of an average site being at the configuration where all
its neighbours have the same spin, thereby maximising
the site’s empowerment by ensuring its actions produce
noticeable changes to its future sensory inputs. Empow-
erment is not affected by the choice of spin-flip dynamics
(Glauber or Metropolis).
3. Variational free energy (active inference)
For the purpose of this study, the active inference
framework is adopted from [82], thus focusing solely on
its intrinsic component. A negative sign is placed before
the expression, effectively transforming the minimisation
problem into maximising the action value −F. For each
possible action at ∈ {−1 (flip), +1 (no-flip)}, we compute
the one-step negative free energy following the derivation
(a) Glauber dynamics
(b) Metropolis dynamics
FIG. 17: Average empowerment plotted against
different values of J, computed from 100 simulations,
each of 20 million time steps on a 50 × 50 square lattice
with periodic boundary conditions. Empowerment
optimises at strong coupling, where the collective
stabilises to a uniformly aligned configuration at
equilibrium. Such configuration maximises an average
site’s ability to retrieve the impact of its action through
future sensory states.
from [82]:
−F(at) = −H(St+1|Wt+1, at)
=
X
st+1
X
wt+1
p(st+1, wt+1|at) logp(st+1|wt+1, at)
(24)
where the probability distribution p(.) is parameterised
by coupling strength J.
The average negative free energy at equilibrium is
15
(a) Glauber dynamics
(b) Metropolis dynamics
FIG. 18: Average negative free energy plotted against
different values of J, computed from 100 simulations,
each of 20 million time steps on a 50 × 50 square lattice
with periodic boundary conditions. Negative free
energy maximises at strong coupling, where the spins
are aligned at equilibrium, and the site experiences
minimum surprise comparing its approximate
distribution of the world state (based on local sensory
information) to the actual distribution that generates
the world states (global property).
computed by weighted average across the proportion of
at = −1 (flip) and at = 1 (no-flip) actions:
− ¯F = −
X
at
p(at)F(at) [bits] (25)
For a given coupling strength J = j, this measure repre-
sents how much discrepancy an average site should expect
between its internal model (based on the local sensory
history) and the external world (described by the lattice
magnetisation); in other words, how well the site’s local
information aligns with the underlying global situation.
The negative free energy plot (Figure 18) reveals a sim-
ilar optimal region for J as empowerment. Maximising
negative free energy effectively minimises surprise. When
J → 0, all the sites are actively flipping their spins. A
chosen site’s local sensory states can take on all possi-
ble values, while the global magnetisation averaged to
zero. This mismatch results in large surprise for an av-
erage site. On the other hand, more atoms are aligned
at large J, increasing the likelihood that an average site
correctly predicts the overall spin direction, hence reduc-
ing surprise. The same trend holds for both Glauber and
Metropolis dynamics.
4. Thermodynamic efficiency
Thermodynamic efficiency η for each corresponding J
is computed as:
η = − dS(J)/dJRJ∗
J I(J′)dJ′
(26)
The numerator is the derivative of the configuration en-
tropy S of the lattice with respect to the control param-
eter J. It represents the reduction of uncertainty in the
lattice’s configuration as a result of a small variation in
the coupling strength J. Using the Kikuchi approxima-
tion, the configuration entropy is [83, 84]:
S = S4 − 2S2 + S1 (27)
where Sk is the entropy of size k sub-lattices.
The denominator in this calculation is the integral of
Fisher information with respect to the control parame-
ter J, representing the work required by the system to
instigate the change δJ. The integration limit extends
from J, the point of evaluation, to J∗, the zero-response
point. Ideally, J∗ = ∞, but in this numerical experi-
ment, setting J∗ = 10 is sufficient, ensuring that the sys-
tem reaches perfect order at equilibrium and no further
work can be done. The method for numerically comput-
ing Fisher information is detailed in Appendix D.
Thermodynamic efficiency reaches optimum when J is
near the critical value Jc ≈ 0.4407 [85], as shown in Fig-
ure 19. At the vicinity of the critical point, even a small
increase in the control parameterJ results in a significant
reduction in the system’s disorder. Consequently, the
work performed to establish order in the system achieves
the highest efficiency. This observation indicates that
the collective systems that optimise thermodynamic ef-
ficiency at the same time operate at the critical regime.
The same result holds for both Glauber and Metropolis
dynamics.
It is important to note that the numerical values of
thermodynamic efficiency tend to exhibit more noise than
other metrics. This is due to the computation of the
16
(a) Glauber dynamics
(b) Metropolis dynamics
FIG. 19: Average thermodynamic efficiency plotted
against different values of J, computed from 100
simulations, each of 20 million time steps on a 50 × 50
square lattice with periodic boundary conditions.
Thermodynamic efficiency optimises at the critical
regime, where a significant portion of the work
expended in tuning the parameter J contributes to
reducing configurational entropy; that is, the collective
is most energetically efficient in creating internal order.
entropy derivative and Fisher information. Thermody-
namic efficiency peaks slightly to the right of the critical
value due to finite-size effect, as shown in figure 20. De-
spite these numerical nuances, the presented computa-
tional results suggest that optimising thermodynamic ef-
ficiency within a collective system is achieved at the crit-
ical regime. This argument frames the thermodynamic
efficiency as an intrinsic utility, and provides an expla-
nation why collective behaviours induced by this utility
often exhibit critical phenomena.
(a) Glauber dynamics
(b) Metropolis dynamics
FIG. 20: Finite-size analysis near the critical coupling
Jc for both Glauber and Metropolis dynamics. Results
show that as lattice size increases, the peak of
thermodynamic efficiency approaches the critical
coupling strength Jc.
VI. PRINCIPLE OF SUPER-EFFICIENCY
Many natural systems with a large number of interact-
ing components exhibit self-organisation, forming larger
structures or coherent collective behaviours without ex-
ternal coordination. Locally interacting neurons collec-
tively perform complex brain functions while processing
diverse stimuli [7, 10]. Active matter comprising self-
catalytic colloidal particles produces polar collective mo-
tion [86, 87]. Starlings form flocks that move in intricate
patterns in response to environmental changes [11, 88].
These self-organising collective behaviours are often ob-
served at critical regimes which seem to balance the flu-
17
idity (or adaptability) offered by amorphous, disordered
groups (e.g., granular materials or liquids) and the stabil-
ity (or persistence) provided by rigid, ordered structures
(e.g., crystalline materials).
The ubiquity of collective behaviours that tend to self-
organise near or at critical regimes suggests that there is
an underlying principle governing such behaviour across
different systems. By abstracting from the specific details
of each system, we may uncover not only why collective
systems self-organise, but also why self-organisation often
brings the collective close to the critical regime. A pos-
sible underlying principle would need to interpret these
behaviours in terms of generic intrinsic utilities.
Motivated by these observations, we propose the prin-
ciple of super-efficiency :
Self-organising collective systems strive to
maximise the thermodynamic efficiency of in-
teractions by gaining maximal predictability
of collective behaviour per unit of the ex-
pended work. This efficiency is maximised at
the critical regime.
In general, the gained predictability enhances coordi-
nation within the system, facilitating efficient interac-
tions. Systems with high or maximal thermodynamic
efficiency tend to operate at or near the critical regime
where long-range correlations and scale invariance bring
collective benefits. Thus, thermodynamic efficiency may
provide an intrinsic utility to the system by balancing en-
ergy costs with group coherence. This would explain the
ubiquity of collective animal behaviours, such as swarm-
ing, herding, flocking, which self-organise to criticality.
Formally we express the principle of super-efficiency
via maximisation of the ratio between the reduction in
configuration entropy and the incurred generalised work
(equation (15)). Given a protocol adjusting the inter-
actions among the constituent components of the sys-
tem, maximisation of the thermodynamic efficiency oc-
curs through tuning the corresponding control parameter
θ1:
θ∗ = arg max
θ
{η(θ)}
= arg max
θ
−∂S/∂θ
∂⟨βWgen⟩/∂θ
(28)
The expression above quantifies the abstract notion of
super-efficiency: a collective system maximises the ratio
of predictability gain to the additional work required to
tune the interactions among its components. In the fol-
lowing subsection, we justify this conjecture by taking
evolutionary, cognitive, and social perspectives.
1 Note that we use a partial derivative to emphasise that this prin-
ciple is applicable for settings with multiple control parameters.
A. Maximising thermodynamic efficiency as an
intrinsic utility
Thermodynamic efficiency can be optimised in two
ways: either by reducing the additional work (i.e., en-
ergy consumption) for a fixed predictability gain or by
increasing the predictability gain given a fixed energy
budget to carry out the additional work.
From an evolutionary perspective, minimising energy
consumption when establishing structural or functional
order preserves resources for survival and reproduction
[89, 90]. Organisms that achieve goals with less energy
per adaptation gain a selective advantage. On the other
hand, given energy constraints on behavioural change, in-
creasing predictability enhances group coordination and
collective survival [91–93].
In terms of cognitive economy [94, 95], individuals
use heuristics and mental shortcuts to reduce the energy
needed for policy and behavioural changes, freeing re-
sources for other cognitive tasks. Under a fixed energy
budget, maximising predictability improves coordination
in complex cognitive and social tasks [96, 97].
For social dynamics, minimising the energy required
for a social transformation conserves resources for the
long-term benefit of society, for both current and future
generations [98, 99]. Alternatively, given energy limita-
tions, maximising predictability enhances strategic plan-
ning and policy implementation, particularly in times of
crisis [100–102].
B. Super-efficiency in canonical models
An earlier study by Nigmatullin et al. [54] derived an
analytical expression for the thermodynamic efficiency of
interactions for the canonical Curie-Weiss model (a fully-
connected ferromagnetic model), showing that it diverges
at the critical point as the temperature θ → θc:
η(θ) =
(
− θc
2kB
(θ − θc)−1 for θ < θc
1
kB
(θ − θc)−1 for θ > θc
(29)
that is:
η(θ) ∝ |θ − θc|−1 . (30)
We can also show that thermodynamic efficiency di-
verges at the critical point for the canonical Ising model
with nearest neighbour interactions. The thermody-
namic efficiency η(J) as the coupling strength J → Jc
is given by (see Appendix E):
η(J) = ln(1 +
√
2)
2 |J − Jc|−1 (31)
that is:
η(J) ∝ |J − Jc|−1 . (32)
18
C. Evidence from simulated systems
Several previous studies of thermodynamic effi-
ciency [51–54], as well as the computer simulation pre-
sented in Section V, exemplify the general principle of
super-efficiency.
Crosato et al. [52] explored this relationship near crit-
icality using a model of self-propelled particles. They de-
fined thermodynamic efficiency η as the reduction in en-
tropy relative to the work done on the system and demon-
strated that as particles undergo a kinetic phase transi-
tion from disordered to coherent motion, η peaks at the
critical regime. In other words, the collective motion be-
comes coherent at the critical point, where the system is
most energetically efficient in reducing its configurational
entropy. Hence we can argue that at this point, the sys-
tem of self-propelled particles is super-efficient (i.e., max-
imally efficient) in coordinating its collective behaviour,
offering a clear utility to the group.
This concept was also applied for urban transforma-
tions [51], where the maximum entropy principle cou-
pled with Lotka-Volterra dynamics was used to study
shifts in population and income distribution in urban ar-
eas. The study considered the thermodynamic efficiency
η expressed as the increase in predictability of the popu-
lation income flows relative to the thermodynamic work
required to adjust the social disposition (i.e., the factor
balancing the suburbs’ attractiveness). The study iden-
tified a phase transition in urban dynamics, where the
number of affluent (i.e., service-abundant) suburbs would
change abruptly in response to a small change in the un-
derlying social disposition parameter. Importantly, the
thermodynamic efficiency was observed to peak at the
phase transition which occurred precisely at the point
balancing monocentric and polycentric urban configura-
tions, providing an intrinsic utility for social dynamics.
In the context of epidemic modelling, Harding et al.
[53] examined the thermodynamic efficiency η of conta-
gions diffusing on a network, defining η as the ratio of
uncertainty reduction in the system to work expenditure
required to quasi-statically change the control parameter
(e.g. the infection transmission rate ). Their numerical
analyses identified a phase transition between sub-critical
(non-epidemic) and super-critical phases (epidemic) as
the infection transmission rate increased, with the high-
est thermodynamic efficiency observed at the critical
regime. In this case, the intrinsic utility of an interven-
tion process, considered as social and public health dy-
namics, is offered by the reduction of pathogen transmis-
sion probability: this utility is represented by thermody-
namic efficiency maximised at the transition from super-
critical to sub-critical phase. Alternatively, the intrinsic
utility for the pathogen evolution, considered as a bio-
logical phenomenon, would be provided by the increase
in the transmission probability: this utility is captured
by thermodynamic efficiency maximised at the transition
from sub-critical to super-critical phase.
In summary, the principle of super-efficiency is sup-
ported by the observed peak in thermodynamic efficiency
at the critical regime in various simulated systems, in-
cluding self-propelled particles, urban transformations,
and epidemic modelling. Despite differences in the appli-
cation settings, the intrinsic utility could be characterised
by maximisation of thermodynamic efficiency achieved at
the critical regime.
D. Evidence from empirical systems
The principle of super-efficiency is further supported
by empirical studies of collective behaviours in natural
systems. While no single empirical study fully justi-
fies the principle, evidence from multiple studies shows
that collective systems tend to maximise predictability
(through internal order) while minimising energy con-
sumption.
Empirical studies of European starling show that their
flocks exhibit scale-free correlations in velocity and speed
fluctuations — a signature of criticality [11, 19, 88].
Long-range correlations enhance collective responsive-
ness to external stimuli (e.g., predators, food resources),
allowing flocks to achieve higher coordination of group
behaviour (i.e., gain predictability). Friman et al. [103]
found that starlings flying in small groups save metabolic
energy compared to flying solo, especially when the in-
dividuals maintain consistent follower positions behind
leaders in V-like formations. These energy savings result
from structural coordination, reflecting group-level en-
ergy optimisation through positional ordering. Together,
these findings suggest that starling flocks operate near
a critical regime, balancing predictability gain with en-
ergy efficiency. Importantly, these studies contrasted dif-
ferent flocking configurations (ranging between extremes
such as solo and coherent groups), so that higher coor-
dination or lower energy consumption can be interpreted
as a gained predictability or additional work required to
change the configuration. The thermodynamic efficiency
is precisely the ratio of these two quantities.
Experiments with ant colonies [6, 104, 105] also show
evidence of disorder-to-order phase transition and signa-
tures of criticality (scale-invariant dynamics) in ant for-
aging activities. These studies suggest that ant colonies
benefit from operating at the vicinity of critical point
where they maintain a certain level of coordination.
These studies show that ants move faster in larger groups,
indicating efficiency gains from social coordination. Por-
firi et al. [106] explained how ant colonies achieve energy
savings through social interactions balancing positive and
negative feedbacks. Such balance keeps metabolic cost
growing sublinearly with colony size, permitting lower
energy consumption per individual for larger colonies.
Collectively, these findings show that ant colonies self-
organise near criticality for maximum efficiency of inter-
actions, where group order is established with low energy
consumption. Again, the comparison between different
group sizes allows for the interpretation of the coordi-
19
nation increase and energy savings as the predictability
gain and additional work respectively. The ratio of these
changes is captured by the thermodynamic efficiency.
The organisation and function of brain network also
show evidence of balancing predictability gain and energy
consumption. Empirical analysis of rat cortex data shows
that sensory representation is achieved by reducing en-
tropy (and conditional entropy) of the neuronal responses
to stimulus [107]. In other words, sensory adaptation
maximises predictability gain in the neuronal activities.
Takagi [108] modelled brain network formation by min-
imising the ratio of activity cost over wiring cost and
found structural similarities — such as hubs and clusters
— between the simulated network and empirical brain
networks from various species. By contrasting differ-
ent network configurations, that is, from completely ran-
dom networks to small-world networks to fully connected
networks, these studies suggest that the brain network
evolves to maximise predictability gain while minimising
energy consumption per adaptation.
E. Implication: why collective systems
self-organise to criticality
The principle of super-efficiency aims to explain why
it is beneficial for a group of interacting agents to oper-
ate at the critical regime, rather than how the system
could self-organise to this point — the latter question
is pursued by SOC models. The underlying rationale is
that for a self-organising system with many interacting
components, being energetically efficient in reducing dis-
order and creating internal coordination is advantageous,
as has been demonstrated by the considered simulation
and empirical studies.
In short, the thermodynamic efficiency offers an intrin-
sic utility for a collective. When this utility is maximised
at a specific configuration (e.g., coupling strength, net-
work connectivity, social disposition), it optimises evolu-
tionary fitness, cognitive economy, or social welfare. In
the considered studies the super-efficiency of collective
behaviour has occurred at the critical regime, which is
expected, as confirmed by the divergence of thermody-
namic efficiency at the critical point in canonical models.
Thus, super-efficiency may provide a general principle for
understanding why collective systems self-organise to the
critical regime.
To re-iterate, a super-efficient self-organising system
approaches the point where it can gain maximal pre-
dictability of its collective behaviour, given the amount
of additional work available to change the control param-
eter. Alternatively, given a desired predictability gain, a
super-efficient system seeks the point where the energy
cost of changing the control parameter is minimal.
VII. DISCUSSION
Is there an intrinsic utility for self-organising collective
systems to operate at the critical regime ? In attempting
to explore this question, we overviewed notable intrinsic
utility measures, using both information-theoretic and
thermodynamic perspectives. The considered measures
were directly compared using a common example which
we constructed in order to connect the canonical 2D Ising
model to the perception-action loop.
The connection is established by conceptualising each
site in the Ising lattice as an agent possessing sensory-
motor capabilities, thereby linking the model to the
perception-action loop framework. The choice of spin-flip
dynamic is analogous to the embodiment of the agents.
Optimisation of the control parameter — the coupling
strength J — may be considered analogous to choosing
the sensory channels [109] that maximises specific intrin-
sic utility given the embodiment.
Optimal J values were computed for different ap-
proaches, including predictive information maximisation,
empowerment maximisation, free energy minimisation,
and thermodynamic efficiency maximisation. For the
considered example, each approach exhibited a distinct
optimal range of parameter values, offering intuitive in-
sights into the underlying driver shaping collective be-
haviour:
• Predictive information maximises at sub-critical
coupling strength for Metropolis dynamics and
near-critical regime for Glauber dynamics, balanc-
ing sensory richness with predictability;
• Empowerment maximises at super-critical coupling
strength, where the individuals have maximal influ-
ence over the environment;
• Free energy minimisation (with intrinsic compo-
nent only) also leads to super-critical coupling
strength, where local observations align most
closely with the global configuration hence surprise
minimised;
• Thermodynamic efficiency maximisation optimises
near the critical regime, achieving maximum en-
tropy reduction per unit of work expended.
Thus, thermodynamic efficiency, measured by the en-
tropy reduction or predictability gain relative to the as-
sociated thermodynamic work carried out, might be a
candidate for the intrinsic utility of criticality.
In the Ising model example, each measure exhibits the
same characteristic behaviour reported in previous stud-
ies — balancing diversity and predictability for predictive
information [32, 33], maximising perceivable influence for
empowerment [39, 40], aligning the internal model with
external generative model for active inference [45, 46],
and optimising entropy reduction relative to work for
thermodynamic efficiency [51, 52]. This consistency in-
dicates that the findings we observed in our simulations
20
reflect the measures’ underlying properties rather than
artifacts of a specific model. We also tested different
Ising model dynamics, both yielding similar results. The
considered model represents a broad class of collective
systems; this establishes an adequate range of applicabil-
ity for our comparative analysis.
One limitation of the considered model is its equilib-
rium dynamics. In contrast, many biological systems op-
erate far from equilibrium, continuously exchanging en-
ergy, matter and information with the external environ-
ments. Here, we restricted our analysis to equilibrium
thermodynamics for two main reasons: (i) previous stud-
ies on thermodynamic efficiency are based on equilibrium
models, making this a logical starting point for a primer,
and (ii) we sought a simplified setting that highlights
the characteristics of self-organising behaviours driven by
each intrinsic utility without introducing too many addi-
tional assumptions.
Extending the study to non-equilibrium systems is an
important direction for future research. For example,
a previous study on non-equilibrium flocking [110] has
shown that as velocities of the birds become more aligned,
the entropic force can break the flock apart unless it is
finely counterbalanced by a cohesive force. Hence, an
intrinsic utility such as thermodynamic efficiency might
serve as a candidate fitness function for achieving such
balance. This would establish whether the principle of
super-efficiency is applicable to non-equilibrium systems
when significant entropic forces are present.
Informed by this analysis, as well as the relevant stud-
ies of thermodynamic efficiency [51–54], we proposed
a general principle, the principle of super-efficiency ,
that may explain why collective systems self-organises
to the critical point. The principle of super-efficiency
states that at the critical point, a self-organising sys-
tem achieves an optimal entropy reduction relative to the
thermodynamic costs. The ability to reduce entropy effi-
ciently grants the collective system an advantage, offering
an intrinsic motivation to operate near the critical point.
We believe that the principle of super-efficiency has im-
plications for the broader field of guided self-organisation,
informing the design of intelligent, adaptive systems that
achieve superior coordination, decision-making, and re-
source management.
ACKNOWLEDGMENTS
We would like to thank Nihat Ay and Jesse van Oost-
rum for an insightful discussion on predictive information
under different Ising model dynamics. Q.C. would like to
thank Joseph Lizier and Jaime Ruiz Serra for discussions
on information theory. Sydney Informatics Hub at the
University of Sydney provided support for HPC compu-
tational resources.
Appendix A: Information-theoretic quantities
We begin by introducing standard notations for
information-theoretic quantities that are relevant to the
subsequent sections. For readers interested in more de-
tails, please refer to [111].
For a discrete random variable X that has probability
mass function p(x) = Pr {X = x}, the entropy H(X)
which measures the uncertainty of random variable X is
defined as:
H(X) = −
X
x
p(x) logp(x) (A1)
where by convention, base 2 logarithms are used through-
out this paper, and the resulting unit is in bits.
If a pair of discrete random variables ( X, Y) follows
joint probability distribution be p(x, y), the conditional
entropy H(Y |X) is defined as:
H(Y |X) = −
X
x
X
y
p(x, y) logp(y|x) (A2)
The conditional entropyH(Y |X) measures the remaining
uncertainty in random variable Y given the knowledge of
random variable X.
The mutual informationbetween two discrete ran-
dom variables X and Y is defined as:
I(X; Y ) =
X
x
X
y
p(x, y) log p(x, y)
p(x)p(y) (A3)
Mutual information is the reduction in surprise about
one random variable given the knowledge of the other.
It is symmetrical and can be expressed as the difference
between entropy and conditional entropy:
I(X; Y ) = H(X) − H(X|Y ) = H(Y ) − H(Y |X) (A4)
The relative entropyor Kullback–Leibler diver-
gence between two probability mass functions p(x) and
q(x) is defined as:
D(p||q) =
X
x
p(x) log p(x)
q(x) (A5)
The KL divergence D(p||q) quantifies how much informa-
tion is lost when an alternative probability distribution
q(x) is assumed as a model instead of the actual distri-
bution p(x).
Appendix B: Thermodynamic preliminaries and
Fisher Information
1. Thermodynamic preliminaries
The configuration of a collective system refers to the
arrangement of the system’s components, usually the ge-
ometric or positional arrangement of the components at
21
a specific moment, for example, the up or down orienta-
tions of all the spins in a ferromagnetic substance. The
configuration entropy represents the amount of uncer-
tainty in the system’s arrangement. Lower configuration
entropy suggests a limited set of possible configurations,
indicating more predictable and coordinated behaviour
for the collective system. It is also easier to control or
guide the system towards a desired state if there is less
uncertainty in the system configurations.
We consider the collective variable Xk(x) defined as
a function of the configuration x, which is coupled to
the conjugate field or control parameter θk. A collec-
tive variable characterises the macroscopic state of the
system resulting from the specific configuration x. An
example of the collective variable and its conjugate con-
trol parameter could be volume and pressure for an ideal
gas system.
The probability of the system being in configuration x
can be expressed by the Gibbs measure [112, 113]:
p(x; ⃗θ) = 1
Z(⃗θ)
e−P
k θkXk(x) (B1)
where Z(⃗θ) is the partition function that normalises this
probability over all configurations. For simplicity, we
consider single-parameter θ for the rest of the discus-
sion, but the same framework applies to multi-parameter
cases.
In thermodynamic context, the configuration entropy
of the system given by the Gibbs ensemble is defined as:
S(θ) = −kB
X
x
p(x; θ) log p(x; θ) (B2)
and can be converted to the Shannon entropy HX =
−P
x p(x) logp(x) by dividing by a factor of kB.
The order parameter ϕ conjugate to the control pa-
rameter θ is related to the expected value of the corre-
sponding collective variable X:
ϕ = −kBT⟨X⟩ (B3)
The Gibbs free energy G is defined as:
G = U − TS − θϕ (B4)
where U is internal energy.
Evaluating the work done to or extracted from the
system requires the specification of a protocol in which
the control parameter varies. Henceforth, we consider
a quasi-static protocol, which means that the change of
control parameter occurs infinitely slowly so that the sys-
tem remains in thermal equilibrium with its surroundings
at all times. The generalised first law of thermodynamics
relates the generalised internal energy ⟨Ugen⟩ = U − θϕ,
the generalised heat flow (from the environment to the
system) ⟨Qgen⟩, and the generalised work ⟨Wgen⟩:
∆⟨Ugen⟩ = ∆⟨Qgen⟩ + ∆⟨Wgen⟩ (B5)
Since a change in the configuration entropy is matched by
the heat flow (∆Qgen = T∆S), the thermodynamic work
equals the change in free energy, that is, ∆⟨Wgen⟩ = ∆G
(the complete argument is presented in Ref. [52]). Taking
the first derivative with respect to control parameter θ
yields:
d⟨Wgen⟩
dθ = dG
dθ (B6)
2. Fisher information
At this stage we turn our attention to the Fisher Infor-
mation which is related to Gibbs free energy. The Fisher
information measures the amount of information that an
observable random variable carries about an unknown
parameter θ which may be influencing the probability of
observations. In order words, Fisher information quan-
tifies the sensitivity of observations to the change of pa-
rameter θ. Mathematically, Fisher information is defined
as the variance of the score function, where the score is
the derivative of the log-likelihood function with respect
to θ [111]:
I(θ) = E
" ∂
∂θ log p(x; θ)
2#
=
Z
x
 ∂
∂θ log p(x; θ)
2
p(x; θ)dx (B7)
It has been established that Fisher information is pro-
portional to the rate of change of the order parameter
with respect to the change in control parameter, being
analogous to susceptibility [113]:
I(θ) = β dϕ
dθ ∝ |θ − θc|−γ (B8)
In explaining the relationship between Fisher informa-
tion and thermodynamic efficiency, we point out that
Fisher information is proportional to the second deriva-
tive of Gibbs free energy [112, 114–116]:
I(θ) = −β d2G
dθ2 (B9)
As established in [52] under a quasi-static protocol, fol-
lowing the first law of thermodynamics, equations (B6)
and (B9) yield:
d⟨βWgen⟩
dθ =
Z θ∗
θ
I(θ′)dθ′ (B10)
where the integral is computed from the point of evalua-
tion θ to the “zero-response point”θ∗ defined as the point
where a perturbation of control parameter θ extracts no
work from the system.
22
Appendix C: Simulation of Ising model
The following settings have been applied to the simu-
lations:
• Lattice: a torus-shape lattice of size 50 ×50;
• Coupling strength J: values taken from range (0,2)
with increments δJ = 0.02;
• Number of simulations: 100 simulations are run for
each value of J;
• Number of time steps: for each simulation, 20.2
million steps are simulated to ensure the system
reaches equilibrium. The transient period (first 20
million steps) is excluded from the analysis;
• Number of samples (for predictive information, em-
powerment and variational free energy): in each
simulation, the last 200k steps are sampled to com-
pute the corresponding information-theoretic quan-
tities. The quantities are then averaged over 100
simulations;
• Number of samples (for thermodynamic efficiency):
in each simulation, configuration distributions are
sampled from the last 200k time steps with a sub-
sampling interval of 2,500 time steps (1 sweep of
the lattice). A total of 8,000 sample distributions
(80 samples/simulation × 100 simulations) are col-
lected for each J. The average distribution over
these 8,000 samples is used for computing the cor-
responding Fisher information. Configuration en-
tropy is computed using the last snapshot of lattice
configuration at each simulation, and then averaged
over 100 simulations;
• Inverse thermodynamic temperature β: chosen to
be constant 1.
The lattice is initialised in a fully ordered state. At
high coupling strength (low temperatures), single spin-
flip algorithms risk trapping the system in a local mini-
mum. Ordered initialisation drives the system to global
energy minima at high coupling strength (low tem-
peratures), without affecting outcomes at low coupling
strength (high temperature). Since only equilibrium
states are analysed and transients are excluded, this
method gives the same result as using random initiali-
sation but significantly accelerates convergence (Figure
21).
We tested two different spin-flip dynamics – Gluaber
dynamics [79] and Metropolis [80, 81] dynamics – to as-
sess the impact of model choice on the results. At each
time step, a site was selected uniformly at random, and
its spin flipped with the probability:
Glauber criterion:
pG(flip) = 0.5 [1− tanh(0.5βdE)] (C1)
Metropolis criterion:
pM (flip) = min

1, e−βdE 
(C2)
where dE = Eafter − Ebefore is the difference in energy
before and after the spin flip of the site. dE depends on
the coupling strength J and the number of neighbours
aligned with the site’s spin.
Figure 22 illustrates spin-flip probabilities for various
coupling strengths under the two different dynamics. As
J → +∞, both dynamics become deterministic: the site
flips its spin in a high energy state, that is, with more
than two misaligned neighbours and remains unchanged
at a low energy state. At low J values, Glauber dynamics
approaches a flipping probability of 0.5 for all different
energy states, while Metropolis dynamics approaches a
flipping probability of 1. Both criteria satisfy the detailed
balance condition at equilibrium; Glauber dynamics is
considered more realistic, and Metropolis dynamics result
in much faster convergence.
The simulation uses 20 million time steps, equivalent to
8000 lattice sweeps, to ensure the system reaches equilib-
rium. The transient period is excluded from the analysis,
and the final results are obtained by averaging over 20
simulations with different initial conditions.
The simulation results presented in Section 4 of the
main text show increased noise in the super-critical re-
gion. This can be attributed to the system’s difficulty in
reaching equilibrium as it approaches the critical point or
enters the super-critical regime. The distribution of mag-
netisation for various values of J (Figure 23) illustrates
this behaviour. In the sub-critical regime, the distribu-
tion is more centred around zero. In contrast, in the
super-critical regime, the magnetisation distribution be-
comes more polarised at equilibrium, with values closer
to -1 or 1, reflecting the dominance of up or down spins.
The central mass in Figure 23c represents simulations
that have not reached equilibrium after 20 million time
steps, contributing to the increased noise observed in this
regime.
Appendix D: Computation of Fisher Information
Let p(x; θ) denote the probability density of random
variable X, parameterised by θ. The Fisher information
is the variance of the score, and a function of the param-
eter θ [111]. The Fisher information can be transformed
23
(a) Predictive information
 (b) Empowerment
(c) Negative free energy
 (d) Thermodynamic efficiency
FIG. 21: Compare results for two different simulation settings: random initial configuration with 250 million time
steps vs fully-ordered initial configuration with 20 million time steps. The results are almost identical across
different initialisation methods when the simulations have converged. Note that the simulations do not fully
converge at high coupling strength (low temperature) when using random initialisation with 20 million time steps.
as follows:
I(θ) =
Z ∂ ln p(x; θ)
∂θ
2
p(x; θ)dx
=
Z ∂p(x; θ)
∂θ
2 1
p(x; θ)dx
=
Z  
∂p(x; θ)
∂θ
1p
p(x; θ)
!2
dx
= 4
Z  
∂
p
p(x; θ)
∂θ
!2
dx
(D1)
In this study, the Fisher information is computed nu-
merically using the discretisation method introduced in
[117]:
I(θ) = 4
X
x
 p
p(x; θ + ∆θ) −
p
p(x; θ − ∆θ)
2∆θ
!2
(D2)
Appendix E: Derivation of thermodynamic efficiency
for canonical Ising model
Analytically, thermodynamic efficiency can be shown
to diverge at the critical point. Consider a 2D square-
lattice Ising model with nearest neighbour interactions
as described in Section 4 of the main text. For isotropic
coupling strength, the dominant singular part of free en-
24
FIG. 22: Compare acceptance probability between Glauber (left) and Metropolis dynamics (right). The acceptance
probability of any site depends on the coupling strength J and the number of neighbours that are aligned with it.
For example, ‘0 align’ means all four neighbours have opposite spins to the centre site, while ‘2 align’ means the
neighbouring sites have equally split spins. Glauber and Metropolis dynamics exhibit distinct behaviours when
coupling strength J is small.
(a) Sub-critical regime
 (b) Near-critical regime
 (c) Super-critical regime
FIG. 23: Distribution of magnetisation under sub-critical, near-critical and super-critical regimes. 500 simulations
run for lattice size 50 × 50, each simulated for 20 million time steps.
ergy density is given by [118]:
− fs
kBT = (1 + k)(1 − k)2
2πk cosh2

2J
kBT
 ln

1 + k
1 − k
 (E1)
where kB is the Boltzmann constant, T is the tempera-
ture, and k = (sinh 2J
kBT )−2. The criticality condition is
given by [85]:

sinh 2J
kBT
2
= 1 (E2)
Hence, given a constant temperature T, we have the fol-
lowing expressions for the critical coupling strength Jc
and the corresponding value of k:
Jc(T) = kB ln
 
1 +
√
2

2 T = C0T (E3)
k(Jc) = 1 (E4)
where C0 =
kB ln(1+
√
2)
2 is a constant. Expanding k(J) =
(sinh 2J
kBT )−2 around Jc gives:
k(J) = k(Jc) + ∂k
∂J

J=Jc
(J − Jc) + O((J − Jc)2) (E5)
The partial derivative term ∂k
∂J is:
∂k
∂J = −4
kBT cosh 2J
kBT
 
sinh 2J
kBT
−3
(E6)
Substituting J = Jc into the above equation and noting
that cosh 2Jc
kBT =
√
2 and sinh 2Jc
kBT = 1, yields:
∂k
∂J

J=Jc
= −4
√
2
kBT (E7)
25
Substituting (E4) and (E7) into the expression (E5)
gives:
k(J) = 1 − 4
√
2
kBT (J − Jc) + O((J − Jc)2)
≈ 1 − 4
√
2
kBT (J − Jc) (E8)
Therefore, near the critical point:
k − 1 ≈ −4
√
2
kBT (J − Jc) (E9)
When J > Jc, k <1, equation (E1) becomes:
fs = −kBT (1 + k)(1 − k)2
2πk cosh2

2J
kBT
 ln
1 + k
1 − k

= −kBT (1 + k)(1 − k)2
2πk cosh2

2J
kBT
 ln(1 + k)
+ kBT (1 + k)(1 − k)2
2πk cosh2

2J
kBT
 ln(1 − k) (E10)
Substituting equation (E9) into (E10) and noting that
cosh2

2Jc
kBT

= 2 and kc = 1, we approximate the singu-
lar part of free energy density as J → J+
c :
fs ≈ C1(J − Jc)2 ln(J − Jc) (E11)
where C1 is a constant. Equation (E11) has the same
form as the expression of fs near the critical temperature
point [118].
Taking the derivative offs with respect to J and keep-
ing only the leading order term gives:
∂fs
∂J = C1

2(J − Jc) ln(J − Jc) + (J − Jc)2 1
J − Jc

= C1(J − Jc)

2 ln(J − Jc) + 1

≈ 2C1(J − Jc) ln(J − Jc) (E12)
Entropy S and free energy F are related by S = −∂F
∂T .
Together with equation (E3), the derivative of entropy
density with respect to J can be expressed as:
∂s
∂J = ∂
∂J
−∂fs
∂T

= ∂
∂T
−∂fs
∂J

= − ∂
∂T

2C1(J − Jc) ln(J − Jc)

= −2C1
∂(J − Jc)
∂T [ln(J − Jc) + 1]
≈ 2C1C0 ln(J − Jc) (E13)
Replacing the free energy density f by its dominant sin-
gular part fs and using equations (E12) and (E13), the
thermodynamic efficiency of interactions η can be com-
puted as J → J+
c . Following the sign convention that
work extracted from the system is positive, the change
in free energy satisfies ∆ f = −∆W:
η(J) = 1
kB
∂s
∂J
.∂f
∂J
≈ 1
kB
2C1C0 ln(J − Jc)
2C1(J − Jc) ln(J − Jc)
= ln(1 +
√
2)
2
1
J − Jc
(E14)
Similarly, when J < Jc, k >1, the dominant part of
the free energy density is given by:
fs = −kBT (1 + k)(1 − k)2
2πk cosh2

2J
kBT
 ln
k + 1
k − 1

(E15)
Using the same procedure as above, we can show forJ →
J−
c :
∂fs
∂J ≈ 2C1(Jc − J) ln(Jc − J) (E16)
∂s
∂J ≈ 2C1C0 ln(Jc − J) (E17)
Hence, the thermodynamic efficiency of interactions η
can be computed as J → J−
c :
η(J) = 1
kB
∂s
∂J
.∂f
∂J
≈ 1
kB
2C1C0 ln(Jc − J)
2C1(Jc − J) ln(Jc − J)
= ln(1 +
√
2)
2
1
Jc − J (E18)
Combining equations (E14) and (E18), we have:
η(J) ∝ |J − Jc|−1 (E19)
This shows that the thermodynamic efficiency diverges
as J approaches Jc from either side.
26
[1] Haken H. Synergetics: An Introduction. 3rd ed.
Springer Series in Synergetics. Springer Berlin, Hei-
delberg; 1983. Available from: https://doi.org/10.
1007/978-3-642-88338-5 .
[2] Haken H. Information and Self-Organization: A
Macroscopic Approach to Complex Systems. 3rd ed.
Springer Series in Synergetics. Springer Berlin, Hei-
delberg; 2006. Available from: https://doi.org/10.
1007/3-540-33023-2 .
[3] Camazine S, Deneubourg JL, Franks NR, Sneyd J,
Theraulaz G, Bonabeau E. Self-organization in biologi-
cal systems. Princeton studies in complexity. Princeton,
NJ: Princeton University Press; 2003.
[4] Kauffman SA. Investigations. New York: Oxford Uni-
versity Press; 2000.
[5] Prokopenko M, Boschetti F, Ryan AJ. An information-
theoretic primer on complexity, self-organization, and
emergence. Complexity. 2009 Sep;15(1):11-28. Available
from: https://doi.org/10.1002/cplx.20249.
[6] Beekman M, Sumpter DJT, Ratnieks FLW. Phase
transition between disordered and ordered forag-
ing in Pharaoh’s ants. Proceedings of the Na-
tional Academy of Sciences. 2001 Aug;98(17):9703-
6. Available from: https://pnas.org/doi/full/10.
1073/pnas.161285298.
[7] Beggs JM, Plenz D. Neuronal Avalanches in Neo-
cortical Circuits. The Journal of Neuroscience. 2003
Dec;23(35):11167-77. Available from: https://www.
jneurosci.org/content/23/35/11167.
[8] Shmulevich I, Kauffman SA, Aldana M. Eukaryotic
cells are dynamically ordered or critical but not chaotic.
Proceedings of the National Academy of Sciences of
the United States of America. 2005;102(38):13439-
44. Available from: https://doi.org/10.1073/pnas.
0506771102.
[9] R¨ am¨ o P, Kesseli J, Yli-Harja O. Perturbation
avalanches and criticality in gene regulatory net-
works. Journal of Theoretical Biology. 2006;242(1):164-
70. Available from: https://doi.org/10.1016/j.
jtbi.2006.02.011.
[10] Beggs JM. The criticality hypothesis: How local
cortical networks might optimize information process-
ing. Philosophical Transactions of the Royal Society
A: Mathematical, Physical and Engineering Sciences.
2008;366(1864):329-43. Available from: https://doi.
org/10.1098/rsta.2007.2092.
[11] Cavagna A, Cimarelli A, Giardina I, Parisi G, San-
tagati R, Stefanini F, et al. Scale-free correla-
tions in starling flocks. Proceedings of the Na-
tional Academy of Sciences. 2010 Jun;107(26):11865-
70. Available from: https://pnas.org/doi/full/10.
1073/pnas.1005766107.
[12] Fessel A, Oettmeier C, Bernitt E, Gauthier NC,
D¨ obereiner HG.Physarum polycephalum Percolation as
a Paradigm for Topological Phase Transitions in Trans-
portation Networks. Physical Review Letters. 2012
Aug;109(7):078103. Available from: https://link.
aps.org/doi/10.1103/PhysRevLett.109.078103.
[13] Krotov D, Dubuis JO, Gregora T, Bialek W. Mor-
phogenesis at criticality. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica. 2014;111(10):3683-8. Available from: https://doi.
org/10.1073/pnas.1324186111.
[14] Cavagna A, Conti D, Creato C, Del Castello L, Giardina
I, Grigera T, et al. Dynamic scaling in natural swarms.
Nature Physics. 2017 Sep;13(9):914-8. Available from:
https://www.nature.com/articles/nphys4153.
[15] Newman MEJ. Power laws, Pareto distributions and
Zipf’s law. Contemporary Physics. 2005 Sep;46(5):323-
51. Available from: https://doi.org/10.1080/
00107510500052444.
[16] Stanley HE. Introduction to Phase Transitions and Crit-
ical Phenomena. 1st ed. Oxford University Press USA;
1971.
[17] Halperin BI, Hohenberg PC. Scaling Laws for Dy-
namic Critical Phenomena. Physical Review. 1969
Jan;177(2):952-71. Available from: https://link.aps.
org/doi/10.1103/PhysRev.177.952.
[18] Binney JJ, Dowrick NJ, Fisher AJ, Newman MEJ. The
theory of critical phenomena: an introduction to the
renormalization group. Oxford science publications. Ox-
ford: Clarendon press; 1992.
[19] Bialek W, Cavagna A, Giardina I, Mora T, Pohl O, Sil-
vestri E, et al. Social interactions dominate speed con-
trol in poising natural flocks near criticality. Proceed-
ings of the National Academy of Sciences of the United
States of America. 2014;111(20):7212-7. Available from:
https://doi.org/10.1073/pnas.1324045111.
[20] Stanley HE, Amaral LAN, Gopikrishnan P, Plerou V.
Scale invariance and universality of economic fluctua-
tions. Physica A: Statistical Mechanics and its Appli-
cations. 2000;283(1-2):31-41. Available from: https:
//doi.org/10.1016/S0378-4371(00)00256-9.
[21] Bettencourt LMA, Lobo J, Helbing D, K¨ uhnert C, West
GB. Growth, innovation, scaling, and the pace of
life in cities. Proceedings of the National Academy
of Sciences. 2007 Apr;104(17):7301-6. Available from:
https://pnas.org/doi/10.1073/pnas.0610172104.
[22] Scheffer M. Critical transitions in nature and society.
Princeton studies in complexity. Princeton, N.J: Prince-
ton University Press; 2009.
[23] Bak P, Tang C, Wiesenfeld K. Self-organized critical-
ity: An explanation of the 1/ f noise. Physical Review
Letters. 1987 Jul;59(4):381-4. Available from: https:
//link.aps.org/doi/10.1103/PhysRevLett.59.381.
[24] Drossel B, Schwabl F. Self-organized critical forest-
fire model. Physical Review Letters. 1992;69(11):1629-
32. Available from: https://doi.org/10.1103/
PhysRevLett.69.1629.
[25] Clar S, Drossel B, Schwabl F. Forest fires and other
examples of self-organized criticality. Journal of Physics
Condensed Matter. 1996;8(37):6803-24.
[26] Malamud BD, Morein G, Turcotte DL. Forest fires:
An example of self-organized critical behavior. Science.
1998;281(5384):1840-2. Available from: https://www.
science.org/doi/10.1126/science.281.5384.1840.
[27] Chen K, Bak P, Obukhov SP. Self-organized criticality
in a crack-propagation model of earthquakes. Physical
Review A. 1991;43(2):625-30. Available from: https:
//doi.org/10.1103/PhysRevA.43.625.
[28] Hesse J, Gross T. Self-organized criticality as a fun-
damental property of neural systems. Frontiers in Sys-
27
tems Neuroscience. 2014 Sep;8. Available from: https:
//doi.org/10.3389/fnsys.2014.00166.
[29] Shew WL, Clawson WP, Pobst J, Karimipanah Y,
Wright NC, Wessel R. Adaptation to sensory in-
put tunes visual cortex to criticality. Nature Physics.
2015;11(8):659-63.
[30] Schmidhuber J. Formal Theory of Creativity, Fun, and
Intrinsic Motivation (1990–2010). IEEE Transactions on
Autonomous Mental Development. 2010 Sep;2(3):230-
47. Available from: http://ieeexplore.ieee.org/
document/5508364/.
[31] Oudeyer PY, Kaplan F. What is intrinsic motivation?
A typology of computational approaches. Frontiers in
Neurorobotics. 2007 Nov;1. Available from: https://
doi.org/10.3389/neuro.12.006.2007.
[32] Ay N, Bertschinger N, Der R, G¨ uttler F, Olbrich E.
Predictive information and explorative behavior of au-
tonomous robots. The European Physical Journal B.
2008 Jun;63(3):329-39. Available from: http://link.
springer.com/10.1140/epjb/e2008-00175-0.
[33] Der R, G¨ uttler F, Ay N. Predictive information and
emergent cooperativity in a chain of mobile robots. In:
Bullock S, Noble J, Watson R, Bedau M, editors. Proc.
Artificial Life XI. MIT Press, Cambridge, MA; 2008. p.
166-72.
[34] Zahedi K, Ay N, Der R. Higher Coordination With Less
Control—A Result of Information Maximization in the
Sensorimotor Loop. Adaptive Behavior. 2010 Jun;18(3-
4):338-55. Available from: https://doi.org/10.1177/
1059712310375314.
[35] Ay N, Bernigau H, Der R, Prokopenko M. Information-
driven self-organization: The dynamical system ap-
proach to autonomous robot behavior. Theory in Bio-
sciences. 2012 Sep;131(3):161-79.
[36] Martius G, Der R, Ay N. Information Driven Self-
Organization of Complex Robotic Behaviors. PLoS
ONE. 2013 May;8(5). Available from: https://doi.
org/10.1371/journal.pone.0063400.
[37] Der R, Martius G. Behavior as broken symme-
try in embodied self-organizing robots. In: ECAL
2013: The Twelfth European Conference on Ar-
tificial Life. Sicily, Italy: MIT Press; 2013. p.
601-8. Available from: https://doi.org/10.1162/
978-0-262-31709-2-ch086 .
[38] Der R. On the Role of Embodiment for Self-
Organizing Robots: Behavior As Broken Symmetry. In:
Prokopenko M, editor. Guided self-organization: In-
ception. vol. 9 of Emergence, Complexity and Com-
putation. Springer, Berlin, Heidelberg; 2014. p. 193-
221. Available from: https://link.springer.com/10.
1007/978-3-642-53734-9_7 .
[39] Klyubin AS, Polani D, Nehaniv CL. Empowerment: A
Universal Agent-Centric Measure of Control. In: 2005
IEEE Congress on Evolutionary Computation. vol. 1.
Edinburgh, Scotland, UK: IEEE; 2005. p. 128-35. Avail-
able from: http://ieeexplore.ieee.org/document/
1554676/.
[40] Capdepuy P, Polani D, Nehaniv CL. Maximization of
Potential Information Flow as a Universal Utility for
Collective Behaviour. In: 2007 IEEE Symposium on
Artificial Life. Honolulu, HI, USA: IEEE; 2007. p. 207-
13. Available from: http://ieeexplore.ieee.org/
document/4218888/.
[41] Klyubin AS, Polani D, Nehaniv CL. Keep Your Options
Open: An Information-Based Driving Principle for Sen-
sorimotor Systems. PLoS ONE. 2008 Dec;3(12):e4018.
Available from: https://doi.org/10.1371/journal.
pone.0004018.
[42] Capdepuy P, Polani D, Nehaniv CL. Perception-action
loops of multiple agents: Informational aspects and the
impact of coordination. Theory in Biosciences. 2012
Sep;131(3):149-59. Available from: https://doi.org/
10.1007/s12064-011-0143-y .
[43] Salge C, Glackin C, Polani D. Empowerment–An In-
troduction. In: Prokopenko M, editor. Guided Self-
Organization: Inception. Berlin, Heidelberg: Springer
Berlin Heidelberg; 2014. p. 67-114. Available from:
https://doi.org/10.1007/978-3-642-53734-9_4 .
[44] Tiomkin S, Nemenman I, Polani D, Tishby N. Intrinsic
Motivation in Dynamical Control Systems. PRX Life.
2024 Aug;2(3):033009. Available from: https://link.
aps.org/doi/10.1103/PRXLife.2.033009.
[45] Friston K, Kilner J, Harrison L. A free energy
principle for the brain. Journal of Physiology-
Paris. 2006 Jul;100(1-3):70-87. Available from:
https://linkinghub.elsevier.com/retrieve/pii/
S092842570600060X.
[46] Friston K. The free-energy principle: A unified brain
theory? Nature Reviews Neuroscience. 2010;11(2):127-
38. Available from: https://doi.org/10.1038/
nrn2787.
[47] Friston K, Rigoli F, Ognibene D, Mathys C, Fitzgerald
T, Pezzulo G. Active inference and epistemic value.
Cognitive Neuroscience. 2015;6(4):187-214. Available
from: https://pubmed.ncbi.nlm.nih.gov/25689102/.
[48] Friston KJ, Lin M, Frith CD, Pezzulo G, Hobson JA,
Ondobaka S. Active Inference, Curiosity and Insight.
Neural Computation. 2017 Oct;29(10):2633-83. Avail-
able from: https://direct.mit.edu/neco/article/
29/10/2633-2683/8300.
[49] Friston K, Da Costa L, Sajid N, Heins C, Ueltzh¨ offer
K, Pavliotis GA, et al. The free energy principle made
simpler but not too simple. Physics Reports. 2023
Jun;1024:1-29. Available from: https://doi.org/10.
1016/j.physrep.2023.07.001.
[50] Heins C, Millidge B, Costa LD, Mann RP, Friston KJ,
Couzin ID. Collective behavior from surprise minimiza-
tion. Proceedings of the National Academy of Sciences.
2024;121(17):e2320239121. Available from: https://
www.pnas.org/doi/abs/10.1073/pnas.2320239121.
[51] Crosato E, Nigmatullin R, Prokopenko M. On criti-
cal dynamics and thermodynamic efficiency of urban
transformations. Royal Society Open Science. 2018
Oct;5(10). Available from: https://doi.org/10.1098/
rsos.180863.
[52] Crosato E, Spinney RE, Nigmatullin R, Lizier JT,
Prokopenko M. Thermodynamics and computa-
tion during collective motion near criticality. Phys-
ical Review E. 2018 Jan;97(1):1-14. Available
from: https://journals.aps.org/pre/abstract/10.
1103/PhysRevE.97.012120.
[53] Harding N, Nigmatullin R, Prokopenko M. Thermody-
namic efficiency of contagions: A statistical mechani-
cal analysis of the SIS epidemic model. Interface Fo-
cus. 2018 Oct;8(6):20180036. Available from: https:
//doi.org/10.1098/rsfs.2018.0036.
[54] Nigmatullin R, Prokopenko M. Thermodynamic effi-
ciency of interactions in self-organizing systems. En-
28
tropy. 2021;23(6):757. Available from: https://doi.
org/10.3390/e23060757.
[55] Stauffer D. Introduction to Percolation Theory Revised
Second Edition. 2nd ed. London: Taylor and Francis;
1992.
[56] Stauffer D. Scaling Theory of Percolation Clusters.
Physics Reports. 1979 Jul;54(1):1-74.
[57] Deckmyn PHAM, Davies GA, Bell DJ. Properties
of Percolating Clusters on Finite Lattices Applied to
Model Filtration Processes. Applied Mathematical
Modelling. 1995 May;19(5):258-69.
[58] Chen Q. qianyangchen/isingModelPALoop: v1.0.1.
Zenodo; 2024. Software version v1.0.1. Available from:
https://doi.org/10.5281/zenodo.13784627.
[59] Grassberger P. Critical Behaviour of the Drossel-
Schwabl Forest Fire Model. New Journal of Physics.
2002 Mar;4:17.
[60] Palmieri L, Jensen HJ. The Emergence of Weak Crit-
icality in SOC Systems. Europhysics Letters. 2018
Aug;123(2):20002.
[61] Ay N, Zahedi K. Causal Effects for Prediction and De-
liberative Decision Making of Embodied Systems. In:
Yamaguchi Y, editor. Advances in Cognitive Neurody-
namics (III). Dordrecht: Springer Netherlands; 2013. p.
499-506.
[62] Ay N, Zahedi K. On the Causal Structure of the Sen-
sorimotor Loop. In: Prokopenko M, editor. Guided
Self-Organization: Inception. Emergence, Complexity
and Computation. vol. 9 of Emergence, Complexity
and Computation. Springer, Berlin, Heidelberg; 2014.
p. 261-94. Available from: https://link.springer.
com/10.1007/978-3-642-53734-9_9 .
[63] Schmidhuber J. Curious model-building control sys-
tems. In: 1991 IEEE International Joint Conference
on Neural Networks. Singapore: IEEE; 1991. p. 1458-
63 vol.2. Available from: https://ieeexplore.ieee.
org/document/170605/.
[64] Bialek W, Nemenman I, Tishby N. Predictabil-
ity, complexity, and learning. Neural Computation.
2001;13(11):2409-63. Available from: https://doi.
org/10.1162/089976601753195969.
[65] Grassberger P. Toward a quantitative theory of self-
generated complexity. International Journal of Theo-
retical Physics. 1986 Sep;25(9):907-38. Available from:
http://link.springer.com/10.1007/BF00668821.
[66] Crutchfield JP, Young K. Inferring statistical com-
plexity. Physical Review Letters. 1989 Jul;63(2):105-8.
Available from: https://link.aps.org/doi/10.1103/
PhysRevLett.63.105.
[67] Prokopenko M, Gerasimov V, Tanev I. Evolving Spa-
tiotemporal Coordination in a Modular Robotic System.
In: Nolfi S, editor. From Animals to Animats 9. vol.
4095. Rome, Italy: Springer, Berlin, Heidelberg.; 2006.
p. 558-69. Available from: http://link.springer.
com/10.1007/11840541_46.
[68] Salge C, Polani D. Empowerment As Replace-
ment for the Three Laws of Robotics. Fron-
tiers in Robotics and AI. 2017 Jun;4:25. Available
from: http://journal.frontiersin.org/article/
10.3389/frobt.2017.00025/full.
[69] Clements M, Polani D. Empowerment as a Generic Util-
ity Function for Agents in a Simple Team Sport Simu-
lation. In: Ronzhin A, Rigoll G, Meshcheryakov R,
editors. Interactive Collaborative Robotics. vol. 10459.
Cham: Springer International Publishing; 2017. p. 37-
49. Series Title: Lecture Notes in Computer Science.
Available from: http://link.springer.com/10.1007/
978-3-319-66471-2_5 .
[70] Grasso C, Bongard J. Empowered neural cellular au-
tomata. In: Proceedings of the Genetic and Evolu-
tionary Computation Conference Companion. Boston
Massachusetts: ACM; 2022. p. 108-11. Available from:
https://dl.acm.org/doi/10.1145/3520304.3529067.
[71] Proietti R, Pezzulo G, Tessari A. An active inference
model of hierarchical action understanding, learning
and imitation. Physics of Life Reviews. 2023 Sep;46:92-
118. Available from: https://pubmed.ncbi.nlm.nih.
gov/37354642/.
[72] Ramstead MJD, Badcock PB, Friston K. Answer-
ing Schr¨ odinger’s question: A free-energy formulation.
Physics of Life Reviews. 2018 Mar;24:1-16. Available
from: https://linkinghub.elsevier.com/retrieve/
pii/S1571064517301409.
[73] Veissi` ere SPL, Constant A, Ramstead MJD, Friston KJ,
Kirmayer LJ. Thinking through other minds: A vari-
ational approach to cognition and culture. Behavioral
and Brain Sciences. 2020;43:e90.
[74] Kaufmann R, Gupta P, Taylor J. An active infer-
ence model of collective intelligence. Entropy. 2021
Jul;23(7):830. Available from: https://doi.org/10.
3390/e23070830.
[75] Gottwald S, Braun DA. The two kinds of free energy and
the Bayesian revolution. PLOS Computational Biology.
2020 Dec;16(12):e1008420. Available from: https://
dx.plos.org/10.1371/journal.pcbi.1008420.
[76] Friston K. The free-energy principle: a rough guide
to the brain? Trends in Cognitive Sciences. 2009
Jul;13(7):293-301. Available from: https://pubmed.
ncbi.nlm.nih.gov/19559644/.
[77] Landauer R. Irreversibility and Heat Generation in
the Computing Process. IBM Journal of Research and
Development. 1961 Jul;5(3):183-91. Available from:
http://ieeexplore.ieee.org/document/5392446/.
[78] Wikipedia contributors. Thermal Efficiency. Wikipedia,
The Free Encyclopedia;. Accessed: 2024-08-22.
Available online: https://en.wikipedia.org/wiki/
Thermal_efficiency. Available from: https://en.
wikipedia.org/wiki/Thermal_efficiency.
[79] Glauber RJ. Time-Dependent Statistics of the Ising
Model. Journal of Mathematical Physics. 1963;4(2):294-
307. Available from: https://doi.org/10.1063/1.
1703954.
[80] Bhanot G. The Metropolis algorithm. Reports on
Progress in Physics. 1988 Mar;51(3):429-57. Avail-
able from: https://iopscience.iop.org/article/
10.1088/0034-4885/51/3/003.
[81] Janke W, Christiansen H, Majumder S. Coarsening in
the long-range Ising model: Metropolis versus Glauber
criterion. In: Journal of Physics: Conference Se-
ries. vol. 1163. Moscow, Russian Federation; 2019. p.
012002. Available from: https://iopscience.iop.
org/article/10.1088/1742-6596/1163/1/012002.
[82] Biehl M, Guckelsberger C, Salge C, Smith SC, Polani
D. Expanding the active inference landscape: More in-
trinsic motivations in the perception-action loop. Fron-
tiers in Neurorobotics. 2018;12:1-26. Available from:
https://doi.org/10.3389/fnbot.2018.00045.
[83] Kikuchi R. A Theory of Cooperative Phenomena.
29
Physical Review. 1951 Mar;81(6):988-1003. Available
from: https://link.aps.org/doi/10.1103/PhysRev.
81.988.
[84] Brandani GB, Schor M, MacPhee CE, Grubm¨ uller H,
Zachariae U, Marenduzzo D. Quantifying Disorder
through Conditional Entropy: An Application to Fluid
Mixing. PLoS ONE. 2013 Jun;8(6):e65617. Available
from: https://dx.plos.org/10.1371/journal.pone.
0065617.
[85] Onsager L. Crystal Statistics. I. A Two-Dimensional
Model with an Order-Disorder Transition. Phys Rev.
1944 Feb;65:117-49. Available from: https://link.
aps.org/doi/10.1103/PhysRev.65.117.
[86] Ramaswamy S. The mechanics and statistics of active
matter. Annual Review of Condensed Matter Physics.
2010;1(1):323-45.
[87] Crosato E, Prokopenko M, Spinney RE. Irreversibil-
ity and emergent structure in active matter. Physical
Review E. 2019;100(4):042613.
[88] Bialek W, Cavagna A, Giardina I, Mora T, Silvestri E,
Viale M, et al. Statistical mechanics for natural flocks of
birds. Proceedings of the National Academy of Sciences.
2012;109(13):4786-91. Available from: https://doi.
org/10.1073/pnas.1118633109.
[89] Kaila VRI, Annila A. Natural selection for least action.
Proc R Soc A. 2008;464:3055-70. Available from: http:
//doi.org/10.1098/rspa.2008.0178.
[90] Trenchard H, Perc M. Energy saving mechanisms, col-
lective behavior and the variation range hypothesis in
biological systems: A review. Biosystems. 2016;147:40-
66. Available from: https://www.sciencedirect.com/
science/article/pii/S0303264716300739.
[91] Sumpter DJ. The principles of collective animal be-
haviour. Philosophical Transactions of the Royal Society
B: Biological Sciences. 2006;361(1465):5-22. Available
from: https://doi.org/10.1098/rstb.2005.1733.
[92] Couzin ID. Collective cognition in animal groups.
Trends in Cognitive Sciences. 2009;13(1):36-43. Avail-
able from: https://doi.org/10.1016/j.tics.2008.
10.002.
[93] Mu˜ noz MA. Colloquium: Criticality and dynami-
cal scaling in living systems. Rev Mod Phys. 2018
Jul;90:031001. Available from: https://link.aps.
org/doi/10.1103/RevModPhys.90.031001.
[94] Simon HA. Rational Choice and the Structure of the
Environment. Psychological Review. 1956;63(2):129-38.
Available from: https://doi.org/10.1037/h0042769.
[95] Johnson SGB, Schotanus PR, Kelso JAS. Minds
and markets as complex systems: an emerging
approach to cognitive economics. Trends in Cogni-
tive Sciences. 2024;28(11):1037-50. Available from:
https://www.sciencedirect.com/science/article/
pii/S1364661324001748.
[96] Dunbar RIM. Neocortex size as a constraint
on group size in primates. Journal of Human
Evolution. 1992;22(6):469-93. Available from:
https://www.sciencedirect.com/science/article/
pii/004724849290081J.
[97] Harr´ e MS, Prokopenko M. The social brain: scale-
invariant layering of Erd˝ os–R´ enyi networks in small-
scale human societies. Journal of The Royal Society
Interface. 2016 May;13(118). Available from: https:
//doi.org/10.1098/rsif.2016.0044.
[98] Hunt A, Watkiss P. Climate change impacts and adap-
tation in cities: A review of the literature. Climatic
Change. 2011;104(1):13-49.
[99] Li Y, Beeton RJS, Zhao X, et al. Advancing ur-
ban sustainability transitions: A framework for un-
derstanding urban complexity and enhancing integra-
tive transformations. Humanit Soc Sci Commun.
2024;11:1064. Available from: https://doi.org/10.
1057/s41599-024-03598-x .
[100] Richardson HW. Economies and Diseconomies of Ag-
glomeration. In: Giersch H, editor. Urban Agglomera-
tion and Economic Growth. Publications of the Egon-
Sohmen-Foundation. Berlin, Heidelberg: Springer;
1995. .
[101] Kapucu N. Collaborative emergency management: bet-
ter community organising, better public preparedness
and response. Disasters. 2008 jun;32(2):239-62.
[102] Khiali-Miab A, Patt A, Kr¨ utli P. Empower-
ing a sustainable urban future: The key role
of coordinated settlement development for opti-
mising energy efficiency and socio-economic wel-
fare. Sustainable Cities and Society. 2024;107:105418.
Available from: https://www.sciencedirect.com/
science/article/pii/S2210670724002464.
[103] Friman SI, Elowe CR, Hao S, Mendez L, Ay-
ala R, Brown I, et al. It pays to follow the
leader: Metabolic cost of flight is lower for trailing
birds in small groups. Proc Natl Acad Sci USA.
2024;121(26):e2319971121. Available from: https://
doi.org/10.1073/pnas.2319971121.
[104] Christensen K, Papavassiliou D, de Figueiredo A,
Franks NR, Sendova-Franks AB. Universality in ant be-
haviour. J R Soc Interface. 2015;12:20140985. Available
from: http://dx.doi.org/10.1098/rsif.2014.0985.
[105] Gallotti R, Chialvo DR. How ants move: individ-
ual and collective scaling properties. J R Soc Inter-
face. 2018;15:20180223. Available from: http://dx.
doi.org/10.1098/rsif.2018.0223.
[106] Porfiri M, Abaid N, Garnier S. Socially driven nega-
tive feedback regulates activity and energy use in ant
colonies. PLoS Comput Biol. 2024;20(11):e1012623.
Available from: https://doi.org/10.1371/journal.
pcbi.1012623.
[107] Adibi M, Clifford CWG, Arabzadeh E. Informational
Basis of Sensory Adaptation: Entropy and Single-Spike
Efficiency in Rat Barrel Cortex. Journal of Neuro-
science. 2013;33(37):14921-6. Available from: https:
//doi.org/10.1523/JNEUROSCI.1313-13.2013.
[108] Takagi K. Energy constraints on brain network forma-
tion. Scientific Reports. 2021;11:11745. Available from:
https://doi.org/10.1038/s41598-021-91250-y .
[109] Olsson L, Nehaniv CL, Polani D. Information Trade-
Offs and the Evolution of Sensory Layouts. In: Pollack
J, Bedau MA, Husbands P, Watson RA, Ikegami T, ed-
itors. Artificial Life IX. The MIT Press; 2004. p. 119-24.
[110] Castellana M, Bialek W, Cavagna A, Giardina I. En-
tropic Effects in a Nonequilibrium System: Flocks of
Birds. Physical Review E. 2016 May;93(5):052416.
[111] Cover TM, Thomas JA. Elements of information theory.
2nd ed. Hoboken, New Jersey: John Wiley & Sons, Inc.;
2005. Available from: https://onlinelibrary.wiley.
com/doi/book/10.1002/047174882X.
[112] Brody D, Rivier N. Geometrical aspects of statistical
mechanics. Physical Review E. 1995 Feb;51(2):1006-11.
Available from: https://link.aps.org/doi/10.1103/
30
PhysRevE.51.1006.
[113] Prokopenko M, Lizier JT, Obst O, Wang XR. Re-
lating Fisher information to order parameters. Phys-
ical Review E - Statistical, Nonlinear, and Soft
Matter Physics. 2011 Oct;84(4):041116. Available
from: https://journals.aps.org/pre/abstract/10.
1103/PhysRevE.84.041116.
[114] Brody DC, Ritz A. Information geometry of finite
Ising models. Journal of Geometry and Physics. 2003
Aug;47(2):207-20. Available from: https://doi.org/
10.1016/S0393-0440(02)00190-0.
[115] Janke W, Johnston DA, Kenna R. Information geom-
etry and phase transitions. Physica A: Statistical Me-
chanics and its Applications. 2004 May;336(1-2):181-6.
Available from: https://linkinghub.elsevier.com/
retrieve/pii/S0378437104000469.
[116] Crooks GE. Measuring Thermodynamic Length.
Physical Review Letters. 2007 Sep;99(10):100602.
Available from: https://link.aps.org/doi/10.1103/
PhysRevLett.99.100602.
[117] S´ anchez-Moreno P, Yanez RJ, Dehesa JS. Discrete Den-
sities and Fisher Information. In: Proceedings of the
14th International Conference on Difference Equations
and Applications. Istanbul, Turkey: Bah¸ cesehir Univer-
sity Press; 2009. p. 291-8.
[118] Baxter RJ. Critical Behaviour. In: Exactly Solved Mod-
els in Statistical Mechanics. London: Academic Press;
1982. p. 88-126. Chapter 7.

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "Why collective behaviours self-organise to criticality: A primer on information-theoretic and thermodynamic utility measures"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - FIX SPACING ISSUES FROM PDF EXTRACTION:
     * PDF extraction may remove spaces between words (e.g., 'dynamicssimulationsand' → 'dynamics simulations and')
     * When extracting quotes, restore missing spaces between words if they appear concatenated
     * Look for patterns like 'word1word2word3' and add spaces: 'word1 word2 word3'
     * This is a common issue from PDF text extraction that needs correction
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.