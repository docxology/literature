=== IMPORTANT: ISOLATE THIS PAPER ===You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.Do NOT mix information from different papers. Only use information from THIS specific paper.Paper Title: Scaling active inferenceCitation Key: tschantz2019scalingAuthors: Alexander Tschantz, Manuel Baltieri, Anil. K. SethREMEMBER: Extract quotes, claims, and findings ONLY from the paper text.Year:2019Abstract: In reinforcement learning (RL), agents often operate in partially observed anduncertain environments. Model-based RL suggests that this is best achieved bylearningandexploitingaprobabilisticmodeloftheworld.‘Activeinference’isanemergingnormativeframeworkincognitiveandcomputationalneurosciencethatoffers a unifying account of how biological agents achieve this. On this frame-work,inference,learningandactionemergefromasingleimperativetomaximizetheBayesianevidenceforanichedmodeloftheworld.However,implementationsofthisprocesshavethusfarbeenrestrictedtolow-dimensionalandidealizedsitu-ations.Here,wepresentaworkingimplementationofactiveinferencethatappliestohigh-dimensionaltasks,withproof-of-principleresultsdemonstratingefficientexploration and an order of magnitude increase in sample efficiency over strongmodel-freebaselines.Ourresultsdemonstratethefeasibilityofapplyingactivein-ferenceatscaleandhighlighttheoperationalhomologiesbetweenactiveinferenceandcurrentmodel-basedapproachestoRL.1 IntroductionIn model-based reinforcement learning (RL), agents first learn a predictive model of the world,beforeusingthismodeltodetermineactions[1]. Encodingamodeloftheworldplausiblyaffordsseveral advantages. For instance, such models can be used to perform perceptual inference [2],implementprospectivecontrol[3,4], quantifyandresolveuncertainty[5], andgeneralizeexistingknowledgetonewtasksandenvironments[6].Assuch,theuseofpredictivemodelshasbeentoutedasapotentialsolutiontothesampleinefficienciesofmodernRLalgorithms[7,8].Atthesametime,thetheoreticalframeworkofactiveinferencehasemergedincognitiveandcom-putational neuroscience as a unifying account of perception, action, and learning [9,10]. Activeinference suggests that biological systems learn a probabilistic model of their habitable environmentandthatthestatesofthesystemchangetomaximize theevidenceforthismodel[11,12].Theresult-ingschemecastsperception,actionandlearningasemergentprocessesof(approximate)Bayesianinference, thereby offering a potentially unifying theory of adaptive biological systems. Despiteits strong theoretical foundations, existing computational implementations have been restricted tolow-dimensional tasks, often with discrete state spaces and actions [11,10,13,14,15]. Here, weestablishaformalconnectionbetweenactiveinferenceandmodel-basedRL.Indoingso,weextendpracticalimplementationsofactiveinferenceothattheyworkeffectivelyatscale, andwesituatemodel-basedRLwithinthebroadtheoreticalcontextofferedbyactiveinference.9102voN42]GL.sc[1v10601.1191:viXraWe present a model of active inference that is applicable in high-dimensional control tasks withboth continuous states and actions. Our model builds upon previous attempts to scale active inference[16,17,18]byincludinganefficientplanningalgorithm, aswellasthequantificationandactiveresolutionofmodeluncertainty.Consistentwiththeactiveinferenceframework,learningandinferenceareachievedbymaximizingsinglelowerboundonBayesianmodelevidence,andpoliciesareselectedtomaximizealowerboundonexpectedBayesianmodelevidence[11].Wedemonstratethatthisunifiednormativeschemeenablessampleefficientlearning,strongperformanceondifficultcontroltasks, andaprincipledapproach toactiveexploration. Moreover, weestablishhomologiesbetweenouractiveinferencebasedmodelandstate-of-the-artapproaches tomodel-basedRL.In what follows, we specify the general mathematical formulation of active inference, before de-scribingourimplementation,whichisapplicableinbothpartially-observedandfully-observedenvironments. We then present preliminary results in three challenging fully-observed continuouscontrol benchmarks, leaving the analysis of partially-observed environments (i.e. pixels) to futurework. Theseresultsdemonstratethatouralgorithmfacilitatesactiveexplorationoverlongtemporalhorizons and significantly outperforms a strong model-free RL baseline, in terms of both sampleefficiencyandperformance.2 ActiveinferenceFollowingpreviouswork[10,11],weconsideractiveinferenceinthecontextofapartiallyobservedMarkovdecisionprocess(POMPD).Ateachtimestept,thetruestateoftheenvironmentˆst∈Rdsˆevolves according to the stochastic transition dynamicsˆst∼ p(ˆst|ˆst−1,at−1), where a ∈ Rdadenotesanagent’sactions. Agentsdonotalwayshaveaccesstothetruestateoftheenvironment,butmightinsteadreceiveobservationsot∈ Rdo,whicharegeneratedaccordingtoot∼ p(ot|ˆst).Assuch,agentsmustoperateonbeliefsst∈Rds aboutthetruestateoftheenvironmentˆst. Inwhatfollows, wedenotethetruedynamicswithuprightlettersp(·)andamodelofthesedynamics(theagent)withitalicsp(·).Active inference proposes that agents implement and update a generative model of their worldp(o˜,˜s,π,θ),wherethetildenotationdenotesasequenceofvariablesthroughtimex˜ ={x ,...,x },0 Tπdenotesapolicy,π ={a ,...,a },andθ ∈Θdenotesparametersofthegenerativemodel,which0 Tarethemselvesrandomvariables. Additionally,agentsmaintainarecognitiondistributionq(˜s,π,θ),representinganagent’s(approximatelyoptimal)beliefsoverstates˜s,policiesπandmodelparame-tersθ.Asnewobservationsaresampled,agentsupdatetheparametersoftheirrecognitiondistributiontominimizevariationalfreeenergyF:F(o˜)=E [lnq(˜s,π,θ)−lnp(o˜,˜s,π,θ)]q(˜s,π,θ)(1)≥−lnp(o˜)This makes the recognition distribution q(˜s,π,θ) converge towards an approximation of the (in-tractable)posteriordistributionp(˜s,π,θ|o˜),therebyimplementingatractableformof(approximate)Bayesianinference[19].Crucially,activeinferencealseproposesthatanagent’sgoalsanddesiresareencodedinthegenera-tivemodelaspriorpreferencesforfavourableobservations[12,20],i.e. bloodtemperatureat37°C.Freeenergythenprovidesaproxyforhowsurprising(i.e.,unlikely)someobservationsareundertheagent’smodel. WhileminimisingEq.1providesanestimateforhowsurprisingsomeobservationsare, it cannot reduce this quantity directly. To achieve this, agents must change their observationsthrough action. Acting to minimise variational free energy ensures the minimisation of surprisal−lnp(o˜),orthemaximisationofthe(Bayesian)modelevidencep(o˜),sincefreeenergyprovidesanupperboundonsurprisal. Activeinference,therefore,proposesthatagent’sselectpoliciesinorderto minimize expected free energy G [12], where the expected free energy for a given policy π atsomefuturetimeτ is:G(π,τ)=E [lnq(s ,θ|π)−lnp(o ,s ,θ|π)]q(oτ,sτ,θ|π) τ τ τ(2)≥−E (cid:2) lnp(o |π) (cid:3)q(oτ|π) τ2Expectedfreeenergyprovidesaboundonexpectedsurprisal,andcanbedecomposedintoextrinsicvalue, which quantifies the degree to which expected observations are congruent with an agent’sprior beliefs, and intrinsic value, which quantifies the amount of information an agent expects togain from enacting some policy [13,11,10]. This decomposition affords a natural interpretation:to avoid being surprised, one should sample unsurprising data, but also learn about the world tomakedatalesssurprisingperse. SelectingpoliciesthatminimizeEq.2will,therefore,ensurethatprobable(i.e.favourable,givenanagent’snormativepriors)observationsarepreferentiallysampled,whilealsoensuringthatagentsgatherinformation