=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: Deep Active Inference for Pixel-Based Discrete Control: Evaluation on the Car Racing Problem
Citation Key: hoeffelen2021deep
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. CRITICAL: The current summary has severe repetition issues. You MUST eliminate all repeated sentences, phrases, and paragraphs. Each idea should be expressed only once. If you find yourself repeating content, remove the duplicates entirely. Focus on variety and uniqueness in your wording.
2. Severe repetition detected: Nested repetition detected: 114 patterns found (severe repetition)

Current draft (first 2000 chars):
Here's a summary of the paper "Deep Active Inference for Pixel-Based Discrete Control: Evaluation on the Car Racing Problem" by Niels van Hoeffelen and Pablo Lanillos, adhering to all your instructions.### OverviewThis paper investigates the potential of deep active inference (DAIF) for controlling a car racing environment, where the agent has no access to the car’s internal state. The research focuses on learning a control policy from high-dimensional visual input, a challenging task for artificial agents. The authors demonstrate that DAIF can achieve comparable performance to deep Q-learning, but doesn’t yet reach state-of-the-art results. The core idea is to minimize the expected free energy, a key concept in active inference, which models the brain’s predictive process.### MethodologyThe DAIF agent was implemented to tackle OpenAI’s car racing benchmark. The system utilizes five deep neural networks to approximate the densities of observation encoding and decoding, the state transition, the policy, and the value. The observation network encodes the high-dimensional input (96x96 RGB) into a latent state representation. The transition network models the evolution of the state over time, while the policy network determines the action to take based on the current state. The value network estimates the expected free energy. The agent employs a replay memory with a batch size of250 and a freeze period for the target network to stabilize learning. The system uses a discrete action space, discretized into11 actions.### ResultsThe DAIF agent’s performance was compared to that of a deep Q-learning (DQN) agent and a random agent. The average reward over100 episodes showed that the DAIF agent achieved a reward of494 ±241, while the DQN agent achieved515 ±162. The authors highlight that the DAIF agent’s performance was on par with the DQN agent, but it didn’t reach the performance of state-of-the-art world model approaches. The slower learning curve of the DAIF agent is attr...

Key terms: brain, learning, deep, racing, pixel, active, evaluation, problem

=== FULL PAPER TEXT ===
Deep Active Inference for Pixel-Based Discrete
Control: Evaluation on the Car Racing Problem(cid:63)
Niels van Hoeffelen and Pablo Lanillos
Department of Artificial intelligence
Donders Institute for Brain, Cognition, and Behaviour
Radboud University
Montessorilaan 3, 6525HR Nijmegen, the Netherlands
niels.vanhoeffelen@ru.nl
p.lanillos@donders.ru.nl
Abstract. Despitethepotentialofactiveinferenceforvisual-basedcon-
trol, learning the model and the preferences (priors) while interacting
with the environment is challenging. Here, we study the performance of
adeepactiveinference(dAIF)agentonOpenAI’scarracingbenchmark,
where there is no access to the car’s state. The agent learns to encode
theworld’sstatefromhigh-dimensionalinputthroughunsupervisedrep-
resentation learning. State inference and control are learned end-to-end
by optimizing the expected free energy. Results show that our model
achieves comparable performance to deep Q-learning. However, vanilla
dAIF does not reach state-of-the-art performance compared to other
world model approaches. Hence, we discuss the current model imple-
mentation’s limitations and potential architectures to overcome them.
Keywords: DeepActiveInference·DeepLearning·POMDP·Visual-
based Control
1 Introduction
Learningfromscratchwhichactionsarerelevanttosucceedinataskusingonly
high-dimensional visual input is challenging and essential for artificial agents
androbotics.Reinforcementlearning(RL)[26]iscurrentlyleadingtheadvances
inpixel-basedcontrol,e.g.,theagentlearnsanactionpolicythatmaximizesthe
accumulateddiscountedrewards.Despiteitsdopaminebiologicalinspiration,RL
is far from capturing the physical processes happening in the brain. We argue,
that prediction in any form (e.g., visual input, muscle feedback or dopamine)
may be the driven motif of the general learning process of the brain [14]. Active
inference[8,4],ageneralframeworkforperception,actionandlearning,proposes
that the brain uses hierarchical generative models to predict incoming sensory
data[6]andtriestominimizethedifferencebetweenthepredictedandobserved
sensorysignals.Thisdifference,mathematicallydescribedasthevariationalfree
(cid:63) 2nd International Workshop on Active Inference IWAI2021, European Conference
on Machine Learning (ECML/PCKDD 2021)
1202
peS
9
]IA.sc[
1v55140.9012:viXra
2 N.T.A. van Hoeffelen, P. Lanillos
energy (VFE), needs to be minimized to generate better predictions about the
worldthatcausesthesesensorysignals[8,17].Throughactingonitsenvironment,
anagentcanaffectsensorysignalstobemoreinlinewithpredictedsignals,which
in turn leads to a decrease of the error between observed and predicted sensory
signals.
AIFmodelshaveshowngreatpotentialinlow-dimensionalanddiscretestate
spaces.Toworkinhigher-dimensionalstatespaces,deepactiveinference(dAIF)
has been proposed, which uses deep neural networks for approximating proba-
bility density functions (e.g., amortised inference) [17,27,29,2,23]. Interestingly,
dAIF can be classified as a generalization of the world models approach [10]
and can incorporate reward-based learning, allowing for a direct comparison to
RL methods. Since the first attempt of dAIF [29], developments have happened
concurrentlyinadaptivecontrol[23,16]andinplanning,exploitingdiscrete-time
optimization, e.g., using the expected free energy [21,28]. In [17], a dIAF agent
was tested on several environments in which the state is observable (Cartpole,
Acrobot, Lunar-lander). In [5], a dAIF agent using Monte-Carlo sampling was
tested on the Animal-AI environment. In [3], dAIF tackled the mountain car
problem and was also tested on OpenAI’s car racing environment. For the car
racing environment, they trained the dAIF agent on a handful of demonstra-
tion rollouts and compared it to a DQN that interacted with the environment
itself. Their results showed that DQN needs a lot more interaction with the
environment to start obtaining rewards compared to dAIF trained on human
demonstrations of the task. Relevant for this work, in [11], a dAIF agent solved
the Cartpole environment as both MDP and as POMPD instances, training the
agent on just visual input.
Inthispaper,westudyadAIFagent1 basedontheproposedarchitecturesin
[17,11] for a more complex pixel-based control POMDP task, namely the Ope-
nAI’s Car Racing environment [13], and discuss its advantages and limitations
compared to other state-of-the-art models. The performance of the dAIF agent
was shown to be in line with previous works and on-par with deep Q-learning.
However,itdidnotachievetheperformanceofotherworldmodelapproaches[1].
Hence, we discuss the reasons for this, as well as architectures that may help to
overcome these limitations.
2 Deep Active Inference Model
ThedAIFarchitecturestudiedisbasedon[11]anddescribedinFig.1.Itmakes
useoffivenetworkstoapproximatethedensitiesofEq.(2):observation(encod-
ing and decoding), transition, policy, and value. The full parameter description
of the networks can be found in the Appendix A.
1 The code can be found at https://github.com/NTAvanHoeffelen/DAIF CarRacing
Deep Active Inference: Evaluation on the Car Racing Problem 3
Fig.1: Deep Active Inference architecture. Five deep artificial neural networks
are used to model the observation encoding and decoding, the state transition,
the policy and the EFE values. The architecture is trained end-to-end by opti-
mizing the expected variational free energy.
Variational Free Energy (VFE). AIF agents infer their actions by minimiz-
ing the VFE expressed at instant k (with explicit action 1-step ahead) as:
F =−KL[q(s ,a )||p(o ,s ,a )] (1)
k k k k 0:k 0:k
Where s ,o ,a are the state, the observation and action respectively, q(s ,a )
k k k k k
is the recognition density, and p(o ,s ,a ) the generative model. Under the
k 0:k 0:k
Markov assumption and factorizing [17,11], Eq. (1) can be rewritten as:
F = E [lnp(o |s )]−KL[q(s )||p(s |s ,a )]−KL[q(a |s )||p(a |s )]
k q(sk) k k k k k−1 k−1 k k k k
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
sensoryprediction stateprediction actionprediction
(2)
Sensory prediction. Theobservationnetworkpredictstheobservations—first
terminEq.(2)—andencodesthehigh-dimensionalinputtostatesq (s |o )
θ k k−N:k
and decodes latent spaces to observations p (o |z ). It can be implemented
ϑ k−N:k k
with a variational autoencoder, where latent representation is described as a
multivariate Gaussian distribution with mean s and variance s . The latent
µ Σ
space z is obtained using the reparametrisation trick. To train the network, we
k
use the binary cross-entropy and the KL regularizer to force the latent space to
be Gaussian:
L =−E [lnp (o |z )]
o,k qθ(sk|ok−N:k) ϑ k−N:k k
1(cid:88) (3)
=BCE(oˆ ,o )− (1+lns −s2 −s )
k−N:k k−N:k 2 Σ,k µ,k Σ,k
State prediction. The transition network models a distribution that allows
the agent to predict the state at time k given the state and action at time
4 N.T.A. van Hoeffelen, P. Lanillos
k − 1, where the input state consists of both the mean s and variance s .
µ Σ
Under the AIF approach this is the difference between the state distribution
(generated by the transition network) and the actual observed state (from the
encoder): KL[q(s ) || p(s |s ,a )]. For the sake of simplicity, we define a
k k k−1 k−1
feed-forward network that computes the maximum-a-posteriori estimate of the
predicted state sˆ =q (s ,a ) and train it using the mean squared error:
k φ k−1 k−1
MSE(s ,q (s ,a )) (4)
µ,k φ k−1 k−1
Action prediction. We use two networks to evaluate action: the policy and
value network. The action prediction part of Eq. (2) is the difference between
the model’s action distribution and the “optimal” true distribution. It is a KL
divergence, which can be split into an energy and an entropy term:
KL[q(a |s )||p(a |s )]=− (cid:88) q(a |s )ln p(a k |s k )
k k k k k k q(a |s )
k k
a
(cid:88) (cid:88)
=− q(a |s )lnp(a |s )− q(a |s )lnq(a |s )
k k k k k k k k
a a
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
energy entropy
(5)
The policy network models the distribution over actions at time k given the
state at time k q (a |s ). It is implemented as a feed-forward neural network
ξ k k
that returns a distribution over actions given a state.
The value network computes the Expected Free Energy (EFE) [21,17,11]
which is used to model the true action posterior p(a |s ). As the true action
k k
posterior is not exactly known, we assume that prior belief makes the agent
select policies that minimize the EFE. We model the distribution over actions
as a precision-weighted Boltzmann distribution over the EFE [21,17,5,24]:
p(a |s )=σ(−γG(s ,o )) (6)
k k k:N k:N
where G(s ,o ) is the EFE for a set of states and observations up to some
k:N k:N
future time N. As we are dealing with discrete time steps, it can be written as
a sum over these time steps:
N
(cid:88)
G(s ,o )= G(s ,o ) (7)
k:N k:N k k
k
TheEFEisevaluatedforeveryaction,becauseofthisweimplicitlyconditioned
on every action [17]. We then define the EFE of a single time step as2:
G(s ,o )=KL[q(s )||p(s ,o )]
k k k k k
≈−lnp(o )+KL[q(s )||q(s |o )] (8)
k k k k
≈−r(o )+KL[q(s )||q(s |o )]
k k k k
2 The full derivation can be found in Appendix D
Deep Active Inference: Evaluation on the Car Racing Problem 5
The negative log-likelihood (or surprise) of an observation −lnp(o ), is re-
t
placed by the reward −r(o ) [17,7,11]. As AIF agents act to minimize their sur-
k
prise, by replacing the surprise with the negative reward, we encode the agent
with the prior that its goal is to maximize reward. This formulation needs the
EFE computation for all of the possible states and observations up to some
time N, making it computationally intractable. Tractability has been achieved
throughbootstrapping[17,11]andcombiningMonte-Carlotreesearchandamor-
tizedinference[5].HerewelearnabootstrappedestimateoftheEFE.Thevalue
network is used to get an estimate of the EFE for all of the possible actions. It
is modelled as a feed-forward neural network G (s ,o )=f (s ).
ρ k k ρ k
Totrainthevaluenetwork,weuseanotherbootstrappedEFEestimatewhich
uses the EFE of the current time step and a β ∈ (0,1] discounted value-net
estimate of the EFE under q(a |s ) for the next time step:
k+1 k+1
Gˆ(s ,o )=−r(o )+KL[q(s )||q(s |o )]+βE [G (s ,o )]
k k k k k k q(ak+1|sk+1) ρ k+1 k+1
(9)
Usinggradientdescent,wecanoptimizetheparametersofthevaluenetwork
by computing the MSE between G (s ,o ) and Gˆ(s ,o ):
ρ k k k k
L =MSE(G (s ,o ),Gˆ(s ,o )) (10)
fρ,k ρ k k k k
In summary, with our implementation, the VFE loss function becomes:
1(cid:88)
−F =BCE(oˆ ,o )− (1+lns −s2 −s )
k k−N:k k−N:k 2 Σ,k µ,k Σ,k
(11)
+MSE(s ,q (s ,a ))
µ,k φ k−1 k−1
+KL[q (a |s )||σ(−γG (s ,o ))]
ξ k k ρ k k
3 Experimental Setup
WeevaluatedthealgorithmonOpenAI’sCarRacing-v0environment[13](Fig.2a).
It is considered a Partial Observable Markov Decision Process (POMDP) prob-
lemasthereisnoaccesstothestateoftheagent/environment.Theinputisthe
top-viewimage(96×96RGB)ofpartoftheracingtrackcentredonthecar.The
goal of this 2D game is to maximize the obtained reward by driving as fast and
precise as possible. A reward of +1000/N is received for every track tile that is
visited, where N is the total number of tiles (placed on the road), and a reward
of -0.1 is received for every frame that passes. Solving the game entails that an
agent scores an average of more than 900 points over 100 consecutive episodes.
By definition, an episode is terminated after the agent visited all track tiles, the
agent strayed out of bounds of the environment, or when 1000 time steps have
elapsed. Every episode, a new race track is randomly generated.
Theagent/carhasthreecontinuouscontrolvariables,namelysteering[−1,1]
(left and right), accelerating [0,1], and braking [0,1]. The action space was dis-
cretized into 11 actions, similarly to [31,25,30], described in Table 2c.
6 N.T.A. van Hoeffelen, P. Lanillos
action values
do nothing [0, 0, 0]
steer sharp left [-1, 0, 0]
steer left [-0.5, 0, 0]
steer sharp right [1 ,0, 0]
(a) CarRacing-v0 steer right [0.5, 0, 0]
accelerate 100% [0, 1, 0]
accelerate 50% [0, 0.5, 0]
accelerate 25% [0, 0.25, 0]
brake 100% [0, 0, 1]
brake 50% [0, 0, 0.5]
brake 25% [0, 0, 0.25]
(c) Discrete Actions
(b) Preprocessed input
4 Results
We compared our dAIF implementation with other state-of-the-art algorithms.
First, Fig. 3 shows the average reward evolution while training for our dAIF
architecture, Deep-Q learning (DQN) [19] (our implementation) and a random
agent. Second, Table 1 shows the average reward over 100 consecutive episodes
for the top methods in the literature. The average reward performance test and
reward per episode for DQN and dAIF are provided in Appendix E.
Fig.3: Moving average reward (MAR) comparison for OpenAI’s CarRacing-v0.
MAR = 0.1CR +0.9MAR , where CR is the cumulative reward of the
e e e−1 e
current episode. In green (solid line), the mean of the dAIF runs that were able
to learn a policy, and in orange (dashed line), the best of all training runs.
Deep Active Inference: Evaluation on the Car Racing Problem 7
For the dAIF and the DQN implementations, observations are first prepro-
cessedbyremovingthebottompartoftheimage.Thispartcontainsinformation
abouttheaccumulatedrewardsofthecurrentepisode,thecar’struespeed,four
ABS sensors, steering wheel position, and gyroscope. Afterwards, the image is
grey-scaled and reshaped to a size of 42×42. The input is defined as a stack of
k to k−N observations to provide temporal information by allowing the encod-
ing of velocity and steering. We use experience replay [15] with a batch size of
250 and memory capacity of 300000 and 100000 transitions for DQN and dAIF
respectively, and make use of target networks [19] (copy of the policy network
for DQN and value network for dAIF) with a freeze period of 50 time steps.
The dAIF agent makes use of a pre-trained VAE which was frozen during
training.Followingasimilarprocedureas[3],theVAEwaspre-trainedonobser-
vations collected by having a human play the environment for 10000 time steps.
Table 1: Average rewards for CarRacing-v0
Method Average Reward
DQN (our implementation) 515 ± 162
dAIF (our implementation) 494 ± 241
A3C (Continuous) [18] 591 ± 45
A3C (Discrete) [12] 652 ± 10
Weight Agnostic Neural Networks [9] 893 ± 74
GA [22] 903 ± 72
World models [10] 906 ± 21
5 Discussion
The dAIF implementation described in this paper has shown to reach perfor-
mance on par with Deep Q-learning. However, there are some remarks. First, it
showed a slower learning curve as described in previous works [11], due to the
needtolearntheworldmodel.Second,weidentifiedsomerunswherethesystem
was not able to learn—See Fig. 3 orange solid line. These runs drag down the
average performance. Finally, it has failed to reach state-of-the-art performance
whencomparingtootherworldmodelapproaches—SeeTable1.Herewediscuss
the limitations of the current implementation and alternative architectures to
overcome the challenge of learning the preferences in dAIF approaches.
Observation and transition model. Our implementation does not fully ex-
ploit temporal representation learning, such as other models that use recurrent
neuralnetworks(RNN).Figure4describestwoarchitecturestolearntheencod-
ingandthetransitioningoftheenvironment.Figure4leftshowstheautoencod-
ing and transition model implemented in this work. This architecture is similar
8 N.T.A. van Hoeffelen, P. Lanillos
Fig.4:Transitionnetworkoutsideoftheobservationnetwork(left)andthetran-
sition network in between the encoder and decoder (right).
to the successful world models [10], but while we used a simple feed-forward
network,theymodelledthestate-transitionwithamixeddensitynetworkRNN.
Interestingly, dAIF also permits the alternative architecture described in Fig. 4
right (also proposed in [20]), in which the network learns to predict future ob-
servations. Here the transition network is part of the autoencoding. By incor-
porating the transition network in the structure of the observation network, we
avoid the need for the dual objectives: perceptual reconstruction and dynamics
learning.Preliminarytestingdidnotshowanyimprovements.Futureworkcould
involve more extensive testing to uncover possible performance improvements.
Dependencyofinputspace TheperformanceofdAIFhasshownastrongde-
pendencyonthelearningoftheobservationmodel.Differentimagepre-processing
methods would lead to improvements of more than 50% in the agent perfor-
mance,asshowninotherDQNimplementationsintheliterature.Testingshowed
that without a pre-trained observation network, the model was unable to learn
consistently and rarely showed performance that would suggest an indication
of task comprehension. By using a pre-trained observation network learning oc-
curredin6outofthe10runs.Toproduceaproperaction-centricrepresentation,
likelihood,transitionandcontrolshouldbelearntconcurrently.However,param-
eters uncertainty may be tackled as the models are being learnt. The current
implementation uses static values for the networks learning rates, future testing
could investigate different variable learning rates for each network or decaying
dropout temperature.
Bootstrapping of the policy and the value. Estimating both the policy
and the value from state encoding has shown end-to-end issues when we do not
pre-train the observation model. In particular, 1-step ahead action formulation
in conjunction with bootstrapping might not capture a proper structure of the
world, which is needed to complete the task, even if we use several consecutive
input images to compute the state. N-step ahead observation optimization EFE
formulations,asproposedin[5,28,20],mayaidlearning.Particularly,whensub-
Deep Active Inference: Evaluation on the Car Racing Problem 9
stituting the negative log surprise by the rewards, the agent might loose the
exploratory AIF characteristic, thus focusing only on goal-oriented behaviour.
Furthermore, and very relevant, the reward implementation in CarRacing-v0
might be not the best way to provide dAIF with rewards for proper preference
learning.
References
1. Openai’s carracing-v0 leaderboard. https://github.com/openai/gym/wiki/
Leaderboard#carracing-v0
2. C¸atal,O.,Nauta,J.,Verbelen,T.,Simoens,P.,Dhoedt,B.:Bayesianpolicyselec-
tion using active inference. arXiv preprint arXiv:1904.08149 (2019)
3. C¸atal, O., Wauthier, S., De Boom, C., Verbelen, T., Dhoedt, B.: Learning gener-
ative state space models for active inference. Frontiers in Computational Neuro-
science 14, 103 (2020)
4. Da Costa, L., Parr, T., Sajid, N., Veselic, S., Neacsu, V., Friston, K.: Active in-
ference on discrete state-spaces: a synthesis. Journal of Mathematical Psychology
99, 102447 (2020)
5. Fountas, Z., Sajid, N., Mediano, P.A., Friston, K.: Deep active inference agents
using monte-carlo methods. arXiv preprint arXiv:2006.04176 (2020)
6. Friston,K.:Atheoryofcorticalresponses.PhilosophicaltransactionsoftheRoyal
Society B: Biological sciences 360(1456), 815–836 (2005)
7. Friston, K., Samothrakis, S., Montague, R.: Active inference and agency: optimal
control without cost functions. Biological cybernetics 106(8), 523–541 (2012)
8. Friston, K.J., Daunizeau, J., Kilner, J., Kiebel, S.J.: Action and behavior: a free-
energy formulation. Biological cybernetics 102(3), 227–260 (2010)
9. Gaier, A., Ha, D.: Weight agnostic neural networks. arXiv preprint
arXiv:1906.04358 (2019)
10. Ha, D., Schmidhuber, J.: World models. arXiv preprint arXiv:1803.10122 (2018)
11. vanderHimst,O.,Lanillos,P.:Deepactiveinferenceforpartiallyobservablemdps.
In: International Workshop on Active Inference. pp. 61–71. Springer (2020)
12. Khan, M., Elibol., O.: Car racing using reinforcement learning (2018), https://
web.stanford.edu/class/cs221/2017/restricted/p-final/elibol/final.pdf
13. Klimov, O.: Carracing-v0. https://gym.openai.com/envs/CarRacing-v0/
14. Lanillos, P., van Gerven, M.: Neuroscience-inspired perception-action in robotics:
applying active inference for state estimation, control and self-perception. arXiv
preprint arXiv:2105.04261 (2021)
15. Lin, L.: Reinforcement learning for robots using neural networks (1992)
16. Meo, C., Lanillos, P.: Multimodal vae active inference controller. arXiv preprint
arXiv:2103.04412 (2021)
17. Millidge,B.:Deepactiveinferenceasvariationalpolicygradients.JournalofMath-
ematical Psychology 96, 102348 (2020)
18. Min J. Jang, S., Lee, C.: Reinforcement car racing with a3c (2017), https://www.
scribd.com/document/358019044/Reinforcement-Car-Racing-with-A3C
19. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,
Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., et al.: Human-level
control through deep reinforcement learning. nature 518(7540), 529–533 (2015)
20. Noel,A.D.,vanHoof,C.,Millidge,B.:Onlinereinforcementlearningwithsparsere-
wardsthroughanactiveinferencecapsule.arXivpreprintarXiv:2106.02390(2021)
10 N.T.A. van Hoeffelen, P. Lanillos
21. Parr, T., Friston, K.J.: Generalised free energy and active inference. Biological
cybernetics 113(5), 495–513 (2019)
22. Risi,S.,Stanley,K.O.:Deepneuroevolutionofrecurrentanddiscreteworldmodels.
In: Proceedings of the Genetic and Evolutionary Computation Conference. pp.
456–462 (2019)
23. Sancaktar, C., van Gerven, M.A., Lanillos, P.: End-to-end pixel-based deep ac-
tive inference for body perception and action. In: 2020 Joint IEEE 10th Interna-
tional Conference on Development and Learning and Epigenetic Robotics (ICDL-
EpiRob). pp. 1–8. IEEE (2020)
24. Schwartenbeck, P., Passecker, J., Hauser, T.U., FitzGerald, T.H., Kronbichler,
M., Friston, K.J.: Computational mechanisms of curiosity and goal-directed ex-
ploration. Elife 8, e41703 (2019)
25. Slik, J.: Deep reinforcement learning for end-to-end autonomous driving (2019)
26. Sutton, R.S., Barto, A.G.: Reinforcement learning: An introduction. MIT press
(2018)
27. Tschantz, A., Baltieri, M., Seth, A.K., Buckley, C.L.: Scaling active inference. In:
2020 International Joint Conference on Neural Networks (IJCNN). pp. 1–8. IEEE
(2020)
28. Tschantz, A., Millidge, B., Seth, A.K., Buckley, C.L.: Reinforcement learning
through active inference. arXiv preprint arXiv:2002.12636 (2020)
29. Ueltzho¨ffer, K.: Deep active inference. Biological cybernetics 112(6), 547–573
(2018)
30. vanderWal,D.,Intelligentie,B.O.K.,Shang,W.:Advantageactor-criticmethods
for carracing (2018)
31. Zhang, Y.: Deep reinforcement learning with mixed convolutional network. arXiv
preprint arXiv:2010.00717 (2020)
A Model Parameters
Table 2: General parameters
Parameter ValueDescription
N 8 Size of the observation stack
screens
N 1 Colour channels of the input image
colour
N 42 Height in pixels of the input image
height
N 42 Width in pixels of the input image
width
N 11 Number of actions the agent can select from
actions
N 1000 Number of episodes the model is trained for
episodes
N 1000 The maximum amount of time steps in an episode
lengthepisode
Freeze period 50 The amount of time steps the target network is frozen
before copying the parameters of the policy/value network
Batch size 250 Number of items in a mini-batch
Deep Active Inference: Evaluation on the Car Racing Problem 11
Table 3: DQN parameters
Parameter Value Description
Policy network Convolutional neural network which estimates Q-values given a state
see Appendix B.
Target network Copy of the Policy network which is updated after each freeze period
see Appendix B.
N 512 Number of hidden units in the policy and target network
hidden
λ 1e-5 Learning rate
γ 0.99 Discount factor
(cid:15) 0.15 → 0.05Probability of selecting a random action. (Starts as 0.15,
decreases linearly per episode with 0.00015 until a minimum of 0.05)
Memory capacity300000 Number of transitions the replay memory can store
Table 4: dAIF parameters
Parameter Value Description
Observation network VAE; see Appendix C.
Transition network Feed-forward neural network of shape:
(2N + 1) x N x N
latent hidden actions
Policy network Feed-forward neural network of shape:
2N x N x N ; with a
latent hidden actions
softmax function on the output
Value network Feed-forward neural network of shape:
2N x N x N
latent hidden actions
Target network Copy of the Value network which is updated
after each freeze period
N 512 Number of hidden units in the transition,
hidden
policy, and value network.
N 128 Size of the latent state
latent
λ 1e-3 Learning rate of the transition network
transition
λ 1e-4 Learning rate of the policy network
policy
λ 1e-5 Learning rate of the value network
value
λ 5e-6 Learning rate of the VAE
VAE
γ 12 Precision parameter
β 0.99 Discount factor
α 18000 1 is multiplied with the VAE loss to scale its
α
size to that of the other term in the VFE
Memory capacity 100000Number of transitions the replay memory can store
12 N.T.A. van Hoeffelen, P. Lanillos
B DQN: Policy network
Table 5: Layers DQN policy network
Type out channelskernelstride input output
conv 64 4 2 (1, 8, 42, 42) (1, 64, 20, 20)
batchnorm
maxpool - 2 2 (1, 64, 20, 20)(1, 64, 10, 10)
relu
conv 128 4 2 (1, 64, 10, 10) (1, 128, 4, 4)
batchnorm
maxpool - 2 2 (1, 128, 4, 4) (1, 128, 2, 2)
relu
conv 256 2 2 (1, 128, 2, 2) (1, 256, 1, 1)
relu
dense - - - 256 512
dense - - - 512 11
Deep Active Inference: Evaluation on the Car Racing Problem 13
C VAE
Table 6: VAE layers
Type out channelskernelstride input output
conv 32 4 2 (1, 8, 42, 42) (1, 32, 20, 20)
b
b
a
a
t
t
c
c
c
c
r
r
o
o
h
h
e
e
n
n
l
l
n
n
u
u
v
v
o
o
r
r
m
m
1
3
2
2
8
4
5
2
2
(1
(
,
1,
32
6
,
4,
20
9
,
,
2
9
0
)
)
(
(
1
1
,
,
1
6
2
4
8
,
,
9
3
,
,
9
3
)
)

Encoder
batchnorm
de
d
n
d
e
c
s
r
r
n
e
o
e
e
e
n
s
n
l
l
u
u
e
l
s
v
o
e
µ
gΣ
25
-
-
-
6 3
-
-
-
2
-
-
-
(1, 12
2
1
1
8
5
2
2
,
6
8
8
3, 3) (1, 25
1
1
1
6
2
2
2
,
8
8
8
1, 1)

dense - - - 128 128 
b
b
a
a
d
d
t
t
d
e
e
c
c
r
e
c
c
h
h
e
n
o
o
l
n
n
u
s
n
n
o
o
e
v
v
r
r
m
m
1
6
2
-
4
8 3
5
-
2
2
-
(
(
1
1
,
,
2
1
5
2
1
6
8
2
,
,
8
1
3
,
,
1
3
)
)
(
(
1
1
,
,
1
6
2
2
4
8
5
,
,
6
9
3
,
,
9
3
)
)

relu Decoder
b
b
a
a
s
d
d
t
t
ig
e
e
c
c
r
r
c
c
m
h
h
e
e
o
o
l
l
n
n
u
u
o
n
n
o
o
i
v
v
d
r
r
m
m
3
8
2 4
4
2
2 (1
(
,
1,
32
6
,
4,
20
9
,
,
2
9
0
)
)
(
(
1
1
,
,
3
8
2
,
,
4
2
2
0
,
,
4
2
2
0
)
)
14 N.T.A. van Hoeffelen, P. Lanillos
D Derivations
Derivation for the EFE for a single time step:
G(s ,o )=KL[q(s )||p(s ,o )]
k k k k k
(cid:90) q(s )
= q(s )ln k
k p(s ,o )
k k
(cid:90)
= q(s )lnq(s )−lnp(s ,o )
k k k k
(cid:90)
= q(s )lnq(s )−lnp(s |o )−lnp(o )
k k k k k
(cid:90)
≈ q(s )lnq(s )−lnq(s |o )−lnp(o )
k k k k k
(cid:90)
≈−lnp(o )+ q(s )lnq(s )−lnq(s |o )
k k k k k
(cid:90) q(s )
≈−lnp(o )+ q(s )ln k
k k q(s |o )
k k
≈−lnp(o )+KL[q(s )||q(s |o )]
k k k k
≈−r(o )+KL[q(s )||q(s |o )]
k k k k
Deep Active Inference: Evaluation on the Car Racing Problem 15
E Average Reward over 100 episodes
Fig.5: Average reward test over 100 episodes for DQN and dAIF. The bright
lines show the mean over episodes. The transparent lines show the reward that
was obtained in a particular episode.

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "Deep Active Inference for Pixel-Based Discrete Control: Evaluation on the Car Racing Problem"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.