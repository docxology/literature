=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model
Citation Key: fujii2025realworld
Authors: Kentaro Fujii, Shingo Murata

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Key Terms: action, real, abstract, hierarchical, model, actions, robot, active, deep, temporally

=== FULL PAPER TEXT ===

Real-World Robot Control by Deep Active Inference
with a Temporally Hierarchical World Model
Kentaro Fujii1 and Shingo Murata 1
Abstractâ€” Robots in uncertain real-world environments must
perform both goal-directed and exploratory actions. However,
most deep learning-based control methods neglect exploration
and struggle under uncertainty. To address this, we adopt
deep active inference, a framework that accounts for human
goal-directed and exploratory actions. Yet, conventional deep
active inference approaches face challenges due to limited
environmental representation capacity and high computational
cost in action selection. We propose a novel deep active
inference framework that consists of a world model, an action
model, and an abstract world model. The world model encodes
environmental dynamics into hidden state representations at
slow and fast timescales. The action model compresses action
sequences into abstract actions using vector quantization, and
the abstract world model predicts future slow states conditioned
on the abstract action, enabling low-cost action selection. We
evaluate the framework on object-manipulation tasks with a
real-world robot. Results show that it achieves high success rates
across diverse manipulation tasks and switches between goal-
directed and exploratory actions in uncertain settings, while
making action selection computationally tractable. These find-
ings highlight the importance of modeling multiple timescale
dynamics and abstracting actions and state transitions.
I. INTRODUCTION
With recent advances in deep learning-based robot control
methods, there is growing expectation for the realization of
robots capable of achieving a wide range of human-like
goals [1]â€“[3]. In real-world environments, the presence or
arrangement of objects required for a task is often uncertain,
and current robots struggle to cope with such uncertainty [4].
In contrast, humans can not only act toward achieving goals
but also explore to resolve environmental uncertaintyâ€”e.g.,
by searching for the location of an objectâ€”thereby adapting
effectively to uncertain situations [5], [6].
To realize robots capable of both goal-directed and ex-
ploratory actions, we focus on deep active inference [7]â€“
[10]â€”a deep learning-based framework grounded in a com-
putational theory that accounts for various cognitive func-
tions [5], [11], [12]. However, deep active inference faces
two key challenges: (1) its performance heavily depends on
the capability of the framework to represent environmental
dynamics [13], and (2) the computational cost is prohibitively
high [9], making it difficult to apply to real-world robots.
To address these challenges, we propose a deep active
inference framework comprising a world model, an action
*This work was supported by JST PRESTO (JPMJPR22C9), JSPS
KAKENHI (JP24K03012), Mori Manufacturing Research and Technology
Foundation.
1Kentaro Fujii and Shingo Murata are with
Graduate School of Integrated Design Engineering,
Keio Universityoakwood.n14.4sp@keio.jp,
murata@elec.keio.ac.jp
model, and an abstract world model. The world model
learns hidden state transitions to represent environmental
dynamics from human-collected robot action and observation
data [14]â€“[16]. The action model maps a sequence of actual
actions to one of a learned set of abstract actions, each cor-
responding to a meaningful behavior (e.g., moving an object
from a dish to a pan) [17]. The abstract world model learns
the relationship between the state representations learned
by the world model and the abstract action representations
learned by the action model [18]. By leveraging the abstract
world model and the abstract action representations, the
framework enables efficient active inference.
To evaluate the proposed method, we conducted robot
experiments in real-world environments with uncertainty.
We investigated whether the framework could reduce com-
putational cost, enable the robot to achieve diverse goals
involving the manipulation of multiple objects, and perform
exploratory actions to resolve environmental uncertainty.
II. RELATED WORK
A. Learning from Demonstration (LfD) for Robot Control
LfD is a method to train robots by imitating human
experts, providing safe, task-relevant data for learning control
policies [19]â€“[24]. A key advancement contributing to recent
progress in LfD for robotics is the idea of generating multi-
step action sequences, rather than only single-step actions
[1]â€“[3], [17], [25]. However, a major challenge in LfD is the
difficulty of generalizing to environments with uncertainty,
even when trained on large amounts of expert demonstrations
[4]. In this work, we focus on the approach that uses
quantized features extracted from action sequences [17], and
treat the extracted features as abstract action representations.
B. World Model
A world model captures the dynamics of the environment
by modeling the relationship between data (observations),
their latent causes (hidden states), and actions. They have
recently attracted significant attention in the context of
model-based reinforcement learning [14], [15], especially in
artificial agents and robotics [26]. However, when robots
learn using a world model, their performance is constrained
by the modelâ€™s capability to represent environmental dynam-
ics [27], [28]. In particular, learning long-term dependencies
in the environment remains a challenge. One solution is
to introduce temporal hierarchy into the model structure
[27], [29]â€“[31]. Furthermore, by incorporating abstract action
representations that capture slow dynamics, the model can
more efficiently predict future observations and states [18].
Â© 2025 IEEE. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or
promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.
The final version of this paper is available at: https://doi.org/10.1109/LRA.2025.3636032
arXiv:2512.01924v1  [cs.RO]  1 Dec 2025
	ğ’›ğ’•ğ¬ 	ğ’…ğ’•#ğ’‰ğ¬AbstractWorldModelğ´!
	ğ’›ğ’•ğŸ		ğ’ğ’•
		ğ’‚ğ’•"ğŸ
World Model	ğ’›ğ’•ğŸ	ğ’›ğ’•ğ¬
	ğ’›ğ’•$ğŸğŸ	ğ’›ğ’•$ğŸğ¬ Action Model		ğ’‚ğ’•
		ğ’‚ğ‘» ãƒ»ãƒ»ãƒ»		ğ’‚$ğ’•
		ğ’‚$ğ‘» ãƒ»ãƒ»ãƒ»ğ´!
1. World and action model learning2. Abstract world model learning
Fig. 1. The overview of the proposed framework. The framework comprises a world model, an action model, and an abstract world model. Here, key
variables are visualized: observationo t and actiona t are processed by the world model to infer hierarchical hidden statesz s
t,z f
t. The action model
compresses action sequences into abstract actionsA t. The abstract world model usesA t to predict the future slow deterministic stated s
t+h.
Temporal hierarchy can be introduced by differentiating
state update frequencies [27], [29], [30] or modulating time
constants of state transitions [16], [32], [33]. In this work,
we adopt the latter to better represent slow dynamics in our
world model [31].
III. THE FORMULATION OF ACTIVE INFERENCE
The free-energy principle [5], [6], [11] is a computational
principle that accounts for various cognitive functions. Ac-
cording to this principle, human observationsoare generated
by unobservable hidden statesz, which evolve in response to
actionsa, following a partially observable Markov decision
process [5]. The brain is assumed to model this genera-
tive process with the world model. Under the free-energy
principle, human perception and action aim to minimize
the surpriseâˆ’logp(o). However, since directly minimizing
surprise is intractable, active inference instead minimizes its
tractable upper bound, the variational free energy [5], [6].
Perception can be formulated as the minimization of the
following variational free energy at time stept[9], [34], [35]:
F(t) =D KL [q(z t)âˆ¥p(z t)]âˆ’E q(zt) [logp(o t |z t)]
â‰¥ âˆ’logp(ot). (1)
Here,q(z t)denotes the approximate posterior over the hid-
den statez t,D KL[q(Â·)||p(Â·)]is the Kullbackâ€“Leibler (KL)
divergence. Note that the first line of (1) is equivalent to the
negative evidence lower bound [36], [37].
Action can be formulated as the minimization of expected
free energy (EFE), which extends variational free energy to
account for future states and observations. LetÏ„ > tbe a
future time step, The EFE is defined as follows [35]:
G(Ï„)â‰ˆ âˆ’Eq(oÏ„ ,zÏ„ |Ï€)[logq(z Ï„ |o Ï„ , Ï€)âˆ’logq(z Ï„ |Ï€)]| {z }
Epistemic value
âˆ’E q(oÏ„ |Ï€)[logp(o Ï„ |o pref)]| {z }
Extrinsic value
.
(2)
Here, the expectation is over the observationo Ï„ because the
future observation is not yet available [35], andÏ€indicates
the policy (i.e. an action sequence). The variableo pref is
referred to as a preference, which encodes the goal, and the
distributionp(o Ï„ |o pref)is called the prior preference. In
(2), the first term referred to as the epistemic value is the
	ğ’”ğ’•"ğŸğŸ	
		ğ’ğ’•"ğŸ
	ğ’…ğ’•"ğŸğŸ
		ğ’‚ğ’•"ğŸ
		ğ’&ğ’•"ğŸ
	ğ’…ğ’•"ğŸğ¬
	ğ’”'ğ’•"ğŸğŸ	
	ğ’”'ğ’•"ğŸğ¬
	ğ’”ğ’•ğŸ	
		ğ’ğ’•
	ğ’…ğ’•ğŸ
		ğ’‚ğ’•
		ğ’&ğ’•
ğ’…ğ’•ğ¬
	ğ’”'ğ’•ğŸ	
	ğ’”'ğ’•ğ¬
	ğ’”ğ’•'ğŸğŸ	
		ğ’ğ’•'ğŸ
	ğ’…ğ’•'ğŸğŸ
		ğ’‚ğ’•'ğŸ
		ğ’&ğ’•'ğŸ
	ğ’…ğ’•'ğŸğ¬
	ğ’”'ğ’•'ğŸğŸ	
	ğ’”'ğ’•'ğŸğ¬	ğ’”ğ’•"ğŸğ¬ 	ğ’”ğ’•ğ¬ 	ğ’”ğ’•'ğŸğ¬
Fig. 2. The world model. It consists of a dynamics model, an encoder,
and a decoder. The dynamics model has two different timescales.
mutual information between the statez Ï„ and the observation
oÏ„ . This term encourages exploratory policies that reduce the
uncertainty in the prior beliefq(z Ï„ |Ï€). On the other hand,
the second term referred to as the extrinsic value encourages
goal-directed policies. Therefore, selecting a policyÏ€that
minimizes the EFE can account for both exploratory and
goal-directed actions [5], [6], [38].
Conventional active inference requires calculating the EFE
over all possible action sequences during task execution,
which is intractable for real-world action spaces [6]. Recent
works have addressed this by using the EFE as a loss function
for training of action generation models [7]â€“[9], but often
ignored exploration capability. In this work, we propose a
novel framework focusing on both goal-achievement perfor-
mance and exploration capability tractably calculating the
EFE during task execution.
IV. METHOD
A. Framework
We propose a framework based on deep active inference
that enables both goal achievement and exploration. The
proposed framework consists of a world model, an action
model, and an abstract world model (Fig. 1).
1) World Model:The world model comprises a dynamics
model, an encoder, and a decoder, all of which are trained
simultaneously (Fig. 2). As the dynamics model, we utilize
a hierarchical model [39], which consists of the slow and
fast states as the hidden statesz t ={z s
t, zf
t}for time stept.
Both deterministicdand stochasticsstates are defined for
each of the slow and fast statesz s
t ={d s
t, ss
t}, zf
t ={d f
t, sf
t},
respectively. These hidden states are calculated as follows:
Slow dynamics
Deterministic state:d s
t =f s
Î¸
 
zs
tâˆ’1

Prior:Ë†s s
t âˆ¼p s
Î¸ (ss
t |d s
t)
Approximate posterior:s s
t âˆ¼q s
Î¸
 
ss
t |d s
t, df
tâˆ’1

.
Fast dynamics
Deterministic State:d f
t =f f
Î¸
 
ss
t, zf
tâˆ’1, atâˆ’1

Prior:Ë†s f
t âˆ¼p f
Î¸
 
sf
t |d f
t

Approximate posterior:s f
t âˆ¼q f
Î¸
 
sf
t |d f
t, ot

(3)
Here,o t is the observation anda tâˆ’1 is the action at the
previous time step. The approximate posterior of the fast dy-
namicsq f
Î¸ is conditioned on the observationo t by receiving
its features extracted by the encoder.
The slow and fast deterministic statesd s
t andd f
t are
computed by multiple timescale recurrent neural network
parameterized with a time constant [32]. When the time
constant is large, the state tends to evolve slowly compared
to when the time constant is small. Therefore, by setting the
time constant for the slow layer larger than one for the fast
layer, the dynamics model represent a temporal hierarchy.
The slow and fast stochastic statess s
t,Ë†ss
t ands f
t,Ë†sf
t are
represented as one-hot vectors sampled from an approximate
posterior or a prior, defined by categorical distributions [40].
The decoder is employed to reconstruct the observation
ot from the hidden statez t, modeling likelihoodp Î¸(ot |z t).
Simultaneously, a networkp Î¸(df
t |z s
t)that predicts the fast
deterministic state Ë†df
t from the slow hidden statez s
t is also
trained. The predicted deterministic state Ë†df
t is then used to
sample the fast stochastic state. By combining both slow
and predicted fast hidden states as inputs to the decoder,
the dynamics model can represent the observation likelihood
pÎ¸(ot |z s
t) 1 based on only the slow hidden statez s
t.
The world model is trained by minimizing the variational
free energyF(t). Here, since the fast deterministic stated f
t
can be regarded as an observation for the slow dynamics, the
variational free energiesF s(t)andF f(t)can be computed
separately for the slow and fast layers, respectively. Further-
more, we also minimize, as an auxiliary task, the negative
log-likelihood of observationo t given the slow hidden state
zs
t, denoted aslogp Î¸(ot |z s
t). In summary, the variational
free energyF(t)in this work is described as follows:
F(t) =Fs(t) +Ff(t)âˆ’logp Î¸(ot|zs
t)
Fs(t) =D KL[sg(qs
Î¸
 
ss
t |d s
t, df
tâˆ’1

)âˆ¥ps
Î¸ (ss
t |d s
t)]
âˆ’logp Î¸(df
t |z s
t)
Ff(t) =D KL[sg(qf
Î¸
 
sf
t |d f
t, ot

)âˆ¥pf
Î¸
 
sf
t |d f
t

]
âˆ’logp Î¸(ot |z t)].
(4)
1Correctly, this distribution is written asp Î¸ (ot |z s
t) =R
pÎ¸ (ot |z t)p f
Î¸
 
sf
t |d f
t

pÎ¸
 
df
t |z s
t

dzf
t. We approximate the marginal
over the fast statesz f
t with a single Monte Carlo sample.
Here, for the KL divergence calculation, we use the KL
balancing technique with a weighting factorw[40].
2) Action Model:The action model consists of an encoder
EÏ• and a decoderD Ï• composed of multilayer perceptron
(MLP), as well as a residual vector quantizer [17], [41], [42]
QÏ• withN q = 2layers. First, the encoderE Ï• embeds the
action sequencea t:t+h of lengthhinto a low-dimensional
featureA t. Next, the featureA t is quantized into Ë†At using the
residual vector quantizerQ Ï•. The residual vector quantizer
includes codebooks{C i}Nq
i=1, each containingKlearnable
codes{c i,j}K
j=1. Specifically, the quantized vector at layer
iis the codec i,k having the smallest Euclidean distance to
the input at layeri. The quantized feature Ë†At is the sum of
outputs from each quantization layer{ Ë†At,i}Nq
i=1 = PNq
i ci,k.
Finally, the decoderD Ï• reconstructs the quantized feature Ë†At
into the action sequenceË†a t:t+h. In summary, the procedure
of the action model is described as follows:
At =E Ï•(at:t+h)
Ë†At =Q Ï•(At)
Ë†at:t+h =D Ï•( Ë†At).
(5)
We treat the feature Ë†At, obtained by the action model, as an
abstract action representing the action sequencea t:t+h.
The encoderE Ï• and decoderD Ï• of the action model are
trained by minimizing the following objective:
LÏ• =Î» MSEâˆ¥at:t+h âˆ’Ë†at:t+hâˆ¥2
2
+Î»commitÎ£Nq
i=1



(At âˆ’Î£ i( Ë†At,iâˆ’1))âˆ’sg (c i,k)



2
2
(6)
where we assume Ë†At,0 = 0. Moreover,Î» MSE andÎ» commit
are coefficients for the reconstruction lossL MSE and the
commitment lossL commit, respectively. The learning of the
codebooks{C i}Nq
i=1 of the residual vector quantizerQ Ï• is
performed using exponential moving averages [17], [41].
3) Abstract World Model:The abstract world modelW Ïˆ
learns a mapping from the current world model statez t and
an abstract actionA t to the future slow deterministic state
ds
t+h. In other words, it provides an abstract representation of
state transitions. The modelW Ïˆ is composed of MLP and
takes the abstract actionA t and the current world model
statez t as inputs to predict the slow deterministic state
ds
t+h. Here, the input abstract actionA t toW Ïˆ can be
any of theK Nq combinations of learned codes from the
action model, denoted as{ Ë†An}KNq
n=1 . Accordingly, for a given
current hidden statez t, the abstract world modelW Ïˆ predicts
KNq possible future slow deterministic states{d s
t+h,n}KNq
n=1 :
{Ë†ds
t+h,n}KNq
n=1 =W Ïˆ(zt,{ Ë†An}KNq
n=1 ). (7)
The abstract world mode is trained by minimizing the
following objective:
LÏˆ = 1
KNq
KNq
X
n=1
âˆ¥Ë†ds
t+h,n âˆ’d s
t+h,nâˆ¥2
2.(8)
Here, to obtain the target slow deterministic states
{ds
t+h,n}KNq
n=1 , we utilize latent imagination of the world
model [15]. To this end, the action sequences{Ë†a 0:h,n}KNq
n=1
are generated from the code combinations{ Ë†An}KNq
n=1 using
the decoderD Ï• of the action model. Then, by leveraging the
prior distribution over the fast states, the slow deterministic
states{d s
t+h,n}KNq
n=1 athsteps ahead are obtained.
B. Action Selection
To make the EFEG(Ï„)calculation tractable, our frame-
work leverages a learned, finite set of abstract actions
{ Ë†An}KNq
n=1 , instead of considering all possible (and thus
infinite) continuous action sequences.
First, we reformulate (2) in accordance with our world
model (for a detailed derivation, see Appendix I):
G(Ï„) =âˆ’E qÎ¸(oÏ„ ,zÏ„ |Ï€)[logq Î¸(zÏ„ |o Ï„ , Ï€)âˆ’logq Î¸(zÏ„ |Ï€)]
âˆ’E qÎ¸(oÏ„ |Ï€)[logp(o Ï„ |o pref)]
â‰ˆ âˆ’EqÎ¸(oÏ„ ,zÏ„ |Ï€)[logq Î¸(sf
Ï„ |z s
Ï„ , oÏ„ )âˆ’logq Î¸(sf
Ï„ |z s
Ï„ )]
âˆ’E qÎ¸(oÏ„ ,zÏ„ |Ï€)[logp(o Ï„ |o pref)].
(9)
Here, the joint distributionq Î¸(oÏ„ , zÏ„ |Ï€)can be decomposed
asq Î¸(oÏ„ , zÏ„ |Ï€) =p Î¸(oÏ„ |z Ï„ )qÎ¸(zf
Ï„ |z s
Ï„ )qÎ¸(zs
Ï„ |Ï€)in
our proposed framework. Note that, given the distribution
qÎ¸(zs
Ï„ |Ï€)over the slow states, all distributions required
to compute the EFEG(Ï„)can be obtained using the world
model, and thusG(Ï„)becomes computable. Here, we replace
the policyÏ€with an abstract action Ë†Aâˆˆ { Ë†An}KNq
n=1 , and
express the distributionq Î¸(zs
Ï„ |Ï€)as follows:
qÎ¸(zs
Ï„ |Ï€)â‰ˆq Î¸(ss
Ï„ |d s
Ï„ , Ë†A)qÏˆ(ds
Ï„ | Ë†A). (10)
In this way, we can use the abstract world modelW Ïˆ to
predict the slow deterministic stated s
Ï„ atÏ„=t+hfrom
the abstract action Ë†A. Using the predicted deterministic state
ds
Ï„ , we can obtain the slow priorq Î¸(zs
Ï„ |Ï€)and compute
the EFE. When computing the EFE, the prior preference
p(oÏ„ |o pref)is assumed to follow a Gaussian distribution
N(o pref, Ïƒ2)with meano pref and varianceÏƒ 2. Therefore, the
EFE can be written as follows:
G(Ï„)â‰ˆ âˆ’EqÎ¸(oÏ„ ,zÏ„ |Ï€)[logq Î¸(sf
Ï„ |z s
Ï„ , oÏ„ )âˆ’logq Î¸(sf
Ï„ |z s
Ï„ )]
âˆ’E qÎ¸(oÏ„ ,zÏ„ |Ï€)[âˆ’Î³(oÏ„ âˆ’o pref)2],
(11)
whereÎ³= 1/2Ïƒ 2 is the preference precision, which balances
the epistemic and extrinsic values, and the expectations in the
EFE are approximated via Monte Carlo sampling [38].
To generate actual robot actions, we first use the abstract
world modelW Ïˆ to predict the slow deterministic states
{ds
t+h,n}KNq
n=1 athsteps into the future for all abstract actions
{ Ë†An}KNq
n=1 , given the current world model statez t. Next,
we predict slow hidden states{z s
t+h,n}KNq
n=1 based on the
predicted slow deterministic states by using (10). Then, for
each predicted state, we compute the EFE and select the
abstract action that yields the minimum EFE. The selected
abstract action is then decoded into an action sequenceË†at:t+h
by the action model, and the robot executes this sequence.
V. EXPERIMENTS
A. Environment Setup
To investigate whether the proposed framework enables
both goal achievement and exploration in real-world envi-
ronmentsâ€”where multiple objects can be manipulated and
uncertainty arises from their placementâ€”we conducted an
experiment using a robot shown in Fig. 4 (left) [43], [44].
The robot had six degrees of freedom, one of which is the
gripper. A camera (RealSense Depth Camera D435; Intel)
was mounted opposite to the robot to capture a view of both
the robot and its environment. From the viewpoint of the
camera, a simple dish, a pot, and a pan were placed on the
right, center, and left, respectively, and a pot lid was placed
closer to the camera than the center pot. Additionally, the
environment was configured such that a blue ball, a red ball,
or both could be present. Note that, therefore, uncertainty
arose when the lid was closed, as the pot might or might not
contain a blue or red ball in this environment.
As training data, we collected object manipulation data
by demonstrating the predetermined eight patterns of poli-
cies (Fig. 4(right)). Each demonstration consists of a se-
quence of two patterns of policies. For all valid combi-
nationsâ€”excluding those in which the policy would result
in no movement (e.g., performing action 3 twice in a
row)â€”we collected five demonstrations per combination by
teleoperating the robot in a leaderâ€“follower manner. There
are36valid action combinations for environments containing
either a blue ball or a red ball, and72combinations for
environments containing both. Each sequence contains100
time steps of joint angles and camera images recorded at
5Hz. Therefore, each pattern of policies had roughly50
time steps. The original RGB images were captured, resized
and clipped to64Ã—80. In this experiment, the robot action
at is defined as the absolute joint angle positions, and the
observationo t is defined as the camera image.
B. Interpretation of the Model Components
In this experiment, we expected the slow hidden states
zs
t to represent the overarching progress of the task, such
as where the balls and the lid were placed. In contrast, we
expected the fast hidden statesz f
t represents more immediate,
transient information. On the other hand, we expected ab-
stract actionsA t to represent a meaningful behavior learned
from the demonstration data. In an ideal case, an abstract
action corresponds to one of the eight policy patterns in Fig.
4(right), such as moving the ball from the dish to the pan.
C. Experimental Criteria
Capability of abstract world model:We evaluated the
capability of the abstract world model. First, we compared
the computation time of our proposed framework against that
of conventional deep active inference approaches [9], [13],
[38], which predicts future states with the world model by
sequentially inputting the action sequenceË†a0:h reconstructed
from an abstract action Ë†Avia the action model.
Second, we evaluated whether different predictions can be
generated from the same initial state for each abstract action
	ğ’›ğ’•ğ’”
	ğ’›ğ’•"ğ’‰,ğŸğ¬AbstractWorldModel
ğ´"!	
		ğ’›ğ’•"ğ’‰,ğŸğ¬		ğ’›ğ’•"ğ’‰,ğŸğ’”	ğ’›ğ’•ğŸ
ActionModel
ğ’¢
ğ´"#	ğ´"$	
ğ´"!	ğ´"#	
ğ´"#	
ğ´"$	
Fig. 3. Action selection based on the minimization of EFE. First, future states are predicted for multiple abstract actions. Then, the EFE is calculated for
each of the predicted future states. Finally, the robot execute action sequence reconstructed from the abstract action that yields the lowest EFE.
îš·2 34568 7
orCamera
Fig. 4. Experimental environment (left) and policy patterns included in the
collected dataset (right). The environment contains either a blue ball, a red
ball, or both. The dataset includes demonstrations of eight different policy
patterns involving the movement of the lid and the balls.
learned by the action model. We also examined whether the
observed outcomes resulting from executing actual actions
generated from a specific abstract action are consistent with
the predictions made by the abstract world model.
Goal achievement performance:
We evaluated the success rate on ball- (140 trials) and
lid-manipulation (24 trials) tasks with varying object config-
urations, such as moving a particular ball or manipulating a
lid. A trial was considered successful if the target object was
placed in its specified goal position within50time steps.
Environment exploration:We evaluated whether the
proposed framework can generate not only goal-directed
actions but also exploratory actions from an uncertain initial
situation. To this end, we set up a scenario in which the
blue ball is initially placed in the pan and the lid is closed,
creating uncertainty about whether the red ball is present
inside the pot. In this scenario, when taking an exploratory
action, it was expected that the robot would open the lid to
resolve the uncertainty.
D. Baseline and Ablation
In the goal-achievement performance experiment, we com-
pared our proposed framework with a baseline and two
ablations described as follows:
â€¢Goal-conditioned diffusion policy (GC-DP).As a
baseline, we implemented a diffusion policy with a U-
Net backbone [1], [45]. In our implementation, this
policy predicted a48-step future actions based on the
two most recent observations and a goal observation.
To stabilize actions, we apply an exponential moving
average of weight0.7to the generated actions.
â€¢Non-hierarchical. As an ablation study, the world
model is replaced by a non-hierarchical dynamics
model [40]. In this variant, the hidden statez t
consists of a single-level deterministic stated t and a
stochastic states t, where the deterministic state is
computed using a gated recurrent unit [46].
â€¢No abstract world model (A WM).As an ablation
study, the robot does not use the abstract world model
for planning. Instead, it calculates the EFE directly over
actual action sequences decoded by the action model.
We did not perform an ablation on the action model itself,
as our framework relies on it to generate the set of candidate
actions (either abstract or actual) for evaluation, making it a
core, indispensable component.
VI. RESULTS
A. Capability of abstract world model
Our proposed framework required only2.37ms to eval-
uate all candidate abstract actions, in contrast to71.8ms
for a sequential evaluation of conventional deep active infer-
ence approaches. This demonstrates the higher computational
tractability of our proposed framework.
As shown in Fig. 5, different abstract actions lead to dis-
tinct predictions. Moreover, for example, by using an action
sequence generated from the abstract action represented by
c1,2 +c 2,7, the ball was successfully moved from the dish
to the pan, consistent with the predicted observation (Fig.
5). These results suggest that the abstract world model has
learned the dependency between abstract actions and the
resulting state transitions, even without directly referring to
actual action sequences. However, the prediction associated
with the abstract action Ë†Arepresented by Ë†A=c 1,8 +c 2,8 in
Fig. 5 shows red balls placed on both the dish and the pan,
which is inconsistent with the initial condition in which only
a blue ball was present. This abstract action corresponded to
moving a ball from the center pot to the pan. Since this action
was not demonstrated when the pot was empty, the abstract
z
ğ‘!,!ğ‘!,#ğ‘!,$ğ‘!,%ğ‘!,&ğ‘!,'ğ‘!,(ğ‘!,)ğ‘#,!ğ‘#,# ğ‘#,%ğ‘#,$ ğ‘#,&ğ‘#,' ğ‘#,)ğ‘#,(
All predictionsInitial
Inconsistent
A. Predictions by the abstract world model 
Consistent
B. Actual robot action 	(ğ‘!,#+ğ‘#,$)
Time 01020304050
Fig. 5. Example of predicted observations using the abstract world model
and actual robot actions. (A) Predicted observations for each abstract action.
Here, eachc i,j denotes thej-th code in thei-th layer of the action model.
The yellow box highlights an example prediction that is consistent with the
initial observation, while the red box indicates an inconsistent prediction. (B)
Actual observations corresponding to the action sequence generated from
the abstract action Ë†Arepresented by Ë†A=c 1,2 +c 2,7 at each time step.
TABLE I
SUCCESS RATE(%).
Manipulation target Ball Lid TotalRed Blue Opening Closing
Proposed 61.4 74.3 75.0100.0 70.7
GC-DP 18.6 25.7 25.0 50.0 24.4
Non-hierarchical 41.4 51.4 75.0 58.3 51.2
No AWM 40.0 21.4 83.366.7 37.2
world model may have learned incorrect dependencies for
unlearned actionâ€“environment combinations.
B. Goal achievement performance
Table I shows the success rates of our proposed frame-
work on goal-directed action generation, evaluated on tasks
involving specific ball and lid manipulations. The proposed
method outperformed the baseline and the ablations across
all goal conditions except the Lid-Opening goal, achieving a
total success rate of over 70%. As a qualitative example, Fig.
6 illustrates the EFE calculation for a scenario where the goal
is to move a ball from a dish to a pan. The abstract action
with the lowest EFE correctly predicts the desired outcome,
and executing the actual actions derived from this abstract
action led to successful task completion. This overall result
confirms that selecting abstract actions by minimizing the
EFE is effective for goal achievement.
The failures in our framework were mainly due to in-
consistent world model predictions, which misled the robot
into believing an inappropriate action would succeed. For
example, the proposed framework selected actions to grasp
nothing but place the (non-grasped) target object at the ap-
Abstract action indexInitialGoalPrediction with minimum EFE
Ã—10!1510EFE500 63
Fig. 6. Example of EFE computed for each abstract action.Top: EFE
values computed for all 64 abstract actions. The action with the lowest EFE
is highlighted as a yellow bar.Bottom: From left to right, the images show
the initial observation, goal, and predicted observation resulting from the
abstract action with the lowest EFE.
TABLE II
EFEVALUES FOR TWO REPRESENTATIVE ABSTRACT ACTIONS
(GOAL-DIRECTED AND EXPLORATORY)IN THE UNCERTAIN SCENARIO.
Preference precision Goal-directed Exploratory
Î³= 102 4.21Ã—10 4 14.5Ã—10 4
Î³= 10âˆ’4 âˆ’4.67Ã—10 0 âˆ’6.11Ã—10 0
propriate location. In contrast, the GC-DP, Non-hierarchical,
and No AWM all exhibited lower success rates. The GC-
DP frequently failed in grasping and placing objects. Both
ablations suffered from more prediction inconsistencies than
our full model, highlighting the importance of temporal hier-
archy and action/state abstraction. The lower performance of
the No AWM ablation suggests that action abstraction was
a particularly critical component for success.
C. Environment exploration
For simplicity, we computed the EFE for two abstract
actions: moving the blue ball from the pan to the dish (goal-
directed), and opening the lid (exploratory), as summarized
in Table II. When preference precisionÎ³was set to10 2,
the EFE for the goal-directed action became lower, and
thus the robot moved the blue ball from the pan to the
dish. In contrast, when preference precisionÎ³was set to
10âˆ’4, the EFE for the exploratory action became lower, and
thus the robot opened the lid. These results indicate that
the proposed framework can assign high epistemic value to
exploratory actions that provide new information, and that
exploratory actions can be induced by appropriately adjusting
the preference precisionÎ³.
VII. CONCLUSIONS
In this work, we introduced a deep active-inference frame-
work that combines a temporally-hierarchical world model,
an action model utilizing vector quantization, and an abstract
world model. By capturing dynamics in a temporal hierarchy
and encoding action sequences as abstract actions, the frame-
work makes the action selection based on active inference
computationally tractable. Real-world experiments on object-
manipulation tasks demonstrated that the proposed frame-
work outperformed the baseline in various goal-directed
settings, as well as the ability to switch from goal-directed
to exploratory actions in uncertain environments.
Despite these promising results, several challenges remain:
1) The action model used a fixed sequence length, which
may not be optimal. 2) The modelâ€™s predictive capability
decreases for action-environment combinations not present
in the dataset. 3) While we validated the capability to take
exploratory actions, we did not evaluate their effectiveness
in solving tasks and the switching to exploratory behavior
still relies on a manually tuned hyperparameter.
Future work will focus on extending the framework to
address these limitations. An immediate step is to evaluate
our framework in environments that require multi-step action
selection and where exploration is necessary to solve the task.
Other promising directions include developing a mechanism
for adaptive switching between goal-directed and exploratory
modes, and extending the action model to represent variable-
length action sequences. Ultimately, this work represents a
significant step toward the long-term goal of creating more
capable robots that can operate effectively in uncertain real-
world environments such as household tasks by leveraging
both goal-directed and exploratory behaviors.
APPENDIXI
EFE DERIVATION
We show the detailed derivation of EFE in our framework:
G(Ï„) =âˆ’E qÎ¸(oÏ„ ,zÏ„ |Ï€)[logq Î¸(zÏ„ |o Ï„ , Ï€)âˆ’logq Î¸(zÏ„ |Ï€)]
âˆ’E qÎ¸(oÏ„ ,zÏ„ |Ï€)[logp(o Ï„ |o pref)]
=âˆ’E qÎ¸(oÏ„ ,zÏ„ |Ï€)[logq Î¸(zf
Ï„ |z s
Ï„ , oÏ„ )qÎ¸(zs
Ï„ |Ï€)
âˆ’logq Î¸(zf
Ï„ |z s
Ï„ )qÎ¸(zs
Ï„ |Ï€)]
âˆ’E qÎ¸(oÏ„ ,zÏ„ |Ï€)[logp(o Ï„ |o pref)]
=âˆ’E qÎ¸(oÏ„ ,zÏ„ |Ï€)[logq Î¸(zf
Ï„ |z s
Ï„ , oÏ„ )âˆ’logq Î¸(zf
Ï„ |z s
Ï„ )]
âˆ’E qÎ¸(oÏ„ ,zÏ„ |Ï€)[logp(o Ï„ |o pref)]
=âˆ’E qÎ¸(oÏ„ ,zÏ„ |Ï€)[logq Î¸(sf
Ï„ |d f
Ï„ , oÏ„ )qÎ¸(df
Ï„ |z s
Ï„ )
âˆ’logq Î¸(sf
Ï„ |d f
Ï„ )qÎ¸(df
Ï„ |z s
Ï„ )]
âˆ’E qÎ¸(oÏ„ ,zÏ„ |Ï€)[logp(o Ï„ |o pref)]
â‰ˆ âˆ’EqÎ¸(oÏ„ ,zÏ„ |Ï€)[logq Î¸(sf
Ï„ |z s
Ï„ , oÏ„ )âˆ’logq Î¸(sf
Ï„ |z s
Ï„ )]
âˆ’E qÎ¸(oÏ„ ,zÏ„ |Ï€)[logp(o Ï„ |o pref)].
(12)
APPENDIXII
ADDITIONAL EXPERIMENTS
To validate the scalability of our framework, we further
evaluated our framework on the CALVIN D benchmark [47],
which provides various unstructured human data. Although
this environment can serve language goal conditioning, we
used only image-based goal conditioning.
For this environment, we compared our proposed
framework with the GC-DP. The evaluation was con-
ducted on eight tasks: move slider left/right (Slider), open/-
close drawer (Drawer), turn on/off lightbulb (Lightbulb),
TABLE III
SUCCESS RATE INCALVIN ENVIRONMET(%).
task Slider Drawer Lightbulb LED Total
Proposed 43.8 93.80.0 11.8 37.5
GC-DP 1.6 68.352.6 16.7 34.8
TABLE IV
HYPERPARAMETERS OF OUR PROPOSED FRAMEWORK
Name Symbol Value
World Model
Training data sequence length â€” 75
Slow dynamics
Deterministic state dimensions â€” 32
Stocahstic state dimensionsÃ—classes â€”4Ã—4
Time constant â€” 32
Fast dynamics
Deterministic state dimensions â€” 128
Stocahstic state dimensionsÃ—classes â€”8Ã—8
Time constant â€” 4
KL balancingw0.8
Action Model
Layers of MLP â€” 2
Hidden dimensions of MLP â€” 128
Action sequence lengthh50
Codebook sizeK8
Abstract action dimensions â€” 32
Learning coefficientsÎ» MSE, Î»commit 1.0,5.0
Abstract World Model
Layers of MLP â€” 2
Hidden dimensions of MLP â€” 512
and turn on/off led (LED). A trial was considered successful
if the task was completed within150timesteps. Our proposed
framework used the same hyperparameters as in our primary
experiments, but the GC-DP was trained to predict a28-step
future action sequence from a four-step observation history
and re-planned every16steps.
As shown in Table III, our proposed method consistently
outperformed GC-DP on the Slider and Drawer tasks, as well
as on the average success rate across all tasks. These results
suggest that our approach, which leverages a temporally
hierarchical world model and abstract actions, is robust and
effective not only in our primary setup but also in more
complex, long-horizon manipulation scenarios.
APPENDIXIII
HYPER PARAMETERS
We show hyperparameters in our experiments in Table IV.
REFERENCES
[1] C. Chi, Z. Xu, S. Feng, E. Cousineau, Y . Du, B. Burchfiel, R. Tedrake,
and S. Song, â€œDiffusion policy: Visuomotor policy learning via ac-
tion diffusion,â€The International Journal of Robotics Research, p.
02783649241273668, 2023.
[2] H. Etukuru, N. Naka, Z. Hu, S. Lee, J. Mehu, A. Edsinger, C. Paxton,
S. Chintala, L. Pinto, and N. M. M. Shafiullah, â€œRobot utility models:
General policies for zero-shot deployment in new environments,â€arXiv
preprint arXiv:2409.05865, pp. 1â€“28, 2024.
[3] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn,
N. Fusai, L. Groom, K. Hausman, B. Ichter,et al., â€œÏ€ 0: A vision-
language-action flow model for general robot control,â€arXiv preprint
arXiv:2410.24164, pp. 1â€“17, 2024.
[4] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch,
T. Armstrong, and P. Florence, â€œInteractive language: Talking to robots
in real time,â€IEEE Robotics and Automation Letters (RA-L), pp. 1â€“8,
2023.
[5] K. Friston, F. Rigoli, D. Ognibene, C. Mathys, T. FitzGerald, and
G. Pezzulo, â€œActive inference and epistemic value,â€Cognitive Neuro-
science, vol. 6, no. 4, pp. 187â€“214, 2015.
[6] K. Friston, T. FitzGerald, F. Rigoli, P. Schwartenbeck, and G. Pezzulo,
â€œActive inference: A process theory,â€Neural Computation, vol. 29, pp.
1â€“49, 2017.
[7] B. Millidge, â€œDeep active inference as variational policy gradients,â€
Journal of Mathematical Psychology, vol. 96, p. 102348, 2020.
[8] Z. Fountas, N. Sajid, P. Mediano, and K. Friston, â€œDeep active
inference agents using monte-carlo methods,â€Advances in Neural
Information Processing Systems, vol. 33, pp. 11 662â€“11 675, 2020.
[9] P. Mazzaglia, T. Verbelen, and B. Dhoedt, â€œContrastive active infer-
ence,â€ inAdvances in Neural Information Processing Systems, vol. 34,
2021, pp. 13 870â€“13 882.
[10] K. Fujii, T. Isomura, and S. Murata, â€œReal-world robot control based
on contrastive deep active inference with demonstrations,â€IEEE
Access, vol. 12, pp. 172 343â€“172 357, 2024.
[11] K. Friston, â€œThe free-energy principle: a unified brain theory?â€Nature
reviews neuroscience, vol. 11, no. 2, pp. 127â€“138, 2010.
[12] P. Schwartenbeck, J. Passecker, T. U. Hauser, T. H. FitzGerald,
M. Kronbichler, and K. J. Friston, â€œComputational mechanisms of
curiosity and goal-directed exploration,â€Elife, vol. 8, p. e41703, 2019.
[13] N. Sajid, P. Tigas, A. Zakharov, Z. Fountas, and K. Friston, â€œExplo-
ration and preference satisfaction trade-off in reward-free learning,â€
arXiv preprint arXiv:2106.04316, pp. 1â€“23, 2021.
[14] D. Ha and J. Schmidhuber, â€œRecurrent world models facilitate policy
evolution,â€ inAdvances in Neural Information Processing Systems,
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, Eds., vol. 31, 2018, pp. 1â€“13.
[15] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and
J. Davidson, â€œLearning latent dynamics for planning from pixels,â€ in
International Conference on Machine Learning, vol. 97. PMLR,
2019, pp. 2555â€“2565.
[16] A. Ahmadi and J. Tani, â€œA novel predictive-coding-inspired variational
rnn model for online prediction and recognition,â€Neural Computation,
vol. 31, no. 11, pp. 2025â€“2074, 2019.
[17] S. Lee, Y . Wang, H. Etukuru, H. J. Kim, N. M. M. Shafiullah, and
L. Pinto, â€œBehavior generation with latent actions,â€arXiv preprint
arXiv:2403.03181, pp. 1â€“18, 2024.
[18] C. Gumbsch, N. Sajid, G. Martius, and M. V . Butz, â€œLearning
hierarchical world models with adaptive temporal abstractions from
discrete latent dynamics,â€ inThe Twelfth International Conference on
Learning Representations, 2024.
[19] H. Ravichandar, A. S. Polydoros, S. Chernova, and A. Billard, â€œRecent
advances in robot learning from demonstration,â€Annual Review of
Control, Robotics, and Autonomous Systems, vol. 3, pp. 297â€“330,
2020.
[20] A. Correia and L. A. Alexandre, â€œA survey of demonstration learning,â€
Robotics and Autonomous Systems, vol. 182, p. 104812, 2024.
[21] M. Zare, P. M. Kebria, A. Khosravi, and S. Nahavandi, â€œA survey of
imitation learning: Algorithms, recent developments, and challenges,â€
IEEE Transactions on Cybernetics, vol. 54, no. 12, pp. 7173â€“7186,
2024.
[22] P. Florence, C. Lynch, A. Zeng, O. A. Ramirez, A. Wahid, L. Downs,
A. Wong, J. Lee, I. Mordatch, and J. Tompson, â€œImplicit behavioral
cloning,â€ inConference on Robot Learning. PMLR, 2022, pp. 158â€“
168.
[23] P. Lancaster, N. Hansen, A. Rajeswaran, and V . Kumar, â€œModem-v2:
Visuo-motor world models for real-world robot manipulation,â€ in2024
IEEE International Conference on Robotics and Automation (ICRA),
2024, pp. 7530â€“7537.
[24] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch,
S. Levine, and C. Finn, â€œBc-z: Zero-shot task generalization with
robotic imitation learning,â€ inConference on Robot Learning. PMLR,
2022, pp. 991â€“1002.
[25] T. Z. Zhao, V . Kumar, S. Levine, and C. Finn, â€œLearning fine-grained
bimanual manipulation with low-cost hardware,â€ inICML Workshop
on New Frontiers in Learning, Control, and Dynamical Systems, 2023,
pp. 1â€“22.
[26] T. Taniguchi, S. Murata, M. Suzuki, D. Ognibene, P. Lanillos, E. Ugur,
L. Jamone, T. Nakamura, A. Ciria, B. Lara,et al., â€œWorld models and
predictive coding for cognitive and developmental robotics: frontiers
and challenges,â€Advanced Robotics, vol. 37, no. 13, pp. 780â€“806,
2023.
[27] W. Cai, T. Wang, J. Wang, and C. Sun, â€œLearning a world model with
multitimescale memory augmentation,â€IEEE Transactions on Neural
Networks and Learning Systems, pp. 1â€“10, 2022.
[28] F. Deng, J. Park, and S. Ahn, â€œFacing off world model backbones:
Rnns, transformers, and s4,â€Advances in Neural Information Pro-
cessing Systems, vol. 36, pp. 1â€“27, 2024.
[29] T. Kim, S. Ahn, and Y . Bengio, â€œVariational temporal abstraction,â€
Advances in Neural Information Processing Systems, vol. 32, pp. 1â€“
10, 2019.
[30] V . Saxena, J. Ba, and D. Hafner, â€œClockwork variational autoen-
coders,â€Advances in Neural Information Processing Systems, vol. 34,
pp. 29 246â€“29 257, 2021.
[31] K. Fujii and S. Murata, â€œHierarchical latent dynamics model with
multiple timescales for learning long-horizon tasks,â€ in2023 IEEE
International Conference on Development and Learning (ICDL), 2023,
pp. 479â€“485.
[32] Y . Yamashita and J. Tani, â€œEmergence of functional hierarchy in a
multiple timescale neural network model: A humanoid robot exper-
iment,â€PLoS Computational Biology, vol. 4, no. 11, p. e1000220,
2008.
[33] A. Spieler, N. Rahaman, G. Martius, B. Sch Â¨olkopf, and A. Levina,
â€œThe expressive leaky memory neuron: an efficient and expressive
phenomenological neuron model can solve long-horizon tasks.â€ inThe
Twelfth International Conference on Learning Representations, 2024,
pp. 1â€“25.
[34] P. Mazzaglia, T. Verbelen, O. C Â¸ atal, and B. Dhoedt, â€œThe free energy
principle for perception and action: A deep learning perspective,â€
Entropy, vol. 24, no. 2, p. 301, 2022.
[35] R. Smith, K. J. Friston, and C. J. Whyte, â€œA step-by-step tutorial
on active inference and its application to empirical data,â€Journal of
Mathematical Psychology, vol. 107, p. 102632, 2022.
[36] D. P. Kingma and M. Welling, â€œAuto-encoding variational bayes,â€ pp.
1â€“14, 2013.
[37] D. J. Rezende, S. Mohamed, and D. Wierstra, â€œStochastic backprop-
agation and approximate inference in deep generative models,â€ in
International Conference on Machine Learning. PMLR, 2014, pp.
1278â€“1286.
[38] K. Igari, K. Fujii, G. W. Haddon-Hill, and S. Murata, â€œSelection of
exploratory or goal-directed behavior by a physical robot implement-
ing deep active inference,â€ in5th International Workshop on Active
Inference, 2024, pp. 1â€“14.
[39] K. Fujii and S. Murata, â€œHierarchical latent dynamics model with
multiple timescales for learning long-horizon tasks,â€ in2023 IEEE
International Conference on Development and Learning (ICDL), 2023,
pp. 479â€“485.
[40] D. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba, â€œMastering atari
with discrete world models,â€ inInternational Conference on Learning
Representations, 2021, pp. 1â€“26.
[41] A. Van Den Oord, O. Vinyals,et al., â€œNeural discrete representa-
tion learning,â€Advances in Neural Information Processing Systems,
vol. 30, pp. 1â€“10, 2017.
[42] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasac-
chi, â€œSoundstream: An end-to-end neural audio codec,â€IEEE/ACM
Transactions on Audio, Speech, and Language Processing, vol. 30,
pp. 495â€“507, 2021.
[43] A. Koch, â€œLow-cost robot arm,â€ https://github.com/
AlexanderKoch-Koch/low cost robot, 2024.
[44] R. Cadene, S. Alibert, A. Soare, Q. Gallouedec, A. Zouitine, and
T. Wolf, â€œLerobot: State-of-the-art machine learning for real-world
robotics in pytorch,â€ https://github.com/huggingface/lerobot, 2024.
[45] O. Ronneberger, P. Fischer, and T. Brox, â€œU-Net: Convolutional
Networks for Biomedical Image Segmentation,â€ inInternational Con-
ference on Medical Image Computing and Computer-Assisted Inter-
vention. Springer, 2015, pp. 234â€“241.
[46] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio, â€œEmpirical evaluation
of gated recurrent neural networks on sequence modeling,â€ inNeural
Information Processing Systems 2014 Workshop on Deep Learning,
2014, pp. 1â€“9.
[47] O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard, â€œCalvin: A
benchmark for language-conditioned policy learning for long-horizon
robot manipulation tasks,â€IEEE Robotics and Automation Letters (RA-
L), vol. 7, no. 3, pp. 7327â€“7334, 2022.

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * FIX SPACING ISSUES FROM PDF EXTRACTION:
       - PDF extraction may remove spaces between words (e.g., 'dynamicssimulationsand' â†’ 'dynamics simulations and')
       - When extracting quotes, restore missing spaces between words if they appear concatenated
       - Look for patterns like 'word1word2word3' and add spaces: 'word1 word2 word3'
       - This is a common issue from PDF text extraction that needs correction
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written (with spacing normalized)
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)
   - FIX SPACING ISSUES FROM PDF EXTRACTION:
     * PDF extraction may remove spaces between words (e.g., 'dynamicssimulationsand' â†’ 'dynamics simulations and')
     * When extracting quotes, restore missing spaces between words if they appear concatenated
     * Look for patterns like 'word1word2word3' and add spaces: 'word1 word2 word3'
     * This is a common issue from PDF text extraction that needs correction

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   âŒ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   âœ… GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   âŒ BAD: Repeating the same claim 3+ times with slight variations
   âœ… GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
