### OverviewThis paper investigates the mapping of Husserlian phenomenology onto active inference. The authors state: “Phenomenology is the rigorous descriptive study of conscious experience.” They note: “Recent attempts to formalize Husserlian phenomenology provide us with a mathematical model of perception as a function of prior knowledge and expectation.” The paper argues: “We propose that key aspects of Husserl’s descriptions of consciousness can be mapped onto aspects of the generative models associated with the active inference approach.” The study demonstrates: “active inference is a corollary of the free energy principle in Bayesian mechanics.” The authors further claim: “active inference describes the dynamics (i.e., observable behavior) of things, so defined, as a path of least action, where the action is defined as time or path integral of an information theoretic quantity called self-information or, more simply, surprisal [7]–[11]. This quantity is also known as the negative log evidence in Bayesian inference.”### MethodologyThe authors explain: “Given the intended audience of this paper, we will only briefly review active inference.” They state: “Active inference is a process theory that can be used to model any physically separable, re-identifiable thing or particle, i.e., anything that persists as a coherent locus of states or paths, over some appreciable timescale.” The paper details: “Active inference describes the dynamics (i.e., observable behavior) of things, so defined, as a path of least action, where the action is defined as time or path integral of an information theoretic quantity called self-information or, more simply, surprisal [7]–[11]. This quantity is also known as the negative log evidence in Bayesian inference.” The study elaborates: “The free energy principle in Bayesian mechanics.” They note: “Active inference describes the dynamics (i.e., observable behavior) of things, so defined, as a path of least action, where the action is defined as time or path integral of an information theoretic quantity called self-information or, more simply, surprisal [7]–[11]. This quantity is also known as the negative log evidence in Bayesian inference.” The authors explain: “The free energy principle in Bayesian mechanics.” They note: “Active inference describes the dynamics (i.e., observable behavior) of things, so defined, as a path of least action, where the action is defined as time or path integral of an information theoretic quantity called self-information or, more simply, surprisal [7]–[11]. This quantity is also known as the negative log evidence in Bayesian inference.” The study details: “This quantity is also known as the negative log evidence in Bayesian inference.” The authors explain: “The free energy principle in Bayesian mechanics.” They note: “Active inference describes the dynamics (i.e., observable behavior) of things, so defined, as a path of least action, where the action is defined as time or path integral of an information theoretic quantity called self-information or, more simply, surprisal [7]–[11]. This quantity is also known as the negative log evidence in Bayesian inference.” The study details: “This quantity is also known as the negative log evidence in Bayesian inference.” The authors explain: “The free energy principle in Bayesian mechanics.” They note: “Active inference describes the dynamics (i.e., observable behavior) of things, so defined, as a path of least action, where the action is defined as time or path integral of an information theoretic quantity called self-information or, more simply, surprisal [7]–[11]. This quantity is also known as the negative log evidence in Bayesian inference.” The study details: “This quantity is also known as the negative log evidence in Bayesian inference.” The authors explain: “The free energy principle in Bayesian mechanics.” They note: “Active inference describes the dynamics (i.e., observable behavior) of things, so defined, as a path of least action, where the action is defined as time or path integral of an information theoretic quantity called self-information or, more simply, surprisal [7]–[11]. This quantity is also known as the negative log evidence in Bayesian inference.” The study details: “This quantity is also known as the negative log evidence in Bayesian inference.”### ResultsThe paper states: “To simulate an agent, we equip it with the states and parameters shown in Table1, which can either be specified a priori by the experimenter, or learned based on real data.” The authors claim: “In active inference, a policy is a set of beliefs about possible courses of action; and action itself is modelled as a kind of self-fulfilling prophecy.” They note: “The value of a policy is determined by estimating a quantity is known as ‘expected free energy’, which encodes how much each policy will minimize surprisal or, equivalently, maximize model evidence, with respect to preferred outcomes.” The study elaborates: “Prior beliefs about base rates of occurrence of states are described by the D vector, with each entry scoring the prior probability of the associated state.” The authors explain: “baseline preferences for policy selection are described by the E vector.”### DiscussionThe paper argues: “We propose that key aspects of Husserl’s descriptions of consciousness can be mapped onto aspects of the generative models associated with the active inference approach.” The authors state: “active inference, like Husserlian phenomenology, is fertile ground for investigation.” The study concludes: “Moving towards computational phenomenology through a connection between Husserlian phenomenology and active inference may allow us to bridge the gaps to fundamental questions such as the explanatory gap and positionality, and extend further into sociological issues of intersectionality, which make fundamental reference to first-person experience.”### AcknowledgmentsThe authors thank Philippe Blouin, Laurence Kirmayer, Magnus Koudahl, Antoine Lutz, Jonas Mago, Jelena Rosic, Anil Seth, Lars Sandved Smith, Dalton Sakthivadivel, and the members of the VERSES Research Lab for useful discussions that shaped the contents of the paper. Special thanks are due to Juan Diego Bogotá, Zak Djebbara, and Karl Friston.