=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Active Inference in Discrete State Spaces from First Principles
Citation Key: kenny2025active
Authors: Patrick Kenny

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2025

Abstract: We seek to clarify the concept of active inference by disentangling it
from the Free Energy Principle. We show how the optimizations that
need to be carried out in order to implement active inference in discrete
state spaces can be formulated as constrained divergence minimization
problems which can be solved by standard mean field methods that
do not appeal to the idea of expected free energy. When it is used
to model perception, the perception/action divergence criterion that
we propose coinci...

Key Terms: entropy, perception, spaces, action, discrete, divergence, principles, free, state, model

=== FULL PAPER TEXT ===

Active Inference in Discrete State Spaces from
First Principles
Patrick Kenny
Patrick.Kenny.000@gmail.com
Centre de recherche informatique de Montr´ eal
November 26, 2025
Abstract
We seek to clarify the concept of active inference by disentangling it
from the Free Energy Principle. We show how the optimizations that
need to be carried out in order to implement active inference in discrete
state spaces can be formulated as constrained divergence minimization
problems which can be solved by standard mean field methods that
do not appeal to the idea of expected free energy. When it is used
to model perception, the perception/action divergence criterion that
we propose coincides with variational free energy. When it is used to
model action, it differs from an expected free energy functional by an
entropy regularizer.
Contents
1 Introduction 3
2 The Bayesian Brain 4
2.1 Probabilities and Beliefs . . . . . . . . . . . . . . . . . . . . . 4
2.2 Perception as Probabilistic Inference . . . . . . . . . . . . . . 5
2.3 Active Inference and the Free Energy Principle . . . . . . . . . 7
2.4 Road Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1
arXiv:2511.20321v1  [cs.AI]  25 Nov 2025
3 Bayesian Machine Learning 10
3.1 Entropy, Cross Entropy and Divergence . . . . . . . . . . . . . 10
3.2 The Mean Field Approximation . . . . . . . . . . . . . . . . . 13
3.3 Variational Free Energy . . . . . . . . . . . . . . . . . . . . . 15
4 Analogies with Statistical Mechanics 16
5 Directed Graphical Models 22
5.1 A Toy Model of Predictive Processing in the Visual Cortex . 22
5.2 Static and Dynamic Bayesian Networks . . . . . . . . . . . . . 25
5.3 Hidden Markov Models . . . . . . . . . . . . . . . . . . . . . . 26
6 The Perception/Action Cycle 27
6.1 Perception and Action as Divergence Minimization . . . . . . 28
6.2 Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
6.3 Time-Domain Renormalization . . . . . . . . . . . . . . . . . 34
7 The Exploration/Exploitation Tradeoff 35
8 Learning 38
8.1 Dirichlet Priors . . . . . . . . . . . . . . . . . . . . . . . . . . 38
8.2 Learning as Divergence Minimization . . . . . . . . . . . . . . 40
9 Whither Expected Free Energy? 42
9.1 Expected Free Energy Functionals . . . . . . . . . . . . . . . . 43
9.2 Beliefs about Policies . . . . . . . . . . . . . . . . . . . . . . . 44
9.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
10 Conclusion 48
References 50
Appendix A Posterior Update Formulas 53
A.1 Predicting the Future . . . . . . . . . . . . . . . . . . . . . . . 53
A.2 Retrodicting the Past . . . . . . . . . . . . . . . . . . . . . . . 54
A.3 Evaluating the Divergence . . . . . . . . . . . . . . . . . . . . 56
2
1 Introduction
The idea that sense perception can be understood as an unconscious pro-
cess of inference dates back to Helmholtz. Among his many other scientific
contributions is the concept of Helmholtz free energy which quantifies the
energy in a system that is available to do physical work such as moving a
piston. Hinton and his collaborators realized that approximate Bayesian in-
ference in machine learning could be viewed as a problem of optimizing an
abstract mathematical quantity which is formally identical to the Helmholtz
free energy and which has come to be known as variational free energy. (Thus
the Helmholtz machine [1] and the Boltzmann machine [2].) In a long series
of papers (notably [3, 4, 5]), Friston has developed the connections between
these ideas and greatly expanded their scope by showing how action as well
as sense perception can be modelled as approximate Bayesian inference.
If perceptions are inferences drawn from bodily sensations, then actions
can be viewed as fulfilling predictions of future sensations. Whereas per-
ceptual inference is thought to consist of computing approximate posterior
probability distributions, active inference is a matter of computing approxi-
mate predictive distributions. Both types of inference can be formulated as
optimization problems which are amenable to the sort of variational inference
algorithms that have been developed in machine learning. In the case of per-
ception, the objective function to be optimized is just variational free energy
and this optimization can be achieved by the well known mean field approx-
imation [6]. In the case of action, a new type of objective function known as
expected free energy has been proposed and new variational methods have
been developed to optimize it [7, 8, 9, 10, 11].
“It is said that the Free Energy Principle is difficult to understand.” Thus
the opening sentence of a review article that purports to simplify the Free
Energy Principle but brooks no compromise on Friston’s program of ground-
ing variational inference by biological agents in statistical physics [12]. For
a physicist studying active inference, it is natural to assume that a biolog-
ical agent models its world as a random dynamical system governed by a
stochastic differential equation and to construe the agent’s striving to main-
tain homeostasis as a pullback attractor. However, in simulating the be-
haviour of biological agents or in designing autonomous AI agents capable of
multistep hierarchical planning, an engineer would assume instead that an
agent models its world as a discrete state space that evolves in discrete time
steps using a Hidden Markov Model or Partially Observable Markov Decision
3
Process. The mathematical apparatus that the engineer needs to deploy is
much simpler than that required by the physicist. In this paper we aim to
give a self-contained and mathematically rigorous account of active inference
in discrete state spaces without appealing to any of the machinery that has
been developed in the context of continuous state spaces.
Although active inference and the Free Energy Principle are usually con-
flated (as in the title of the active inference textbook [13], for example) we
will distinguish between the two because, as we will see, active inference in
discrete state spaces (understood as the problem of inferring predictive prob-
ability distributions for an agent’s future sensations) is amenable to treat-
ment by standard mean field methods which do not appeal to the idea of
expected free energy. We will show how this approach enables us to model
the perception/action cycle in a unified way by optimizing a single Kullback-
Leibler divergence criterion rather than by optimizing variational free energy
to model perception and expected free energy to model action, as is usually
done.
We will briefly review the Bayesian Brain Hypothesis, and basic ideas
in Bayesian machine learning and statistical mechanics before getting down
to business. Readers who have had no previous exposure to the subject of
active inference may find it useful to consult popularizations by Andy Clark
[14], Anil Seth [15] and Mark Solms [16].
2 The Bayesian Brain
In this section, we lay out a roadmap for the paper after briefly reviewing the
Bayesian Brain Hypothesis, active inference and the Free Energy Principle.
2.1 Probabilities and Beliefs
“In the greatest part of our concernments, [God] has afforded us only the
twilight, as I may so say, of probability; suitable, I presume, to that state
of mediocrity and probationership he has been pleased to place us in here;
wherein, to check our over-confidence and presumption, we might, by every
day’s experience, be made sensible of our short-sightedness and liableness to
error.”
Thus John Locke, writing in 1689 [17]. What Locke understood by prob-
ability is unclear and, although the mathematical theory of probability has
4
long been well established, there is still no consensus among statisticians, en-
gineers and physicists about what probabilities mean. Neuroscientists how-
ever have settled on the Bayesian interpretation of probabilities as credences
or degrees of belief. (This explains why the terms “probability distribution”
and “belief” have come to be used interchangeably in the active inference
literature.)
Under this interpretation, probabilities refer to the state of an observer’s
knowledge of events rather than to objective knowledge of the events them-
selves. Physical scientists generally balk at this interpretation for obvious
reasons, but it has the great advantage of formalizing the problem of rea-
soning under uncertainty in a very parsimonious way: all inference, without
exception, reduces to the sum and product rules for combining probabilities.
Provided that numerical values are correctly assigned to beliefs and probabil-
ity calculations are carried out exactly, Bayesian decision theory is provably
optimal.
On the Bayesian Brain Hypothesis, the brain is a repository of beliefs
about how the body and environment work and how they interact with each
other. Bayesian beliefs about events are subject to revision as more informa-
tion becomes available. (More formally: prior probabilities are converted to
posterior probabilities by applying Bayes’ Rule.) As the brain collects sense
data from the body in a given situation, it infers the causes of its sensations
— in other words, what is going on in its world — by approximate Bayesian
inference which converts its prior beliefs (about how the world works in gen-
eral) into posterior beliefs (about what the world is doing right now). These
posterior beliefs are what we think of as reality.
2.2 Perception as Probabilistic Inference
As a first step towards a general formulation of the Bayesian Brain Hypoth-
esis, consider that the primary function of the brain is to regulate the body
and to act on the environment in such a way as to maintain the body in a
state of homeostasis. All of the brain’s knowledge of what is going on in its
world (which includes the body as well as the environment) at a given time is
mediated by incoming sensory data. We can model this state of affairs with a
steady state joint probability distributionp(s,o|S) wheresis a vector whose
components represent the state of the organism’s world andois a vector com-
prised of all of the incoming sensory data (interoceptive and proprioceptive
as well as exteroceptive). The state vectorscan be thought of as compris-
5
ing the causes of the observation vectoro. The marginal distributionp(s|S)
encodes the brain’s prior knowledge of its world. The state of the world at
a given time is hidden in the sense that it cannot be directly observed but
has to be inferred from incoming sense data. From this perspective, sense
perception is just this process of inference.
The terminology “observation” should not be read as suggesting that
sense data are generally accessible to consciousness. We are normally not
consciously aware of most of our visceral and proprioceptive sensations, just
as we are unaware of the sensations of sound pressure waves impinging on our
ears (except in the case of loud noises) or of light waves impinging on our eyes
(except in the case of sudden flashes). Consciousness appears mysterious to
us because, along with many of the sensations themselves, the process which
converts sensations to perceptions is hidden from us.
For a given observation vectoro, the marginal probabilityp(o|S) is known
as themodel evidenceand its negative logarithm,−lnp(o|S), as thesurprisal.
(The less probable an event, the more surprising it is.) In principle the
evidence or surprisal could be evaluated by summing over all possible states
of the world:
p(o|S) =
X
s
p(s,o|S)
but this summation is obviously intractable. However if the evidence could be
evaluated, the problem of inferring the state of the world from sensory data
could be solved by a direct calculation of the posterior probabilityp(s|o, S):
p(s|o, S) =p(s,o|S)
p(o|S) .(2.1)
Approximations cannot be avoided and, given the scale of the problem,
it is reasonable to assume that the brain uses variational approximations like
those that have been developed in machine learning rather than Monte Carlo
methods. So we postulate that the brain calculates some sort of mean field
approximation to the posterior distributionp(s|o, S). As a byproduct, this
calculation gives an approximation to the model evidence which is known
in machine learning as the evidence lower bound or ELBO.Variational free
energyis defined by changing the sign on the ELBO, so that one talks about
minimizing the variational free energy rather than maximizing the ELBO
and minimizing the surprisal rather than maximizing the evidence.
On this account, variational free energy is constantly being minimized as
the approximate posterior distribution is updated from one moment to the
6
next. It is proposed in [18] that qualia can be understood as transductions
of bodily sensations which are encoded in the brain by the parameters which
specify this approximate posterior distribution in much the same way as
photographs are encoded in jpeg files. (By a happy coincidence, variational
posterior distributions in machine learning are traditionally denoted byq
rather thanpwhich is reserved for priors and exact posteriors.)
Note that approximating the full posterior distributionp(s|o, S) (rather
than merely computing a point estimate ofs) enables the brain to assess
the uncertainty attaching to its explanation of the sense datao. This is
important because an agent cannot act effectively on its environment if it is
uncertain about the state of its world at a given time or in its immediate
future. In such a situation, gathering information with a view to reducing
uncertainty is a primary imperative.
2.3 Active Inference and the Free Energy Principle
In order to extend the Bayesian Brain Hypothesis to account for action as
well as perception, Friston observes that biological agents always seem to
act in ways that can be interpreted as striving to maximize the evidence
for their model of the world or, since the model evidence is intractable, to
minimize variational free energy [19]. This behaviour is often referred to as
“self-evidencing” in the active inference literature. Thus the agent is said to
act on its world in such a way as to maximize the evidence for its continued
existence or to minimize its surprise at what happens to it.
To this end, a biological agent needs to be able to predict its future
sensations in much the same way as large language models predict text.
Whereas the predictions made by large language models are driven by a
past history and the statistics of natural languages, the predictions made
by a biological agent are driven by its history up to the present moment
and the requirement that future sensations are drawn from the steady state
marginal distributionp(o|S). Action is then construed as fulfilling these
predictions. More specifically, actions ultimately reduce to autonomic and
motor reflexes (that is, secreting hormones and contracting muscles) which
fulfill predictions of interoceptive and proprioceptive sensations [20]. This
move brings action under the umbrella of the Bayesian Brain Hypothesis
without invoking extrinsic ideas about goals, rewards or utility functions.
Action is governed by prediction, that is, by probabilistic inference of the
future states of the world. Hence the termactive inference.
7
Predictions in exteroceptive sense modalities such as seeing and hearing
over which the agent has limited control may be less precise, but navigating
an environment obviously requires the ability to anticipate events and not
merely to react to events after the fact. Indeed, it is difficult to see how
a biological agent could learn to act effectively on the world it finds itself
inhabiting other than by learning to predict the sensory consequences of its
behaviour in much the same way as large language models learn the statistics
of natural languages by learning to predict text one word at a time.
Predicting the future requires a probability model which is capable of
tracking trajectories in the state space over time. (The steady state distribu-
tionp(s,o|S) that we have been considering is not adequate in this respect
since it treats states at successive times as being statistically independent.)
The obvious choice is to model state trajectories as a Markov chain by as-
signing to each state a probability distribution over states that can be visited
next. This type of model is usually referred to as a Hidden Markov Model
(HMM) rather than a Markov chain since the state occupied at a given time
cannot be directly observed but has to be inferred probabilistically. If the
states were observable, trajectories in the state space would appear to be
directed towards regions that have high probability under the steady state
marginal distributionp(s|S), subject to perturbations due to random shocks
coming from the agent’s environment.
A HMM is a joint distribution on sequences of states and observations,
rather than on individual states and observations as in the case of the steady
state distribution. Under this sort of dynamic model, perception is under-
stood to be a matter of recognizing patterns that unfold over time (rather
than static objects) and variational free energies are associated with se-
quences of observations (rather than with individual observations). So an
agent contemplating an action could assign a variational free energy to it if
it could foretell how the sensory consequences of the action would unfold.
Of course the agent cannot foresee the future but, for each action that is
possible in a given situation, it could calculate an expected value for the
variational free energy of the sense data it would encounter if the action was
performed. This would enable the agent to assign approximate probabilities
to the various possible actions that are open it at a given time and use these
probabilities in inferring a predictive distribution for its future sensations
by variational methods. TheFree Energy Principleasserts that biological
agents “decide” on courses of action in this way. (On Friston’s account [21],
the ability to look ahead in performing this sort of variational inference is a
8
hallmark of conscious behaviour.)
On the other hand, we will show in this paper that the problem of active
inference, understood as the problem of inferring a probability distribution
for an agent’s future sensations, can be solved by standard mean field meth-
ods that do not appeal to the idea of expected free energy.
2.4 Road Map
The Free Energy Principle suggests how an objective function known as ex-
pected free energy might be defined which would enable active inference to
be cast as an optimization problem in the same way as perceptual inference.
But, because it purports to be a general principle like Hamilton’s principle
of stationary action, the Free Energy Principle does not commit to a pre-
cise definition of this objective function. In this paper, we will proceed from
the assumption that, for an agent that uses a HMM to model its world, the
Kullback-Leibler divergence of a predictive distribution from the HMM is
a natural measure of the quality of the predictive distribution. So rather
than appealing to the Free Energy Principle, we posit that a (suitably con-
strained) Kullback-Leibler divergence can serve as the objective function for
active inference and we show how it can be optimized by standard mean field
methods.
The mean field approximation is generally used to infer approximate pos-
terior distributions of hidden variables after data has been collected. Thus,
given a HMMp(s,o) and a sequence of observations up to timetdenoted
byo ≤t, minimizing variational free energy provides an approximation to the
posterior distributionp(s ≤t|o≤t) wheres ≤t is the hidden state sequence that
accounts foro ≤t. (The lower bound on the model evidencep(o ≤t) known as
the ELBO is a byproduct of this calculation.) We will explain the mean field
approximation in Sections 3 and 4. We will show how it can be used to pro-
vide an approximation to the predictive distributionp(s>t,o >t|o≤t) on future
states and observations in addition to the posterior distributionp(s ≤t|o≤t) in
Section 6. The core idea is very simple: it consists in treating future states
and observationss >t ando >t as hidden variables on the same footing ass ≤t
and calculating a variational posterior (conditioned on the observationso ≤t)
for this augmented set of hidden variables. This perspective enables us to
treat the perception/action cycle in a unified way. We model it by minimiz-
ing a single Kullback-Leibler divergence functional rather than minimizing
variational free energy in the case of perception and expected free energy in
9
the case of action, as is usually done.
It is shown in [22] how a wide variety of reinforcement learning algo-
rithms can be formulated as solving constrained divergence minimization
problems and the exploration/exploitation tradeoff in reinforcement learning
is analyzed in this general setting. We discuss how this applies to HMMs in
Section 7.
Section 8 shows how mean field methods apply to the problem of learning
HMM parameters. Although this material is not new, we include it for
completeness.
We discuss the relationship between our perception/action divergence cri-
terion and various expected free energy functionals in Section 9. We explain
the (implicit or explicit) approximations that are used to derive these ex-
pected free energy functionals and how the divergence criterion avoids them.
It turns out that, when it is used to calculate predictive distributions, the
perception/action divergence criterion differs from a free energy functional
by an entropy regularizer, although the functional in question is not one that
has been used in the active inference literature.
3 Bayesian Machine Learning
This section explains the mean field approximation and the concept of varia-
tional free energy. We begin with an overview of the rudiments of information
theory and Bayesian machine learning.
3.1 Entropy, Cross Entropy and Divergence
We use the notationp(x) to refer to a generic probability distribution of a vec-
tor valued random variablexandq(x) to refer to an approximate probability
distribution. Typically, the distributionp(x) is computationally intractable
andq(x) is constrained to belong to a family of tractable distributions. For
the most part, we take the values ofxto be discrete.
In situations where we need to distinguish between a random variablex
and a value it takes, we denote such a value by ¯x. For example, we use the
notationδ ¯x(x) to refer to the probability distribution all of whose mass is
concentrated on the value ¯x:
δ¯x(x) =
(
1 ifx= ¯x
0 otherwise.
10
Theinformation contentorsurprisalof the event that the random variable
takes the value ¯xis defined to be the negative log probability,−lnp( ¯x).
(The more unlikely an event, the greater the surprise at its occurrence.) The
entropyofp(x) and thecross entropyofp(x) relative toq(x) are defined by
averaging the information with respect top(x) andq(x) respectively:
Ep(x) [−lnp(x)] =−
X
x
p(x) lnp(x)
Eq(x) [−lnp(x)] =−
X
x
q(x) lnp(x).
We denote the entropy ofp(x) by H [p(x)]. This has the property that
0≤H [p(x)]≤lnK
whereKis the number of values thatxcan take. The minimum value of 0 is
attained by point mass distributions and the maximum value by the uniform
distribution which assigns equal probabilities to all values ofx. So H [p(x)]
can be thought of as a measure of how flat the distributionp(x) is.
The Kullback-Leibler divergence ofq(x) fromp(x) is the difference be-
tween the cross entropy and the entropy:
KL [q(x)∥p(x)] =E q(x) [−lnp(x)]−H [q(x)] (3.1)
=
X
x
q(x) ln q(x)
p(x).
It has the property that KL [q(x)∥p(x)]≥0 with equality holding iffq(x)
andp(x) are identical. Note that the divergence is not symmetric in its
arguments (for more on this see [23]).
Either theforward divergenceKL [p(x)∥q(x)] (the “divergence ofp(x)
fromq(x)”) or thereverse divergenceKL [q(x)∥p(x)] (the “divergence ofq(x)
fromp(x)”) can be used to evaluate how well the target distributionp(x) is
approximated byq(x). We will mostly use reverse divergences in this paper
as we will usually be dealing with situations where the target distribution
p(x) is computationally intractable and the approximating distributionq(x)
is tractable so that the reverse divergence can be calculated and optimized
whereas the forward divergence cannot.
A situation where it is natural to use the forward divergence as the opti-
mization criterion arises when the target distributionp(x) is given empirically
11
in the form of a training data set. Since the entropy H [p(x)] is independent
ofq(x), minimizing the forward divergence KL [p(x)∥q(x)] is equivalent to
minimizing the cross entropyE p(x) [−lnq(x)]. This is generally referred to as
the cross-entropy loss function in non-Bayesian machine learning and mini-
mizing it is equivalent to maximum likelihood estimation ofq(x) [23]. It is
well known that maximum likelihood estimation is vulnerable to overfitting.
On the other hand, when the reverse divergence is used as the optimization
criterion, the the entropy H [q(x)] which appears with a negative sign in (3.1)
mitigates against this tendency by penalizing approximating distributions of
low entropy. It is often referred to as an “entropy regularizer” for this reason.
Reverse divergences arise naturally when posterior distributions need to
be approximated. Suppose we seek to minimize the divergence KL [q(x)∥p(x)]
subject to the constraint thatq(x) = 0 outside of a setAso that the opti-
mand is X
x∈A
q(x) ln q(x)
p(x).
Define a probability distributionp ′(x) by
p′(x) =
(
1
p(A) p(x) (x∈A)
0 (x/∈A)
(3.2)
so that
X
x∈A
q(x) ln q(x)
p(x) =
X
x∈A
q(x) ln q(x)
p′(x) −lnp(A).
The first term on the right-hand side is just the divergence ofq(x) fromp ′(x)
and we can ignore the second term since it is independent ofq(x). If no
additional constraints are imposed onq(x), then the divergence assumes the
minimum value of 0 whenq(x) =p ′(x). Since the expression in the first line
of (3.2) is just the conditional distribution ofxgiven thatx∈A, what this
calculation shows is that Bayes rule can be derived by solving a constrained
divergence minimization problem. All of the calculations that we carry out
in this paper will follow this pattern.
We will frequently need to calculate the divergence of one joint distribu-
tion from another. For this we will use the following “chain rule”. Given two
joint distributionsq(x,y) andp(x,y), the divergence KL [q(x,y)∥p(x,y)]
12
can be written in either of the forms
KL [q(x)∥p(x)] +E q(x) [KL [q(y|x)∥p(y|x)]]
KL [q(y)∥p(y)] +E q(y) [KL [q(x|y)∥p(x|y)]] (3.3)
This is a straightforward consequence of the definition of the divergence.
3.2 The Mean Field Approximation
We can associate anenergy functionE(x) with a probability distribution
p(x) by settingE(x) =−lnp(x) so thatp(x) = exp (−E(x)).(In physics,
low energy states are more probable than high energy states. Hence the
negative sign.) Conversely, given an energy functionE(x), we can define a
probability distributionp(x) by setting
p(x) = 1
Z e−E(x)
where thepartition functionZis determined by the requirement that prob-
abilities sum to 1.
A well known example is the multivariate Gaussian distribution which is
defined by an energy function of the formx ⊤Σ−1x. In a situation where the
state space is of high dimension and sparsity constraints are imposed on the
precision matrixΣ −1 this distribution is referred to as a Gaussian Markov
Random Field. Ifxis constrained to be binary valued, the corresponding
distribution is known as the Ising model in statistical physics and as the
Boltzmann machine in machine learning. All of these distributions are in-
tractable in the case of high dimensional state spaces in the sense that the
partition function cannot be evaluated.
Regardless of whetherp(x) is tractable or not, we can seek to approximate
it by a tractable distributionq(x) using the reverse divergence KL [q(x)∥p(x)]
as the optimization criterion. In the mean field approximation,xis decom-
posed into subvectorsx 0, . . . ,xN and statistical independence constraints are
imposed so that the approximating distributionq(x) factorizes as
q(x0)q(x1). . . q(xN ).(3.4)
The procedure that we will present for calculatingq(x) (which is referred
to as coordinate ascent variational inference in [6]), requires that the factors
13
are updatedasynchronously(that is, one at a time rather than in parallel).
The divergence KL [q(x)∥p(x)] is guaranteed to decrease on each update
but, although the procedure is guaranteed to converge, it may converge to a
local rather than a global minimum of the divergence. So it may need to be
initialized carefully.
The rule for updating a factorq(x n) holding the remaining factors fixed
is very simple:q(x n) is the distribution defined by the energy function
Eq(x\n) [E(x)] wherex \n denotes the set of components ofxother thanx n. To
see that this update is guaranteed to decrease the divergence KL [q(x)∥p(x)]
let us write the divergence in the form
Eq(xn)
h
Eq(x\n) [lnq(x)−lnp(x)]
i
.
We need to minimize this with respect toq(x n). Using the notation +. . .to
indicate terms which are independent ofx n and hence irrelevant,
lnq(x) = lnq(x n) +. . . ,
so minimizing the divergence is equivalent to minimizing
Eq(xn) [lnq(x n)−ln ˜q(xn)] (3.5)
where ˜q(xn) is defined by
ln ˜q(xn) =E q(x\n) [lnp(x)].
The expression (3.5) has the form of a KL divergence so if ˜q(x n) were a
probability distribution it would be minimized by settingq(x n) = ˜q(xn). If
˜q(xn) is rescaled to ensure that probabilities sum to 1, (3.5) differs from a
true divergence by an irrelevant additive constant. So we can summarize the
update formula forq(x n) as
q(xn)∝˜q(xn) where ln ˜q(xn) =E q(x\n) [lnp(x)] (3.6)
Implementing these update formulas does not require that the divergence
that is being minimized ever needs to be evaluated. However, the fact that the
divergence is guaranteed to decrease whenever one of the factors is updated
is very useful for debugging.
In machine learning, the terms “mean field approximation” and “varia-
tional inference” are generally used interchangeably. As in the calculus of
14
variations, the mean field updates (3.6) are derived without imposing any
conditions the functional forms of the factors in (3.4) even in the case of con-
tinuous distributions. Hence the epithet “variational”. The term “inference”
generally refers to calculating exact or approximate posterior distributions
of hidden variables in a probability model. (We will see how the mean field
approximation accomplishes this in the next section.) However, as we have
presented it, the mean field approximation can be applied to the problem of
calculating a factorized approximationq(x) toanytarget distributionp(x)
provided that this problem is formulated as one of minimizing the reverse di-
vergenceKL [q(x)∥p(x)]. We will see in Section 6 how variational methods
can be brought to bear on active inference as well as perceptual inference by
casting the problem of inferring predictive distributions as one of optimizing
a (suitably constrained) KL divergence rather than an expected free energy
functional.
3.3 Variational Free Energy
Suppose now that some of the components ofxare observable but others
are not. In keeping with the notation that we will use later on, we set
x= (s,o) whereostands for the observable variables andsfor the hidden
variables. Here we explain how variational inference can be used to derive
an approximation to the posterior distributionp(s|o) in situations where the
exact posterior is intractable.
Suppose thatois observed to take the value ¯o. As we saw in Section 3.1,
the exact posteriorp(s|o) can be viewed as the solution to the problem of
minimizing the divergence KL [q(s,o)∥p(s,o)] subject to the constraint that
q(s,o) = 0 unlesso= ¯oor, equivalently, thatq(s,o) can be written in the
form
q(s,o) =q(s)δ ¯o(o).
Ifq(s) is required to factorize as in (3.4), then the global minimum of the
divergence may not be attainable but the mean field algorithm returns an
approximation to the exact posterior whose quality can be measured by the
numerical value of the divergence. This numerical value is known as the
variational free energy(VFE):
VFE = KL [q(s,o)∥p(s,o)] whereq(s,o) =q(s)δ ¯o(o). (3.7)
This is the most convenient way of defining variational free energy for our
15
purposes but we can use the chain rule (3.3) to express it in more familiar
ways. The divergence KL [q(s,o)∥p(s,o)] can be written in either of the
forms
KL [q(s)∥p(s)] +E q(s) [KL [q(o|s)∥p(o|s)]]
or
KL [q(o)∥p(o)] +E q(o) [KL [q(s|o)∥p(s|o)]]
which simplify to
KL [q(s)∥p(s)] +E q(s) [−lnp( ¯o|s)] (3.8)
and
KL [q(s)∥p(s| ¯o)]−lnp( ¯o) (3.9)
under the assumption thatq(s,o) =q(s)δ ¯o(o). In the active inference litera-
ture, the first term in (3.8) is known as thecomplexityas it can be interpreted
as penalizing distributionsq(s) whose divergence from the priorp(s) is large.
The second term is referred to as theaccuracysince it can be interpreted as
the average error incurred in reconstructing ¯oby sampling from the distri-
butionq(s).
Since the first term in (3.9) is non-negative, the variational free energy
bounds the surprise−lnp( ¯o) from above. So choosingq(s) to minimize the
variational free energy can serve as a proxy for minimizing the surprise in
situations where the marginal distributionp(o) is intractable.
4 Analogies with Statistical Mechanics
Particularly in his early writings on the Free Energy Principle, Friston fre-
quently conflates variational free energy and thermodynamic free energy, the
energy in a system which is available to perform physical work such as mov-
ing a piston. This move has confused many of his readers and it is not needed
to understand active inference, but the analogies with statistical mechanics
are so fruitful that it is worth making an effort to understand how far they
can be pushed. The ideas presented in this section are speculative and they
will not be needed later so readers who are not interested in this topic are
invited to skip it.
16
A physicist using statistical mechanical methods to study the thermody-
namics of a neural population or of the brain as a whole would aim to write
down an energy functionE(x) which would, at least in principle, enable her
to calculate the energy of the system given its microstatex. This energy
function would have the property that the probability distributionp(x) of
microstates when the system is in thermal equilibrium with its environment
is a Boltzmann distribution. That is,
p(x) = 1
Z(β) e−βE(x) (4.1)
where the parameterβis related to the temperatureTof the system by
β= 1
kBT
(kB is Boltzmann’s constant). Takingβ= 1, the free energy of the system
is the total energy minus the entropy. The total energy is calculated by
averaging over microstates and the entropy is the Shannon entropy ofp(x).
So the free energy F [p(x)] is given by the expression
Ep(x) [E(x)]−H [p(x)]
which reduces to−lnZ. If the neural population is maintained in a non-
equilibrium steady state by sensory stimulation from the outside world and
q(x) is the distribution of microstates under this condition, then the non-
equilibrium free energy F [q(x)] is given by
Eq(x) [E(x)]−H [q(x)]
which can be written as
KL [q(x)∥p(x)]−lnZ.
So the equilibrium and non-equilibrium free energies are related by
F [q(x)] = KL [q(x)∥p(x)] + F [p(x)].
Thus the divergence KL [q(x)∥p(x)] can be interpreted as the additional
thermodynamic free energy that is dissipated as heat to the environment as
the system relaxes towards equilibrium. Regarded in this way, the imperative
17
to minimize the divergence fromq(x) top(x) is mandated by the second law
of thermodynamics.
We have assumed that the non-equilibrium steady state is the result of
external simulation of sensory neurons so that some of the components of
xare fixed. We can restate this assumption in the notation introduced in
Section 3.3:x= (s,o) andotakes the value ¯o, so that
KL [q(x)∥p(x)] = KL [q(s,o)∥p(s,o)]
and
q(s,o) =q(s)δ ¯o(o).
So by (3.7), the divergence KL [q(x)∥p(x)] can be interpreted as a variational
free energy as well as a thermodynamic free energy.
The Boltzmann distribution arises as the solution of a constrained op-
timization problem, namely the problem of finding the maximum entropy
distribution on microstates for which the expected value ofE(x) coincides
with a given observed value. (The parameterβarises as a Lagrange mul-
tiplier.) More generally, given similar constraints on a collection of energy
functions{E (c)(x)|c= 1, . . . , C}the maximum entropy distribution is a gen-
eralized Boltzmann distribution of the form
p(x) = 1
Z(β) exp
 
−
CX
c=1
β(c)E(c)(x)
!
(4.2)
whereβ= (β (1), . . . , β(C)). This way of assigning probability distributions
is known as the maximum entropy principle [24, 25]. Since maximizing the
entropy of a distribution is equivalent to minimizing its divergence from a
uniform distribution, the maximum entropy principle can be regarded as
another instance of constrained divergence minimization.
Considering that all of the probability distributions in statistical mechan-
ics have the form of generalized Boltzmann distributions, the question arises:
Can the brain as a whole be modelled in this way? For this we would need
to define a collection of energy functions which are localized in the sense
that each of them of is a function of the microstates of a (relatively small)
neuronal population. For the sake of argument, let us imagine representing
the brain by a sparsely connected graph as in Fig. 1. The nodes represent
neural populations (grey matter) and the branches represent communication
18
channels between populations (white matter). The variablesx 0,x 1, . . .asso-
ciated with the nodes are the microstates of the corresponding populations.
We could associate energy functionsE (0)(x0), E(1)(x1), . . .with each of the
nodes and define a probability distribution by invoking (4.2) but this would
result in a distribution under which the variablesx 0,x 1, . . .are statistically
independent which is obviously inadequate. In the case of a Gaussian Markov
random field, this could be corrected by adding off-diagonal terms to the pre-
cision matrix. Analogously, we could model the dependencies between neural
populations by adding energy functions depending on two sets of variables
for each branch in the graph. For example, the branch joining node 0 and
node 3 would contribute an energy functionE (03)(x0,x 3) whose role is to
model the dependency betweenx 0 andx 3.
x0
x1
x2
x3
x4
x5
Figure 1:A toy model of the brain as a Markov Random Field. Nodes cor-
respond to grey matter and branches to white matter. Energy functions are
associated with both nodes and branches. Energy functions associated with
the nodes model the microstates of neural populations. Energy functions as-
sociated with the branches model the dependencies between the microstates of
the neural populations associated with the nodes. One or more of the nodes
receives sensory data from the external world which changes from moment
to moment. The neural populations are continually striving to dissipate free
energy by exciting or inhibiting the populations with which they are in com-
munication via the branches.
We assume that one of nodes in the graph is distinguished from the others
by being in communication with the world outside the brain (which includes
the body as well as the environment). The neural population corresponding
to this node consists of sensory neurons and the microstate of this popula-
tion at a given time depends on what is going on in the world as well as the
19
microstates of the populations with it which communicates. In this picture,
the “brain” tracks changes in the body and environment by continually up-
dating approximate variational posterior distributions for each of the neural
populations.
An attractive feature of this model is that it suggests that a similar picture
obtains at finer scales of resolution. Implementing the mean field update
formula 3.6 for each of the nodes in the graph Fig. 1 requires reading off the
variational posteriors associated with the nodes with which it communicates
and only those nodes. For example, the variational posteriorq(x 0) is updated
by calculating the energy functionE q(x\0) [E(x)] which is given (up to an
irrelevant additive constant) by
E(0)(x0) +E q(x1)

E(01)(x0,x 1)

+E q(x3)

E(03)(x0,x 3)

.
So nodes 1 and 3 stand in the same relationship to node 0 as the sensory neu-
rons to the “brain” as a whole. Thus if the neural population corresponding
to node 0 admits of a graphical representation similar to Fig. 1, then the
mechanics of updating the local variational posteriorq(x 0) will be similar to
that of updating the global variational posteriorq(x 0,x 1, . . .). The same sort
of analysis may be applicable to neural subpopulations at different scales,
perhaps even down to the scale of individual neurons.
There are no restrictions on the topology of the graph in Fig. 1. Vari-
ational free energy is continually being minimized by passing messages be-
tween communicating neural populations but this does not require that the
variational free energy of the brain as a whole be evaluated from moment
to moment. Hence there is no need for a “central processing unit” or neo-
Cartesian “inner screen” [26] in the picture we have sketched. All that is
going on is that each neural population and subpopulation is continually
striving to dissipate its own free energy by exciting and/or inhibiting the
activity of the other populations with which it communicates.
The Lagrange multipliers in (4.1) and (4.2) have an interesting role to
play. Decreasingβ(that is to say, increasing the temperature) in (4.1) has
the effect of increasing the entropy of the distributionp(x) which quanti-
fies the uncertainty about the value thatxactually takes. This operation
is usually referred to asprecision weightingsince the effect of applying it
to a Gaussian distribution is to leave the mean unchanged but scale the
precision (or inverse variance) by a factor ofβ. Similarly, changing the mul-
tipliers associated with the branches in the graph in Fig. 1 has the effect
20
of modulating the strength of the interactions between neural populations.
Precision weighting is thought to be the mechanism underlying attention [27]
and arousal modulation [16] and the computational neuroscience literature
abounds in (more or less plausible) just-so stories which use this mechanism
to explain neurological anomalies such as aphantasia, akinesia and apathy in
dementia, and the sensory effects of psychedelics.
An agent cannot act effectively on its environment if it is uncertain about
the current state of its world or about the accuracy of its predictions of future
states. In such a situation actions have to be guided by the imperative to
reduce uncertainty. (We will discuss the exploration/exploitation tradeoff in
Section 7.) So an intelligent agent needs to assess the degree of confidence
it has in its inferences and predictions before it acts in a given situation.
Treating the Lagrange multipliers as random variables subject to probabilis-
tic inference would endow the agent with the flexibility needed to do this
well. This topic has hardly begun to be explored in the active inference lit-
erature ([28] is an exception but the “universal generative model” of [29, 30]
does not include precision weighting). So we will not dwell on it here except
to point out that there doesn’t seem to be any major obstacles in the way of
a fully probabilistic treatment should that prove to be useful.
For example, if we assume a joint distribution on microstates and multi-
pliers of the form
p(β,x)∝exp
 
−
CX
c=1
β(c)E(c)(x)
!
and a variational approximation of the form
q(β,x) =
Y
c
q(β(c))
Y
n
q(xn)
the factorsq(β (c)) turn out to be exponential distributions. The assumption
here that theβ (c)’s are statistically independent in the approximating distri-
bution is arguably unsatisfactory if the intention is to model attention since
attention is usually thought of as a limited resource which is concentrated
on one object at a time. This objection might be answered by assuming a
joint distribution of the form
p(µ,x)∝
Y
c
 
µ(c)E(c)(x)
21
where theµ (c)’s are non-negative and sum to 1. Under these conditions, a
variational approximation of the form
q(µ,x) =q(µ)
Y
n
q(xn)
yields a Dirichlet distribution forq(µ). (The Dirichlet distribution is dis-
cussed in Section 8.1.)
5 Directed Graphical Models
Energy based models lend themselves naturally to variational approximation
methods but they are difficult to train in situations where the partition func-
tion cannot be evaluated and this is generally the case if the state space is
of high dimension. For this reason, probability distributions are not usually
specified by energy functions in machine learning but by directed graphical
models, also known as Bayesian networks.
5.1 A Toy Model of Predictive Processing in the Visual
Cortex
For example, the graph in Fig. 2 represents a toy model of hierarchical pre-
dictive processing in the visual cortex [31]. The variablex N represents the
state of the retina, the variablex N−1 represents the proximate causes ofx N
and the directed arrow joining the two indicates that the causal relationship
is specified by a conditional probability distribution of the formp(x N |xN−1 ),
and so on up the hierarchy. So if we setx= (x 0, . . . ,xN ), the assump-
tion is that the joint distributionp(x) factorizes as a product of conditional
probability distributions
p(x) =p(x 0)p(x1|x0)···p(x N |xN−1 ).(5.1)
As a warm-up exercise for the calculations that we will perform in Section 6
and the appendix, consider the problem of approximatingp(x) by a factorized
distribution of the form
q(x) =q(x 0)q(x1)···q(x N ) (5.2)
by variational methods. In the case wherex N is not observed (panel (a) of
Fig. 2), we update all of the factors one at time so that, on convergence,
22
q(xN ) is an approximation to the marginal distributionp(x N ). In the case
wherex N is observed (panel (b) of Fig. 2) we impose the additional constraint
that the distributionq(x N ) is concentrated on the observed value ¯xN . So
q(xN ) is never updated and, on convergence,q(x) is an approximation to the
posterior distributionp(x|x N = ¯xN ).
x0 x1 ··· xN−1 xN
(a)x N is not observed
x0 x1 ··· xN−1 xN
(b)x N is observed
Figure 2:A toy model for predictive processing in the visual cortex. Hatching
indicates variables whose values are not observed.
If we assume that forn= 0, . . . , N,xn can takeKpossible values, then
we can representx n as a 1-hotK×1 vector and write
p(x0) =x ⊤
0 p0
q(xn) =x ⊤
n qn
p(xn|xn−1) =x ⊤
n−1An xn
wherep 0 andq n areK×1 vectors andA n is aK×Kmatrix. Thus
lnp(x) =x ⊤
0 lnp 0 +
NX
n=1
x⊤
n−1 lnA n xn
where the logarithms of the vectors and matrices on the right-hand side are
taken element wise.
Recall that by (3.6), the recipe for updating one of the factorsq(x n) in
(5.2) holding the others fixed calls for evaluating ˜q(xn) defined by
ln ˜q(xn) =E q(x\n) [lnp(x)].
23
This gives
ln ˜q(xn) =



x⊤
0 (lnp 0 + lnA1 q1) +. . .(n= 0)
x⊤
n
 
lnA ⊤
n qn−1 + lnAn+1 qn+1

+. . .(0< n < N)
x⊤
N lnA ⊤
N−1 qN +. . .(n=N)
where the ellipses indicate terms which are independent ofx n and hence can
be ignored. 1 (We have used the fact that the scalarq ⊤
n−1 lnA n xn can be
rewritten asx ⊤
n lnA ⊤
n qn−1.) So the update formula forq n is
lnq n =



lnp 0 + lnA1q1 +. . .(n= 0)
lnA ⊤
n qn−1 + lnAn+1qn+1 +. . .(0< n < N)
lnA ⊤
N−1 qN +. . .(n=N)
where the ellipses indicate constants whose role is to ensure that the compo-
nents ofq n sum to 1. In other words, for eachn= 0, . . . , N, the vectorqn is
derived from the corresponding vector on the right-hand side by passing it
through the softmax function. In the case werex N is observed, the update
formula is not applied in the casen=N, andq N is set to be the 1-hot vector
corresponding to the observed value ofx N .
To evaluate the divergence KL [q(x)∥p(x)], note that
Eq(x) [lnp(x)] =q ⊤
0 lnp 0 +
NX
n=1
q⊤
n−1 lnA n qn
and
Eq(x) [lnq(x)] =
NX
n=0
qn lnq n
so that
KL [q(x)∥p(x)] =
NX
n=0
q⊤
n lnq n −q ⊤
0 lnp 0 −
NX
n=1
q⊤
n−1 lnA n qn.
1We use the convention +. . .to indicate irrelevant additive terms throughout.
24
5.2 Static and Dynamic Bayesian Networks
A tree structure as in Fig. 3, rather than the linear structure in Fig. 2,
enables our toy predictive processing model to be extended to embrace mul-
tiple sensory modalities. Associated with each nodenin the tree there is a
variablex n, and a conditional probability distributionp(x n′|xn) is specified
for each branchn→n ′. Think of the leaf nodes of the tree as the various
sense gates and of the variables associated with nodes at higher levels as
progressively more abstract representations of the sensorium.
Variational message passing in a tree works in much the same way as in
the linear graph Fig. 3: updating the variational posterior associated with
a given node depends on messages received from its parent and its children
(rather than from its successor and predecessor as in the case of Fig. 2) but
not from any other nodes.
Oddly enough, the term “predictive processing” in the neuroscience lit-
erature is primarily used to refer to prediction from top to bottom in a
hierarchical network such as Figs. 2 and 3 rather than prediction from past
to future which is evidently just as important. Fig. 4 illustrates a general
procedure for converting a given Bayesian Network to aDynamicBayesian
Network which enables temporal as well as top down prediction.
The construction consists in replacing a node in the original network rep-
resenting a random variablex n by a chain of nodes representing instances
ofx n at successive times. This entails replacing the conditional distribution
p(xn′|xn) associated with a given branch in the original graph by a con-
ditional distribution of the formp(x n′,t|xn,t,x n′,t−1) for each timet. This
probability distribution is usually taken to be independent oft, although we
will encounter situations where time-dependent transition probabilities are
needed.
An important point to note is that this construction can be implemented
with slower clock speeds at higher levels of the hierarchy so that the various
levels can be thought of as representing the sensorium on different temporal
scales as well as on different spatial scales. Philosophy of mind has tradi-
tionally been stumped by the binding problem in sense perception but this
schema shows how the Bayesian Brain Hypothesis deals with it straightfor-
wardly.
25
Figure 3:A toy model for simultaneous predictive processing in multiple sen-
sory modalities. The leaf nodes of the tree correspond to sensors.
5.3 Hidden Markov Models
Henceforth, we will use the notationsto indicate asequenceof hidden vari-
ables or “states”s 0,s 1, . . . ,sT in a Dynamic Bayesian Network,oto indicate
a sequence of observable variableso 1, . . . ,oT and ¯o1, . . . ,¯oT to indicate a
sequence of observed values. For each timet= 1, . . . , T−1,s>t denotes the
sequences t+1, . . . ,sT and so forth.
AHidden Markov Model(HMM) is a joint probability distributionp(s,o)
on state sequencessand observation sequencesoof the form
p(s,o) =p(s 0)
TY
t=1
p(st|st−1)p(ot|st).(5.3)
So the sequence of states is a Markov chain and, at each timet,o t is pre-
dicted froms t alone rather than from the pair (s t,o t−1) (as would be the
case in more general Dynamic Bayesian Networks). The intuition here is
that sensory data (as distinct from, say, the text data which large language
models have to predict) is so noisy that useful predictions are only possible
in the space of hidden variables (or embeddings, to use the language of ar-
tificial intelligence [32, 33]). In a good probability model, the state space
will be of lower dimension than the observation space so that inferring states
26
xn
(a)Static Network.
xn,t−1 xn,t xn,t+1··· ···
(b)Dynamic Network.
Figure 4:Constructing a Dynamic Bayesian Network from a static network.
The static network supports prediction from top to bottom (as in figs. 2 and
3). The dynamic network supports prediction from past to future as well.
from observations can be thought of as a denoising operation which can be
expected to enhance the accuracy of perceptions and predictions.
6 The Perception/Action Cycle
Recall the basic tenets of the Bayesian Brain Hypothesis: A biological agent
infers the state of its limbs and viscera at a given time from its propriocep-
tive and interoceptive sensations and the state of its environment from its
exteroceptive sensations. This is known as perceptual inference. It consists
in calculating an approximate posterior distribution of the hidden variables
in the agent’s model of its world by minimizing variational free energy. The
agent navigates its world by calculating a predictive distribution on its fu-
ture sensations. This is known as active inference. If active inference is to
be cast as an optimization problem in the same way as perceptual inference
then an objective function analogous to variational free energy needs to be
27
defined. For an agent that models its world as a HMM, we propose that the
divergence of a predictive distribution from the HMM can serve as such an
objective function.
We have already seen in Sections 3.3 and 5.1 how calculating approximate
posterior and predictive distributions can be formulated as constrained di-
vergence minimization problems that can be solved by variational methods.
In this section we show how a suitably constrained mean field approximation
to the HMM that models the agent’s world can serve both as a posterior dis-
tribution for perceptual inference and as a predictive distribution for active
inference.
When it is used to model perception, the divergence criterion that we
optimize coincides with variational free energy. When it used to model action,
it turns out to be closely related to but different from the expected free energy
functionals that have previously been used in active inference. We will discuss
this relationship in Section 9 below.
6.1 Perception and Action as Divergence Minimiza-
tion
s0 s1 s2 ··· st st+1 ··· sT
o1 o2 ot ot+1 oT
Figure 5: Directed graphical model for the Perception/Action cycle. At time
t,o ≤t has been observed ando >t is yet to be observed.
The title of this section is borrowed from [22] where it is shown how a
wide range of reinforcement learning algorithms can be viewed as solving
constrained divergence minimization problems. Given a HMMp(s,o) and
an observation history ¯o1, . . . ,¯ot, we show how perceptual and active infer-
ence can be cast as a problem of optimizing a joint divergence functional
KL [q(s,o)∥p(s,o)] whereq(s,o) a suitably constrained variational approx-
imation top(s,o). Our approach enables us to treat the perception/action
cycle in a unified way rather than modelling perception by optimizing a
28
variational free energy functional and action by optimizing an expected free
energy functional (as is usually done).
The graphical model for the calculation we will perform is shown in fig.
5. We assume that the planning horizonTis fixed in advance. At a given
timet= 0, . . . , T, the variableso1, . . . ,ot have been observed and we use the
observed values ¯o1, . . . ,¯ot to calculate posterior distributions for the entire
state sequences 0, . . . ,sT up to timeT. So, at each timet, we are retrodicting
the past as well as predicting the future. Retrodiction has to be supported
if an agent is to have the ability to update its beliefs about past events as
more information becomes available.
Following the standard convention in the active inference literature, we
usetto denote thepresenttime andτto denote any time (past, present, or
future) in the interval 0, . . . , T. For a given present timet, we assume that
q(s,o) factorizes as
q(s,o) =q(s 0)
TY
τ=1
q(sτ ,o τ )
whereq(s 0) =p(s 0) and
q(sτ ,o τ ) =
(
q(sτ )δ¯oτ (oτ ) (τ= 1, . . . , t)
q(sτ )p(oτ |sτ ) (τ=t+ 1, . . . T). (6.1)
Another (arguably better) possibility which we will not explore here would
be to factorizeq(s) as a product of conditional distributions so that
q(s) =q(s 0)
TY
τ=1
q(sτ |sτ−1 ).
This is the most commonly used factorization in Bayesian treatments of
HMMs [34]. In the context of active inference, this factorization is discussed
in [35, 13] where it is referred to as the Bethe approximation.
Note that because of the presence of the termp(o τ |sτ ) in (6.1) the fac-
torization we are using is not a full mean field approximation top(s,o). We
will explain the reason for this choice in Section 7 below.
Note also that, although our notation fails to reflect these dependencies,
the value of the divergence fromq(s,o) top(s,o) depends on the starting
distributionp(s 0) (through (5.3)) as well as on the observations up to timet
29
(through (6.1)). We will treat the starting distribution as context-dependent
in the sense that it is determined by the agent’s activity prior to time 0 and
we will take the variational posteriorq(s 0) to be identical top(s 0). Equa-
tion (6.14) below explains the rationale for this choice. Our handling of the
starting distribution departs from the tradition in the active inference liter-
ature where it is specified by a vector denoted byDwhich is assumed to be
context-independent and the variational posterior at time 0 is updated in the
same way as the variational posteriors at other times.
The casest= 0 (the beginning of the cycle) andt=T(the end of the
cycle) are of particular interest. In the caset= 0, the variableso 1, . . . ,oT
are yet to be observed and the observation history is empty. So so by (6.1),
q(o|s) =p(o|s). Thus the divergence KL [q(o|s)∥p(o|s)] is 0 so by the chain
rule (3.3)),
KL [q(s,o)∥p(s,o)] = KL [q(s)∥p(s)].(6.2)
In the caset=T, all of the variableso 1, . . . ,oT have been observed so by
(6.1),q(s,o) =q(s)δ ¯o(o) and, by (3.7), the divergence KL [q(s,o)∥p(s,o)] is
just the variational free energy of the observations ¯o1, . . . ,¯oT calculated with
the approximate posteriorq(s).
Throughout this section, we ignore any structure that the state space of
the HMM may have and we assume that the number of states sufficiently
small that the emission probability distributions and transition probability
distributions that define the HMM can be precomputed and stored as ma-
trices which we denote byAandBrespectively (following the conventions
in the active inference literature).
As in Section 5.1 we represents τ ando τ by 1-hot vectors so that
p(oτ |sτ ) =s ⊤
τ Aoτ
p(sτ |sτ−1 ) =s ⊤
τ−1 Bsτ
p(s0) =s ⊤
0 p0 (6.3)
wherep 0 is the vector that represents the starting distributionp(s 0). With
these conventions,
lnp(s,o) =s ⊤
0 lnp 0 +
TX
τ=1
 
s⊤
τ lnA o τ +s ⊤
τ−1 lnB s τ

.(6.4)
Likewise, we store each posterior distributionq(s τ ) as a vectorq τ so that
q(sτ ) =s ⊤
τ qτ .(6.5)
30
Finally, we store the entropies of the emission probability distributions as a
vectorhso that
H [p(oτ |sτ )] =s ⊤
τ h.(6.6)
The components ofhare the diagonal entries of the matrix−A(lnA) ⊤.
We derive the formulas for updating each posteriorq τ holding the others
fixed in Sections A.1 and A.2 in the appendix. In the case whereτ > t, the
update formula forq τ is given by (A.4):
lnq τ =
(
−h+ lnB ⊤qτ−1 + lnB qτ+1 +. . .(τ= 1, . . . , T−1)
−h+ lnB ⊤qT−1 +. . .(τ=T). (6.7)
In the case whereτ≤t, the update formula forq τ is given by (A.8):
lnq τ =



lnp 0 (τ= 0)
lnA ¯oτ + lnB⊤qτ−1 + lnB qτ+1 . . .(τ= 1, . . . , T−1)
lnA ¯oT + lnB⊤qT−1 +. . .(τ=T).
(6.8)
Broadly speaking, there are two ways of implementing these update for-
mulas at each present timet. Both implementations can be carried out in
real time, but they have different computational requirements. The sim-
plest, known as Bayesian filtering (by analogy with Kalman filtering), holds
q0, . . . ,qt−1 fixed and updates the posteriors for the present and future times
qt, . . . ,qT asynchronously at timet. The alternative, known as Bayesian
smoothing, updates all of the posteriorsq 1, . . . ,qT asynchronously at every
time present timet. This results in a lower value for the divergence criterion.
An obvious compromise between the two approaches would be to limit the
extent to which beliefs about the past are updated in Bayesian smoothing.
After calculating the posteriors at present timet, the divergence ofq(s,o)
fromp(s,o) can be evaluated using (A.10):
KL [q(s,o)∥p(s,o)] =
TX
τ=1
q⊤
τ lnq τ −
tX
τ=1
q⊤
τ lnA ¯oτ −
TX
τ=1
q⊤
τ−1 lnB q τ .(6.9)
We can summarize these calculations in the casest= 0 (the beginning
of the cycle) andt=T(the end of the cycle) as follows. In the caset= 0,
31
the variableso 1, . . . ,oT are yet to be observed, the posteriorsq 1, . . . ,qT are
updated using the prediction update formulas (6.7), and the expression for
the divergence ofq(s,o) fromp(s,o) is
TX
τ=1
q⊤
τ lnq τ −
TX
τ=1
q⊤
τ−1 lnB q τ .(6.10)
As we saw in (6.2)), this is just the marginal divergence ofq(s) fromp(s).
In the caset=T, all of the variableso 1, . . . ,oT have been observed,
the posteriorsq 1, . . . ,qT are updated using the retrodiction update formulas
(6.8) and the expression for the divergence ofq(s,o) fromp(s,o) is
TX
τ=1
q⊤
τ lnq τ −
TX
τ=1
q⊤
τ lnA ¯oτ −
TX
τ=1
q⊤
τ−1 lnB q τ .(6.11)
By (3.7), this is just the variational free energy of the observed sequence
¯o1, . . . ,¯oT .
More generally, fort= 0, . . . , T, we can split the right-hand side of (6.9)
into two terms, a contribution from the past an a contribution from the
future. That is,
KL [q(s,o)∥p(s,o)] =F ≤t +F >t (6.12)
where
F≤t =
tX
τ=1
q⊤
τ lnq τ −
tX
τ=1
q⊤
τ lnA ¯oτ −
tX
τ=1
q⊤
τ−1 lnB q τ
= KL [q(s≤t,o ≤t)∥p(s ≤t,o ≤t)] (6.13)
which, by (3.7), is just the variational free energy of the observations up to
the present timet, and
F>t =
TX
τ=t+1
q⊤
τ lnq τ −
TX
τ=t+1
q⊤
τ−1 lnB q τ
= KL [q(s>t)∥p(s >t)] (6.14)
where this divergence is evaluated usingq(s t) as the starting distribution at
timetrather thanp(s 0).
32
6.2 Planning
Consider now an agent having the ability to perform several different types
of action, each modelled by a transition probability matrix. Suppose that
the agent is charged with a task which is characterized by the requirement
that the states of world at timeTare sampled from a stationary preference
distributionp(s τ |C) such as the steady state distributionp(s τ |S) that we
referred to in our discussion of homeostasis in Section 2.2. Suppose further
that the agent is given a list of the possible action sequences ending at time
Tthat are available to it. These action sequences are calledpoliciesin the
active inference literature.
Assume that at a given present timet= 0, . . . , T−1, the agent has
performed actionsa 1, . . . , at and recorded observations ¯o1, . . . ,¯ot. Which
action should the agent perform at timet+ 1?
One way to answer this question would be to start from the observation
that the calculations in Section 6.1 can be modified straightforwardly to
accommodate time-dependent transition probability matricesB πτ for each
policyπand timeτ. So for each policyπthat is consistent with the actions
that have already been performed up to the present timet, we can use the
observations ¯o1, . . . ,¯ot to calculate variational posteriorsq(s τ |π) forτ=
1, . . . , Tand use these posteriors to evaluate the divergence ofq(s>t|π) from
p(s>t|C). The agent would then plan to execute the policy which minimizes
this divergence and this would determine the action that the agent performs
at timet+ 1, as required.
Another possibility, which is much less computationally expensive but
does not seem to have been noticed in the active inference literature, would
be to calculate divergences in the forward rather than the reverse direction
(as discussed in Section 3.1). Elsewhere in this paper we use reverse diver-
gences rather than forward divergences because in those situations the target
distribution is intractable and hence has to be approximated. In this case, the
target distribution, namelyp(s >t|C), is already factorized and the divergence
KL [p(s>t|C)∥p(s >t|π)] can be evaluated straightforwardly without having
to use a variational approximationq(s >t|π) top(s >t|π). Indeed, ifp C is the
vector representation ofp(s τ |C) (which we are assuming to be independent
ofτ), then
KL [p(s>t|C)∥p(s >t|π)] =
TX
τ=t+1
p⊤
C lnp C −
TX
τ=t+1
p⊤
C lnB πτ pC
33
by (6.14).
6.3 Time-Domain Renormalization
The approach to planning that we have just outlined has the drawback that
the list of policies to be searched can be expected to increase exponentially
with the planning horizon,T. This explosion could be avoided by treating the
process of generating a policy as a hidden stochastic process about which the
agent makes probabilistic inferences. The simplest possibility is to assume
that action sequences are generated by a Markov chain so that
p(a0, . . . , aT ) =p(a 0)
TY
τ=1
p(aτ |aτ−1 ).(6.15)
More generally, we could assume as a generative model for action sequences
a second HMM whose states we will denote byσ τ so that
p(a) =p(σ 0)
TY
τ=1
p(στ |στ−1 )p(aτ |στ ) (6.16)
whereais the sequence of actionsa 1, . . . , aT . Under a model of this sort, the
search space grows linearly rather than exponentially inT.
Note that the two HMMs under consideration can be integrated into a sin-
gle HMM whose states are triples (σ τ , aτ ,s τ ) and whose transition, emission
and starting probability distributions are given by the equations
p(στ , aτ ,s τ |στ−1 , aτ−1 ,s τ−1 ) =p(σ τ |στ−1 )p(aτ |στ )p(sτ |sτ−1 , aτ )
p(oτ |στ , aτ ,s τ ) =p(o τ |sτ )
p(σ0,s 0) =p(σ 0)p(s0).(6.17)
Iterating this construction enables a hierarchical structure to be built up.
This process is known astime domain renormalizationbecause it can easily
accommodate slower clock speeds at higher levels of the hierarchy [30].
Although variational inference is traditionally thought of as a method of
inferring state occupancies and transitions and active inference as a method of
inferring actions that are currently underway or yet to be performed, folding
actions into states as in (6.17) does away with this distinction.
Of course the prediction and retrodiction updates (6.7) and (6.8) would
need to be modified if variational posteriors are to be calculated efficiently
34
with a structured HMM of this sort. We won’t go into details but just point
out that a natural mean field factorization to use for this purpose would be
to set
q(σ,a,s,o) =q(σ 0)q(s0)
TY
τ=1
q(στ )p(aτ |στ )q(sτ ,o τ |aτ )
whereq(σ 0) =p(σ 0),q(s 0) =p(s 0) andq(s τ ,o τ |aτ ) factorizes in the same
way as (6.1). Updating a full set of variational posteriorsq(σ τ , aτ ,s τ ,o τ )
(whereτranges from 1 toT) at the present timetgives, in particular, a
predictive distributionq(a t+1) on the action that will be performed at time
t+ 1. So the agent would decide what to do next by drawing a sample from
this distribution.
The reader may wonder what has happened to the stationary preference
distributionp(s τ |C) introduced in the last section. It is generally the case
that a finite state Markov chain has a unique steady state distribution [23].
Applied to the integrated HMM (6.17), this implies that, regardless of the
starting distributionsp(σ 0) andp(σ 0), the triples (σ τ , aτ ,s τ ) will eventu-
ally be distributed according to a steady state distribution. The same will
therefore be true of the statess τ . The latter steady state distribution is
determined by the HMM (6.16), rather than by an externally prescribed
preference distributionp(s τ |C). In order to ensure that the steady state and
the preference distribution agree, the parameters of the HMM (6.16) would
have to be learned from an appropriate training dataset. (We will discuss
learning in Section 8.)
7 The Exploration/Exploitation Tradeoff
As shown in [22], many well known reinforcement learning algorithms can
be viewed as solving constrained divergence minimization problems. This
perspective enables the exploration/exploitation tradeoff in reinforcement
learning to be understood in a principled way. In this section, we show how
this analysis plays out in the case of HMMs.
The present timetis fixed throughout this section so we will use the
notationo > rather thano >t to indicate future observations and similarly for
s>. Using the factorizations
q(s>,o >) =q(o >|s>)q(s>)
p(s>,o >) =p(o >)p(s>|o>),
35
we can write
lnq(s >,o >)−lnp(s >,o >) =
= (lnq(o >|s>)−lnp(o >)) + (lnq(s>)−lnp(s >|o>)).
So we can write the divergence KL [q(s >,o >)∥p(s >,o >)] in the form
Eq(s>) [KL [q(o>|s>)∥p(o >)]] +E q(s>,o>) [lnq(s >)−lnp(s >|o>)].(7.1)
Since the divergence ofq(s >|o>) fromp(s >|o>) is non-negative,
Eq(s>|o>) [lnq(s >|o>)]≥E q(s>|o>) [lnp(s >|o>)]
which implies that
Eq(s>,o>) [lnq(s >)−lnp(s >|o>)]≥E q(s>,o>) [lnq(s >)−lnq(s >|o>)]
=−E q(s>) [KL [q(o>|s>)∥q(o >)]]
=−KL [q(s >,o >)∥q(s >)q(o>)].(7.2)
So combining (7.1) and (7.2), we obtain
KL [q(s>,o >)∥p(s >,o >)]≥
Eq(s>) [KL [q(o>|s>)∥p(o >)]]−KL [q(s >,o >)∥q(s >)q(o>)].(7.3)
The expression on the right hand side here is referred to as thefree energy
of the expected futurein [36, 37].
The divergence KL [q(s>,o >)∥q(s >)q(o>)] is themutual informationbe-
tweens > ando >. This is a measure of the degree to whichs > ando > fail
to be statistically independent. In the active inference literature, the mu-
tual information is usually referred to as theexpected information gainas it
can be interpreted as the reduction in uncertainty abouts > contingent on
observingo > given by the expression
H [q(s>)]−E q(o>) [H [q(s>|o>)]]
which is just another way of writing the mutual information betweens > and
o>. The reason why we did not use a full mean field approximation in (6.1)
is to ensure that this reduction in uncertainty is non-zero.
Because the mutual information appears in (7.3) with a negative sign,
minimizing the divergence ofq(s >,o >) fromp(s >,o >) can be expected to
36
increase the information gain, although it is not guaranteed to do so: firstly,
because this is an inequality not an equality, and secondly, because the first
term on the right-hand side of (7.3) depends onq(s >,o >). This term can
be viewed as a measure of how well the marginal distributionp(o >) is ap-
proximated byq(s >,o >). So it is apragmatic valueorextrinsic valuein the
terminology of active inference. It can be decreased by acting on the world
in a way that ensures that future observations have high probability under
the marginal distributionp(o >). But this sort of action is not feasible if the
agent is uncertain as to the current state of its world. In such a situation,
the right-hand side of (7.3) can only be minimized by acting in such a way as
to maximize the information gain. Thus (7.3) can be viewed as formalizing
the tradeoff between exploration and exploitation that an intelligent agent
needs to be able to make as it navigates its world.
Sinceo > is yet to be observed, (6.1) implies thatq(o >|s>) =p(o >|s>)
and we can use this to rewrite (7.3) as follows. For the first term on the
right-hand side of (7.3) we have
Eq(s>) [KL [q(o>|s>)∥p(o >)]]
=E q(s>) [KL [p(o>|s>)∥p(o >)]]
=E q(s>)

Ep(o>|s>) [lnp(o >|s>)−lnp(o >)]

=E q(o>) [−lnp(o >)]−E q(s>) [H [p(o>|s>)]].
In the active inference literature, the terms extrinsic or pragmatic value
are usually reserved for the cross entropy term on the right-hand side of
this equation and the expected entropy term is called theambiguity. As
for the joint divergence on the left-hand side of (7.3), the assumption that
q(o>|s>) =p(o >|s>) implies (by the chain rule (3.3)) that
KL [q(s>,o >)∥p(s >,o >)] = KL [q(s>)∥p(s >)].
So (7.3) now takes the form
KL [q(s>)∥p(s >)]≥
Eq(o>) [−lnp(o >)]−E q(s>) [H [p(o>|s>)]]
−KL [q(s>,o >)∥q(s >)q(o>)].(7.4)
and, since the mutual information betweens > ando > can be written in the
form
H [q(s>)]−E q(s>) [H [p(o>|s>)]],
37
this simplifies to
KL [q(s>)∥p(s >)]≥E q(o>) [−lnp(o >)]−H [q(s >)].(7.5)
Transposing the ambiguity term on the right-hand side of (7.4) to the
left-hand side gives
KL [q(s>)∥p(s >)] +E q(s>) [H [p(o>|s>)]]≥
Eq(o>) [−lnp(o >)]−KL [q(s >,o >)∥q(s >)q(o>)].(7.6)
In the active inference literature, the expression on the right-hand side of (7.6)
is the standard way of definingexpected free energy, although it is suggested
in [13] that the expression on the left-hand side might be used instead.
We will discuss expected free energy functionals in Section 9. For the
moment we note that inspection of (7.5) and (7.6) shows that optimizing the
right-hand side of (7.6) tends to (but is not guaranteed to) decrease the cross-
entropy term and increase the mutual information term whereas optimizing
the right-hand side of (7.5) tends to increase the the entropy H [q(s>)] instead.
Optimizing the left-hand side of (7.6) tends to decrease both the divergence
term and the ambiguity.
8 Learning
A fully Bayesian approach to the problem of estimating the transition and
emission probability distributions that define a HMM requires that these
probability distributions be treated as random variables. Thus a prior prob-
ability distribution is assigned to the HMM parameters before learning and
a posterior distribution after learning. This posterior distribution can be
updated on an ongoing basis (as more training data becomes available) and
point estimates of the HMM parameters can be calculated from the poste-
rior as needed. The Dirichlet distribution is the natural choice for a prior on
discrete probability distributions and on discrete HMMs.
8.1 Dirichlet Priors
The Dirichlet distribution is a multivariate distribution onK-tuples (µ1, . . . , µK)
whose components are non-negative and sum to 1. It is defined by the prob-
38
ability density
1
B(α)
KY
k=1
µαk−1
k .
HereB(α) is the beta function defined by
B(α) = Γ(α1). . .Γ(αK)
Γ(α0)
whereα 0 =α 1 +. . .+αK. Theα’s are referred to as concentration pa-
rameters or Dirichlet counts. Ifµ k is interpreted as the probability that an
integer valued random variable takes the valuek,α k is usually thought of as
the number of times this event is observed to happen in a virtual training
set although theα’s are not required to be whole numbers. The Dirichlet
distribution has the property that
E[lnµ k] =ψ(α k)−ψ(α 0) (8.1)
fork= 1, . . . , K. Hereψis the digamma function (that is, the derivative of
ln Γ).
Ifq(µ) andp(µ) are Dirichlet distributions defined by concentration pa-
rametersα ′ andαrespectively then the divergence KL [q(µ)∥p(µ)] is given
by
lnB(α)−lnB(α ′) +
KX
k=1
(α′
k −α k)) (ψ(α′
k)−ψ(α ′
0)).(8.2)
Recall that a HMMMis specified by emission and transition probability
distributions stored as rows of matrices which we denoted byAandB. We
can define a prior onMby assigning Dirichlet priors to each of the rows of the
these matrices. It is convenient to write this prior in matrix form as follows.
IfC A is the matrix whose entries are the concentration parameters for the
emission probabilities then we can write the prior onAin the logarithmic
domain as
lnp(A) = tr

lnA(C A −1) ⊤

+. . . .
where1is the matrix all of whose entries are equal to 1. Similarly,
lnp(B) = tr

lnB(C B −1) ⊤

+. . . .
whereC B is the matrix whose entries are the concentration parameters for
the transition probabilities. So the prior onMhas the form
lnp(M) = tr

lnA(C A −1) ⊤

+ tr

lnB(C B −1) ⊤

+. . . .(8.3)
39
8.2 Learning as Divergence Minimization
Suppose we are given a Dirichlet priorp(M) and a training set consisting
of an observation sequence ¯oof lengthT. We can calculate a variational
posterior onM— which turns out to be a Dirichlet distribution defined by
another set of concentration parameters — as follows.
We define a joint prior on triples (M,s,o) by setting
p(M,s,o) =p(M)p(s,o|M).
and seek a variational approximationq(M,s,o) top(M,s,o) of the form
q(M,s,o) =q(M)q(s)δ ¯o(o)
where
q(s) =
TY
τ=0
q(sτ ).
As in Section 6, we represent each distributionq(sτ ) by a vectorqτ . Following
the prescription (3.6), we alternate between updatingq(M) holdingq(s) fixed
and updatingq(s) holdingq(M) fixed.
Thus we updateq(M) by settingq(M)∝˜q(M) where
ln ˜q(M) =Eq(s,o) [lnp(M,s,o)]
=E q(s) [lnp(M,s, ¯o)]
= lnp(M) +E q(s) [lnp(s, ¯o|M)].(8.4)
Recall that by (6.4),
lnp(s, ¯o|M) =s ⊤
0 lnp 0 +
TX
τ=1
 
s⊤
τ lnA ¯oτ +s ⊤
τ−1 lnB s τ

.(8.5)
so the second term on the right hand side of (8.4) is equal to
TX
τ=1
q⊤
τ lnA ¯oτ +
TX
τ=1
q⊤
τ−1 lnB q τ
=tr
 
lnA
TX
τ=1
¯oτ q⊤
τ
!
+ tr
 
lnB
TX
τ=1
qτ q⊤
τ−1
!
.
40
Combining this with (8.3) and collecting terms, (8.4) becomes
ln ˜q(M) = tr
 
lnA(C ′
A −1) ⊤
+ tr
 
lnB(C ′
B −1) ⊤
+. . .
where
C′
A =C A +
TX
τ=1
qτ ¯o⊤
τ
C′
B =C B +
TX
τ=1
qτ−1 q⊤
τ .(8.6)
So, likep(M),q(M) is of the form (8.3). Thus it too is a Dirichlet distribu-
tion.
To updateq(s) holdingq(M) fixed, we setq(s 0) =p(s 0) and for each
τ= 1, . . . , T, we updateq(sτ ) by settingq(s τ )∝˜q(sτ ) where ˜q(sτ ) is defined
by
ln ˜q(sτ ) =E q(s\τ )

Eq(M,o) [lnp(M,s,o)]

=E q(s\τ )

Eq(M) [lnp(M,s, ¯o)]

=E q(s\τ )

Eq(M) [lnp(s, ¯o|M)]

+. . .(8.7)
Recall that by (6.4),
lnp(s, ¯o|M) =s ⊤
0 lnp 0 +
TX
τ=1
 
s⊤
τ lnA ¯oτ +s ⊤
τ−1 lnB s τ

.(8.8)
So
Eq(M) [lnp(s, ¯o|M)] =s ⊤
0 lnp 0 +
TX
τ=1
 
s⊤
τ ⟨lnA⟩ ¯oτ +s ⊤
τ−1 ⟨lnB⟩s τ

(8.9)
where the notation⟨lnA⟩indicates the expectation of lnAtaken with re-
spect toq(M) and similarly for⟨lnB⟩. These expectations can be calculated
using (8.1). Hence the update formula forq τ is formally the same as the
retrodiction update formula (6.8):
lnq τ =



lnp 0 (τ= 0)
⟨lnA⟩ ¯oτ +⟨lnB⟩ ⊤ qτ−1 +⟨lnB⟩q τ+1 +. . .(τ= 1, . . . , T−1)
⟨lnA⟩ ¯oT +⟨lnB⟩ ⊤ qT−1 +. . .(τ=T).
41
It remains to evaluate the joint divergence KL [q(M,s,o)∥p(M,s,o)].
By the chain rule (3.3), we can write this as
KL [q(M)∥p(M)] +E q(M) [KL [q(s,o)∥p(s,o|M)]].
The first term here is given by (8.2). For the second, sinceois observed to
take the value ¯o, the divergence KL [q(s,o|M)∥p(s,o|M)] is given by the
variational free energy formula (6.11):
KL [q(s,o|M)∥p(s,o|M)]
=
TX
τ=1
q⊤
τ lnq τ −
TX
τ=1
q⊤
τ lnA ¯oτ −
TX
τ=1
q⊤
τ−1 lnB q τ
so that
Eq(M) [KL [q(s,o)∥p(s,o|M)]]
=
TX
τ=1
q⊤
τ lnq τ −
TX
τ=1
q⊤
τ ⟨lnA⟩ ¯oτ −
TX
τ=1
q⊤
τ−1 ⟨lnB⟩q τ .
Extending these derivations to accommodate the structured state spaces
discussed in Section 6.3 is straightforward. Consider the matrixPT
τ=1 qτ−1 q⊤
τ
appearing the second line of (8.6). For each transition from one state to
another, there is a corresponding entry in this matrix whose value is the
expected number of times the transition occurs in the training data. If states
are decomposed as in (6.17) or into more complex hierarchies, then all that
is required is to accumulate matrices of expected transition counts for each
level in the hierarchy.
9 Whither Expected Free Energy?
Recall that, given observations that have already been made, variational free
energy is an approximation to the surprisal of these observations. Evaluating
it requires a generative modelp(s,o) (such as a HMM), the observationso
themselves, and an approximation to the posterior distributionp(s|o).
If we can calculate an approximate posteriorq(s >|o>) for observationso >
that have yet to be made, and in addition we have an approximate marginal
distributionq(o >) on these observations, then we can calculate an expected
value for the variational free energy of future observations. This expected
value is traditionally denoted byG. How shouldGbe evaluated?
42
9.1 Expected Free Energy Functionals
Note that, taken together,q(o >) andq(s >|o>) define a joint distribution
q(s>,o >). We assume that this joint distribution satisfies the condition that
q(o>|s>) =p(o >|s>) in order to ensure that the mutual information between
s> ando > is non zero (as explained in Section 7).
Although this assumption seems to be needlessly restrictive (as seems to
have been first pointed out in [36]), the approximation
q(s>|o>)≈q(s >) (9.1)
enablesGto be evaluated easily. The variational free energy foro > is
Eq(s>) [−lnp(s >,o >)]−H [q(s >)]
so, taking the expectation with respect toq(o >),
G=E q(o>)q(s>) [−lnp(s >,o >)]−H [q(s >)]
=−E q(s>) [lnp(s >)]−H [q(s >)] +E q(s>)q(o>) [−lnp(o >|s>)]
= KL [q(s>)∥p(s >)] +E q(s>) [H [p(o>|s>)]] (9.2)
which is just the left-hand side of (7.6). However, it is the expression on the
right-hand side of (7.6) that is taken as standard definition of expected free
energy in the active inference literature. This can be derived by appealing
to an additional approximation, namely
p(s>,o >)≈q(s >|o>)p(o>).(9.3)
Using both of these approximations, we have
G=E q(s>,o>) [−lnp(s >,o >)]−E q(o>) [H [q(s>|o>)]]
≈E q(s>,o>) [−lnq(s >|o>)−lnp(o >)]−H [q(s >)]
=E q(s>,o>) [−lnq(s >,o >) +q(s >) +q(o >)] +E q(o>) [−lnp(o >)]
=−KL [q(s >,o >)∥q(s >)q(o>)] +E q(o>) [−lnp(o >)] (9.4)
which is just the right-hand side of (7.6), as required.
What happens if we proceed without making either of these approxima-
tions? The variational free energy foro > is
Eq(s>|o>) [−lnp(s >,o >)]−H [q(s >|o>)]
43
so, taking the expectation with respect toq(o >), we obtain the following
expression for the expected free energy:
G=E q(s>,o>) [−lnp(s >,o >)]−E q(o>) [H [q(s>|o>)]].(9.5)
Since
H [q(s>,o >)] = H [q(o>)] +E q(o>) [H [q(s>|o>)]],
we can rewrite this as
G=E q(s>,o>) [−lnp(s >,o >)]−H [q(s >,o >)] + H [q(o>)]
= KL [q(s>,o >)∥p(s >,o >)] + H [q(o>)]
= KL [q(s>)∥p(s >)] + H [q(o>)] (9.6)
since we are assuming thatq(o >|s>) =p(o >|s>). So if the expected free
energyGis defined in this way, the marginal divergence KL [q(s >)∥p(s >)]
andGare very closely related:
KL [q(s>)∥p(s >)] =G−H [q(o >)].(9.7)
Comparing this with the general relationship (3.1)
KL [q(x)∥p(x)] =E q(x) [−lnp(x)]−H [q(x)],
which we discussed in Section 3.1, we see that the entropy term H [q(o >)] in
(9.7) can be interpreted as a regularizer which can serve to prevent overfitting
when the divergence KL [q(s>)∥p(s >)] is used as a criterion for approximat-
ingp(s >) byq(s >).
9.2 Beliefs about Policies
Expected free energy is usually thought of as the objective function which an
agent needs to optimize in order to make probabilistic inferences about ac-
tions which are currently underway or yet to be performed, or more generally,
to update beliefs about policies.
Suppose that, at present timet, an agent has a prior distributionp(π)
over policies where each policy is a sequence of actionsa 1, . . . , aT and that
observations ¯o≤t have been recorded. According to equation (B.9) of [13],
the agent’s posterior belief about policies is given by
q(π) =σ(lnp(π)−F(π)−G(π))
44
whereσis the softmax function and, for each policyπ,F(π) is the variational
free energy associated with the observation history, andG(π) is the expected
free energy associated with future observations.
To see how such an equation might be derived, let us define a joint dis-
tribution on triples (π,s,o) by setting
p(π,s,o) =p(π)p(s,o|π),(9.8)
We can obtain an approximate posterior distributionq(π) onπby assuming
a variational approximation to this joint distribution of the formq(π)q(s,o)
whereq(s,o) factorizes as in (6.1). Ifq(s,o) is held fixed then, by (3.6), the
update formula forq(π) isq(π)∝˜q(π) where
ln ˜q(π) = lnp(π) +Eq(s,o) [lnp(s,o|π)]
= lnp(π)−KL [q(s,o)∥p(s,o|π)] +. . .
since by (3.1),
KL [q(s,o)∥p(s,o|π)] =E q(s,o) [−lnp(s,o|π)]−H [q(s,o)]
and the entropy term H [q(s,o)] is independent ofπ. As in (6.12), we can
write
KL [q(s,o)∥p(s,o|π)] =F ≤(π) +F >(π)
where
F≤(π) = KL [q(s≤,o ≤)∥p(s ≤,o ≤|π)]
and
F>(π) = KL [q(s>)∥p(s >|π)] (9.9)
where this divergence is evaluated usingq(s t) as the starting distribution at
the present timet. So the update formula forq(π) is
q(π) =σ(lnp(π)−F ≤(π)−F >(π)) (9.10)
which yields equation (B.9) of [13] ifG(π) is taken to beF >(π).
Alternatively, we could take
G(π) =E q(s>,o>) [−lnp(s >,o >|π)]−E q(o>) [H [q(s>|o>)]] (9.11)
45
since, as in the derivation of (9.7) from (9.5),
F>(π) =G(π)−H [q(o >)].(9.12)
The fact that the entropy term on the right-hand side here is independent
ofπimplies that the values returned by the softmax function would be
unaffected by substituting the right-hand side of (9.11) forF >(π) in (9.10).
The expression forG(π) in (9.11) has the same form as the free energy
functional in (9.5) which we derived without appealing to either of the ap-
proximations (9.1) and (9.3). Note however that the argument we have just
given depends on calculatingF ≤(π) andF >(π) with the distributionq(s,o)
postulated by the variational approximation to the joint distribution defined
by (9.8). This isnotthe same as the policy dependent mean field approxi-
mation top(s,o|π) that would be obtained by applying the prediction and
retrodiction update formulas (6.7) and (6.8) to each policy individually. So
this way of defining variational and expected free energy for policies depends
on how the distributionq(s,o) is estimated. The correct estimation proce-
dure is to updateq(s,o) andq(π) alternately in the usual way.
We have already derived the update formula forq(π) holdingq(s,o) fixed
so it only remains to derive the update formula forq(s,o) holdingq(π) fixed.
By (3.6),q(s,o)∝˜q(s,o) where
ln ˜q(s,o) =Eq(π) [lnp(π,s,o))].
By (6.4),
lnp(s,o|π) =s ⊤
0 lnp 0 +
TX
τ=1
 
s⊤
τ lnA o τ +s ⊤
τ−1 lnB πτ sτ

.
So
Eq(π) [lnp(s,o|π)] =s ⊤
0 lnp 0 +
TX
τ=1
 
s⊤
τ lnA o τ +s ⊤
τ−1 lnB τ sτ

whereB τ is defined by
lnB τ =
X
π
q(π) lnBπτ
So if the matricesB τ are defined in this way, the update formulas forq(s)
have the same form as the prediction and retrodiction update formulas (6.7)
and (6.8).
46
9.3 Discussion
Although it is not strictly speaking an expected free energy, the expression
on the right-hand side of (7.6) is generally taken as the definition of expected
free energy in the active inference literature thanks to its interpretation in
terms of pragmatic value and expected information gain. We have seen how
this is usually justified by appealing to the approximation (9.3). (This ap-
proximation is stated explicitly in [10, 8] and it seems to be used implicitly
in [13]. Other survey articles and tutorials take the right-hand side of (7.6)
as the definition of expected free energy without attempting to justify it
[7, 9, 11].)
Inspecting the derivation (9.4) shows that it also invokes the approxima-
tion (9.1). If it is assumed thatq(s >|o>) =q(s >) and thatp(s >,o >) =
q(s>|o>)p(o>), thens > ando > are statistically independent under both
the the target distributionp(s >,o >) and the approximating distribution
q(s>,o >) so that the mutual information betweens > ando > is zero. This is
clearly problematic.
If neither approximation is used then, as we saw in (9.7), the expres-
sion obtained for the expected free energy only differs from the divergence
KL [q(s>)∥p(s >)] by an entropy regularizer, H [q(o >)]. Whether this sort
of regulariztion is practically useful can only be determined by running ex-
periments but is easy to imagine that a guardrail against making over con-
fident predictions about future events could prove useful for an agent en-
gaged in planning over extended time scales. As we discussed in Section 3.2,
the variational updates (3.6) are designed to optimize divergences (such as
KL [q(s>)∥p(s >)]) rather than functionals of the form (9.6). So if an ex-
pected free energy functional such asG(or, for that mattter, any criterion
other than a reverse KL divergence) were to be used as the objective function
for active inference, another algorithm would need to be developed in order
to optimize it.
Expected free energy is usually thought of as furnishing an objective
function for variational inference about actions and policies as distinct from
states and transitions as in (9.10). However the distinction between states
and actions is not absolute as it hinges on the way states are defined. (For
example, this distinction is mooted by folding actions into states as in Section
6.3.) Although we had no need to appeal to an equation like (B.9) of [13]
in the body of this paper, we showed how such an equation can be derived
by defining variational and expected free energies for policies in terms of a
47
mean field approximation to the joint distribution defined by (9.8). This
entails defining the expected free energy associated with a policy by (9.9) or
by (9.11) rather than by the right-hand side of (7.6) as is usually done in the
active inference literature.
10 Conclusion
This work was motivated by a desire to tighten up the mathematical ar-
guments in the active inference text book [13] but the difficulties which we
have just outlinned led us to develop a new approach to the problem of active
inference which can be summarized succinctly as follows:
Assuming that an agent models its world by a Hidden Markov Model
p(s,o), it can solve the problems of perceptual and active inference in a
unified way by calculating a suitably constrained mean field approximation
q(s,o) top(s,o) using the divergence fromq(s,o) top(s,o) as the optimiza-
tion criterion. At a given time present timet, the variableso ≤t have been
observed to take the values ¯o≤t and the constraint onq(s,o) is
q(sτ ,o τ ) =
(
q(sτ )δ¯oτ (oτ ) (τ≤t)
q(sτ )p(oτ |sτ ) (τ > t)
as in (6.1). The update formulas needed to calculateq(s,o) subject to this
constraint are given in the appendix. The derivation of these formulas makes
no appeal to the Free Energy Principle.
We teased out the relationship between this divergence criterion and the
notion of expected free energy in Section 9. Equation (6.12) shows that the
divergence fromq(s,o) top(s,o) can be decomposed as
KL [q(s≤t,o ≤t)∥p(s ≤t,o ≤t)] + KL [q(s>t)∥p(s >t)].
where the divergence in the second term is calculated with the starting dis-
tributionq(s t) at the present timet. By (3.7), the first term here is the
variational free energy of the observations up to timetcalculated with the
variational posteriorq(s ≤t). The second term can be written in the form
KL [q(s>t)∥p(s >t)] =G−H [q(o >t)]
whereGis the expected free energy defined by (9.5)
G=E q(s>t,o>t) [−lnp(s >t,o >t)]−E q(o>t) [H [q(s>t|o>t)]].
48
This was derived without appealing to either of the approximations (9.1) or
(9.3). So our approach is faithful to the spirit of the Free Energy Principle
even as it dispenses with the need to appeal to it.
49
References
[1] P. Dayan, G. E. Hinton, R. M. Neal, and R. S. Zemel, “The Helmholtz
Machine,” inAdvances in Neural Information Processing Systems 7
(NIPS 1994), pp. 3–9, MIT Press, 1995.
[2] R. Salakhutdinov and G. E. Hinton, “Deep Boltzmann Machines,” in
Proceedings of the 12th International Conference on Neural Information
Processing Systems (NeurIPS 2009), pp. 448–455, Curran Associates,
Inc., 2009.
[3] K. J. Friston, “Learning and Inference in the Brain,”Neural Networks,
vol. 16, no. 9, pp. 1325–1352, 2003.
[4] K. J. Friston, “A Theory of Cortical Responses,”Philosophical Trans-
actions of the Royal Society B: Biological Sciences, vol. 360, no. 1456,
pp. 815–836, 2005.
[5] K. J. Friston, “The Free-Energy Principle: A Unified Brain Theory?,”
Nature Reviews Neuroscience, vol. 11, no. 2, pp. 127–138, 2010.
[6] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, “Variational Inference:
A Review for Statisticians,”Journal of the American Statistical Associ-
ation, vol. 112, no. 518, pp. 859–877, 2017.
[7] J. v. Oostrum, C. Langer, and N. Ay, “A Concise Mathematical De-
scription of Active Inference in Discrete Time,”Journal of Mathematical
Psychology, vol. 125, May 2025. arXiv:2406.07726 [cs].
[8] R. Smith, K. J. Friston, and C. J. Whyte, “A Step-by-Step Tutorial
on Active Inference and its Application to Empirical Data,”Journal of
Mathematical Psychology, vol. 107, pp. 1–60, 2022.
[9] L. D. Costa, T. Parr, N. Sajid, S. Veseli´ c, V. Neac¸ su, and K. J. Friston,
“Active Inference on Discrete State-Spaces: A Synthesis,”Journal of
Mathematical Psychology, vol. 99, 2020.
[10] N. Sajid, P. J. Ball, T. Parr, and K. J. Friston, “Active Inference: De-
mystified and Compared,”Neural Computation, vol. 33, no. 3, pp. 674–
712, 2021.
50
[11] Z. Zhang and F. Xu, “An Overview of the Free Energy Principle and
Related Research,”Neural Computation, vol. 36, no. 5, pp. 963–1021,
2024.
[12] K. Friston, L. D. Costa, N. Sajid, C. Heins, K. Ueltzh¨ offer, G. A. Pavli-
otis, and T. Parr, “The Free Energy Principle Made Simpler but not
Too Simple,”Physics Reports, vol. 1024, pp. 1–29, 2023.
[13] T. Parr, G. Pezzulo, and K. J. Friston,Active Inference: The Free En-
ergy Principle in Mind, Brain, and Behavior. MIT Press, 2022.
[14] A. Clark,Surfing Uncertainty: Prediction, Action, and the Embodied
Mind. Oxford / New York: Oxford University Press, 2015.
[15] A. K. Seth,Being You: A New Science of Consciousness. New York:
Dutton (Penguin Random House), 2021.
[16] M. Solms,The Hidden Spring: A Journey to the Source of Conscious-
ness. New York: W. W. Norton & Company, 2021.
[17] J. Locke,An Essay Concerning Human Understanding. New York: Ox-
ford University Press, 1689/2008.
[18] J. A. Hobson and K. J. Friston, “Consciousness, Dreams, and Inference:
The Cartesian Theatre Revisited,”Journal of Consciousness Studies,
vol. 21, no. 1-2, pp. 6–32, 2014.
[19] K. Friston, “Life as we know it,”Journal of the Royal Society Interface,
vol. 10, no. 86, 2013.
[20] R. A. Adams, S. Shipp, and K. J. Friston, “Predictions not Commands:
Active Inference in the Motor System,”Brain Structure and Function,
vol. 218, pp. 611 – 643, 2012.
[21] K. Friston, “The Mathematics of Mind-Time.”https://aeon.co/
essays/consciousness-is-not-a-thing-but-a-process-of-inference,
May 2017.
[22] D. Hafner, P. A. Ortega, J. Ba, T. Parr, K. Friston, and N. Heess,
“Action and Perception as Divergence Minimization,” Feb. 2022.
arXiv:2009.01791 [cs].
51
[23] K. P. Murphy,Probabilistic Machine Learning: Advanced Topics. Adap-
tive Computation and Machine Learning Series, Cambridge, Mas-
sachusetts: The MIT Press, 2023.
[24] E. Jaynes, “Information Theory and Statistical Mechanics,”Physical
Review, vol. 106, pp. 620–630, 1957.
[25] E. Jaynes, “Information Theory and Statistical Mechanics II,”Physical
Review, vol. 108, pp. 171–190, 1957.
[26] M. J. D. Ramstead, M. Albarracin, A. Kiefer, B. Klein, C. Fields,
K. Friston, and A. Safron, “The Inner Screen Model of Consciousness:
Applying the Free Energy Principle Directly to the Study of Conscious
Experience,” Jan. 2024. arXiv:2305.02205 [q-bio].
[27] H. Feldman and K. J. Friston, “Attention, Uncertainty, and Free-
Energy,”Frontiers in Human Neuroscience, vol. 4, p. 215, 2010.
[28] L. Sandved-Smith, C. Hesp, J. Mattout, K. Friston, A. Lutz, and
M. J. D. Ramstead, “Towards a Computational Phenomenology of
Mental Action: Modelling Meta-Awareness and Attentional Control
with Deep Parametric Active Inference,”Neuroscience of Conscious-
ness, vol. 2021, August 2021.
[29] K. J. Friston, L. D. Costa, A. Tschantz, A. Kiefer, T. Salvatori,
V. Neacsu, M. Koudahl, C. Heins, N. Sajid, D. Markovic, T. Parr,
T. Verbelen, and C. L. Buckley, “Supervised Structure Learning,” Nov.
2023. arXiv:2311.10300 [cs].
[30] K. Friston, C. Heins, T. Verbelen, L. D. Costa, T. Salvatori,
D. Markovic, A. Tschantz, M. Koudahl, C. Buckley, and T. Parr,
“From Pixels to Planning: Scale-Free Active Inference,” July 2024.
arXiv:2407.20292 [cs].
[31] B. Millidge, A. K. Seth, and C. L. Buckley, “Predictive Coding: A
Theoretical and Experimental Review,” 2021. arXiv:2107.12979 [cs].
[32] A. Dawid and Y. LeCun, “Introduction to Latent Variable Energy-Based
Models: A Path Toward Autonomous Machine Intelligence,”Journal of
Statistical Mechanics: Theory and Experiment, vol. 2024, Oct. 2024.
52
[33] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap, “Mastering Diverse
Domains through World Models,” Apr. 2024. arXiv:2301.04104 [cs].
[34] M. J. Beal,Variational Algorithms for Approximate Bayesian Inference.
PhD thesis, Gatsby Computational Neuroscience Unit, University Col-
lege London, 2003. Ph.D. Thesis.
[35] T. Parr, D. Markovi´ c, and K. J. Friston, “Neuronal Message Passing
using Mean-field, Bethe, and Marginal Approximations,”Scientific Re-
ports, vol. 9, no. 1, pp. 1–18, 2019.
[36] B. Millidge, A. Tschantz, and C. L. Buckley, “Whence the Expected
Free Energy?,”Neural Computation, vol. 33, no. 2, pp. 447–482, 2021.
[37] A. Tschantz, B. Millidge, A. K. Seth, and C. L. Buckley, “Reinforcement
Learning through Active Inference,” 2020. arXiv:2002.12636.
A Posterior Update Formulas
A.1 Predicting the Future
Fix a present timetand a timeτin the ranget+ 1, . . . , T. We derive the
formula for updatingq(s τ ) holding fixed the other posteriorsq(s τ′) (whereτ ′
takes values in the range 1, . . . , Tother thanτ).
By (3.6), to updateq(s τ ) we need to evaluate ˜q(sτ ) defined by
ln ˜q(sτ ) =E q(s\τ ,o) [lnp(s,o)].
Let us rewrite this as
ln ˜q(sτ ) =E q(s\τ ) [lnp(s)] +E q(s\τ ,o) [lnp(o|s)].(A.1)
To evaluate the first term on the right-hand side of (A.1), note that by (6.5),
Eq(s\τ ) [sτ′] =
(
sτ ifτ ′ =τ
qτ′ otherwise
and
Eq(s\τ ) [sτ′−1 lnB s τ′] =



s⊤
τ lnB q τ+1 ifτ ′ =τ−1
q⊤
τ−1 lnB s τ ifτ ′ =τ
q⊤
τ−1 lnB q τ otherwise
53
so that
Eq(s\τ ) [lnp(s)] =E q(s\τ )
"
s⊤
0 lnp 0 +
TX
τ′=1
s⊤
τ′−1 lnB s τ′
#
=
(
s⊤
τ
 
lnB ⊤qτ−1 + lnB qτ+1

+. . .(τ= 1, . . . , T−1)
s⊤
T lnB ⊤qT−1 +. . .(τ=T).
(A.2)
To evaluate the second term on the right-hand side of (A.1), note that since
we are assuming thatτ > t, (6.1) and (6.6) imply that
Eq(s\τ ,o) [lnp(o τ′|sτ′)] =
(
−s⊤
τ hifτ ′ =τ
−q⊤
τ′hifτ ′ ̸=τ
so that
Eq(s\τ ,o) [lnp(o|s)] =
TX
τ′=1
Eq(s\τ ,o) [lnp(o τ′|sτ′)]
=−s ⊤
τ h+. . . .(A.3)
Combining (A.2) and (A.3),
ln ˜q(sτ ) =
(
s⊤
τ
 
−h+ lnB ⊤qτ−1 + lnB qτ+1

+. . .(τ= 1, . . . , T−1)
s⊤
T
 
−h+ lnB ⊤qT−1

+. . .(τ=T)
so that, by (3.6), the update formula forq τ is
lnq τ =
(
−h+ lnB ⊤qτ−1 + lnB qτ+1 +. . .(τ= 1, . . . , T−1)
−h+ lnB ⊤qT−1 +. . .(τ=T). (A.4)
ifτ > t. We refer to this as theprediction update formula.
A.2 Retrodicting the Past
Now fix a present timetand a timeτin the range 1, . . . , t. We derive the
formula for updatingq(s τ ) holding fixed the other posteriorsq(s τ′) (whereτ ′
takes any value in the range 1, . . . , Tother thanτ).
54
As in (A.1), we need to evaluate ˜q(sτ ) where
ln ˜q(sτ ) =E q(s\τ ) [lnp(s)] +E q(s\τ ,o) [lnp(o|s)] (A.5)
The first term on the right-hand side of (A.5) is handled in the same way as
(A.2) so that
Eq(s\τ ) [lnp(s)] =E q(s\τ )
"
s⊤
0 lnp 0 +
TX
τ′=1
s⊤
τ′−1 lnB s τ′
#
=
(
s⊤
τ
 
lnB ⊤qτ−1 + lnB qτ+1

+. . .(τ= 1, . . . , T−1)
s⊤
T lnB ⊤qT−1 +. . .(τ=T).
(A.6)
To evaluate the second term on the right-hand side of (A.5), note that since
we are assuming thatτ≤t, (6.1) implies that
Eq(s\τ ,o) [lnp(o τ′|sτ′)] =
(
s⊤
τ lnA ¯oτ (τ′ =τ)
q⊤
τ′ lnA ¯oτ′ (τ′ ̸=τ)
so that
Eq(s\τ ,o) [lnp(o|s)] =
TX
τ′=1
Eq(s\τ ,o) [lnp(o τ′|sτ′)]
=s ⊤
τ lnA ¯oτ +. . . .(A.7)
Combining (A.6) and (A.7),
ln ˜q(sτ ) =
(
s⊤
τ
 
lnA ¯oτ + lnB⊤qτ−1 + lnB qτ+1

+. . .(τ= 1, . . . , T−1)
s⊤
T
 
lnA ¯oT + lnB⊤qT−1

+. . .(τ=T)
and the update formula forq τ is
lnq τ =
(
lnA ¯oτ + lnB⊤qτ−1 + lnB qτ+1 . . .(τ= 1, . . . , T−1)
lnA ¯oT + lnB⊤qT−1 +. . .(τ=T). (A.8)
ifτ≤t. We refer to this as theretrodiction update formula.
55
A.3 Evaluating the Divergence
After updating all of the posteriors at the present timet, we can evaluate
the divergence KL [q(s,o)∥p(s,o)] by rewriting it as
KL [q(s,o)∥p(s,o)]
=E q(s,o) [lnq(s,o)−lnp(s,o)]
=E q(s) [lnq(s)−lnp(s)] +E q(s,o) [lnq(o|s)−lnp(o|s)] (A.9)
and evaluating these expectations separately. Thus
Eq(s) [lnq(s)−lnp(s)]
=E q(s)
" TX
τ=0
s⊤
τ lnq τ −s ⊤
0 lnp 0 −
TX
τ=1
s⊤
τ−1 lnB s τ
#
=
TX
τ=0
q⊤
τ lnq τ −q ⊤
0 lnp 0 −
TX
τ=1
q⊤
τ−1 lnB q τ
=
TX
τ=1
q⊤
τ lnq τ −
TX
τ=1
q⊤
τ−1 lnB q τ
since we are assuming thatq 0 =p 0. And
Eq(s,o) [lnq(o|s)−lnp(o|s)]
=
tX
τ=1
Eq(sτ ,oτ ) [lnq(o τ |sτ )−lnp(o τ |sτ )]
=−
tX
τ=1
Eq(sτ ) [lnp( ¯oτ |sτ )] by (6.1)
=−
tX
τ=1
Eq(sτ )

s⊤
τ lnA ¯oτ

=−
tX
τ=1
q⊤
τ lnA ¯oτ by (6.5).
So (A.9) gives
KL [q(s,o)∥p(s,o)]
=
TX
τ=1
q⊤
τ lnq τ −
tX
τ=1
q⊤
τ lnA ¯oτ −
TX
τ=1
q⊤
τ−1 lnB q τ .(A.10)
56

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Active Inference in Discrete State Spaces from First Principles"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * FIX SPACING ISSUES FROM PDF EXTRACTION:
       - PDF extraction may remove spaces between words (e.g., 'dynamicssimulationsand' → 'dynamics simulations and')
       - When extracting quotes, restore missing spaces between words if they appear concatenated
       - Look for patterns like 'word1word2word3' and add spaces: 'word1 word2 word3'
       - This is a common issue from PDF text extraction that needs correction
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written (with spacing normalized)
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)
   - FIX SPACING ISSUES FROM PDF EXTRACTION:
     * PDF extraction may remove spaces between words (e.g., 'dynamicssimulationsand' → 'dynamics simulations and')
     * When extracting quotes, restore missing spaces between words if they appear concatenated
     * Look for patterns like 'word1word2word3' and add spaces: 'word1 word2 word3'
     * This is a common issue from PDF text extraction that needs correction

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
