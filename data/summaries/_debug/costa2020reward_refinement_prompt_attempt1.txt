=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: Reward Maximisation through Discrete Active Inference
Citation Key: costa2020reward
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. CRITICAL: The current summary has severe repetition issues. You MUST eliminate all repeated sentences, phrases, and paragraphs. Each idea should be expressed only once. If you find yourself repeating content, remove the duplicates entirely. Focus on variety and uniqueness in your wording.
2. Severe repetition detected: Same sentence appears 55 times (severe repetition)

Current draft (first 2000 chars):
Reward Maximisation through Discrete Active InferenceThis paper investigates the application of active inference to maximise reward in a partially observable environment. Active inference, a computational framework for modelling agency and control, provides a robust approach to solving complex problems where the agent’s actions and the environment’s dynamics are not fully known. The authors demonstrate that active inference can be used to learn and execute policies that maximise reward, offering a novel approach to reinforcement learning.The authors state: “Active inference provides a robust approach to solving complex problems where the agent’s actions and the environment’s dynamics are not fully known.” They note: “The authors state: “Active inference provides a robust approach to solving complex problems where the agent’s actions and the environment’s dynamics are not fully known.” The paper argues: “The authors state: “Active inference provides a robust approach to solving complex problems where the agent’s actions and the environment’s dynamics are not fully known.”The authors state: “Active inference provides a robust approach to solving complex problems where the agent’s actions and the environment’s dynamics are not fully known.”The authors state: “Active inference provides a robust approach to solving complex problems where the agent’s actions and the environment’s dynamics are not fully known.”The authors state: “Active inference provides a robust approach to solving complex problems where the agent’s actions and the environment’s dynamics are not fully known.”The authors state: “Active inference provides a robust approach to solving complex problems where the agent’s actions and the environment’s dynamics are not fully known.”The authors state: “Active inference provides a robust approach to solving complex problems where the agent’s actions and the environment’s dynamics are not fully known.”The authors state: “Active inference provides a robust approach...

Key terms: learning, imperial, active, costa, optimal, reward, discrete, london

=== FULL PAPER TEXT ===
Reward Maximisation through Discrete Active Inference
Lancelot Da Costa l.da-costa@imperial.ac.uk
Department of Mathematics
Imperial College London
London, SW7 2AZ, UK
Noor Sajid noor.sajid.18@ucl.ac.uk
Thomas Parr thomas.parr.12@ucl.ac.uk
Karl Friston k.friston@ucl.ac.uk
Wellcome Centre for Human Neuroimaging
University College London
London, WC1N 3AR, UK
Ryan Smith rsmith@laureateinstitute.org
Laureate Institute for Brain Research
Tulsa, OK 74136, United States
Abstract
Activeinferenceisaprobabilisticframeworkformodellingthebehaviourofbiologicalandartificial
agents,whichderivesfromtheprincipleofminimisingfreeenergy. Inrecentyears,thisframework
has successfully been applied to a variety of situations where the goal was to maximise reward,
offeringcomparableandsometimessuperiorperformancetoalternativeapproaches. Inthispaper,
we clarify the connection between reward maximisation and active inference by demonstrating
how and when active inference agents perform actions that are optimal for maximising reward.
Precisely, we show the conditions under which active inference produces the optimal solution to
the Bellman equation—a formulation that underlies several approaches to model-based reinforce-
ment learning and control. On partially observed Markov decision processes, the standard active
inferenceschemecanproduceBellmanoptimalactionsforplanninghorizonsof1,butnotbeyond.
In contrast, a recently developed recursive active inference scheme (sophisticated inference) can
produce Bellman optimal actions on any finite temporal horizon. We append the analysis with a
discussion of the broader relationship between active inference and reinforcement learning.
Keywords: Generativemodel;controlasinference;dynamicprogramming;Bellmanoptimality;
model-based reinforcement learning; discrete-time stochastic optimal control; Bayesian inference;
Markov decision process
Contents
1 Introduction 2
1.1 Active inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Reward maximisation through active inference? . . . . . . . . . . . . . . . . . . . . . 3
1.3 Organisation of paper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Reward maximisation on finite horizon MDPs 4
2.1 Basic definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.2 Bellman optimal state-action policies . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.3 Backward induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3 Active inference on finite horizon MDPs 7
3.1 Perception as inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.2 Planning as inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1
2202
luJ
11
]IA.sc[
4v11180.9002:viXra
4 Reward maximisation on MDPs through active inference 9
4.1 Reward maximisation as reaching preferences . . . . . . . . . . . . . . . . . . . . . . 9
4.2 Reward maximisation on MDPs with a temporal horizon of 1 . . . . . . . . . . . . . 10
4.3 Reward maximisation on MDPs with finite temporal horizons . . . . . . . . . . . . . 12
5 Generalisation to POMDPs 13
5.1 Active inference on finite horizon POMDPs . . . . . . . . . . . . . . . . . . . . . . . 13
5.1.1 Perception as inference. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
5.1.2 Planning as inference. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
5.2 Maximising reward on POMDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
6 Discussion 16
6.1 Decision-making beyond reward maximisation . . . . . . . . . . . . . . . . . . . . . . 16
6.2 Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
6.3 Scaling active inference. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
7 Conclusion 18
A Active inference and reinforcement learning 28
A.1 Main differences between active inference and reinforcement learning . . . . . . . . . 29
A.2 Reward learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
A.3 Solving the exploration-exploitation dilemma . . . . . . . . . . . . . . . . . . . . . . 32
B Proofs 33
B.1 Proof of Proposition 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
B.2 Proof of Proposition 10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
B.3 Proof of Proposition 11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
B.4 Proof of Lemma 14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
B.5 Proof of Theorem 15 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
B.6 Proof of Theorem 16 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
B.7 Proof of Proposition 18 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
1. Introduction
1.1 Active inference
Active inference is a normative framework for modelling intelligent behaviour in biological and
artificial agents. It simulates behaviour by numerically integrating equations of motion thought to
describe the behaviour of biological systems, a description based on the free energy principle (Barp
et al., 2022; Friston, 2010; Friston et al., 2022; Ramstead et al., 2022). Active inference comprises
a collection of algorithms for modelling perception, learning, and decision-making in the context of
both continuous and discrete state spaces (Barp et al., 2022; Buckley et al., 2017; Da Costa et al.,
2020;Fristonetal.,2021,2010,2017c). Briefly,buildingactiveinferenceagentsentails: 1)equipping
theagentwitha(generative)modeloftheenvironment,2)fittingthemodeltoobservationsthrough
approximate Bayesian inference by minimising variational free energy (i.e., optimising an evidence
lower bound (Beal, 2003; Bishop, 2006; Blei et al., 2017; Jordan et al., 1998)) and 3) selecting
actions that minimise expected free energy, a quantity that that can be decomposed into risk (i.e.,
the divergence between predicted and preferred paths) and ambiguity, leading to context-specific
combinationsofexploratoryandexploitativebehaviour(Millidge,2021;Schwartenbecketal.,2019).
Thisframeworkhasbeenusedtosimulateandexplainintelligentbehaviourinneuroscience(Adams
etal.,2013;Parr,2019;Parretal.,2021;Sajidetal.,2022),psychologyandpsychiatry(Smithetal.,
2020a,b,d, 2021a,b,c,d, 2022b), machine learning (Çatal et al., 2020; Fountas et al., 2020; Mazzaglia
2
et al., 2021; Millidge, 2020; Tschantz et al., 2019, 2020a) and robotics (Çatal et al., 2021; Lanillos
et al., 2020; Oliver et al., 2021; Pezzato et al., 2020; Pio-Lopez et al., 2016; Sancaktar et al., 2020;
Schneider et al., 2022).
1.2 Reward maximisation through active inference?
In contrast, the traditional approaches towards simulating and explaining intelligent behaviour—
stochastic optimal control (Bellman, 1957; Bertsekas and Shreve, 1996) and reinforcement learning
(RL;BartoandSutton(1992))—derivefromthenormativeprincipleofexecutingactionstomaximise
rewardscoringtheutilityaffordedbyeachstateoftheworld. Thisideadatesbacktoexpectedutility
theory (Von Neumann and Morgenstern, 1944), an economic model of rational choice behaviour,
which also underwrites game theory (Von Neumann and Morgenstern, 1944) and decision theory
(Berger,1985;DayanandDaw,2008). Severalempiricalstudieshaveshownthatactiveinferencecan
successfullyperformtasksthatinvolvecollectingreward,often(butnotalways)showingcomparative
or superior performance to RL (Cullen et al., 2018; Marković et al., 2021; Mazzaglia et al., 2021;
Millidge, 2020; Paul et al., 2021; Sajid et al., 2021a; Smith et al., 2020d, 2021b,c, 2022b; van der
Himst and Lanillos, 2020), and marked improvements when interacting with volatile environments
(Marković et al., 2021; Sajid et al., 2021a). Given the prevalence and historical pedigree of reward
maximisation, we ask:
How and when do active inference agents execute actions that are optimal with respect to reward
maximisation?
1.3 Organisation of paper
Inthispaper,weexplain(andprove)howandwhenactiveinferenceagentsexhibit(Bellman)optimal
reward maximising behaviour.
Forthis,westartbyrestrictingourselvestothesimplestproblem: maximisingrewardonafinite
horizon Markov decision process (MDP) with known transition probabilities—a sequential decision-
makingtaskwithcompleteinformation. Inthissetting,wereviewthebackwardinductionalgorithm
from dynamic programming, which forms the workhorse of many optimal control and model-based
RLalgorithms. ThisalgorithmfurnishesaBellmanoptimalstate-actionmapping,whichmeansthat
it provides provably optimal decisions from the point of view of reward maximisation (Section 2).
We then introduce active inference on finite horizon MDPs (Section 3)—a scheme consisting of
perception as inference followed by planning as inference, which selects actions so that future states
best align with preferred states.
In Section 4, we show how and when active inference maximises reward in MDPs. Specifically,
whenthepreferreddistributionisa(uniformmixtureof)Diracdistribution(s)overrewardmaximis-
ing trajectories, selecting action sequences according to active inference maximises reward (Section
4.1). Yet, active inference agents, in their standard implementation, can select actions that max-
imise reward only when planning one step ahead (Section 4.2). It takes a recursive, sophisticated
form of active inference to select actions that maximise reward—in the sense of a Bellman optimal
state-action mapping—on any finite time-horizon (Section 4.3).
InSection5,weintroduceactiveinferenceonpartiallyobservableMarkovdecisionprocesseswith
known transition probabilities—a sequential decision-making task where states need to be inferred
from observations—and explain how the results from the MDP setting generalise to this setting.
Our findings are summarised in Section 7.
All of our analyses assume that the agent knows the environmental dynamics (i.e., transition
probabilities) and reward function. In Appendix A, we discuss how active inference agents can
learn their world model and rewarding states when these are initially unknown—and the broader
relationship between active inference and RL.
3
Figure 1: Finite horizon Markov decision process. This is a Markov decision process pictured
as a Bayesian network (Jordan et al., 1998; Pearl, 1998). A finite horizon MDP comprises
a finite sequence of states, indexed in time. The transition from one state to the next
state depends on action. As such, for any given action sequence, the dynamics of the
MDP form a Markov chain on state-space. In this fully observed setting, actions can be
selectedunderastate-actionpolicy,Π,indicatedwithadashedline: thisisaprobabilistic
mapping from state-space and time to actions.
2. Reward maximisation on finite horizon MDPs
Inthissection,weconsidertheproblemofrewardmaximisationinMarkovdecisionprocesses(MDPs)
with known transition probabilities.
2.1 Basic definitions
MDPsareaclassofmodelsspecifyingenvironmentaldynamicswidelyusedindynamicprogramming,
model-based RL, and more broadly in engineering and artificial intelligence (Barto and Sutton,
1992; Stone, 2019). They are used to simulate sequential decision-making tasks with the objective
of maximising a reward or utility function. An MDP specifies environmental dynamics unfolding in
discrete space and time under the actions pursued by an agent.
Definition 1 (Finite horizon MDP). A finite horizon MDP comprises the following collection of
data:
• S a finite set of states.
• T = {0,...,T} a finite set which stands for discrete time. T is the temporal horizon (a.k.a.
planning horizon).
• A is a finite set of actions.
• P(s =s(cid:48) |s =s,a =a) is the probability that action a∈A in state s∈S at time t−1
t t−1 t−1
will lead to state s(cid:48) ∈S at time t. s are random variables over S that correspond to the state
t
being occupied at time t=0,...,T.
• P(s =s) specifies the probability of being at state s∈S at the start of the trial.
0
• R(s) is the finite reward received by the agent when at state s∈S.
The dynamics afforded by a finite horizon MDP (see Figure 1) can be written globally as a proba-
bility distribution over state trajectories s := (s ,...,s ), given a sequence of actions a :=
0:T 0 T 0:T−1
(a ,...,a ), which factorises as follows:
0 T−1
T
(cid:89)
P(s |a )=P(s ) P(s |s ,a ).
0:T 0:T−1 0 τ τ−1 τ−1
τ=1
4
Remark 2 (On the definition of reward). More generally, the reward function can be taken to be
dependent on the previous action and previous state: R (s(cid:48) |s) is the reward received after transi-
a
tioning from state s to state s(cid:48), due to action a (Barto and Sutton, 1992; Stone, 2019). However,
given an MDP with such a reward function, we can recover our simplified setting by defining a new
MDP where the new states comprise the previous action, previous state, and current state in the
original MDP. By inspection, the resulting reward function on the new MDP depends only on the
current state (i.e., R(s)).
Remark 3 (Admissible actions). In general, it is possible that only some actions can be taken at
each state. In this case, one defines A to be the finite set of (allowable) actions from state s ∈ S.
s
All forthcoming results concerning MDPs can be extended to this setting.
To formalise what it means to choose actions in each state, we introduce the notion of a state-
action policy.
Definition4(State-actionpolicy). Astate-actionpolicyΠisaprobabilitydistributionoveractions,
that depends on the state that the agent occupies, and time. Explicitly,
Π:A×S×T→[0,1]
(a,s,t)(cid:55)→Π(a|s,t)
(cid:88)
∀(s,t)∈S×T: Π(a|s,t)=1.
a∈A
When s =s, we will write Π(a|s ):=Π(a|s,t). Note that the action at the temporal horizon T is
t t
redundant, as no further reward can be reaped from the environment. Therefore, one often specifies
state-action policies only up to time T −1, as Π:A×S×{0,...,T −1}→[0,1]. The state-action
policy—as defined here—can be regarded as a generalisation of a deterministic state-action policy
that assigns the probability of 1 to an available action and 0 otherwise.
Remark 5 (Conflicting terminologies: policy in active inference). In active inference, a policy is
defined as a sequence of actions indexed in time1. To avoid terminological confusion, we use action
sequences to denote policies under active inference.
At time t, the goal is to select an action that maximises future cumulative reward:
T
(cid:88)
R(s ):= R(s ).
t+1:T τ
τ=t+1
Specifically,thisentailsfollowingastate-actionpolicyΠthatmaximisesthestate-valuefunction:
v (s,t):=E [R(s )|s =s]
Π Π t+1:T t
for any (s,t) ∈ S×T. The state-value function scores the expected cumulative reward if the
agent pursues state-action policy Π from the state s = s. When the state s = s is clear from
t t
context, we will often write v (s ) := v (s,t). Loosely speaking, we will call the expected reward
Π t Π
the return.
Remark 6 (Notation E ). Whilst standard in RL (Barto and Sutton, 1992; Stone, 2019), the
Π
notationE [R(s )|s =s]canbeconfusing. Itdenotestheexpectedreward,underthetransition
Π t+1:T t
probabilities of the MDP and a state-action policy Π
E [R(s )].
P(st+1:T|at:T−1,st=s)Π(at:T−1|st+1:T−1,st=s) t+1:T
It is important to keep this correspondence in mind, as we will use both notations depending on
context.
1. These are analogous to temporally extended actions or options introduced under the options framework in
RL(StolleandPrecup,2002).
5
Remark 7 (Temporal discounting). In infinite horizon MDPs (i.e., when T is infinite), RL often
seeks to maximise the discounted sum of rewards
(cid:34) ∞ (cid:35)
(cid:88)
v (s,t):=E γτ−tR(s )|s =s ,
Π Π τ+1 t
τ=t
for a given temporal discounting term γ ∈ (0,1) (Barto and Sutton, 1992; Bertsekas and Shreve,
1996;Kaelblingetal.,1998). Infact, temporaldiscountingisaddedtoensurethattheinfinitesumof
future rewards converges to a finite value (Kaelbling et al., 1998). In finite horizon MDPs temporal
discounting is not necessary so we set γ =1 (c.f., (Schmidhuber, 2006, 2010)).
To find the best state-action policies, we would like to rank them in terms of their return. We
introduceapartialorderingsuchthatastate-actionpolicyisbetter thananotherifityieldsahigher
return in any situation:
Π≥Π(cid:48) ⇐⇒ ∀(s,t)∈S×T:v (s,t)≥v (s,t).
Π Π(cid:48)
Similarly, astate-actionpolicyΠisstrictly better thananotherΠ(cid:48) ifityieldsstrictlyhigherreturns:
Π>Π(cid:48) ⇐⇒ Π≥Π(cid:48) and ∃(s,t)∈S×T:v (s,t)>v (s,t).
Π Π(cid:48)
2.2 Bellman optimal state-action policies
A state-action policy is Bellman optimal if it is better than all alternatives.
Definition 8 (Bellman optimality). A state-action policy Π∗ is Bellman optimal if and only if it is
better than all other state-action policies:
Π∗ ≥Π,∀Π.
In other words, it maximises the state-value function v (s,t) for any state s at time t.
Π
It is important to verify that this concept is not vacuous.
Proposition 9 (Existence of Bellman optimal state-action policies). Given a finite horizon MDP
as specified in Definition 1, there exists a Bellman optimal state-action policy Π∗.
AproofcanbefoundinAppendixB.1. NotethatuniquenessoftheBellmanoptimalstate-action
policy is not implied by Proposition 9; indeed, multiple Bellman optimal state-action policies may
exist (Bertsekas and Shreve, 1996; Puterman, 2014).
Now that we know that Bellman optimal state-action policies exist, we can characterise them as
a return-maximising action followed by a Bellman optimal state-action policy.
Proposition 10 (Characterisation of Bellman optimal state-action policies). For a state-action
policy Π, the following are equivalent:
1. Π is Bellman optimal.
2. Π is both
(a) Bellman optimal when restricted to {1,...,T}. In other words, ∀ state-action policy Π(cid:48)
and (s,t)∈S×{1,...T}
v (s,t)≥v (s,t).
Π Π(cid:48)
(b) At time 0, Π selects actions that maximise return:
Π(a|s,0)>0 ⇐⇒ a∈argmaxE [R(s )|s =s,a =a], ∀s∈S. (1)
Π 1:T 0 0
a∈A
A proof can be found in Appendix B.2. Note that this characterisation offers a recursive way to
constructBellmanoptimalstate-actionpoliciesbysuccessivelyselectingthebestaction,asspecified
by (1), starting from T and inducting backwards (Puterman, 2014).
6
2.3 Backward induction
Proposition 10 suggests a straightforward recursive algorithm to construct Bellman optimal state-
action policies known as backward induction (Puterman, 2014). Backward induction has a long
history. It was developed by the German mathematician Zermelo in 1913 to prove that chess has
Bellman optimal strategies (Zermelo, 1913). In stochastic control, backward induction is one of the
mainmethodsforsolvingtheBellmanequation(AddaandCooper,2003;MirandaandFackler,2002;
Sargent, 2000). In game theory, the same method is used to compute sub-game perfect equilibria in
sequential games (Fudenberg and Tirole, 1991).
Backwardinductionentailsplanningbackwardsintime,fromagoalstateattheendofaproblem,
byrecursivelydeterminingthesequenceofactionsthatenablesreachingthegoal. Itproceedsbyfirst
consideringthelasttimeatwhichadecisionmightbemadeandchoosingwhattodoinanysituation
at that time in order to get to the goal state. Using this information, one can then determine what
todoatthesecond-to-lastdecisiontime. Thisprocesscontinuesbackwardsuntilonehasdetermined
the best action for every possible situation or state at every point in time.
Proposition11(Backwardinduction: constructionofBellmanoptimalstate-actionpolicies). Back-
ward induction
Π(a|s,T −1)>0 ⇐⇒ a∈argmaxE[R(s )|s =s,a =a], ∀s∈S
T T−1 T−1
a∈A
Π(a|s,T −2)>0 ⇐⇒ a∈argmaxE [R(s )|s =s,a =a], ∀s∈S
Π T−1:T T−2 T−2
a∈A
(2)
.
.
.
Π(a|s,0)>0 ⇐⇒ a∈argmaxE [R(s )|s =s,a =a], ∀s∈S
Π 1:T 0 0
a∈A
defines a Bellman optimal state-action policy Π. Furthermore, this characterisation is complete: all
Bellman optimal state-action policies satisfy the backward induction relation (2).
A proof can be found in Appendix B.3.
Example 1 (Intuitionforbackwardinduction). Togiveaconcreteexampleofthiskindofplanning,
backward induction (2) would consider the actions below in the following order:
1. Desired goal: I would like to go to the grocery store,
2. Intermediate action: I need to drive to the store,
3. Current best action: I should put my shoes on.
Proposition11tellsusthattobeoptimalwithrespecttorewardmaximisation,onemustplanlike
backward induction. This will be central to our analysis of reward maximisation in active inference.
3. Active inference on finite horizon MDPs
We now turn to introducing active inference agents on finite horizon MDPs with known transition
probabilities. We assume that the agent’s generative model of its environment is given by the
previously defined finite horizon MDP (see Definition 1). We do not consider the case where the
transitions have to be learned but comment on it in the Appendix A.2 (see also (Da Costa et al.,
2020; Friston et al., 2016)).
In what follows, we fix a time t≥0 and suppose that the agent has been in states s ,...,s . To
0 t
ease notation, we let (cid:126)s := s ,(cid:126)a := a be the future states and future actions. We define Q to
t+1:T t:T
be the predictive distribution, which encodes the predicted future states and actions given that the
agent is in state s
t
T−1
(cid:89)
Q((cid:126)s,(cid:126)a|s ):= Q(s |a ,s )Q(a |s ).
t τ+1 τ τ τ τ
τ=t
7
3.1 Perception as inference
Inactiveinference,perceptionentailsinferencesaboutfuture,past,andcurrentstatesgivenobserva-
tionsandasequenceofactions. Whenstatesarepartiallyobserved, thisisdonethroughvariational
Bayesian inference by minimising a free energy functional (a.k.a. an evidence bound (Beal, 2003;
Bishop, 2006; Blei et al., 2017; Wainwright and Jordan, 2007)).
IntheMDPsetting,pastandcurrentstatesareknown,soitisonlynecessarytoinferfuturestates
giventhecurrentstateandactionsequenceP((cid:126)s|(cid:126)a,s ). TheseposteriordistributionsP((cid:126)s|(cid:126)a,s )can
t t
be computed exactly in virtue of the fact that the transition probabilities of the MDP are known;
hence variational inference becomes exact Bayesian inference.
T−1
(cid:89)
Q((cid:126)s|(cid:126)a,s ):=P((cid:126)s|(cid:126)a,s )= P(s |s ,a ). (3)
t t τ+1 τ τ
τ=t
3.2 Planning as inference
Nowthattheagenthasinferredfuturestatesgivenalternativeactionsequences,wemustassessthese
alternative plans by examining the resulting state trajectories. The objective that active inference
agentsoptimise—inordertoselectthebestpossibleactions—istheexpectedfree energy (Barpetal.,
2022; Da Costa et al., 2020; Friston et al., 2021). Under active inference, agents minimise expected
freeenergyinordertomaintainthemselvesdistributedaccordingtoatargetdistributionC overthe
state-space S encoding the agent’s preferences.
Definition 12 (Expected free energy on MDPs). On MDPs, the expected free energy of an action
sequence(cid:126)a starting from s is defined as (Barp et al., 2022):
t
G((cid:126)a|s )=D [Q((cid:126)s|(cid:126)a,s )|C((cid:126)s)] (4)
t KL t
Therefore, minimising expected free energy corresponds to making the distribution over predicted
states close to the distribution C that encodes prior preferences. Note that the expected free energy
in partiallyobservedMDPscomprisesanadditionalambiguityterm(seeSection5),whichisdropped
here as there is no ambiguity about observed states.
Since the expected free energy assesses the goodness of inferred future states under a course of
action, we can refer to planning as inference (Attias, 2003; Botvinick and Toussaint, 2012). The
expected free energy may be rewritten as
G((cid:126)a|s )=E [−logC((cid:126)s)]− H[Q((cid:126)s|(cid:126)a,s )] (5)
t Q((cid:126)s|(cid:126)a,st) t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Expectedsurprise Entropyoffuturestates
Hence,minimisingexpectedfreeenergyminimisestheexpectedsurpriseofstates2accordingtoC and
maximises the entropy of Bayesian beliefs over future states (a maximum entropy principle (Jaynes,
1957a), which is sometimes cast as keeping options open (Klyubin et al., 2008)).
Remark 13 (Numerical tractability). The expected free energy is straightforward to compute using
linear algebra. Given an action sequence (cid:126)a, C((cid:126)s) and Q((cid:126)s |(cid:126)a,s ) are categorical distributions over
t
ST−t. Let their parameters be c,s ∈[0,1]|S|(T−1), where |·| denotes the cardinality of a set. Then
(cid:126)a
the expected free energy reads
G((cid:126)a|s )=sT(logs −logc). (6)
t (cid:126)a (cid:126)a
2. Thesurprise(a.k.a. selfinformationorsurprisal)ofstates−logC((cid:126)s)isinformationtheoreticnomenclature(Stone,
2015) that scores the extent to which an observation is unusual under C. It does not imply that the agent
experiencessurpriseinasubjectiveordeclarativesense.
8
Notwithstanding, (6) is expensive to evaluate repeatedly when all possible action sequences are con-
sidered. In practice, one can adopt a temporal mean field approximation over future states (Millidge
et al., 2020a):
T
(cid:89)
Q((cid:126)s|(cid:126)a,s )≈ Q(s |(cid:126)a,s ),
t τ t
τ=t+1
which yields the simplified expression
T
(cid:88)
G((cid:126)a|s )≈ D [Q(s |(cid:126)a,s )|C(s )]. (7)
t KL τ t τ
τ=t+1
Expression (7) is much easier to handle: for each action sequence (cid:126)a, 1) one evaluates the sum-
mands sequentially τ =t+1,...,T, and 2) if and when the sum up to τ becomes significantly higher
than the lowest expected free energy encountered during planning, G((cid:126)a | s ) is set to an arbitrarily
t
high value. Setting G((cid:126)a|s ) to a high value is equivalent to pruning away unlikely trajectories. This
t
bears some similarity to decision tree pruning procedures used in RL (Huys et al., 2012). It finesses
exploration of the decision-tree in full depth and provides an Occam’s window for selecting action
sequences.
Complementary approaches can help make planning tractable. For example, hierarchical gener-
ative models factorise decisions into multiple levels. By abstracting information at a higher-level,
lower-levels entertain fewer actions (Friston et al., 2018)—which reduces the depth of the decision
tree by orders of magnitude. Another approach is to use algorithms that search the decision-tree
selectively, such as Monte-Carlo tree search (Champion et al., 2021a,b; Fountas et al., 2020; Maisto
et al., 2021; Silver et al., 2016), and amortising planning using artificial neural networks (i.e.,
learning to plan) (Çatal et al., 2020; Fountas et al., 2020; Sajid et al., 2021b).
4. Reward maximisation on MDPs through active inference
Here, we show how active inference solves the reward maximisation problem.
4.1 Reward maximisation as reaching preferences
From the definition of expected free energy (4), active inference on MDPs can be thought of as
reaching and remaining at a target distribution C over state-space.
The idea that underwrites this section is that when the stationary distribution has all of its
mass on reward maximising states, the agent will maximise reward. To illustrate this, we define a
preferencedistributionC ,β >0overstate-spaceS,suchthatpreferredstatesarerewardingstates3
β
expβR(σ)
C (σ):= ∝exp(βR(σ)), ∀σ ∈S
β (cid:80) expβR(ς)
ς∈S
⇐⇒ −logC (σ)=−βR(σ)−c(β), ∀σ ∈S, for some c(β)∈R constant w.r.t σ.
β
The (inverse temperature) parameter β >0 scores how motivated the agent is to occupy reward
maximising states. Note that states s ∈ S that maximise the reward R(s) maximise C (s) and
β
minimise −logC (s) for any β >0.
β
Usingtheadditivepropertyoftherewardfunction,wecanextendC toaprobabilitydistribution
β
over trajectories (cid:126)σ := (σ ,...,σ ) ∈ ST. Specifically, C scores to what extent a trajectory is
1 T β
preferred over another trajectory:
3. Notetheconnectionwithstatisticalmechanics: βisaninversetemperatureparameter,−Risapotentialfunction
andC isthecorrespondingGibbsdistribution(Pavliotis,2014;RahmeandAdams,2019).
β
9
T T
C ((cid:126)σ):= expβR((cid:126)σ) = (cid:89) expβR(σ τ ) = (cid:89) C (σ ), ∀(cid:126)σ ∈ST
β (cid:80) expβR((cid:126)ς) (cid:80) expβR(ς) β τ
(cid:126)ς∈ST
τ=1
ς∈S
τ=1 (8)
T
(cid:88)
⇐⇒ −logC ((cid:126)σ)=−βR((cid:126)σ)−c(cid:48)(β)=− βR(σ )−c(cid:48)(β), ∀(cid:126)σ ∈ST,
β τ
τ=1
where c(cid:48)(β):=c(β)T ∈R is constant w.r.t (cid:126)σ.
When the preferences are defined in this way, the zero-temperature limit β → +∞ is the case
where the preferences C are non-zero only for states or trajectories that maximise reward. In this
β
case, lim C is a uniform mixture of Dirac distributions over reward maximising trajectories:
β→+∞ β
(cid:88)
lim C ∝ Dirac
β (cid:126)s
β→+∞
(cid:126)s∈IT−t (9)
I :=argmaxR(s).
s∈S
This is because, for a reward maximising state σ, exp(βR(σ)) will converge to +∞ more quickly
thanexp(βR(σ(cid:48)))foranon-rewardmaximisingstateσ(cid:48). SinceC isconstrainedtobenormalisedto
β
1(asitisaprobabilitydistribution),C (σ(cid:48))− β − → −− + − ∞ →0. Hence,inthelimitβ →+∞,C isnon-zero
β β
(and uniform) only on reward maximising states.
We now show how reaching preferred states can be formulated as reward maximisation:
Lemma 14. The sequence of actions that minimises expected free energy also maximises expected
reward in the zero temperature limit β →+∞ (9):
lim argminG((cid:126)a|s )⊆argmaxE [R((cid:126)s)]
β→+∞ (cid:126)a
t
(cid:126)a
Q((cid:126)s|(cid:126)a,st)
Furthermore, of those action sequences that maximise expected reward, the expected free energy
minimisers will be those that maximise the entropy of future states H[Q((cid:126)s|(cid:126)a,s )].
t
A proof can be found in Appendix B.4. In the zero temperature limit β → +∞, minimising
expected free energy corresponds to choosing the action sequence(cid:126)a such that Q((cid:126)s |(cid:126)a,s ) has most
t
mass on reward maximising states or trajectories (see Figure 2). Of those reward maximising
candidates, the minimiser of expected free energy maximises the entropy of future states H[Q((cid:126)s |
(cid:126)a,s )], thus leaving options open.
t
4.2 Reward maximisation on MDPs with a temporal horizon of 1
Inthissection,wefirstconsiderthecaseofasingle-stepdecisionproblem(i.e.,atemporalhorizonof
T =1)anddemonstratehowthestandardactiveinferenceschememaximisesrewardonthisproblem
inthelimitβ →+∞. Thiswillactasanimportantbuildingblockforwhenwesubsequentlyconsider
more general multi-step decision problems.
The standard decision-making procedure in active inference consists of assigning each action
sequence with a probability given by the softmax of the negative expected free energy (Barp et al.,
2022; Da Costa et al., 2020; Friston et al., 2017a)
Q((cid:126)a|s )∝exp(−G((cid:126)a|s )).
t t
Agents then select the most likely action under this distribution
(cid:88)
a ∈argmaxQ(a|s )=argmax Q(a|(cid:126)a)Q((cid:126)a|s )
t t t
a∈A a∈A
(cid:126)a
(cid:88) (cid:88)
=argmax Q(a|(cid:126)a)exp(−G((cid:126)a|s ))=argmax exp(−G((cid:126)a|s )).
t t
a∈A a∈A
(cid:126)a (cid:126)a
((cid:126)a) =a
t
10
Figure 2: Reaching preferences and the zero temperature limit. We illustrate how active
inference selects actions such that Q((cid:126)s|(cid:126)a,s ) most closely matches the preference distri-
t
bution C (top-right). In this example, the discrete state-space is a discretisation of a
β
continuous interval in R, and the preferences and predictive distributions over states have
a Gaussian shape. The predictive distribution Q is assumed to have a fixed variance with
respecttoactionsequences,suchthattheonlyparameterthatcanbeoptimisedbyaction
selectionisitsmean. Crucially, inthezerotemperaturelimit(9), lim C becomesa
β→+∞ β
Diracdistributionovertherewardmaximisingstate(bottom). Thus,minimisingexpected
free energy corresponds to selecting the action such that the predicted states assign most
mass to the reward maximising state (bottom-right). Q∗ := Q((cid:126)s | (cid:126)a∗,s ) denotes the
t
predictive distribution over states given the action sequence that minimises expected free
energy(cid:126)a∗ =argmin G((cid:126)a|s ).
(cid:126)a t
In summary, this scheme selects the first action within action sequences that, on average, max-
imise their exponentiated negative expected free energies. As a corollary, if the first action is in a
sequence with a very low expected free energy, this adds an exponentially large contribution to the
selection of this particular action. We summarise this scheme in Table 1.
Process Computation
Perceptual inference Q((cid:126)s|(cid:126)a,s )=P((cid:126)s|(cid:126)a,s )= (cid:81)T−1P(s |s ,a )
t t τ=t τ+1 τ τ
Planning as inference G((cid:126)a|s )=D [Q((cid:126)s|(cid:126)a,s )|C((cid:126)s)]
t KL t
Decision-making Q((cid:126)a|s )∝exp(−G((cid:126)a|s ))
t t
Action selection a
t
∈argmax a∈A[Q(a
t
=a|s
t
)= (cid:80)
(cid:126)a
Q(a
t
=a|(cid:126)a)Q((cid:126)a|s
t
)]
Table 1: Standard active inference scheme on finite horizon MDPs (Barp et al., 2022).
Theorem 15. In MDPs with known transition probabilities and in the zero temperature limit β →
+∞ (9), the scheme of Table 1
(cid:88)
a ∈ lim argmax exp(−G((cid:126)a|s )), G((cid:126)a|s )=D [Q((cid:126)s|(cid:126)a,s )|C ((cid:126)s)] (10)
t t t KL t β
β→+∞ a∈A
(cid:126)a
((cid:126)a) =a
t
is Bellman optimal for the temporal horizon T =1.
11
A proof can be found in Appendix B.5. Importantly, the standard active inference scheme (10)
falls short in terms of Bellman optimality on planning horizons greater than one; this rests upon
the fact that it does not coincide with backward induction. Recall that backward induction offers a
complete description of Bellman optimal state-action policies (Proposition 11). In contrast, active
inference plans by adding weighted expected free energies of each possible future course of action.
In other words, unlike backward induction, it considers future courses of action beyond the subset
that will subsequently minimise expected free energy, given subsequently encountered states.
4.3 Reward maximisation on MDPs with finite temporal horizons
To achieve Bellman optimality on finite temporal horizons, we turn to the expected free energy of
an action given future actions that also minimise expected free energy. To do this we can write the
expectedfreeenergyrecursively,astheimmediateexpectedfreeenergy,plustheexpectedfreeenergy
thatonewouldobtainbysubsequentlyselectingactionsthatminimiseexpectedfreeenergy(Friston
etal.,2021). Theresultingschemeconsistsofminimisinganexpectedfreeenergydefinedrecursively,
from the last time step to the current timestep. In finite horizon MDPs, this reads
G(a |s )=D [Q(s |a ,s )|C (s )]
T−1 T−1 KL T T−1 T−1 β T
G(a |s )=D [Q(s |a ,s )|C (s )]
τ τ KL τ+1 τ τ β τ+1
+E [G(a |s )], τ =t,...,T −2,
Q(aτ+1,sτ+1|aτ,sτ) τ+1 τ+1
where, at each time-step, actions are chosen to minimise expected free energy
Q(a |s )>0 ⇐⇒ a ∈argminG(a|s ). (11)
τ+1 τ+1 τ+1 τ+1
a∈A
To make sense of this formulation, we unravel the recursion
G(a |s )=D [Q(s |a ,s )|C (s )]+E [G(a |s )]
t t KL t+1 t t β t+1 Q(at+1,st+1|at,st) t+1 t+1
=D [Q(s |a ,s )|C (s )]+E [D [Q(s |a ,s )|C (s )]]
KL t+1 t t β t+1 Q(at+1,st+1|at,st) KL t+2 t+1 t+1 β t+2
+E [G(a |s )]
Q(at+1:t+2,st+1:t+2|at,st) t+2 t+2
T−1
(cid:88)
=...=E D [Q(s |a ,s )|C (s )]
Q((cid:126)a,(cid:126)s|at,st) KL τ+1 τ τ β τ+1
τ=t
=E D [Q((cid:126)s|(cid:126)a,s )|C ((cid:126)s)],
Q((cid:126)a,(cid:126)s|at,st) KL t β
(12)
which shows that this expression is exactly the expected free energy under action a , if one is to
t
pursue future actions that minimise expected free energy (11). We summarise this ’sophisticated
inference’ scheme in Table 2.
Process Computation
Perceptual inference Q(s |a ,s )=P(s |a ,s )
τ+1 τ τ τ+1 τ τ
Planning as inference G(a |s )=D [Q(s |a ,s )|C (s )]...
τ τ KL τ+1 τ τ β τ+1
...+E [G(a |s )]
Q(aτ+1,sτ+1|aτ,sτ) τ+1 τ+1
Decision-making Q(a
τ
|s
τ
)>0 ⇐⇒ a
τ
∈argmin a∈AG(a|s
τ
)
Action selection a ∼Q(a |s )
t t t
Table 2: Sophisticated active inference scheme on finite horizon MDPs (Friston et al., 2021).
The crucial improvement over the standard active inference scheme (Table 1) is that planning is
now performed based on subsequent counterfactual actions that minimise expected free energy, as
12
opposedtoconsideringallfuturecoursesofaction. Translatingthisintothelanguageofstate-action
policies yields ∀s∈S
a (s)∈argminG(a|s =s)
T−1 T−1
a∈A
a (s)∈argminG(a|s =s)
T−2 T−2
a∈A
.
. (13)
.
a (s)∈argminG(a|s =s)
1 1
a∈A
a (s)∈argminG(a|s ).
0 0
a∈A
Equation (13) is strikingly similar to the backward induction algorithm (Proposition 11), and
indeed we recover backward induction in the limit β →+∞.
Theorem 16 (Backward induction as active inference). In MDPs with known transition probabili-
ties, and in the zero temperature limit β →+∞ (9), the scheme of Table 2
Q(a |s )>0 ⇐⇒ a ∈ lim argminG(a|s )
τ τ t τ
β→+∞ a∈A (14)
G(a |s )=D [Q(s |a ,s )|C (s )]+E [G(a |s )]
τ τ KL τ+1 τ τ β τ+1 Q(aτ+1,sτ+1|aτ,sτ) τ+1 τ+1
is Bellman optimal on any finite temporal horizon as it coincides with the backward induction algo-
rithm from Proposition 11. Furthermore, if there are multiple actions that maximise future reward,
thosethatareselectedbyactiveinferencealsomaximisetheentropyoffuturestatesH[Q((cid:126)s|(cid:126)a,a,s )].
0
Notethatmaximisingtheentropyoffuturestateskeepstheagent’soptionsopen(Klyubinetal.,
2008) in the sense of committing the least to a specified sequence of states. A proof of Theorem 16
can be found in Appendix B.6.
5. Generalisation to POMDPs
Partially observable Markov decision processes (POMDPs) generalise MDPs in that the agent ob-
serves a modality o , which carries incomplete information about the current state s , as opposed to
t t
the current state itself.
Definition 17 (Finite horizon POMDP). A finite horizon POMDP is an MDP (see Definition 1)
with the following additional data:
• O a finite set of observations.
• P(o = o | s = s) is the probability that the state s ∈ S at time t will lead to the observation
t t
o ∈ O at time t. o are random variables over O that correspond to the observation being
t
sampled at time t=0,...,T.
5.1 Active inference on finite horizon POMDPs
WebrieflyintroduceactiveinferenceagentsonfinitehorizonPOMDPswithknowntransitionprob-
abilities (for more details, see (Da Costa et al., 2020; Parr et al., 2022; Smith et al., 2022a)). We
assume that the agent’s generative model of its environment is given by the previously defined
POMDP (Definition 17)4.
4. WedonotconsiderthecasewherethemodelparametershavetobelearnedbutcommentonitinAppendixA.2
(detailsin(DaCostaetal.,2020;Fristonetal.,2016)).
13
Let (cid:126)s := s ,(cid:126)a := a be all states and actions (past, present, and future), let o˜:= o be
0:T 0:T−1 0:t
the observations available up to time t, and (cid:126)o := o be the future observations. The agent has
t+1:T
a predictive distribution over states given actions
T−1
(cid:89)
Q((cid:126)s|(cid:126)a,o˜):= Q(s |a ,s ,o˜).
τ+1 τ τ
τ=0
that is continuously updated following new observations.
5.1.1 Perception as inference
In active inference, perception entails inferences about (past, present, and future) states given ob-
servations and a sequence of actions. When states are partially observed, the posterior distribution
P((cid:126)s | (cid:126)a,o˜) is intractable to compute directly. Thus, one approximates it by optimising a varia-
tional free energy functional F (a.k.a. an evidence bound (Beal, 2003; Bishop, 2006; Blei et al.,
(cid:126)a
2017; Wainwright and Jordan, 2007)) over a space of probability distributions Q(· |(cid:126)a,o˜) called the
variational family
P((cid:126)s|(cid:126)a,o˜)=argminF [Q((cid:126)s|(cid:126)a,o˜)]=argminD [Q((cid:126)s|(cid:126)a,o˜)|P((cid:126)s|(cid:126)a,o˜)]
(cid:126)a KL
Q Q (15)
F [Q((cid:126)s|(cid:126)a,o˜)]:=E [logQ((cid:126)s|(cid:126)a,o˜)−logP(o˜,(cid:126)s|(cid:126)a)].
(cid:126)a Q((cid:126)s|(cid:126)a,o˜)
Here, P(o˜,(cid:126)s | (cid:126)a) is the POMDP, which is supplied to the agent, and P((cid:126)s | (cid:126)a,o˜). When the free
energy minimum (15) is reached, the inference is exact
Q((cid:126)s|(cid:126)a,o˜)=P((cid:126)s|(cid:126)a,o˜). (16)
For numerical tractability, the variational family may be constrained to a parametric family of
distributions, in which case equality is not guaranteed
Q((cid:126)s|(cid:126)a,o˜)≈P((cid:126)s|(cid:126)a,o˜). (17)
5.1.2 Planning as inference
Theobjectivethatactiveinferenceminimisesinordertheselectthebestpossiblecoursesofactionis
theexpected free energy (Barpetal.,2022;DaCostaetal.,2020;Fristonetal.,2021). InPOMDPs,
the expected free energy reads (Barp et al., 2022)
G((cid:126)a|o˜)=D [Q((cid:126)s|(cid:126)a,o˜)|C ((cid:126)s)]+E H[P((cid:126)o|(cid:126)s)].
KL β Q((cid:126)s|(cid:126)a,o˜)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Risk Ambiguity
The expected free energy on POMDPs is the expected free energy on MDPs plus an extra term
called ambiguity. This ambiguity term accommodates the uncertainty implicit in partially observed
problems. The reason that this resulting functional is called expected free energy is because it com-
prisesarelativeentropy(risk)andexpectedenergy(ambiguity). Theexpectedfreeenergyobjective
subsumes several decision-making objectives that predominate in statistics, machine learning, and
psychology, which confers it with several useful properties when simulating behaviour (see Figure 3
for details).
5.2 Maximising reward on POMDPs
Crucially, our reward maximisation results translate to the POMDP case. To make this explicit, we
rehearse Lemma 14 in the context of POMDPs.
14
Figure 3: Active inference. The top panels illustrate the perception-action loop in active infer-
ence, in terms of minimisation of variational and expected free energy. The lower panels
illustrate how expected free energy relates to several descriptions of behaviour that pre-
dominate in the psychological, machine learning, and economics. These descriptions are
disclosedwhenoneremovesparticulartermsfromtheobjective. Forexample,ifweignore
extrinsic value, we are left with intrinsic value, variously known as expected information
gain (Lindley, 1956; MacKay, 2003). This underwrites intrinsic motivation in machine
learning and robotics (Barto et al., 2013; Deci and Ryan, 1985; Oudeyer and Kaplan,
2007) and expected Bayesian surprise in visual search (Itti and Baldi, 2009; Sun et al.,
2011)andtheorganisationofourvisualapparatus(Barlow,1961,1974;Linsker,1990;Op-
ticanandRichmond,1987). Intheabsenceofambiguity, weareleftwithminimisingrisk,
whichcorrespondstoaligningpredictedstatestopreferredstates. Thisleadstoriskaverse
decisions in behavioural economics (Kahneman and Tversky, 1979) and formulations of
control as inference in engineering such as KL control (van den Broek et al., 2010). If we
then remove intrinsic value, we are left with expected utility in economics (Von Neumann
andMorgenstern,1944)thatunderwritesRLandbehaviouralpsychology(BartoandSut-
ton, 1992). Bayesian formulations of maximising expected utility under uncertainty are
alsothebasisofBayesiandecisiontheory(Berger,1985). Finally,ifweonlyconsiderafully
observed environment with no preferences, minimising expected free energy corresponds
toamaximumentropyprincipleoverfuturestates(Jaynes,1957a,b). NotethathereC(o)
denotes the preferences over observations derived from the preferences over states. These
are related by P(o|s)C(s)=P(s|o)C(o).
15
Proposition18(RewardmaximisationonPOMDPs). InPOMDPswithknowntransitionprobabil-
ities, provided that the free energy minimum is reached (16), the sequence of actions that minimises
expected free energy also maximises expected reward in the zero temperature limit β →+∞ (9):
lim argminG((cid:126)a|o˜)⊆argmaxE [R((cid:126)s)].
Q((cid:126)s|(cid:126)a,o˜)
β→+∞ (cid:126)a (cid:126)a
Furthermore, of those action sequences that maximise expected reward, the expected free energy
minimisers will be those that maximise the entropy of future states minus the (expected) entropy of
outcomes given states H[Q((cid:126)s|(cid:126)a,o˜)]−E H[P((cid:126)o|(cid:126)s)]].
Q((cid:126)s|at,o˜)
From Proposition 18 we see that if there are multiple maximise reward action sequences, those
that are selected maximise
H[Q((cid:126)s|(cid:126)a,o˜)] − E [H[P((cid:126)o|(cid:126)s)]] .
Q((cid:126)s|at,o˜)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Entropyoffuturestates Entropyofobservationsgivenfuturestates
Inotherwords,theyleastcommittoaprespecifiedsequenceoffuturestatesandensurethattheir
expected observations are maximally informative of states. Of course, when inferences are inexact,
the extent to which Proposition 18 holds depends upon the accuracy of the approximation (17). A
proof of Proposition 18 can be found in Appendix B.7.
The schemes of Table 1 & 2 exist in the POMDP setting, (e.g., (Barp et al., 2022) and (Fris-
ton et al., 2021), respectively). Thus, in POMDPs with known transition probabilities, provided
that inferences are exact (16) and in the zero temperature limit β → +∞ (9), standard active
inference (Barp et al., 2022) maximises reward on temporal horizons of 1 but not beyond, and a
recursive scheme such as sophisticated active inference (Friston et al., 2021) maximises reward on
finite temporal horizons. Note that, for computational tractability, the sophisticated active infer-
ence scheme presented in (Friston et al., 2021) does not generally perform exact inference; thus, the
extent to which it will maximise reward in practice will depend upon the accuracy of its inferences.
Nevertheless,ourresultsindicatethatsophisticatedactiveinferencewillvastlyoutperformstandard
active inference in most reward maximisation tasks.
6. Discussion
Inthispaper,wehaveexaminedaspecificnotionofoptimality;namely,Bellmanoptimality;defined
as selecting actions to maximise future expected rewards. We demonstrated how and when active
inference is Bellman optimal on finite horizon POMDPs with known transition probabilities and
reward function.
These results highlight important relationships between active inference, stochastic control, and
RL,aswellasconditionsunderwhichtheywouldandwouldnotbeexpectedtobehavesimilarly(e.g.,
environmentswithmultiplereward-maximisingtrajectories,thoseaffordingambiguousobservations,
etc.). We refer the reader to Appendix A for a broader discussion of the relationship between active
inference and reinforcement learning.
6.1 Decision-making beyond reward maximisation
More broadly, it is important to ask if reward maximisation is the right objective underwriting
intelligent decision-making? This is an important question for decision neuroscience. That is, do
humans optimise a reward signal, expected free energy, or other planning objectives. This can be
addressed by comparing the evidence for these competing hypotheses based on empirical data (e.g.,
see(Smithetal.,2020d,2021b,c,2022b)). Currentempiricalevidencesuggeststhathumansarenot
purely reward-maximising agents: they also engage in both random and directed exploration (Daw
et al., 2006; Gershman, 2018; Mirza et al., 2018; Schulz and Gershman, 2019; Wilson et al., 2014,
16
2021; Xu et al., 2021) and keep their options open (Schwartenbeck et al., 2015b). As we have
illustrated, active inference implements a clear form of directed exploration through minimising
expected free energy. Although not covered in detail here, active inference can also accommodate
random exploration by sampling actions from the posterior belief over action sequences, as opposed
to selecting the most likely action as presented in Tables 1 and 2.
Note that behavioural evidence favouring models that do not solely maximise reward within
reward maximisation tasks—i.e., where "maximise reward" is the explicit instruction—is not a
contradiction. Rather, gathering information about the environment (exploration) generally helps
to reap more reward in the long run, as opposed to greedily maximising reward based on imperfect
knowledge(Cullenetal.,2018;Sajidetal.,2021a). Thisobservationisnotnewandmanyapproaches
to simulating adaptive agents employed today differ significantly from their reward maximising
antecedents (Appendix A.3).
6.2 Learning
Whenthetransitionprobabilitiesorrewardfunctionareunknowntotheagent,theproblembecomes
oneofreinforcementlearning(RL)(Shohametal.,2003)asopposedtostochasticcontrol. Although
we did not explicitly consider it above, this scenario can be accommodated by active inference by
simplyequippingthegenerativemodelwithaprior,andupdatingthemodelviavariationalBayesian
inferencetobestfitobserveddata. Dependingonthespecificlearningproblemandgenerativemodel
structure, this can involve updating the transition probabilities and/or the target distribution C.
In POMDPs it can also involve updating the probabilities of observations under each state. We
refer to Appendix A.2 for discussion of reward learning through active inference and connections to
representativeRLapproaches,and(DaCostaetal.,2020;Fristonetal.,2016)forlearningtransition
probabilities through active inference.
6.3 Scaling active inference
When comparing RL and active inference approaches generally, one outstanding issue for active
inference is whether it can be scaled up to solve the more complex problems currently handled by
RL in machine learning contexts (Çatal et al., 2020, 2021; Fountas et al., 2020; Mazzaglia et al.,
2021; Millidge, 2020; Tschantz et al., 2019). This is an area of active research.
One important issue along these lines is that planning ahead by evaluating all or many possible
sequences of actions is computationally prohibitive in many applications. Three complementary
solutionsthathaveemergedare: 1)employinghierarchicalgenerativemodelsthatfactorisedecisions
into multiple levels and reduce the size of the decision tree by orders of magnitude (Çatal et al.,
2021;Fristonetal.,2018;Parretal.,2021),2)efficientlysearchingthedecisiontreeusingalgorithms
like Monte Carlo tree search (Champion et al., 2021a,b; Fountas et al., 2020; Maisto et al., 2021;
Silver et al., 2016), and 3) amortising planning using artificial neural networks (Çatal et al., 2020;
Fountas et al., 2020; Sajid et al., 2021b).
Another issue rests upon learning the generative model. Active inference may readily learn the
parameters of a generative model; however, more work needs to be done on devising algorithms for
learning the structure of generative models themselves (Friston et al., 2017b; Smith et al., 2020c).
This is an important research problem in generative modelling, called Bayesian model selection or
structure learning (Gershman and Niv, 2010; Tervo et al., 2016).
Note that these issues are not unique to active inference. Model-based RL algorithms deal with
thesamecombinatorialexplosionwhenevaluatingdecisiontrees,whichisoneprimarymotivationfor
developing efficient model-free RL algorithms. However, other heuristics have also been developed
for efficiently searching and pruning decision trees in model-based RL, e.g., (Huys et al., 2012;
Lally et al., 2017). Furthermore, model-based RL suffers the same limitation regarding learning
generative model structure. Yet, RL may have much to offer active inference in terms of efficient
17
implementation and the identification of methods to scale to more complex applications (Fountas
et al., 2020; Mazzaglia et al., 2021).
7. Conclusion
In summary, we have shown that under the specification that the active inference agent prefers
maximising reward (9):
1. On finite horizon POMDPs with known transition probabilities, the objective optimised for
action selection in active inference (i.e., expected free energy) produces reward maximising
action sequences when state-estimation is exact. When there are multiple reward maximising
candidates, this selects those sequences that maximise the entropy of future states—thereby
keeping options open—and that minimise the ambiguity of future observations so that they
are are maximally informative. More generally, the extent to which action sequences will be
reward maximising will depend on the accuracy of state-estimation.
2. The standard active inference scheme (e.g., (Barp et al., 2022)) produces Bellman optimal
actions for planning horizons of 1 when state-estimation is exact, but not beyond.
3. A sophisticated active inference scheme (e.g., (Friston et al., 2021)) produces Bellman opti-
mal actions on any finite planning horizon when state-estimation is exact. Furthermore, this
scheme generalises the well-known backward induction algorithm from dynamic programming
to partially observed environments. Note that, for computational efficiency, the scheme pre-
sented in (Friston et al., 2021) does not generally perform exact state-estimation; thus, the
extent to which it will maximise reward in practice will depend upon the accuracy of its infer-
ences. Nevertheless, it is clear from our results that sophisticated active inference will vastly
outperform standard active inference in most reward maximisation tasks.
Note that, for computational tractability, the sophisticated active inference scheme presented
in (Friston et al., 2021) does not generally perform exact inference; thus, the extent to which it
will maximise reward in practice will depend upon the accuracy of its inferences. Nevertheless, it
is clear from these results that sophisticated active inference will vastly outperform standard active
inference in most reward maximisation tasks.
Acknowledgements
The authors thank Dimitrije Markovic and Quentin Huys for providing helpful feedback during the
preparation of the manuscript.
Funding information
LD is supported by the Fonds National de la Recherche, Luxembourg (Project code: 13568875).
NS is funded by the Medical Research Council (MR/S502522/1) and 2021-2022 Microsoft PhD
Fellowship. KF is supported by funding for the Wellcome Centre for Human Neuroimaging (Ref:
205103/Z/16/Z), a Canada-UK Artificial Intelligence Initiative (Ref: ES/T01279X/1) and the Eu-
ropeanUnion’sHorizon2020FrameworkProgrammeforResearchandInnovationundertheSpecific
Grant Agreement No. 945539 (Human Brain Project SGA3). RS is supported by the William K.
WarrenFoundation,theWell-BeingforPlanetEarthFoundation,theNationalInstituteforGeneral
Medical sciences (P20GM121312), and the National Institute of Mental Health (R01MH123691).
This publication is based on work partially supported by the EPSRC Centre for Doctoral Training
in Mathematics of Random Systems: Analysis, Modelling and Simulation (EP/S023925/1).
18
Author contributions
LD: conceptualisation, proofs, writing – first draft, review and editing. NS, TP, KF, RS: conceptu-
alisation, writing – review and editing.
References
Rick A. Adams, Klaas Enno Stephan, Harriet R. Brown, Christopher D. Frith, and Karl J. Friston.
TheComputationalAnatomyofPsychosis. Frontiers in Psychiatry,4,2013. ISSN1664-0640. doi:
10.3389/fpsyt.2013.00047.
Jerome Adda and Russell W. Cooper. Dynamic Economics Quantitative Methods and Applications.
MIT Press, 2003.
Hagai Attias. Planning by Probabilistic Inference. In 9th Int. Workshop on Artificial Intelligence
and Statistics, page 8, 2003.
H. B. Barlow. Possible Principles Underlying the Transformations of Sensory Messages. The MIT
Press, 1961. ISBN 978-0-262-31421-3.
H B Barlow. Inductive Inference, Coding, Perception, and Language. Perception, 3(2):123–134,
June 1974. ISSN 0301-0066. doi: 10.1068/p030123.
Alessandro Barp, Lancelot Da Costa, Guilherme França, Karl Friston, Mark Girolami, Michael I.
Jordan,andGrigoriosA.Pavliotis. GeometricMethodsforSampling,Optimisation,Inferenceand
Adaptive Agents. In Geometry and Statistics, number 46 in Handbook of Statistics. Academic
Press, 2022. ISBN 978-0-323-91345-4.
Andrew Barto and Richard Sutton. Reinforcement Learning: An Introduction. 1992.
Andrew Barto, Marco Mirolli, and Gianluca Baldassarre. Novelty or Surprise? Frontiers in Psy-
chology, 4, 2013. ISSN 1664-1078. doi: 10.3389/fpsyg.2013.00907.
Matthew James Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis,
University of London, 2003.
Richard E. Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, US, 1957.
ISBN 978-0-691-14668-3.
Richard E. Bellman and Stuart E. Dreyfus. Applied Dynamic Programming. Princeton University
Press, December 2015. ISBN 978-1-4008-7465-1.
James O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer Series in Statis-
tics. Springer-Verlag, New York, second edition, 1985. ISBN 978-0-387-96098-2. doi: 10.1007/
978-1-4757-4286-2.
Oded Berger-Tal, Jonathan Nathan, Ehud Meron, and David Saltz. The Exploration-Exploitation
Dilemma: AMultidisciplinaryFramework. PLOS ONE,9(4):e95693,April2014. ISSN1932-6203.
doi: 10.1371/journal.pone.0095693.
Dimitri P. Bertsekas and Steven E. Shreve. Stochastic Optimal Control: The Discrete Time Case.
Athena Scientific, 1996. ISBN 978-1-886529-03-8.
ChristopherM.Bishop. Pattern Recognition and Machine Learning. InformationScienceandStatis-
tics. Springer, New York, 2006. ISBN 978-0-387-31073-2.
19
David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational Inference: A Review for Statis-
ticians. Journal of the American Statistical Association, 112(518):859–877, April 2017. ISSN
0162-1459, 1537-274X. doi: 10.1080/01621459.2017.1285773.
Matthew Botvinick and Marc Toussaint. Planning as inference. Trends in Cognitive Sciences, 16
(10):485–488, October 2012. ISSN 13646613. doi: 10.1016/j.tics.2012.08.006.
Christopher L. Buckley, Chang Sub Kim, Simon McGregor, and Anil K. Seth. The free energy
principleforactionandperception: Amathematicalreview. Journal of Mathematical Psychology,
81:55–79, December 2017. ISSN 00222496. doi: 10.1016/j.jmp.2017.09.004.
Ozan Çatal, Tim Verbelen, Johannes Nauta, Cedric De Boom, and Bart Dhoedt. Learning Per-
ception and Planning With Deep Active Inference. In ICASSP 2020 - 2020 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3952–3956, May 2020.
doi: 10.1109/ICASSP40776.2020.9054364.
Ozan Çatal, Tim Verbelen, Toon Van de Maele, Bart Dhoedt, and Adam Safron. Robot navigation
as hierarchical active inference. Neural Networks, 142:192–204, October 2021. ISSN 0893-6080.
doi: 10.1016/j.neunet.2021.05.010.
Théophile Champion, Howard Bowman, and Marek Grześ. Branching Time Active Inference: Em-
pirical study and complexity class analysis. arXiv:2111.11276 [cs], November 2021a.
Théophile Champion, Lancelot Da Costa, Howard Bowman, and Marek Grześ. Branching Time
Active Inference: The theory and its generality. arXiv:2111.11107 [cs], November 2021b.
Maell Cullen, Ben Davey, Karl J. Friston, and Rosalyn J. Moran. Active Inference in OpenAI Gym:
A Paradigm for Computational Investigations Into Psychiatric Illness. Biological Psychiatry:
Cognitive Neuroscience and Neuroimaging, 3(9):809–818, September 2018. ISSN 24519022. doi:
10.1016/j.bpsc.2018.06.010.
LancelotDaCosta,ThomasParr,NoorSajid,SebastijanVeselic,VictoritaNeacsu,andKarlFriston.
Active inference on discrete state-spaces: A synthesis. Journal of Mathematical Psychology, 99:
102447, December 2020. ISSN 0022-2496. doi: 10.1016/j.jmp.2020.102447.
Lancelot Da Costa, Pablo Lanillos, Noor Sajid, Karl Friston, and Shujhat Khan. How Active
Inference Could Help Revolutionise Robotics. Entropy, 24(3):361, March 2022a. ISSN 1099-4300.
doi: 10.3390/e24030361.
Lancelot Da Costa, Samuel Tenka, Dominic Zhao, and Noor Sajid. Active Inference as a Model of
Agency. In Workshop on RL as a Model of Agency, 2022b.
NathanielD.Daw,JohnP.O’Doherty,PeterDayan,BenSeymour,andRaymondJ.Dolan. Cortical
substratesforexploratorydecisionsinhumans. Nature,441(7095):876–879,June2006. ISSN1476-
4687. doi: 10.1038/nature04766.
P.DayanandN.D.Daw.Decisiontheory,reinforcementlearning,andthebrain.Cognitive,Affective,
& Behavioral Neuroscience, 8(4):429–453, December 2008. ISSN 1530-7026, 1531-135X. doi:
10.3758/CABN.8.4.429.
Edward Deciand Richard M.Ryan. Intrinsic Motivation and Self-Determination in Human Behav-
ior. Perspectives in Social Psychology. Springer US, New York, 1985. ISBN 978-0-306-42022-1.
doi: 10.1007/978-1-4899-2271-7.
Benjamin Eysenbach and Sergey Levine. If maxent rl is the answer, what is the question? arXiv
preprint arXiv:1910.01913, 2019.
20
ZafeiriosFountas,NoorSajid,PedroA.M.Mediano,andKarlFriston. Deepactiveinferenceagents
using Monte-Carlo methods. arXiv:2006.04176 [cs, q-bio, stat], June 2020.
Karl Friston. The free-energy principle: A unified brain theory? Nature Reviews Neuroscience, 11
(2):127–138, February 2010. ISSN 1471-003X, 1471-0048. doi: 10.1038/nrn2787.
Karl Friston, Spyridon Samothrakis, and Read Montague. Active inference and agency: Optimal
control without cost functions. Biological Cybernetics, 106(8):523–541, October 2012. ISSN 1432-
0770. doi: 10.1007/s00422-012-0512-8.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, John O’Doherty, and
Giovanni Pezzulo. Active inference and learning. Neuroscience & Biobehavioral Reviews, 68:
862–879, September 2016. ISSN 01497634. doi: 10.1016/j.neubiorev.2016.06.022.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pezzulo.
Active Inference: A Process Theory. Neural Computation, 29(1):1–49, January 2017a. ISSN
0899-7667, 1530-888X. doi: 10.1162/NECO_a_00912.
Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, and Thomas Parr. Sophisticated
Inference. Neural Computation, 33(3):713–763, February 2021. ISSN 0899-7667. doi: 10.1162/
neco_a_01351.
Karl Friston, Lancelot Da Costa, Noor Sajid, Conor Heins, Kai Ueltzhöffer, Grigorios A. Pavliotis,
and Thomas Parr. The free energy principle made simpler but not too simple. arXiv:2201.06387
[cond-mat, physics:nlin, physics:physics, q-bio], January 2022.
KarlJ.Friston, JeanDaunizeau, andStefanJ.Kiebel. ReinforcementLearningorActiveInference?
PLoS ONE, 4(7):e6421, July 2009. ISSN 1932-6203. doi: 10.1371/journal.pone.0006421.
Karl J. Friston, Jean Daunizeau, James Kilner, and Stefan J. Kiebel. Action and behavior: A free-
energy formulation. Biological Cybernetics, 102(3):227–260, March 2010. ISSN 1432-0770. doi:
10.1007/s00422-010-0364-z.
KarlJ.Friston,MarcoLin,ChristopherD.Frith,GiovanniPezzulo,J.AllanHobson,andSashaOn-
dobaka. Active Inference, Curiosityand Insight. Neural Computation, 29(10):2633–2683, October
2017b. ISSN 0899-7667, 1530-888X. doi: 10.1162/neco_a_00999.
Karl J. Friston, Thomas Parr, and Bert de Vries. The graphical brain: Belief propagation and
active inference. Network Neuroscience, 1(4):381–414, December 2017c. ISSN 2472-1751. doi:
10.1162/NETN_a_00018.
Karl J. Friston, Richard Rosch, Thomas Parr, Cathy Price, and Howard Bowman. Deep temporal
models and active inference. Neuroscience & Biobehavioral Reviews, 90:486–501, July 2018. ISSN
01497634. doi: 10.1016/j.neubiorev.2018.04.004.
Drew Fudenberg and Jean Tirole. Game Theory. MIT Press, 1991. ISBN 978-0-262-06141-4.
Samuel J. Gershman. Deconstructing the human algorithms for exploration. Cognition, 173:34–42,
April 2018. ISSN 1873-7838. doi: 10.1016/j.cognition.2017.12.014.
Samuel J. Gershman and Yael Niv. Learning latent structure: Carving nature at its joints. Current
Opinion in Neurobiology, 20(2):251–256, April 2010. ISSN 1873-6882. doi: 10.1016/j.conb.2010.
02.008.
Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian reinforcement
learning: A survey. arXiv preprint arXiv:1609.04436, 2016.
21
A. Guez, D. Silver, and P. Dayan. Scalable and Efficient Bayes-Adaptive Reinforcement Learn-
ing Based on Monte-Carlo Tree Search. Journal of Artificial Intelligence Research, 48:841–883,
November 2013a. ISSN 1076-9757. doi: 10.1613/jair.4117.
Arthur Guez, David Silver, and Peter Dayan. Efficient Bayes-Adaptive Reinforcement Learning
using Sample-Based Search. arXiv:1205.3109 [cs, stat], December 2013b.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. CoRR, abs / 1801.01290,
2018. URL http://arxiv.org/abs/1801.01290.
Quentin J. M. Huys, Neir Eshel, Elizabeth O’Nions, Luke Sheridan, Peter Dayan, and Jonathan P.
Roiser. Bonsai Trees in Your Head: How the Pavlovian System Sculpts Goal-Directed Choices
by Pruning Decision Trees. PLoS Computational Biology, 8(3):e1002410, March 2012. ISSN
1553-7358. doi: 10.1371/journal.pcbi.1002410.
Laurent Itti and Pierre Baldi. Bayesian surprise attracts human attention. Vision research, 49(10):
1295–1306, May 2009. ISSN 0042-6989. doi: 10.1016/j.visres.2008.09.007.
E.T.Jaynes. InformationTheoryandStatisticalMechanics. Physical Review, 106(4):620–630, May
1957a. doi: 10.1103/PhysRev.106.620.
E. T. Jaynes. Information Theory and Statistical Mechanics. II. Physical Review, 108(2):171–190,
October 1957b. doi: 10.1103/PhysRev.108.171.
MichaelI.Jordan,ZoubinGhahramani,TommiS.Jaakkola,andLawrenceK.Saul. AnIntroduction
toVariationalMethodsforGraphicalModels. InMichaelI.Jordan, editor, Learning in Graphical
Models, pages 105–161. Springer Netherlands, Dordrecht, 1998. ISBN 978-94-010-6104-9 978-94-
011-5014-9. doi: 10.1007/978-94-011-5014-9_5.
Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in
partially observable stochastic domains. Artificial Intelligence, 101(1):99–134, May 1998. ISSN
0004-3702. doi: 10.1016/S0004-3702(98)00023-X.
Daniel Kahneman and Amos Tversky. Prospect Theory: An Analysis of Decision under Risk.
Econometrica, 47(2):263–291, 1979. ISSN 0012-9682. doi: 10.2307/1914185.
Hilbert J. Kappen, Vicenç Gómez, and Manfred Opper. Optimal control as a graphical model
inferenceproblem. Machine Learning, 87(2):159–182, May2012. ISSN0885-6125, 1573-0565. doi:
10.1007/s10994-012-5278-7.
Alexander S. Klyubin, Daniel Polani, and Chrystopher L. Nehaniv. Keep Your Options Open:
An Information-Based Driving Principle for Sensorimotor Systems. PLOS ONE, 3(12):e4018,
December 2008. ISSN 1932-6203. doi: 10.1371/journal.pone.0004018.
Níall Lally, Quentin J. M. Huys, Neir Eshel, Paul Faulkner, Peter Dayan, and Jonathan P. Roiser.
The Neural Basis of Aversive Pavlovian Guidance during Planning. Journal of Neuroscience, 37
(42):10215–10229,October2017. ISSN0270-6474,1529-2401. doi: 10.1523/JNEUROSCI.0085-17.
2017.
PabloLanillos,JordiPages,andGordonCheng. Robotself/otherdistinction: Activeinferencemeets
neuralnetworkslearninginamirror. InEuropean Conference on Artificial Intelligence.IOSpress,
April 2020.
22
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.
arXiv preprint arXiv:1805.00909, 2018a.
SergeyLevine. ReinforcementLearningandControlasProbabilisticInference: TutorialandReview.
arXiv:1805.00909 [cs, stat], May 2018b.
D. V. Lindley. On a Measure of the Information Provided by an Experiment. The Annals of
Mathematical Statistics, 27(4):986–1005, 1956. ISSN 0003-4851.
R Linsker. Perceptual Neural Organization: Some Approaches Based on Network Models and
InformationTheory. Annual Review of Neuroscience, 13(1):257–281, 1990. doi: 10.1146/annurev.
ne.13.030190.001353.
DavidJ.C.MacKay.InformationTheory,InferenceandLearningAlgorithms.CambridgeUniversity
Press, Cambridge, UK ; New York, sixth printing 2007 edition edition, September 2003. ISBN
978-0-521-64298-9.
Domenico Maisto, Francesco Gregoretti, Karl Friston, and Giovanni Pezzulo. Active Tree Search in
Large POMDPs. arXiv:2103.13860 [cs, math, q-bio], March 2021.
Dimitrije Marković, Hrvoje Stojić, Sarah Schwöbel, and Stefan J. Kiebel. An empirical evaluation
of active inference in multi-armed bandits. Neural Networks, 144:229–246, December 2021. ISSN
0893-6080. doi: 10.1016/j.neunet.2021.08.018.
Pietro Mazzaglia, Tim Verbelen, and Bart Dhoedt. Contrastive Active Inference. In Advances in
Neural Information Processing Systems, May 2021.
Beren Millidge. Deep active inference as variational policy gradients. Journal of Mathematical
Psychology, 96:102348, June 2020. ISSN 0022-2496. doi: 10.1016/j.jmp.2020.102348.
Beren Millidge. Applications of the Free Energy Principle to Machine Learning and Neuroscience.
arXiv:2107.00140 [cs], June 2021.
Beren Millidge, Alexander Tschantz, and Christopher L. Buckley. Whence the Expected Free En-
ergy? arXiv:2004.08128 [cs], April 2020a.
BerenMillidge,AlexanderTschantz,AnilK.Seth,andChristopherL.Buckley. OntheRelationship
Between Active Inference and Control as Inference. In Tim Verbelen, Pablo Lanillos, Christo-
pher L. Buckley, and Cedric De Boom, editors, Active Inference, Communications in Computer
and Information Science, pages 3–11, Cham, 2020b. Springer International Publishing. ISBN
978-3-030-64919-7. doi: 10.1007/978-3-030-64919-7_1.
Mario J. Miranda and Paul L. Fackler. Applied Computational Economics and Finance. The MIT
Press,Cambridge,Mass.London,newededitionedition,September2002. ISBN978-0-262-63309-
3.
M. Berk Mirza, Rick A. Adams, Christoph Mathys, and Karl J. Friston. Human visual exploration
reduces uncertainty about the sensed world. PLOS ONE, 13(1):e0190429, 2018. ISSN 1932-6203.
doi: 10.1371/journal.pone.0190429.
Guillermo Oliver, Pablo Lanillos, and Gordon Cheng. An empirical study of active inference on a
humanoid robot. IEEE Transactions on Cognitive and Developmental Systems, pages 1–1, 2021.
ISSN 2379-8939. doi: 10.1109/TCDS.2021.3049907.
L.M.OpticanandB.J.Richmond.Temporalencodingoftwo-dimensionalpatternsbysingleunitsin
primate inferior temporal cortex. III. Information theoretic analysis. Journal of Neurophysiology,
57(1):162–178, January 1987. ISSN 0022-3077. doi: 10.1152/jn.1987.57.1.162.
23
Pierre-Yves Oudeyer and Frederic Kaplan. What is Intrinsic Motivation? A Typology of Com-
putational Approaches. Frontiers in Neurorobotics, 1:6, November 2007. ISSN 1662-5218. doi:
10.3389/neuro.12.006.2007.
Thomas Parr. The Computational Neurology of Active Vision. PhD thesis, University College
London, London, 2019.
Thomas Parr, Dimitrije Markovic, Stefan J. Kiebel, and Karl J. Friston. Neuronal message passing
using Mean-field, Bethe, and Marginal approximations. Scientific Reports, 9(1):1889, December
2019. ISSN 2045-2322. doi: 10.1038/s41598-018-38246-3.
Thomas Parr, Jakub Limanowski, Vishal Rawji, and Karl Friston. The computational neurology
of movement under active inference. Brain, March 2021. ISSN 0006-8950. doi: 10.1093/brain/
awab085.
Thomas Parr, Giovanni Pezzulo, and Karl J. Friston. Active Inference: The Free Energy Principle
in Mind, Brain, and Behavior. MIT Press, Cambridge, MA, USA, March 2022. ISBN 978-0-262-
04535-3.
Aswin Paul, Noor Sajid, Manoj Gopalkrishnan, and Adeel Razi. Active Inference for Stochastic
Control. arXiv:2108.12245 [cs], August 2021.
Grigorios A. Pavliotis. Stochastic Processes and Applications: Diffusion Processes, the Fokker-
Planck and Langevin Equations. Number volume 60 in Texts in Applied Mathematics. Springer,
New York, 2014. ISBN 978-1-4939-1322-0.
Judea Pearl. Graphical Models for Probabilistic and Causal Reasoning. In Philippe Smets, editor,
QuantifiedRepresentationofUncertaintyandImprecision,HandbookofDefeasibleReasoningand
Uncertainty Management Systems, pages 367–389. Springer Netherlands, Dordrecht, 1998. ISBN
978-94-017-1735-9. doi: 10.1007/978-94-017-1735-9_12.
Corrado Pezzato, Riccardo Ferrari, and Carlos Hernández Corbato. A Novel Adaptive Controller
for Robot Manipulators Based on Active Inference. IEEE Robotics and Automation Letters, 5(2):
2973–2980, April 2020. ISSN 2377-3766. doi: 10.1109/LRA.2020.2974451.
LéoPio-Lopez,AngeNizard,KarlFriston,andGiovanniPezzulo. Activeinferenceandrobotcontrol:
A case study. Journal of The Royal Society Interface, 13(122):20160616, September 2016. doi:
10.1098/rsif.2016.0616.
MartinL.Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, August 2014. ISBN 978-1-118-62587-3.
Jad Rahme and Ryan P. Adams. A Theoretical Connection Between Statistical Physics and Rein-
forcement Learning. arXiv:1906.10228 [cond-mat, stat], June 2019.
MaxwellJ.D.Ramstead,DaltonA.R.Sakthivadivel,ConorHeins,MagnusKoudahl,BerenMillidge,
Lancelot Da Costa, Brennan Klein, and Karl J. Friston. On Bayesian Mechanics: A Physics of
and by Beliefs, May 2022.
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On Stochastic Optimal Control and Re-
inforcementLearningbyApproximateInference. InTwenty-Third International Joint Conference
on Artificial Intelligence, June 2013.
StéphaneRoss,JoellePineau,BrahimChaib-draa,andPierreKreitmann. ABayesianApproachfor
Learning and Planning in Partially Observable Markov Decision Processes. page 42.
24
Stephane Ross, Brahim Chaib-draa, and Joelle Pineau. Bayes-Adaptive POMDPs. In J. C. Platt,
D.Koller,Y.Singer,andS.T.Roweis,editors,AdvancesinNeuralInformationProcessingSystems
20, pages 1225–1232. Curran Associates, Inc., 2008.
Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of
Operations Research, 39(4):1221–1243, 2014.
DanielRussoandBenjaminVanRoy. Aninformation-theoreticanalysisofthompsonsampling. The
Journal of Machine Learning Research, 17(1):2442–2471, 2016.
Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on
thompson sampling. arXiv preprint arXiv:1707.02038, 2017.
Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. Active Inference: Demystified and
Compared. Neural Computation, 33(3):674–712, January 2021a. ISSN 0899-7667. doi: 10.1162/
neco_a_01357.
Noor Sajid, Panagiotis Tigas, Alexey Zakharov, Zafeirios Fountas, and Karl Friston. Exploration
and preference satisfaction trade-off in reward-free learning. arXiv:2106.04316 [cs, q-bio], July
2021b.
Noor Sajid, Emma Holmes, Lancelot Da Costa, Cathy Price, and Karl Friston. A mixed generative
model of auditory word repetition, January 2022.
Anna C. Sales, Karl J. Friston, Matthew W. Jones, Anthony E. Pickering, and Rosalyn J. Moran.
Locus Coeruleus tracking of prediction errors optimises cognitive flexibility: An Active Inference
model. PLOS Computational Biology, 15(1):e1006267, January 2019. ISSN 1553-7358. doi:
10.1371/journal.pcbi.1006267.
Cansu Sancaktar, Marcel van Gerven, and Pablo Lanillos. End-to-End Pixel-Based Deep Active
Inference for Body Perception and Action. arXiv:2001.05847 [cs, q-bio], May 2020.
R. W. H. Sargent. Optimal control. Journal of Computational and Applied Mathematics, 124(1):
361–371, December 2000. ISSN 0377-0427. doi: 10.1016/S0377-0427(00)00418-0.
Jürgen Schmidhuber. Developmental robotics, optimal artificial curiosity, creativity, music, and
the fine arts. Connection Science, 18(2):173–187, June 2006. ISSN 0954-0091. doi: 10.1080/
09540090600768658.
Jürgen Schmidhuber. Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990–2010).
IEEE Transactions on Autonomous Mental Development, 2(3):230–247, September 2010. ISSN
1943-0604, 1943-0612. doi: 10.1109/TAMD.2010.2056368.
Tim Schneider, Boris Belousov, Hany Abdulsamad, and Jan Peters. Active Inference for Robotic
Manipulation, June 2022.
EricSchulzandSamuelJ.Gershman.Thealgorithmicarchitectureofexplorationinthehumanbrain.
CurrentOpinioninNeurobiology,55:7–14,2019.ISSN1873-6882.doi: 10.1016/j.conb.2018.11.003.
PhilippSchwartenbeck, ThomasH.B.FitzGerald, ChristophMathys, RayDolan, andKarlFriston.
The Dopaminergic Midbrain Encodes the Expected Certainty about Desired Outcomes. Cerebral
Cortex (New York, N.Y.: 1991), 25(10):3434–3445, October 2015a. ISSN 1460-2199. doi: 10.
1093/cercor/bhu159.
Philipp Schwartenbeck, Thomas H. B. FitzGerald, Christoph Mathys, Ray Dolan, Martin Kron-
bichler, and Karl Friston. Evidence for surprise minimization over value maximization in choice
behavior. Scientific Reports, 5:16575, November 2015b. ISSN 2045-2322. doi: 10.1038/srep16575.
25
PhilippSchwartenbeck,JohannesPassecker,TobiasUHauser,ThomasHBFitzGerald,MartinKron-
bichler,andKarlJFriston. Computationalmechanismsofcuriosityandgoal-directedexploration.
eLife, page 45, 2019.
Yoav Shoham, Rob Powers, and Trond Grenager. Multi-agent reinforcement learning: A critical
survey. Technical report, 2003.
DavidSilver,AjaHuang,ChrisJ.Maddison,ArthurGuez,LaurentSifre,GeorgevandenDriessche,
JulianSchrittwieser,IoannisAntonoglou,VedaPanneershelvam,MarcLanctot,SanderDieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with
deepneuralnetworksandtreesearch. Nature,529(7587):484–489,January2016. ISSN0028-0836,
1476-4687. doi: 10.1038/nature16961.
Ryan Smith, Philipp Schwartenbeck, Thomas Parr, and Karl J. Friston. An active inference model
of concept learning. bioRxiv, page 633677, May 2019. doi: 10.1101/633677.
Ryan Smith, Rayus Kuplicki, Justin Feinstein, Katherine L. Forthman, Jennifer L. Stewart, Mar-
tin P. Paulus, Tulsa 1000 Investigators, and Sahib S. Khalsa. A Bayesian computational model
reveals a failure to adapt interoceptive precision estimates across depression, anxiety, eating, and
substance use disorders. PLOS Computational Biology, 16(12):e1008484, December 2020a. ISSN
1553-7358. doi: 10.1371/journal.pcbi.1008484.
Ryan Smith, Rayus Kuplicki, Adam Teed, Valerie Upshaw, and Sahib S. Khalsa. Confirmatory evi-
dencethathealthyindividualscanadaptivelyadjustpriorexpectationsandinteroceptiveprecision
estimates, September 2020b.
Ryan Smith, Philipp Schwartenbeck, Thomas Parr, and Karl J. Friston. An Active Inference Ap-
proach to Modeling Structure Learning: Concept Learning as an Example Case. Frontiers in
Computational Neuroscience, 14, May 2020c. ISSN 1662-5188. doi: 10.3389/fncom.2020.00041.
Ryan Smith, Philipp Schwartenbeck, Jennifer L. Stewart, Rayus Kuplicki, Hamed Ekhtiari, and
Martin P. Paulus. Imprecise Action Selection in Substance Use Disorder: Evidence for Active
LearningImpairmentsWhenSolvingtheExplore-ExploitDilemma. Drugandalcoholdependence,
215:108208, October 2020d. ISSN 0376-8716. doi: 10.1016/j.drugalcdep.2020.108208.
Ryan Smith, Sahib S. Khalsa, and Martin P. Paulus. An Active Inference Approach to Dissecting
Reasons for Nonadherence to Antidepressants. Biological Psychiatry. Cognitive Neuroscience and
Neuroimaging, 6(9):919–934, September 2021a. ISSN 2451-9030. doi: 10.1016/j.bpsc.2019.11.012.
Ryan Smith, Namik Kirlic, Jennifer L. Stewart, James Touthang, Rayus Kuplicki, Sahib S. Khalsa,
Justin Feinstein, Martin P. Paulus, and Robin L. Aupperle. Greater decision uncertainty char-
acterizes a transdiagnostic patient sample during approach-avoidance conflict: A computational
modelling approach. Journal of psychiatry & neuroscience: JPN, 46(1):E74–E87, January 2021b.
ISSN 1488-2434. doi: 10.1503/jpn.200032.
Ryan Smith, Namik Kirlic, Jennifer L. Stewart, James Touthang, Rayus Kuplicki, Timothy J.
McDermott, Samuel Taylor, Sahib S. Khalsa, Martin P. Paulus, and Robin L. Aupperle. Long-
termstabilityofcomputationalparametersduringapproach-avoidanceconflictinatransdiagnostic
psychiatric patient sample. Scientific Reports, 11(1):11783, June 2021c. ISSN 2045-2322. doi:
10.1038/s41598-021-91308-x.
RyanSmith,AhmadMayeli,SamuelTaylor,ObadaAlZoubi,JessycaNaegele,andSahibS.Khalsa.
Gut inference: A computational modelling approach. Biological Psychology, 164:108152, Septem-
ber 2021d. ISSN 0301-0511. doi: 10.1016/j.biopsycho.2021.108152.
26
Ryan Smith, Karl J. Friston, and Christopher J. Whyte. A step-by-step tutorial on active inference
and its application to empirical data. Journal of Mathematical Psychology, 107:102632, April
2022a. ISSN 0022-2496. doi: 10.1016/j.jmp.2021.102632.
Ryan Smith, Samuel Taylor, Jennifer L. Stewart, Salvador M. Guinjoan, Maria Ironside, Namik
Kirlic, Hamed Ekhtiari, Evan J. White, Haixia Zheng, Rayus Kuplicki, Tulsa 1000 Investigators,
andMartinP.Paulus. SlowerLearningRatesfromNegativeOutcomesinSubstanceUseDisorder
over a 1-Year Period and Their Potential Predictive Utility. Computational Psychiatry, 6(1):
117–141, June 2022b. ISSN 2379-6227. doi: 10.5334/cpsy.85.
SusanneStillandDoinaPrecup.Aninformation-theoreticapproachtocuriosity-drivenreinforcement
learning. Theory in Biosciences = Theorie in Den Biowissenschaften, 131(3):139–148, September
2012. ISSN 1611-7530. doi: 10.1007/s12064-011-0142-z.
Martin Stolle and Doina Precup. Learning options in reinforcement learning. In International
Symposium on abstraction, reformulation, and approximation, pages 212–223. Springer, 2002.
James V. Stone. Information Theory: A Tutorial Introduction. Sebtel Press, England, 1st edition
edition, February 2015. ISBN 978-0-9563728-5-7.
James V Stone. Artificial Intelligence Engines: A Tutorial Introduction to the Mathematics of Deep
Learning. 2019.
Yi Sun, Faustino Gomez, and Juergen Schmidhuber. Planning to Be Surprised: Optimal Bayesian
Exploration in Dynamic Environments. arXiv:1103.5708 [cs, stat], March 2011.
Toshiyuki Tanaka. A Theory of Mean Field Approximation. page 10, 1999.
D. Gowanlock R. Tervo, Joshua B. Tenenbaum, and Samuel J. Gershman. Toward the neural
implementation of structure learning. Current Opinion in Neurobiology, 37:99–105, April 2016.
ISSN 1873-6882. doi: 10.1016/j.conb.2016.01.014.
Emanuel Todorov. Linearly-solvable markov decision problems. In Advances in neural information
processing systems, pages 1369–1376, 2007.
Emanuel Todorov. General duality between optimal control and estimation. 2008 47th IEEE Con-
ference on Decision and Control, pages 4286–4292, 2008. doi: 10.1109/cdc.2008.4739438.
EmanuelTodorov. Efficientcomputationofoptimalactions. Proceedings of the national academy of
sciences, 106(28):11478–11483, 2009.
Michel Tokic and Günther Palm. Value-Difference Based Exploration: Adaptive Control between
Epsilon-Greedy and Softmax. In Joscha Bach and Stefan Edelkamp, editors, KI 2011: Advances
in Artificial Intelligence, Lecture Notes in Computer Science, pages 335–346, Berlin, Heidelberg,
2011. Springer. ISBN 978-3-642-24455-1. doi: 10.1007/978-3-642-24455-1_33.
Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of
the 26th Annual International Conference on Machine Learning, ICML ’09, pages 1049–1056,
Montreal,Quebec,Canada,June2009.AssociationforComputingMachinery. ISBN978-1-60558-
516-1. doi: 10.1145/1553374.1553508.
Alexander Tschantz, Manuel Baltieri, Anil K. Seth, and Christopher L. Buckley. Scaling active
inference. arXiv:1911.10601 [cs, eess, math, stat], November 2019.
Alexander Tschantz, Beren Millidge, Anil K. Seth, and Christopher L. Buckley. Reinforcement
Learning through Active Inference. In ICLR, February 2020a.
27
Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. Learning action-oriented models
through active inference. PLOS Computational Biology, 16(4):e1007805, April 2020b. ISSN 1553-
7358. doi: 10.1371/journal.pcbi.1007805.
Bart van den Broek, Wim Wiegerinck, and Bert Kappen. Risk sensitive path integral control. UAI,
2010.
Otto van der Himst and Pablo Lanillos. Deep Active Inference for Partially Observable MDPs.
arXiv:2009.03622 [cs, stat], 1326:61–71, 2020. doi: 10.1007/978-3-030-64919-7_8.
J.VonNeumannandO.Morgenstern. Theory of Games and Economic Behavior. TheoryofGames
and Economic Behavior. Princeton University Press, Princeton, NJ, US, 1944.
Martin J. Wainwright and Michael I. Jordan. Graphical Models, Exponential Families, and Vari-
ational Inference. Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2007. ISSN
1935-8237, 1935-8245. doi: 10.1561/2200000001.
Robert C. Wilson, Andra Geana, John M. White, Elliot A. Ludvig, and Jonathan D. Cohen. Hu-
mans Use Directed and Random Exploration to Solve the Explore–Exploit Dilemma. Journal
of experimental psychology. General, 143(6):2074–2081, December 2014. ISSN 0096-3445. doi:
10.1037/a0038199.
Robert C. Wilson, Elizabeth Bonawitz, Vincent D. Costa, and R. Becket Ebitz. Balancing ex-
ploration and exploitation with information and randomization. Current Opinion in Behavioral
Sciences, 38:49–56, 2021. ISSN 2352-1554. doi: 10.1016/j.cobeha.2020.10.001.
He A Xu, Alireza Modirshanechi, Marco P Lehmann, Wulfram Gerstner, and Michael H Herzog.
Novelty is not surprise: Human exploratory and adaptive behavior in sequential decision-making.
PLOS Computational Biology, 17(6):e1009070, 2021.
Ernst Zermelo. über eine Anwendung der Mengenlehre auf die Theorie des Schachspiels. 1913.
B.Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy.
PhD thesis, Carnegie Mellon University, Pittsburgh, 2010.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. 2008.
Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann,
and Shimon Whiteson. VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-
Learning. arXiv:1910.08348 [cs, stat], February 2020.
Luisa M Zintgraf, Leo Feng, Cong Lu, Maximilian Igl, Kristian Hartikainen, Katja Hofmann, and
ShimonWhiteson. Explorationinapproximatehyper-statespaceformetareinforcementlearning.
In International Conference on Machine Learning, pages 12991–13001. PMLR, 2021.
Appendix A. Active inference and reinforcement learning
This paper considered how active inference can solve the stochastic control problem. In this ap-
pendix, we discuss the broader relationship between active inference and RL.
Loosely speaking, RL is the field of methodologies and algorithms that learn reward-maximising
actions from data and seek to maximise reward in the long run. Because RL is a data-driven field,
algorithmsareselectedbasedonhowwelltheyperformonbenchmarkproblems. Thishasproduced
a plethora of diverse algorithms, many designed to solve specific problems, each with their own
28
strengths and limitations. This makes RL difficult to characterise as a whole. Thankfully, many
approachestomodel-basedRLandcontrolcanbetracedbacktoapproximatingtheoptimalsolution
to the Bellman equation (Bellman and Dreyfus, 2015; Bertsekas and Shreve, 1996) (although this
maybecomecomputationallyintractableinhigh-dimensions(BartoandSutton,1992)). Ourresults
showed how and when decisions under active inference and such RL approaches are similar.
ThisappendixdiscusseshowactiveinferenceandRLrelateanddiffermoregenerally. Theirrela-
tionshiphasbecomeincreasinglyimportanttounderstand, asagrowingbodyofresearchhasbegun
to1)comparetheperformanceofactiveinferenceandRLmodelsinsimulatedenvironments(Cullen
etal.,2018;Millidge,2020;Sajidetal.,2021a), 2)applyactiveinferencetomodelhumanbehaviour
on reward learning tasks (Smith et al., 2020d, 2021b,c, 2022b), and 3) consider the complementary
predictions and interpretations they each offer in computational neuroscience, psychology, and psy-
chiatry (Cullen et al., 2018; Huys et al., 2012; Schwartenbeck et al., 2015a, 2019; Tschantz et al.,
2020b).
A.1 Main differences between active inference and reinforcement learning
Philosophy. Active inference and RL differ profoundly in their philosophy. RL derives from
the normative principle of maximising reward (Barto and Sutton, 1992), while active inference
describessystemsthatmaintaintheirstructuralintegrityovertime(Barpetal.,2022;Fristonetal.,
2022). Despite this difference, there are many practical similarities between these frameworks. For
example,recallthatbehaviourinactiveinferenceiscompletelydeterminedbytheagent’spreferences,
determined as priors in their generative model. Crucially, log priors can be interpreted as reward
functions and vice-versa, which is how behaviour under RL and active inference can be related.
Modelbasedandmodelfree. Activeinferenceagentsalways embodyagenerative(i.e.,forward)
model of their environment, while RL comprises both model-based algorithms and simpler model-
free algorithms. In brief, ’model-free’ means that agents learn a reward-maximising state-action
mapping, based on updating cached state-action pair values, through initially random actions that
do not consider future state transitions. In contrast, model-based RL algorithms attempt to extend
stochastic control approaches by learning the dynamics and reward function from data. Recall that
stochasticcontrolcallsonstrategiesthatevaluatedifferentactionsonacarefullyhandcraftedforward
model of dynamics (i.e., known transition probabilities) to finally execute the reward-maximising
action. Under this terminology, all active inference agents are model-based.
Modelling exploration. Exploratorybehaviour—whichcanimproverewardmaximisationinthe
long run—is implemented differently in the two approaches. In most cases, RL implements a simple
formofexplorationbyincorporatingrandomnessindecision-making(TokicandPalm,2011;Wilson
et al., 2014), where the level of randomness may or may not change over time as a function of
uncertainty. In other cases, RL incorporates ad-hoc information bonuses in the reward function
or other decision-making objectives to build in directed exploratory drives (e.g., upper confidence
bound algorithms or Thompson sampling). In contrast, directed exploration emerges naturally
within active inference through interactions between the risk and ambiguity terms in the expected
free energy (Da Costa et al., 2020; Schwartenbeck et al., 2019). This addresses the explore-exploit
dilemma and confers the agent with artificial curiosity (Friston et al., 2017b; Schmidhuber, 2010;
Schwartenbecketal.,2019;StillandPrecup,2012),asopposedtotheneedtoaddad-hocinformation
bonus terms (Tokic and Palm, 2011). We expand on this relationship further in Appendix A.3.
Controlandlearningasinference. Activeinferenceintegratesstate-estimation,learning,decision-
making, and motor control under the single objective of minimising free energy (Da Costa et al.,
2020). In fact, active inference extends previous work on the duality between inference and con-
trol(Kappenetal.,2012;Rawliketal.,2013;Todorov,2008;Toussaint,2009)tosolvemotorcontrol
problems via approximate inference (i.e., planning as inference) (Attias, 2003; Botvinick and Tou-
ssaint, 2012; Friston et al., 2012, 2009; Millidge et al., 2020b). Therefore, some of the closest RL
29
methods to active inference are control as inference, also known as maximum entropy RL (Levine,
2018b;Millidgeetal.,2020b;Ziebart,2010),thoughonemajordifferenceisinthechoiceofdecision-
making objective. Loosely speaking, these aforementioned methods minimise the risk term of the
expected free energy, while active inference also minimises ambiguity.
Useful features of active inference.
1. Active inference allows great flexibility and transparency when modelling behaviour. It affords
explainable decision-making as a mixture of information- and reward-seeking policies that are ex-
plicitly encoded (and evaluated in terms of expected free energy) in the generative model as priors,
which are specified by the user (Da Costa et al., 2022a). As we have seen, the kind of behaviour
that can be produced includes the optimal solution to the Bellman equation.
2. Active inference accommodates deep hierarchical generative models combining both discrete and
continuous state-spaces (Friston et al., 2017c, 2018; Parr et al., 2021).
3. The expected free energy objective optimised during planning subsumes many approaches used
to describe and simulate decision-making in the physical, engineering, and life sciences, affording
it various interesting properties as an objective (Figure 3 and (Friston et al., 2021)). For exam-
ple, exploratory and exploitative behaviour are canonically integrated, which finesses the need for
manually incorporating ad-hoc exploration bonuses in the reward function (Da Costa et al., 2022b).
4. Activeinferencegoesbeyondstate-actionpoliciesthatpredominateintraditionalRLtosequential
policy optimisation. In sequential policy optimisation, one relaxes the assumption that the same
action is optimal given a particular state—and acknowledges that the sequential order of actions
may matter. This is similar to the linearly-solvable MDP formulation presented by (Todorov, 2007,
2009), where transition probabilities directly determine actions, and an optimal policy specifies
transitions that minimise some divergence cost. This way of approaching policies is perhaps most
apparent in terms of exploration. Put simply, it is clearly better to explore and then exploit than
theconverse. Becauseexpectedfreeenergyisafunctionalofbeliefs,explorationbecomesanintegral
part of decision-making—in contrast with traditional RL approaches that try to optimise a reward
function of states. In other words, active inference agents will explore until enough uncertainty is
resolved for reward maximising, goal-seeking imperatives to start to predominate.
Such advantages should motivate future research to better characterise the environments in which
these properties offer useful advantages—such as where performance benefits from learning and
planning at multiple temporal scales, and from the ability to select policies that resolve both state
and parameter uncertainty.
A.2 Reward learning
Given the focus on relating active inference to the objective of maximising reward, it is worth
briefly illustrating how active inference can learn the reward function from data and its potential
connections to representative RL approaches. One common approach for active inference to learn
a reward function (Smith et al., 2020d, 2022b) is to set preferences over observations rather than
states, which corresponds to assuming that inferences over states given outcomes are accurate
D [Q((cid:126)s|(cid:126)a,o˜)|C((cid:126)s)]=D [Q((cid:126)o|(cid:126)a,o˜)|C((cid:126)o)]+E [D [Q((cid:126)s|(cid:126)o,o˜,(cid:126)a)|P ((cid:126)s|(cid:126)o)]]
KL KL Q((cid:126)o|(cid:126)a,o˜) KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Risk(states) Risk(outcomes) ≈0
≈D [Q((cid:126)o|(cid:126)a,o˜)|C((cid:126)o)],
KL
(cid:124) (cid:123)(cid:122) (cid:125)
Risk(outcomes)
i.e., equality holds whenever the free energy minimum is reached (16). Then one sets the prefer-
ence distribution such that the observations designated as rewards are most preferred. In the zero
30
temperature limit (9), preferences only assign mass to reward-maximising observations. When for-
mulated in this way, the reward signal is treated as sensory data, as opposed to a separate signal
from the environment. When one sets allowable actions (controllable state transitions) to be fully
deterministic such that the selection of each action will transition the agent to a given state with
certainty, the emerging dynamics are such that the agent chooses actions to resolve uncertainty
about the probability of observing reward under each state. Thus, learning the reward probabilities
of available actions amounts to learning the likelihood matrix P((cid:126)o | (cid:126)s) := o ·As , where A is a
t t
stochastic matrix. This is done by setting a prior a over A, i.e., a matrix of non-negative compo-
nents, the columns of which are Dirichlet priors over the columns of A. The agent then learns by
accumulating Dirichlet parameters. Explicitly, at the end of a trial or episode, one sets (Da Costa
et al., 2020; Friston et al., 2016)
T
(cid:88)
a←a+ o ⊗Q(s |o ) (18)
τ τ 0:T
τ=0
In (18), Q(s |o ) is seen as a vector of probabilities over the state-space S, corresponding to the
τ 0:T
probability of having been in one or another state at time the τ after having gathered observations
throughout the trial. This rule simply amounts to counting observed state-outcome pairs, which is
equivalent to state-reward pairs when the observation modalities correspond to reward.
One should not conflate this approach with the update rule consisting of accumulating state-
observation counts in the likelihood matrix
T
(cid:88)
A←A+ o ⊗Q(s |o ) (19)
τ τ 0:T
τ=0
and then normalising its columns to sum to one when computing probabilities. The latter simply
approximatesthelikelihoodmatrixAbyaccumulatingthenumberofobservedstate-outcomepairs.
Thisisdistinctfromtheapproachoutlinedabove,whichencodesuncertaintyoverthematrixA,asa
probabilitydistributionoverpossibledistributionsP(o |s ). Theagentisinitiallyveryunconfident
t t
about A, which means that it doesn’t place high probability mass on any specification of P(o |
t
s ). This uncertainty is gradually resolved by observing state-observation (or state-reward) pairs.
t
Computationally, it is a general fact of Dirichlet priors that an increase in elements of a causes the
entropy of P(o | s ) to decrease. As the terms added in (18) are always positive, one choice of
t t
distribution P(o | s )—which best matches available data and prior beliefs—is ultimately singled
t t
out. In other words, the likelihood mapping is learned.
Theupdateruleconsistingofaccumulatingstate-observationcountsinthelikelihoodmatrix(19)
(i.e., not incorporating Dirichlet priors) bears some similarity to off-policy learning algorithms such
asQ-learning. InQ-learning,theobjectiveistofindthebestactiongiventhecurrentobservedstate.
Forthis,theQ-learningagentaccumulatesvaluesforstate-actionpairswithrepeatedobservationof
rewarding/punishing action outcomes—much like state-observation counts. This allows it to learn
the Q-value function that defines a reward maximising policy.
As always in partially observed environments, we cannot guarantee that the true likelihood
mapping will be learned in practice. Please see (Smith et al., 2019) for examples where, although
not in an explicit reward-learning context, learning the likelihood can be more or less successful in
different situations. Learning the true likelihood fails when the inference over states is inaccurate,
such as when using too severe of a mean-field approximation to the free energy (Blei et al., 2017;
Parr et al., 2019; Tanaka, 1999), which causes the agent to misinfer states and thereby accumulate
Dirichletparametersinthewronglocations. Intuitively, thisamountstojumpingtoconclusionstoo
quickly.
Remark 19. If so desired, reward learning in active inference can also be equivalently formulated
as learning transition probabilities P(s |s ,a ). In this alternative setup (as exemplified in (Sales
t+1 t t
31
et al., 2019)), mappings between reward states and reward outcomes in A are set as identity ma-
trices, and the agent instead learns the probability of transitioning to states that deterministically
generate preferred (rewarding) observations given the choice of each action sequence. The transition
probabilitiesundereachactionarelearnedinasimilarfashionasabove (18), byaccumulatingcounts
on a Dirichlet prior over P(s |s ,a ). See (Da Costa et al., 2020, Appendix) for details.
t+1 t t
Given the model-based Bayesian formulation of active inference, more direct links can be made
betweentheactiveinferenceapproachtorewardlearningdescribedaboveandotherBayesianmodel-
basedRLapproaches. Forsuchlinkstoberealised,theBayesianRLagentwouldberequiredtohave
a prior over a prior (e.g., a prior over the reward function prior or transition function prior). One
waytoimplicitlyincorporatethisisthroughThompsonsampling(Ghavamzadehetal.,2016;Russo
and Van Roy, 2014, 2016; Russo et al., 2017). While not the focus of this paper, future work could
furtherexaminethelinksbetweenrewardlearninginactiveinferenceandmodel-basedBayesianRL
schemes.
A.3 Solving the exploration-exploitation dilemma
An important distinction between active inference and reinforcement learning schemes is how they
solve the exploration-exploitation dilemma.
The exploration-exploitation dilemma (Berger-Tal et al., 2014) arises whenever an agent has
incomplete information about its environment, such as when the environment is partially observed,
or the generative model has to be learned. The dilemma is then about deciding whether to execute
actionsaimingtocollectrewardbasedonimperfectinformationabouttheenvironment,ortoexecute
actions aiming to gather more information—allowing the agent to reap more reward in the future.
Intuitively,itisalwaysbesttoexploreandthenexploit,butoptimisingthistrade-offcanbedifficult.
Activeinferencebalancesexplorationandexploitationthroughminimisingtheriskandambiguity
inherent in the minimisation of expected free energy. This balance is context-sensitive and can be
adjusted by modifying the agent’s preferences (Da Costa et al., 2022a). In turn, the expected free
energy is obtained from a description of agency in biological systems derived from physics (Barp
et al., 2022; Friston et al., 2022).
Modern RL algorithms integrate exploratory and exploitative behaviour in many different ways.
Oneoptioniscuriosity-drivenrewardstoencourageexploration. MaximumentropyRLandcontrol-
as-inference make decisions by minimising a KL divergence to the target distribution (Eysenbach
and Levine, 2019; Haarnoja et al., 2017, 2018; Levine, 2018a; Todorov, 2008; Ziebart et al., 2008),
which combines reward maximisation with maximum entropy over states. This is similar to active
inference on MDPs (Millidge et al., 2020b). Similarly, the model-free Soft Actor-Critic (Haarnoja
et al., 2018) algorithm maximises both expected reward and entropy. This outperforms other state-
of-the-art algorithms in continuous control environments and has been shown to be more sample
efficient than its reward-maximising counterparts (Haarnoja et al., 2018). Hyper Zintgraf et al.
(2021) proposes reward maximisation alongside minimising uncertainty over both external states
and model parameters. Bayes-adaptive RL (Guez et al., 2013a,b; Ross et al., 2008; Zintgraf et al.,
2020)providespoliciesthatbalanceexplorationandexploitationwiththeaimofmaximisingreward.
Thompsonsamplingprovidesawaytobalanceexploitingcurrentknowledgetomaximiseimmediate
performanceandaccumulatingnewinformationtoimprovefutureperformance(Russoetal.,2017).
This reduces to optimising dual objectives, reward maximisation and information gain, similar to
activeinferenceonPOMDPs. Empirically,Sajidetal.(2021a)demonstratedthatanactiveinference
agent and a Bayesian model-based RL agent using Thompson sampling exhibit similar behaviour
when preferences are defined over outcomes. They also highlighted that, when completely removing
therewardsignalfromtheenvironment,thetwoagentsbothselectpoliciesthatmaximisesomesort
of information gain.
In general, the way each of these approaches to the exploration-exploitation dilemma differ in
theory and in practice remains largely unexplored.
32
Appendix B. Proofs
B.1 Proof of Proposition 9
Note that a Bellman optimal state-action policy Π∗ is a maximal element according to the partial
ordering ≤. Existence thus consists of a simple application of Zorn’s lemma. Zorn’s lemma states
that if any increasing chain
Π ≤Π ≤Π ≤... (20)
1 2 3
has an upper bound that is a state-action policy, then there is a maximal element Π∗.
Giventhechain(20), weconstructanupperbound. WeenumerateA×S×Tby(α ,σ ,t ),...,
1 1 1
(α ,σ ,t ). Then the state-action policy sequence
N N N
Π (α |σ ,t ), n=1,2,3,...
n 1 1 1
is bounded within [0,1]. By the Bolzano-Weierstrass theorem, there exists a subsequence Π (α |
nk 1
σ ,t ), k = 1,2,3,... that converges. Similarly, Π (α | σ ,t ) is also a bounded sequence, and
1 1 nk 2 2 2
by Bolzano-Weierstrass it has a subsequence Π (a | σ ,t ) that converges. We repeatedly take
subsequences until N. To ease notation, call the
nk
rjesu
2
lting
2
su
2
bsequence Π , m=1,2,3,...
m
Withthis,wedefineΠˆ =lim Π . ItisstraightforwardtoseethatΠˆ isastate-actionpolicy:
m→∞ m
Πˆ(α|σ,t)= lim Π (α|σ,t)∈[0,1], ∀(α,σ,t)∈A×S×T,
m
m→∞
(cid:88) Πˆ(α|σ,t)= lim (cid:88) Π (α|σ,t)=1, ∀(σ,t)∈S×T.
m
m→∞
α∈A α∈A
To show that Πˆ is an upper bound, take any Π in the original chain of state-action policies (20).
Thenbythedefinitionofanincreasingsubsequence,thereexistsanindexM ∈Nsuchthat∀k ≥M:
Π ≥Π. Since limits commute with finite sums, we have v (s,t)=lim v (s,t)≥v (s,t)≥
k Πˆ m→∞ Πm Πk
v (s,t) for any (s,t) ∈ S×T. Thus, by Zorn’s lemma there exists a Bellman optimal state-action
Π
policy Π∗.
B.2 Proof of Proposition 10
1)⇒2): We only need to show assertion (b). By contradiction, suppose that ∃(s,α)∈S×A such
that Π(α|s,0)>0 and
E [R(s )|s =s,a =α]<maxE [R(s )|s =s,a =a].
Π 1:T 0 0 Π 1:T 0 0
a∈A
We let α(cid:48) be the Bellman optimal action at state s and time 0 defined as
α(cid:48) :=argmaxE [R(s )|s =s,a =a].
Π 1:T 0 0
a∈A
Then, we let Π(cid:48) be the same state-action policy as Π except that Π(cid:48)(·|s,0) assigns α(cid:48) deterministi-
cally. Then,
(cid:88)
v (s,0)= E [R(s )|s =s,a =a]Π(a|s,0)
Π Π 1:T 0 0
a∈A
<maxE [R(s )|s =s,a =a]
Π 1:T 0 0
a∈A
=E [R(s )|s =s,a =α(cid:48)]Π(cid:48)(α(cid:48) |s,0)
Π(cid:48) 1:T 0 0
(cid:88)
= E [R(s )|s =s,a =a]Π(cid:48)(a|s,0)
Π(cid:48) 1:T 0 0
a∈A
=v (s,0).
Π(cid:48)
33
So Π is not Bellman optimal, which is a contradiction.
1)⇐2):WeonlyneedtoshowthatΠmaximisesv (s,0),∀s∈S. Bycontradiction,thereexists
Π
a state-action policy Π(cid:48) and a state s∈S such that
v (s,0)<v (s,0)
Π Π(cid:48)
(cid:88) (cid:88)
⇐⇒ E [R(s )|s =s,a =a]Π(a|s,0)< E [R(s )|s =s,a =a]Π(cid:48)(a|s,0).
Π 1:T 0 0 Π(cid:48) 1:T 0 0
a∈A a∈A
By (a) the left hand side equals
maxE [R(s )|s =s,a =a].
Π 1:T 0 0
a∈A
Unpacking the expression on the right-hand side:
(cid:88)
E [R(s )|s =s,a =a]Π(cid:48)(a|s,0)
Π(cid:48) 1:T 0 0
a∈A
(cid:88)(cid:88)
= E [R(s )|s =σ]P(s =σ |s =s,a =a)Π(cid:48)(a|s,0)
Π(cid:48) 1:T 1 1 0 0
a∈Aσ∈S
(21)
(cid:88)(cid:88)
= {E [R(s )|s =σ]+R(σ)}P(s =σ |s =s,a =a)Π(cid:48)(a|s,0)
Π(cid:48) 2:T 1 1 0 0
a∈Aσ∈S
(cid:88)(cid:88)
= {v (σ,1)+R(σ)]P(s =σ |s =s,a =a)Π(cid:48)(a|s,0)
Π(cid:48) 1 0 0
a∈Aσ∈S
Since Π is Bellman optimal when restricted to {1,...,T} we have v (σ,1) ≤ v (σ,1),∀σ ∈ S.
Π(cid:48) Π
Therefore,
(cid:88)(cid:88)
{v (σ,1)+R(σ)]P(s =σ |s =s,a =a)Π(cid:48)(a|s,0)
Π(cid:48) 1 0 0
a∈Aσ∈S
(cid:88)(cid:88)
≤ {v (σ,1)+R(σ)]P(s =σ |s =s,a =a)Π(cid:48)(a|s,0).
Π 1 0 0
a∈Aσ∈S
Repeating the steps above (21), but in reverse order, yields
(cid:88) (cid:88)
E [R(s )|s =s,a =a]Π(cid:48)(a|s,0)≤ E [R(s )|s =s,a =a]Π(cid:48)(a|s,0)
Π(cid:48) 1:T 0 0 Π 1:T 0 0
a∈A a∈A
However,
(cid:88)
E [R(s )|s =s,a =a]Π(cid:48)(a|s,0)<maxE [R(s )|s =s,a =a]
Π 1:T 0 0 Π 1:T 0 0
a∈A
a∈A
which is a contradiction.
B.3 Proof of Proposition 11
• We first prove that state-action policies Π defined as in (2) are Bellman optimal by induction
on T.
T =1:
Π(a|s,0)>0 ⇐⇒ a∈argmaxE[R(s )|s =s,a =a], ∀s∈S
1 0 0
a
isaBellmanoptimalstate-actionpolicyasitmaximisesthetotalrewardpossibleintheMDP.
34
LetT >1befiniteandsupposethatthePropositionholdsforMDPswithatemporalhorizon
of T −1. This means that
Π(a|s,T −1)>0 ⇐⇒ a∈argmaxE[R(s )|s =s,a =a], ∀s∈S
T T−1 T−1
a
Π(a|s,T −2)>0 ⇐⇒ a∈argmaxE [R(s )|s =s,a =a], ∀s∈S
Π T−1:T T−2 T−2
a
.
.
.
Π(a|s,1)>0 ⇐⇒ a∈argmaxE [R(s )|s =s,a =a], ∀s∈S
Π 2:T 1 1
a
is a Bellman optimal state-action policy on the MDP restricted to times 1 to T. Therefore,
since
Π(a|s,0)>0 ⇐⇒ a∈argmaxE [R(s )|s =s,a =a], ∀s∈S
Π 1:T 0 0
a
Proposition 10 allows us to deduce that Π is Bellman optimal.
• We now show that any Bellman optimal state-action policy satisfies the backward induction
algorithm (2).
Supposebycontradictionthatthereexistsastate-actionpolicyΠthatisBellmanoptimalbut
does not satisfy (2). Say, ∃(a,s,t)∈A×S×T,t<T, such that
Π(a|s,t)>0 and a∈/ argmaxE [R(s )|s =s,a =α].
Π t+1:T t t
α∈A
This implies
E [R(s )|s =s,a =a]<maxE [R(s )|s =s,a =α].
Π t+1:T t t Π t+1:T t t
α∈A
Let a˜ ∈ argmax E [R(s ) | s = s,a = α]. Let Π˜ be a state-action policy such that
α Π t+1:T t t
Π˜(· | s,t) assigns a˜ ∈ A deterministically, and such that Π˜ = Π otherwise. Then we can
contradict the Bellman optimality of Π as follows
v (s,t)=E [R(s )|s =s]
Π Π t+1:T t
(cid:88)
= E [R(s )|s =s,a =α]Π(α|s,t)
Π t+1:T t t
α∈A
<maxE [R(s )|s =s,a =α]
Π t+1:T t t
α∈A
=E [R(s )|s =s,a =a˜]
Π t+1:T t t
=E [R(s )|s =s,a =a˜]
Π˜ t+1:T t t
= (cid:88) E [R(s )|s =s,a =α]Π˜(α|s,t)
Π˜ t+1:T t t
α∈A
=v (s,t).
Π˜
35
B.4 Proof of Lemma 14
lim argminD [Q((cid:126)s|(cid:126)a,s )|C ((cid:126)s)]
KL t β
β→+∞ (cid:126)a
= lim argmin−H[Q((cid:126)s|(cid:126)a,s )]+E [−logC ((cid:126)s)]
β→+∞ (cid:126)a
t Q((cid:126)s|(cid:126)a,st) β
= lim argmin−H[Q((cid:126)s|(cid:126)a,s )]−βE [R((cid:126)s)]
β→+∞ (cid:126)a
t Q((cid:126)s|(cid:126)a,st)
= lim argmaxH[Q((cid:126)s|(cid:126)a,s )]+βE [R((cid:126)s)]
β→+∞ (cid:126)a
t Q((cid:126)s|(cid:126)a,st)
⊆ lim argmaxβE [R((cid:126)s)]
β→+∞ (cid:126)a
Q((cid:126)s|(cid:126)a,st)
=argmaxE [R((cid:126)s)]
(cid:126)a
Q((cid:126)s|(cid:126)a,st)
The inclusion follows from the fact that, as β → +∞, a minimiser of the expected free energy
hastomaximiseE [R((cid:126)s)]. Amongsuchactionsequences,theexpectedfreeenergyminimisers
Q((cid:126)s|(cid:126)a,st)
are those that maximise the entropy of future states H[Q((cid:126)s|(cid:126)a,s )].
t
B.5 Proof of Theorem 15
When T =1 the only action is a . We fix an arbitrary initial state s =s∈S. By Proposition 10, a
0 0
Bellmanoptimalstate-actionpolicyisfullycharacterisedbyanactiona∗ thatmaximisesimmediate
0
reward
a∗ ∈argmaxE[R(s )|s =s,a =a].
0 a∈A 1 0 0
RecallthatbyRemark6, thisexpectationstandsforreturnunderthetransitionprobabilitiesofthe
MDP
a∗ ∈argmaxE [R(s )].
0 a∈A P(s1|a0=a,s0=s) 1
Since transition probabilities are assumed to be known (3), this reads
a∗ ∈argmaxE [R(s )].
0 a∈A Q(s1|a0=a,s0=s) 1
On the other hand,
a ∈ lim argmaxexp(−G(a|s ))
0 t
β→+∞ a∈A
= lim argminG(a|s ).
t
β→+∞ a∈A
By Lemma 14, this implies
a ∈argmaxE [R(s )],
0
a∈A
Q(s1|a0=a,s0=s) 1
which concludes the proof.
B.6 Proof of Theorem 16
We prove this result by induction on the temporal horizon T of the MDP.
The proof of the Theorem when T =1 can be seen from the proof of Theorem 15. Now suppose
that T >1 is finite and that the Theorem holds for MDPs with a temporal horizon of T −1.
Our induction hypothesis says that Q(a | s ), as defined in (14), is a Bellman optimal state-
τ τ
action policy on the MDP restricted to times τ = 1,...,T. Therefore, by Proposition 10, we only
need to show that the action a selected under active inference satisfies
0
a ∈argmaxE [R((cid:126)s)|s ,a =a].
0 Q 0 0
a∈A
36
This is simple to show as
argmaxE [R((cid:126)s)|s ,a =a]
Q 0 0
a∈A
=argmaxE [R((cid:126)s)] (by Remark 6)
a∈A
P((cid:126)s|a1:T,a0=a,s0)Q((cid:126)a|s1:T)
=argmaxE [R((cid:126)s)] (as the transitions are known)
a∈A
Q((cid:126)s,(cid:126)a|a0=a,s0)
= lim argmaxE [βR((cid:126)s)]
β→+∞ a∈A
Q((cid:126)s,(cid:126)a|a0=a,s0)
⊇ lim argmaxE [βR((cid:126)s)]−H[Q((cid:126)s|(cid:126)a,a =a,s )]
β→+∞ a∈A
Q((cid:126)s,(cid:126)a|a0=a,s0) 0 0
= lim argminE [−logC ((cid:126)s)]−H[Q((cid:126)s|(cid:126)a,a =a,s )] (by (8))
β→+∞ a∈A
Q((cid:126)s,(cid:126)a|a0=a,s0) β 0 0
= lim argminE D [Q((cid:126)s|(cid:126)a,a =a,s )|C ((cid:126)s)]
β→+∞ a∈A
Q((cid:126)s,(cid:126)a|a0=a,s0) KL 0 0 β
= lim argminG(a =a|s ) (by (12)).
0 0
β→+∞ a∈A
Therefore, an action a selected under active inference is a Bellman optimal state-action policy on
0
finite temporal horizons. Furthermore, the inclusion follows from the fact that if there are multiple
actions that maximise expected reward, that which is selected under active inference maximises the
entropy of beliefs about future states.
B.7 Proof of Proposition 18
Unpacking the zero temperature limit
lim argminG((cid:126)a|o˜)
β→+∞ (cid:126)a
= lim argminD [Q((cid:126)s|(cid:126)a,o˜)|C ((cid:126)s)]+E H[P((cid:126)o|(cid:126)s)]
KL β Q((cid:126)s|(cid:126)a,o˜)
β→+∞ (cid:126)a
= lim argmin−H[Q((cid:126)s|(cid:126)a,o˜)]+E [−logC ((cid:126)s)]+E H[P((cid:126)o|(cid:126)s)]
Q((cid:126)s|(cid:126)a,o˜) β Q((cid:126)s|(cid:126)a,o˜)
β→+∞ (cid:126)a
= lim argmin−H[Q((cid:126)s|(cid:126)a,o˜)]−βE [R((cid:126)s)]+E H[P((cid:126)o|(cid:126)s)] (by (8))
Q((cid:126)s|(cid:126)a,o˜) Q((cid:126)s|(cid:126)a,o˜)
β→+∞ (cid:126)a
⊆ lim argmaxβE [R((cid:126)s)]
Q((cid:126)s|(cid:126)a,o˜)
β→+∞ (cid:126)a
=argmaxE [R((cid:126)s)]
Q((cid:126)s|(cid:126)a,o˜)
(cid:126)a
The inclusion follows from the fact that as β → +∞ a minimiser of the expected free energy
has first and foremost to maximise E [R((cid:126)s)]. Among such action sequences, the expected free
Q((cid:126)s|(cid:126)a,o˜)
energy minimisers are those that maximise the entropy of (beliefs about) future states H[Q((cid:126)s|(cid:126)a,o˜)]
and resolve ambiguity about future outcomes by minimising E H[P((cid:126)o|(cid:126)s)].
Q((cid:126)s|(cid:126)a,o˜)
37

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "Reward Maximisation through Discrete Active Inference"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.