=== IMPORTANT: ISOLATE THIS PAPER ===
You are revising a summary for ONLY the paper below. Do NOT reference or use content from any other papers.
Paper Title: Active inference: demystified and compared
Citation Key: sajid2019active
REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Issues to fix:
1. Title mismatch: Paper title 'Active inference: demystified and compared' not found in summary
2. Too short: 13 words (minimum 200)

Current draft:
Okay, I understand. I will follow these instructions precisely to generate the summary.

Key terms: demystified, learning, sajid, compared, account, agents, reinforcement, active

=== FULL PAPER TEXT ===
Active inference: demystified and compared
1 2 1 1
Noor Sajid ,Philip J. Ball ,Thomas Parr and Karl J. Friston
1
Wellcome Centre for Human Neuroimaging, UCL Queen Square Institute of Neurol-
ogy,London,UKWC1N3AR.
2
Machine Learning Research Group, Department of Engineering Science, University
ofOxford.
Correspondence: NoorSajid
TheWellcomeCentreforHumanNeuroimaging,
UCLQueenSquareInstituteofNeurology,
London,UKWC1N3AR.
+44(0)2034484362
noor.sajid.18@ucl.ac.uk
Abstract
Active inference is a first principle account of how autonomous agents operate in dy-
namic, non-stationary environments. This problem is also considered in reinforcement
learning,butlimitedworkexistsoncomparingthetwoapproachesonthesamediscrete-
0202
tcO
03
]IA.sc[
3v36801.9091:viXra
stateenvironments. Inthispaper,weprovide: 1)anaccessibleoverviewofthediscrete-
state formulation of active inference, highlighting natural behaviors in active inference
that are generally engineered in reinforcement learning; 2) an explicit discrete-state
comparison between active inference and reinforcement learning on an OpenAI gym
baseline. Webeginbyprovidingacondensedoverviewoftheactiveinferenceliterature,
in particular viewing the various natural behaviors of active inference agents through
the lens of reinforcement learning. We show that by operating in a pure belief-based
setting, active inference agents can carry out epistemic exploration — and account for
uncertainty about their environment — in a Bayes–optimal fashion. Furthermore, we
showthattherelianceonanexplicitrewardsignalinreinforcementlearningisremoved
inactiveinference,whererewardcansimplybetreatedasanotherobservationwehave
a preference over; even in the total absence of rewards, agent behaviors are learned
through preference learning. We make these properties explicit by showing two sce-
nariosinwhichactiveinferenceagentscaninferbehaviorsinreward-freeenvironments
comparedtobothQ-learningandBayesianmodel-basedreinforcementlearningagents;
by placing zero prior preferences over rewards and by learning the prior preferences
overtheobservationscorrespondingtoreward. Weconcludebynotingthatthisformal-
ismcanbeappliedtomorecomplexsettings;e.g.,roboticarmmovement,Atarigames,
etc., if appropriate generative models can be formulated. In short, we aim to demystify
the behavior of active inference agents by presenting an accessible discrete state-space
and time formulation, and demonstrate these behaviors in a OpenAI gym environment,
alongsidereinforcementlearningagents.
Keywords: active inference, variational Bayesian inference, free energy principle,
2
generativemodels,reinforcementlearning
1 Introduction
Active inference provides a framework (derived from first principles) for solving and
understandingthebehaviorofautonomousagentsinsituationsrequiringdecision-making
under uncertainty [Friston et al., 2017a,b]. It uses the free energy principle to describe
the properties of random dynamical systems (such as an agent in an environment), and
byminimizingexpectedfreeenergyovertime,Bayes–optimalbehaviorcanbeobtained
for a given environment [Friston et al., 2014, Friston, 2019]. More concretely, this op-
timal behavior is determined by evaluating evidence (i.e., marginal likelihood) under
an agent’s generative model of outcomes [Friston et al., 2016]. The agent’s generative
model of the environment is an abstraction, which assumes certain internal (hidden)
states give rise to these outcomes. One goal of the agent is to infer what these hidden
states are, given a set of outcomes. The generative model also provides a way, through
searching and planning, to form beliefs about the future. Thus, the agent can make
informed decisions over which sequence of actions (i.e., policies) it is most likely to
choose. In active inference, due to its Bayesian formulation, the most likely policies
leadtoBayes–optimaloutcomes(i.e.,thosemostcoherentwithpriorbeliefs). Thisfor-
mulation has two complementary objectives: 1) infer Bayes–optimal behavior, and 2)
optimize the generative model based on the agent’s ability to infer which hidden states
gave rise to the observed data. Both can be achieved, simultaneously, by minimizing
free energy functionals. This free energy formulation gives rise to realistic behaviors,
3
such as natural exploration-exploitation trade-offs, and — by being fully Bayesian —
is amenable to on-line learning settings, where the environment is non-stationary. This
follows from the ability to model uncertainty over contexts [Friston et al., 2015, Parr
andFriston,2017].
Active inference can also be seen as providing a formal framework for jointly opti-
mizing action and perception [Millidge et al., 2020a]. In the context of machine learn-
ing, this is often referred to as planning as inference [Attias, 2003, Botvinick and Tou-
ssaint, 2012, Baker and Tenenbaum, 2014, Millidge et al., 2020a], and in the case of
non-equilibrium physics, it is analogous to self-organization or self-assembly [Crauel
andFlandoli,1994,Seifert,2012,Friston,2019].
The main contributions of active inference, in contrast to analogous reinforcement
learning(RL)frameworks,followfromitscommitmentstoapurebelief-basedscheme.
Reinforcement learning is a broad term used in different fields. To make meaningful
comparisons between active inference and reinforcement learning, we commit to the
definition of reinforcement learning in [Sutton et al., 1998, Sutton and Barto, 2018]:
“reinforcement learning is learning what to do – how to map situations to actions – so
as to maximize a numerical reward signal”. RL algorithms, under this definition, can
be model-based or model-free. Model-based methods learn a model of environmental
dynamicswhichisusedtoinferapolicythatmaximizeslong-termreward,whilemodel-
free RL estimates a policy that maximizes long-term reward directly from trajectory
data. Throughoutthepaper,RLreferstobothmodel-basedandmodel-freeunlessstated
otherwise. This definition rests on the reward hypothesis, that “any goal or purpose
can be well thought of as maximization of the expected value of the cumulative sum
4
of a received scalar signal (reward)” [Sutton and Barto, 2018]. Here, reward is — by
definition — some outcome that reinforces behavior, and hence a circular definition of
reward behavior. For example going to the cafe and buying coffee can be explained
from two perspectives: 1) a cup of coffee in the morning is intrinsically rewarding and
therefore I go out to the cafe to get the coffee and 2) going to the cafe to get coffee
means, tautologically, that coffee is rewarding. In short, rewards reinforce behaviors
thatsecurerewards. Traditionally,these(model-free)RLalgorithmsoperatedirectlyon
the state of the environment, but (model-based) RL algorithms that operate on beliefs
alsorepresentanactiveareaofresearch[Igletal.,2018].
Conversely,inactiveinferenceanagent’sinteractionwiththeenvironmentisdeter-
mined by action sequences that minimize expected free energy (and not the expected
value of a reward signal). Additionally, unlike in reinforcement learning, the reward
signal is not differentiated from other types of sensory outcomes. That is any type of
outcomemaybemoreorlesspreferred. Thismeansthattheimplicitrewardassociated
with any outcome is a feature of the creature seeing the observation - not the environ-
menttheyinhabit. Thismaybedifferentfordifferentagents,orevenforthesameagent
atdifferentpointsintime. Thishighlightsthatthetwoframeworkshavefundamentally
different objectives: reward-maximization in reinforcement learning and free energy
minimizationinactiveinference.
In this paper, we reveal circumstances in which behavior might be the same and
whenitmaydifferunderthesetwodistinctobjectives. Weshowthatthemaincontribu-
tions of active inference, in comparison to reinforcement learning, include: a) reward
functions (i.e., prior preferences) are not always necessary because any policy has an
5
epistemic value, even in the absence of prior preferences, b) agents can learn their own
reward function and this becomes a way of describing how the agent expects itself to
behave —– as opposed to getting something from the environment, c) a principled ac-
countofepistemicexplorationandintrinsicmotivationasminimizinguncertainty[Parr
and Friston, 2017, Schwartenbeck et al., 2019] and d) incorporating uncertainty as a
natural part of belief updating [Parr and Friston, 2017]. Why are these contributions
of interest? In standard reinforcement learning, the reward function defines the agent’s
goal and allows it to learn how to best act within the environment [Sutton et al., 1998].
However, defining the reward function is difficult; if it is specific signal from the envi-
ronment based on action, then is it unanimously good in that environment; e.g., should
an agent controlling the thermostat only get positive reward for turning on the heating
during winter? Consequently, crafting appropriate reward functions is not easy, and
it is possible for agents to learn sub-optimal actions, if the reward function is poorly
specified [Amodei et al., 2016]. However, active inference bypasses this problem by
replacingthetraditionalrewardfunction,usedinreinforcementlearning,withpriorbe-
liefsaboutpreferredoutcomes. Thiscausestheagenttoactinaway—viathebeliefsit
holds—suchthattheobservedoutcomesmatchpriorpreferences. Thisisusefulwhen
we have imprecise or no prior preferences; since active inference endows agents with
the ability to learn prior preferences from interacting with the environment itself — by
learning and empirical prior distribution over preferred outcomes. In other words, an
agent can learn the kinds of outcomes it can achieve, and these become its prior pref-
erences (in virtue of the fact that these outcomes are achievable, they underwrite the
agent’s viability in that environment). This way of defining the reward function (prior
6
preferences) highlights that whether a state is rewarding (or not) is a function of the
agent themselves, and not the environment. This reward function conceptualization is
distinctfromrewardfunctionsunderreinforcementlearning.
Another challenge within reinforcement learning is balancing the ratio between ex-
ploration and exploitation; i.e., what actions should the agent take at any given point
in time? Should the agent continue to explore and find more valuable actions or ex-
ploit its (current) most valuable action sequence? Many different algorithms have been
used to address this; including (cid:15)-greedy [Vermorel and Mohri, 2005, Mnih et al., 2013,
2016], action selection based on action-utility [Sutton, 1990] and counter-based strate-
gies [Wiering and Schmidhuber, 1998, Tijsma et al., 2016], etc. However, even with
these exploratory mechanisms in place, most reinforcement learning formulations, call
on a temperature hyper-parameter, to weight extrinsic reward (from the environment)
against the intrinsic motivation (from the agent). There is no such hyper-parameter
in active inference — although the precision of various priors plays an analogous role
— because the distinction between extrinsic value (i.e., expected reward) and intrinsic
value(i.e.,intrinsicmotivation)isjustonewayofdecomposingexpectedfreeenergy. In
active inference, everything minimizes free energy, including hyper-parameters. Usu-
ally, these hyper-parameters transpire to be precision over various beliefs. This allows
for a natural trade-off between epistemic exploration and pragmatic behavior. This
meansthatalltherequiredmachineryisinplayfromthestartbutshouldonlybeadded
to the reinforcement framework (by definition) if it helps maximize expected return.
Consequently, state-of-art reinforcement learning approaches can be regarded as a se-
ries of refinements to the base algorithm that help resolve problems as they are en-
7
countered including the need to marginalize out hyper-parameters instead of defining
particularvaluese.g.,[Cesa-Bianchietal.,2017].
Here, we unpack these properties of active inference, with appropriate ties to the
reinforcement learning literature, under the discrete state-space and time formulation;
thereby providing a brief overview of the theory. Furthermore, we demonstrate these
properties, and points of contact, with reinforcement learning agents on a series of ex-
periments using a modified FrozenLake OpenAI baseline. This is purely an illustration
of the conceptual premises; not a demonstration of their implications. Thus, while our
simulationsofreinforcementlearningcouldhaveincludedmorecomplex(i.e.,context-
aware) aspects — such as in [Cao and Ray, 2012, Lloyd and Leslie, 2013, Padakandla
et al., 2019] –— these approaches describe various ways to perform inference, without
explicitreferencetotheirimpactonbehavior. Indeed,asshownby[O’Donoghueetal.,
2020],state-of-the-artapproaches,whichcanbeseenasframingreinforcementlearning
asprobabilisticinference,makesimplificationsforpracticalreasons. Thesesubtlemod-
elingchoices,thattrade-offtractabilityforaccuracy,canresultinsub-optimalbehavior
(e.g.,failingtoaccountforQ-valueuncertainty,andensuing‘dithering’behavior). Ac-
tive inference avoids this ambiguity by clearly defining how latent variable models are
constructed to solve POMDP problems, and how inference should proceed, using gra-
dient descent on expected free energy. This allows the agent to consider the effect of
its own actions upon future rewards (i.e., preferred outcomes) when evaluating the ex-
pected free energy of all plausible policies (i.e., action trajectories), based upon the
anticipated consequences of those policies. However, these are Bellman–optimal for
one time-step, but Bayes–optimal (i.e., realize prior beliefs and minimise free energy)
8
for distal time horizons. Additionally, by minimising expected free energy, the agent
balances exploration and exploitation resulting in a Bayes-optimal arbitration between
the two – which may not be reward-maximizing from an RL perspective. In contrast
to RL, active inference accounts for epistemic uncertainty by operating in an explic-
itly belief-based framework [Levine, 2018]. Additionally, the conceptual approach of
active inference means that all the appropriate terms — relating to intrinsic value of
information —are in play fromthe start but shouldonly be added tothe reinforcement
framework(bydefinition)iftheyhelpmaximizelong-termreward. Consequently,state-
of-art reinforcement learning approaches can be regarded as a series of refinements to
the base algorithm that help resolve problems as they are experienced. Therefore, for
explicit behavioral comparison, we simulate performance of the two frameworks after
removal of the reward signal from the FrozenLake environment i.e., a flat value func-
tion. In this setting, there is no motivation for adding any additional information gain
termbecausetheycannotbejustifiedintermsofincreasingexpectedvalueofreward.
The review comprises three sections. The first section considers the discrete state-
spaceandtimeformulationofactiveinference,andprovidescommentaryonitsderiva-
tion, implementation, and connections to reinforcement learning. The second section
providesaconcreteexampleofthekeycomponentsofthegenerativemodelandupdate
rulesin play,using amodifiedversion ofOpenAI’s FrozenLake environment. Through
these simulations, we compared the performance of three types of agents: active infer-
ence, Q-learning [Watkins and Dayan, 1992] using (cid:15)-greedy exploration and Bayesian
model-based reinforcement learning using Thompson sampling [Poupart, 2018] in sta-
tionary and non-stationary environments. We note that whilst all agents are able to
9
perform appropriately in a stationary setting, active inference’s ability to carry out on-
line planning allows for Bayes–optimal behavior in the non-stationary environment.
Thesimulationsdemonstratethatintheabsenceofarewardsignal,theactiveinference
exhibits information seeking behavior (to build a better model of its environment), in
contrasttoQ-learningagents,butsimilartotheBayesianreinforcement-learningagent.
We make explicit the conceptual differences in reward function under active inference
andreinforcementlearningthroughlearningofpriorpreferenceswhichenabletheagent
tosettleintoitsniche. Wehighlightthatfromtheperspectiveofreinforcementlearning,
this niche might be counter-intuitive i.e., reward minimising. We conclude with a brief
discussion of how this formalism could be applied in (more complex) engineering ap-
plications; e.g., robotic arm movement, Atari games, etc., if the appropriate underlying
probabilitydistributionorgenerativemodelcanbeformulated.
2 Active Inference
Motivation
Active inference describes how (biological or artificial) agents navigate dynamic, non-
stationary environments [Friston et al., 2017a,b]. It postulates that agents maintain
homeostasisbyresidingin(attracting)statesthatminimizesurprise[Fristonetal.,2011,
Bogacz,2017].
Definition 1 (Surprise). is defined as the negative log probability of an outcome. For
this, we introduce a random variable, o ∈ O, that corresponds to a particular outcome
10
receivedbytheagentandO isafinitesetofallpossibleoutcomes.
S(o) = −logP(o) (1)
whereP denotesaprobabilitydistribution.
In active inference, the agent determines how to minimize surprise by maintaining
a generative model of the (partially observable) world. This is necessary because the
agent does not have access to a ‘true’ measurement of its current state (i.e., the state
of the actual generative process). Instead, it can only perceive itself and the world
around via outcomes [Friston et al., 2017a,c]. This allows the problem to be framed as
a partially observable Markov decision process (POMDP) [Astrom, 1965], where the
generative model allows us to make inferences about ‘true’ states given outcomes. In
active inference, the agent makes choices based on its beliefs about these states of the
worldandnotbasedonthe‘value’ofthestates[Fristonetal.,2016]. Thisdistinctionis
key: instandardmodel-basedreinforcementlearningframeworkstheagentisinterested
inoptimizingthevaluefunctionofthestates[Suttonetal.,1998];i.e.,makingdecisions
thatmaximizeexpectedvalue. Inactiveinference,weareinterestedinoptimizingafree
energy functional of beliefs about states; i.e., making decisions that minimize expected
freeenergy. Putevenmoresimply,inreinforcementlearningweareinterestedinresid-
ing in high-value states under a reward function, whilst in active inference we wish to
reside in states that give rise to outcomes that match our prior preferences (i.e., a target
distribution). In one sense, this is a false distinction, as we could interpret a reward
function as a log prior preference or vice versa. However, ensuring consistency with
11
a distribution is different to maximizing reward. The latter implies we try to spend all
of our time in the most rewarding state, while the former nuances this with an impera-
tivetospendtimeinstatesthatgenerateoutcomesproportionatetothepriorprobability
associated with those outcomes. However, particular RL algorithms e.g., SMM [Lee
et al., 2019], also optimize similar objectives of matching state marginal distributions
tosometargetdistribution.
From an implementation perspective, this means replacing the traditional reward
function used in reinforcement learning with prior beliefs about preferred outcomes.
The agent’s prior preferences, logP(o), are defined only to within an additive constant
(i.e., a single negative or positive number). This means that the prior probability of an
outcomeisasoftmaxfunctionofutility: P(o) = σ(U(o))andthereforedependsonrel-
ative differences between rewarding [Chong et al., 2016] and unrewarding (surprising)
outcomes. Additionally,byincorporatinglearning-overpriors-activeinferenceagent
canbeequippedwiththeabilitytolearntheirownpreferencesoveroutcomes. Thereby,
bypassing the need for an explicit reward signal from the environment and the agent to
exhibitself-evidencingbehavior(seeSection3).
Variational Free Energy
Starting from a simple generative model for outcomes, it is possible to derive a varia-
tional free energy formulation, as motivated by Figure 1; this gives the starting point
for the full active inference derivation. First, we introduce the random variable, s ∈ S,
to represent a particular hidden state of the world, where S is a finite set of all possible
(hidden)states.
12
Figure 1: Graphical representation of the generative process (based on true states, s∗)
in the world and the corresponding (internal) generative model (based on probabilistic
beliefsrandomvariables,s,thatstandinfortruestatesthatarehidden)thatbestexplain
the outcomes, o. This graphic, highlights that the outcomes are shared between the
generativeprocessandmodel.
The generative model abstraction asserts that the world has a true (hidden) state
s∗, which results in the outcomes o (via the generative process); s∗ ∈ S. The agent
correspondinglyhasaninternalrepresentationof(ordistributionover)s,whichitinfers
fromo(viaitsgenerativemodel). Thehiddenstateisacombinationoffeaturesrelevant
to the agent (e.g., location, color, etc.) and the outcome is the information from the
environment (e.g., feedback, velocity, reward, etc.). By the reverse process of mapping
from its hidden state to the outcomes (through Bayesian model inversion), the agent
can explain the outcomes in terms of how they were caused by hidden states. This is
Bayesianmodelinversionorinference.
Definition 2 (Generative Model). is defined as a partially observable MDP that rests
onthefollowing(simplified)jointprobability: P(o,s)whereo ∈ Oands ∈ S asstated
13
previously. ThejointprobabilitycanbefactorizedintoalikelihoodfunctionP(o|s)and
prior over internal states P(s) (see supplementary materials for a full specification of
thegenerativemodel):
P(o,s) = P(o|s)P(s). (2)
Weknowthatfortheagenttominimizeitssurprise,weneedtomarginalizeoverall
possible states that could lead to a given outcome. This can be achieved by using the
abovefactorization:
(cid:88)
P(o) = P(o|s)P(s) (3)
s∈S
Thisisnotatrivialtask,sincethedimensionalityofthehiddenstate(andsequences
ofactions)spacecanbeextremelylarge. Instead,weutilizeavariationalapproximation
ofthisquantity,P(o),whichistractableandallowsustoestimatequantitiesofinterest.
Definition 3 (Variational free energy). Variational free energy, F, is defined as the
upper bound on surprise; definition 1. It is derived using Jensen’s inequality and com-
monly,knownasthe(negative)evidencelowerbound(ELBO)inthevariationalinfer-
enceliterature[Bleietal.,2017]:
(cid:88)
−logP(o) = −log P(o,s) (4)
s∈S
(cid:88) P(o,s)
≤ − Q(s)log (5)
Q(s)
s∈S
(cid:88) Q(s)
= Q(s)log (6)
P(o,s)
s∈S
Here,Q(.)isthevariationaldistribution.
14
To make the link more concrete, we further manipulate the variational free energy
quantity,F:
(cid:88) Q(s)
F = Q(s)log (7)
P(o,s)
s∈S
(cid:88) Q(s)
= Q(s)log (8)
P(s|o)P(o)
s∈S
(cid:18) (cid:19)
(cid:88) Q(s)
= Q(s) log −logP(o) (9)
P(s|o)
s∈S
= D [Q(s)||P(s|o)]−logP(o) (10)
KL
By rearranging the last equation, the connection between surprise and variational free
energyismadeexplicit:
−logP(o) = F −D [Q(s)||P(s|o)] (11)
KL
Additionally, we can express variational free energy as a function of these posterior
beliefsinmanyforms:
F = D [Q(s|π)||P(s|o,π)]−logP(o) (12)
KL
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
evidencebound logevidence
= D [Q(s|π)||P(s|π)]−E [logP(o|s)] (13)
KL s∼Q(s)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
complexity accuracy
Since KL divergences cannot be less than zero, from Equation 12 we see that free
energyisminimizedwhentheapproximateposteriorbecomesthetrueposterior. Inthat
15
instance, the free energy would simply be the negative log evidence for the generative
model [Beal, 2003]. This shows that minimizing free energy is equivalent to maxi-
mizing (generative) model evidence. In other words, it is minimizing the complexity
of accurate explanations for observed outcomes, as seen in Equation 13. Note that we
have conditioned the probabilities in Equation 12 and 13 on policies, π. These policies
can be regarded as hypotheses about how to act that, as we will see below, pertain to
probabilistictransitionsamonghiddenstates. Forthemoment,theintroductionofpoli-
ciessimplymeansthatthevariationalfreeenergyabovecanbeevaluatedforanygiven
actionsequence.
Expected Free Energy
Variational free energy gives us a way to perceive the environment (i.e., determine s
from o), and addresses one part of the problem; namely, making inferences about the
world(i.e.,the‘inference’inactiveinference). However,the‘active’partoftheformu-
lationisstilllacking;wehavenotaccountedforthefactthattheagentcantakeactions.
To motivate this, we note that we would like to minimize not only our variational free
energy F calculated from past and present observations, but also our expected free en-
ergy G, which depends upon anticipated observations in the future. Minimization of
expected free energy allows the agent to influence the future by taking actions in the
present, which are selected from policies. We will first consider the definition of a pol-
icy, and later determine how to evaluate their likelihoods from the generative model,
whichultimatelyleadstotheactionselectedbytheagent.
16
Definition 4 (Policy). is defined as a sequence of actions at time, τ, that enable an
agent to transition between hidden states; τ ∈ [1,2,...,T], where T is the total number
of time-steps in a given experiment under the generative model. For this, we introduce
two random variables: 1) u ∈ U to represent a particular action at time τ where U is
τ
afinitesetofallpossibleactions,and2)π ∈ Π. torepresentaparticularpolicy,where
Πisafinitesetofallowablepoliciesi.e.,sequencesofactionsinthesenseofsequential
policyoptimization;e.g.,[Alagozetal.,2010].
π = {u ,u ,...,u } (14)
1 2 τ
uptoagiventimehorizon,τ. Theexplicitlinkbetweenpolicyandactionis
u = π(τ) (15)
τ
Remark1(Connectionstostate-actionpolicies). Fromdefinition4,inactiveinference
apolicyissimplyasequenceofchoicesforactionsthroughtimei.e.,asequentialpolicy.
This contrasts with state-action policies in reinforcement learning, where π denotes
RL
amappingofstatestoactions;e.g. [Bellman,1952,Sutton,1990]
π (u,s) = P(u | s)
RL
The two policy types – sequential, π and state-action π – are equivalent, under a
RL
POMDP formulation, when τ = 1 i.e., π = {u } [Friston et al., 2016]. Note that the
1
Bayes–optimal policy is selected from these policies. For the remainder of the paper, a
17
policy(π)referstosequentialpolicy.
To derive the expected free energy, we first extend the variational free energy def-
inition to be dependent on time (τ) and policy (π) (and present its matrix formulation:
Equation18):
(cid:88) Q(s |π)
τ
F(τ,π) = Q(s |π)Q(s |π)log (16)
τ τ−1
P(o ,s |s ,π)
τ τ τ−1
sπ
τ
= E (cid:2) D [Q(s |π)||P(s |s ,π)] (cid:3) −E (cid:2) lnP(o |s ) (cid:3) (17)
Q(sτ−1|π) KL τ τ τ−1 Q(sτ|π) τ τ
= sπ · (cid:0) logsπ −logBπ sπ −logA·o (cid:1) (18)
τ τ τ−1 τ−1 τ
Heresπ istheexpectedstateconditionedoneachpolicy;Bπ isthetransitionprobability
τ τ
forhiddenstates,contingentuponpursuingagivenpolicy,ataparticulartime;Aisthe
expected likelihood matrix mapping from hidden states to outcomes and o represents
τ
theoutcomes. Thesearesimplyvectors(sπ)ormatrices(Bπ andA)specifyingaprob-
τ τ
abilityforeachalternativestateoroutcome. Forthematriceseachcolumncorresponds
to a different value to the variable we condition upon (here, hidden states), while rows
give the probability of each hidden state at the next time or the outcome at the current
time step, respectively. Now having developed this functional dependency on time, we
simply take an expectation with respect to the posterior distribution of outcomes from
ourgenerativemodel,P(o |s ).
τ τ
Definition 5 (Expected free energy). is defined as a free energy functional of future
trajectories, G. It effectively evaluates evidence for plausible policies based on out-
comes that have yet to be observed [Parr and Friston, 2019a]. Heuristically, we can
18
obtain G from Equation 16 by making two moves. The first is to include beliefs about
future outcomes in the expectation; i.e., supplementing the expectation under the ap-
proximate posterior with the likelihood, resulting in a predictive distribution given by
P(o |s )Q(s |π). The second is to (implicitly or explicitly) condition the joint proba-
τ τ τ
bilities of states and observations in the generative model upon some desired state of
affairs(C),asopposedtoaspecificpolicy. Thesetwomovesensure(1)wecanevaluate
thisquantitybeforetheobservationsareobtainedand(2)minimizationofGencourages
1
policieswhoseresultisconsistentwithC .
1
Thereareotherwaysinwhichwecouldconsiderconstructingfreeenergyfunctionalstodealwith
outcomes that have yet to be observed. While some of the alternatives are plausible from a theoretical
perspective,theytendtodispensewithaspectsofobservedbehaviorthatweseektocapture–andthere-
foredonotapplytothekindsofsystemweareinterestedinhere. Forexample,Millidgeetal[Millidge
etal.,2020b]proposeanalternativethatsubtractstheconditionalentropyofthelikelihood(i.e.,ambigu-
ity)fromtheexpectedfreeenergyshownhere. Thisleadstoagentswhoarelessambiguityaverse,and
donotseekinformation. Asactiveinferencedealswithcuriousagents, weretainthisambiguityinthe
expectedfreeenergyfunctional.
19
(cid:88) Q(s |π)
τ
G(τ,π) = P(o |s )Q(s |π)Q(s |π)log (19)
τ τ τ τ−1
P(o ,s |s ,C)
τ τ τ−1
sτ,oτ
= E [log(Q(s |π)−log(P(o ,s |s ,C))] (20)
Q˜ τ τ τ τ−1
= E [log(Q(s |π)−log(P(s |o ,s ))−log(P(o |C))] (21)
Q˜ τ τ τ τ−1 τ
≥ E [log(Q(s |π)−log(Q(s |o ,s ,π))]−E [log(P(o |C))] (22)
Q˜ τ τ τ τ−1 Q˜ τ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
-vemutualinformation expectedlogevidence
= E [log(Q(o |π)−log(Q(o |s ,s ,π))]−E [log(P(o |C))] (23)
Q˜ τ τ τ τ−1 Q˜ τ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
-veepistemicvalue extrinsicvalue
= D [Q(o |π)||P(o |C)]+E [H[P(o |s )]] (24)
KL τ τ Q(sτ|sτ−1,π) τ τ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
expectedcost expectedambiguity
= oπ ·(oπ −C )+sπ ·H (25)
τ τ τ τ
˜
wherethefollowingnotationisused: Q = P(o |s )Q(s |π);Q(o |s ,π) = P(o |s );
τ τ τ τ τ τ τ
C = logP(o |C) is the logarithm of prior preference over outcomes, o is the vector
τ τ τ
of posterior predictive outcomes (i.e., Asπ) and H = −diag (cid:0)E [A ].E [A] (cid:1) is the
τ Q i,j Q
vectorencodingtheambiguityoveroutcomesforeachhiddenstate.
Whenminimizingexpectedfreeenergy,wecanregardEquation23ascapturingthe
imperative to maximize the amount of information gained, from observing the envi-
ronment, about the hidden state (i.e., maximizing epistemic value), whilst maximizing
expected value – as scored by the (log) preferences (i.e., extrinsic value). This entails
a clear trade-off: the former (epistemic) component promotes curious behavior, with
exploration encouraged as the agent seeks out salient states to minimize uncertainty
20
about the environment, and the latter (pragmatic) component encourages exploitative
behavior, through leveraging knowledge that enables policies to reach preferred out-
comes. Inotherwords,theexpectedfreeenergyformulationenablesactiveinferenceto
treat exploration and exploitation as two different ways of tackling the same problem:
minimizinguncertainty.
This natural curiosity can be contrasted with handcrafted exploration in reinforce-
ment learning schemes, where curiosity is replaced by random action selection [Mnih
et al., 2013] or through the use of ad hoc novelty bonuses, which are appended to the
reward function [Pathak et al., 2017]. Information theoretic approaches have also been
explored in a reinforcement learning context e.g., [Still and Precup, 2012, Mohamed
andRezende,2015,Blauetal.,2019]. Someoftheseapproachesleveragebeliefsabout
latent states [Blau et al., 2019, Sekar et al., 2020]. For example, Blau et al, [Blau
et al., 2019] is a model-free algorithm that implicitly accounts for beliefs over the la-
tent states. Additionally, Seker et al. [Sekar et al., 2020] is very close in its treatment
of latent states to active inference but leverages an ensemble of belief states to inform
epistemic exploration, rather than a true Bayesian posterior. Additionally, curiosity as
formulatedunderactiveinferencecanemergeinreinforcementlearningunderPOMDP
formulations. Example algorithms may incorporate inductive biases [Igl et al., 2018]
and uncertainty over state transitions or outcomes [Ross et al., 2008, Kolter and Ng,
2009, Zintgraf et al., 2019]. The key aspect of these belief-POMDP schemes is that
theydealwithbeliefstates(i.e.,aspaceofprobabilitydistributionsoverhiddenstates).
This is crucial for exploration and minimizing uncertainty, because uncertainty is an
attributeofabeliefabouthiddenstates,notthehiddenstatesperse.
21
Normativelyspeaking,activeinferencedispenseswiththeBellmanoptimalityprin-
ciple and replaces it with a (variational) principle of least action – please see [Friston
et al., 2012] for further discussion. However, recent Bayesian RL schemes have used
variationalprinciplese.g. maintaininglatentovertheMDP[Zintgrafetal.,2019]orthe
explicit beliefs [Igl et al., 2019]. While in many of these settings, the distance between
the two schools of thought may seem to be closing, a fundamental distinction—-that
has yet to be bridged—-is the situation in which there are no rewards or, in active in-
ference, when prior preferences are uninformative. In a scheme motivated by reward
maximization, no meaningful behavior can be generated in this setting. This is not a
criticism of such schemes, but a statement of their scope and the problems they are
designed to solve. In contrast, the intrinsic value of seeking information—-regardless
of its potential to evince reward—-in active inference means that in the absence of any
rewarding outcomes, agents are still driven by a curiosity that helps them build a better
model of their world. Furthermore, active inference agents can learn their priors over
observations, and will exhibit ambiguity minimizing behavior in order to fulfill these
prior expectations [Friston et al., 2016]. In short, they can learn epistemic habits in the
absenceofextrinsicrewards.
Equation 24 offers an alternative perspective on the same objective; i.e., an agent
wishes to minimize the ambiguity and the degree to which outcomes (under a given
policy) deviate from prior preferences P(o |C). Thus, ambiguity is the expectation of
τ
the conditional entropy — or uncertainty about outcomes — under the current policy.
Low entropy suggests that outcomes are salient and uniquely informative about hidden
states(e.g.,visualcuesinawell-litenvironment—asopposedtothedark). Inaddition,
22
the agent would like to pursue policy dependent outcomes (Q(o |π)) that resemble its
τ
preferred outcomes (P(o |C)). This is achieved when the KL divergence between pre-
τ
dicted and preferred outcomes (i.e. expected cost) is minimized by a particular policy.
Furthermore, prior beliefs about future outcomes equip the agent with goal-directed
behavior(i.e. towardsstatestheyexpecttooccupyandfrequent).
Itisnowalsopossibletospecifypriorsoverpoliciesusingtheexpectedfreeenergy.
Policies,apriori,minimizetheexpectedfreeenergyterm,G[Fristonetal.,2017a]. This
has sometimes been framed in terms ofa heuristic reductio ad absurdum argument that
if selected policies realize prior beliefs and minimise free energy, then the only tenable
priorbeliefsarepoliciesthatwillminimisefreeenergy[ParrandFriston,2019a]. Ifthis
were not true, then an active inference agent would not have prior belief that it selects
policies that minimise expected free energy and it would infer (and pursue) policies
that were not free energy minimising. As such, it would not be an active inference
(i.e., free energy minimising) agent, which is a contradiction. This leads to the agent’s
priorbeliefthatitwillselectpoliciesthatminimisethefreeenergyexpectedunderthat
policy. There are some subtleties to this argument that leave some room for maneuver.
Specifically,thisdoesnottellushowtoconstructanexpectedfreeenergyfunctional-–
but this is typically chosen to be consistent with Definition 5. This choice ensures both
exploratory and exploitative behavior and is therefore sufficiently flexible to deal with
thekindofproblemweareinterestedinforthispaper.
This can be realized by expressing the probability of any policy with a softmax
23
function(i.e.,normalizedexponential)ofexpectedfreeenergy:
P(π) = σ[−β−1 ·G(π)] (26)
whereσ denotesasoftmaxfunctionandβ isatemperatureparameter.
Thisillustratesthe‘self-evidencing’behaviorofactiveinference. Actionsequences
(policies) that result in lower expected free energy are more likely. Intuitively this
makessense;sinceallnotionsofhowtoactintheworld(i.e.,exploration,exploitation)
arewrappedupintheexpectedfreeenergyG,policyselectionsimplybecomesamatter
ofdetermining(throughsearch)thesetofactionswhichgetusclosesttothisgoal(i.e.,
theattractingsetdefinedbypriorpreferencesP(o|C)).
Note the similarities to Dyna-style/planning model-based reinforcement learning
[Sutton, 1990]: hypothetical roll-outs are used to model the consequences of each pol-
icy. However, the actual controller in active inference is derived through an approach
similar to model predictive control [Camacho and Alba, 2013], where a search is per-
formedoverpossibleactionsequencesateachtime-step.
Now that we have priors over policies, we can incorporate these into the generative
model,andintothefreeenergy. Thisgives:
F = E [F(π)]+D [Q(π)||P(π)] (27)
Q(π) KL
Here, the free energy of the model conditioned upon the model plays the role of a
negative log marginal likelihood, giving this the form of an accuracy and complexity
term in the space of beliefs about policies. Often, the temperature parameter β is also
24
equipped with priors (normally using a gamma distribution) and posteriors, which add
anadditionalcomplexitytermtothefreeenergy.
Optimizing Free Energy
From this free energy formulation, we can optimize expectations about hidden states,
policies, and precision through inference, and optimize model parameters (likelihood,
transition states) through learning (via a learning rate: η). This optimization requires
finding sufficient statistics of posterior beliefs that minimize variational free energy
[Friston et al., 2017c]. Under variational Bayes, this would mean iterating the appro-
priate formulations (for inference and learning) until convergence. Under the active
inference scheme, we calculate the solution by using a gradient descent (with a default
step size, ζ, of 4) on free energy F, which allows us to optimize both action-selection
andinferencesimultaneously,usingamean-fieldapproximation[Becketal.,2012,Parr
et al., 2019]. The gradients of the negative free energy with respect to states and preci-
sions,respectively,are
επ = (logA·o +logBπ sπ +logBπ ·sπ )−logsπ (28)
τ τ τ−1 τ−1 τ τ+1 τ
εγ = (β −β )+(π −π )·G (29)
τ 0
where β = β + (π − π ).G; β = 1 encodes posterior beliefs about (inverse) pre-
τ 0 γ
cision (i.e., temperature); π represents the policies specifying action sequences and
π = σ(−γ.G). G in this equation is a vector whose elements are the expected free
0
energiesforeachpolicyunderconsideration.
25
This involves converting the discrete updates, defined in Equation 28 and 29, into
dynamicsforinferencethatminimizestateandprecisionpredictionerrors: επ = −∂ F
τ s
andεγ = −∂ F . Thesepredictionerrorsarefreeenergygradients. Gradientflowsthen
γ
produceposteriorexpectationsthatminimizefreeenergytoprovideBayesianestimates
of hidden variables. This particular optimization scheme means expectations about
hidden variables are updated over several time scales. During each outcome or trial,
beliefs about each policy is evaluated based upon prior beliefs about future outcomes -
whichgetinthroughtheexpectedfreeenergy. Thisisdeterminedbyupdatingposterior
beliefs about hidden states (i.e., state estimation under each policy, Q(s|π)) on a fast
timescale,whileposteriorbeliefsfindnewextrema(i.e.,asnewoutcomesaresampled,
P(s|o,π)). These posterior beliefs are then used to compute the posterior predictive
probabilities of future outcomes, Q(o|π), which themselves contribute to the expected
freeenergy,andthroughthisthepriorsoverpolicies. Thisintroducesanunusualfeature
oftheonlineinferenceschemesunderactiveinferencerarelyseeninBayesianaccounts
- the priors over policies change with new outcomes. This is a key distinction between
active inference and standard Bayesian RL perspectives, and can only occur when the
bestpoliciestoengageinarefunctionalsofbeliefswhichmaybeupdated.
Usingthiskindofbeliefupdating,wecancalculatetheposteriorbeliefsabouteach
policy;namely,asoftmaxfunctionbasedonexpectedfreeenergyandthelikelihoodof
past observations under that policy (approximated with F(π)), extending the definition
of the prior covered in Equation 26. The softmax function is a generalized sigmoid for
vector input, and can, in a neurobiological setting, be regarded as a firing rate function
of neuronal depolarization [Friston et al., 2017b]. Having optimized posterior beliefs
26
about policies, they are used to form a Bayesian model average of the next outcome
(i.e., under these beliefs about what I will do next, which observations would I expect
on average), which is realized through action selected to conform to this. Practically,
action selection is often achieved more simply by sampling from the distribution over
actionsatagiventimeimpliedbyposteriorbeliefsaboutpolicies.
In active inference, the scope and depth of the policy search is exhaustive, in the
sense that any policy entertained by the agent is encoded explicitly, and any hidden
state over the sequence of actions entailed by policy are continuously updated. How-
ever,inpractice,thiscanbecomputationallyexpensive;therefore,apolicyisnolonger
evaluated if its log evidence is ζ (default 20) times less likely than the (current) most
plausiblepolicy. This,ζ,canbetreatedasanadjustablehyper-parameter. Additionally,
at the end of each sequence of outcomes, the expected parameters are updated to allow
forlearningacrosstrials. ThisislikeMonte-Carloreinforcementlearning,wheremodel
parameters are updated at the end of each trial. Lastly, temporal discounting emerges
naturally from active inference, where the generative model determines the nature of
discounting (based on parameter capturing precision), with predictions in the distant
futurebeinglesspreciseandthusdiscountedinaBayes–optimalfashion[Fristonetal.,
2017a]. Practically, this involves the inclusion of a hyper-parameter β = γ−1 that can
be regarded as the (inverse precision or temperature) of posterior beliefs over policies.
This is marginalized out during belief updating; enabling the exploration-exploitation
trade-off to be inferred. This highlights the flexibility of active inference; in the sense
it can be applied to any generative model. It is unclear how similar parameterizations
couldbeadoptedinconventionalreinforcementlearning,whereoptimizingtemperature
27
parameters typically involves a grid search over value. Having said this, sophisticated
formulations of reinforcement learning, e.g., soft actor-critic [Haarnoja et al., 2018,
Cesa-Bianchietal.,2017]haveaddressedthisissue.
Theabovediscussionsuggeststhat,fromagenericgenerativemodel,wecanderive
Bayesianupdatesthatclarifyhowperception,policyselectionandactionsshapebeliefs
about hidden states and subsequent outcomes in a dynamic (non-stationary) environ-
ment. This formulation can be extended to capture a more representative generative
process by defining a hierarchical (deep temporal) generative model as described in
[Fristonetal.,2017a,c,ParrandFriston,2017],continuousstatespacesmodels[Buck-
leyetal.,2017,ParrandFriston,2019b,Ueltzho¨ffer,2018]ormixedmodelswithboth
discrete and continuous states as described in [Friston et al., 2017c, Parr and Friston,
2018]. Inthecaseofacontinuousformulation,thegenerativemodelstate-spacecanbe
defined in terms of generalized coordinates of motion (i.e., the coefficients of a Taylor
seriesexpansionintime-asopposedtoaseriesofdiscretetime-steps),whichgenerally
have a non-linear mapping to the observed outcomes. Additionally, future work looks
to evaluate how these formulations (agents) may interact with each other to emulate
multi-agentexchanges.
The implicit variational updates presented here have previously been used to sim-
ulate a wide range of neuronal processing (using a gradient descent on variational
free energy): ranging from single cell responses (including place-cell activity) [Fris-
ton et al., 2017a], midbrain dopamine activity [Friston et al., 2014], to evoked poten-
tials, including those associated with mismatch negative (MMN) paradigms [Friston
etal.,2017a]. Additionally,therehasbeensomeevidenceimplicatingthesevariational
28
inferences with neuromodulatory systems: action selection (dopaminergic), attention
andexpecteduncertainty(cholinergic)andvolatilityandunexpecteduncertainty(nora-
drenergic) [Parr and Friston, 2017, 2019a]. Please see [Friston et al., 2017a, Parr and
Friston,2019a,DaCostaetal.,2020],foradetailedoverview.
Inwhatfollows,weprovideasimpleworkedexampletoshowpreciselythebehav-
iorsthatemerge—naturally—underactiveinference.
3 Simulations
This section considers inference using simulations of a modified version of OpenAI
gym’s FrozenLake environment: for simplicity, we have chosen this paradigm (note
that more complex simulations have been explored in the literature; e.g., behavioral
economics trust games [Moutoussis et al., 2014, Schwartenbeck et al., 2015], narrative
construction and reading [Friston et al., 2017b], saccadic searches and scene construc-
tion[Mirzaetal.,2016],Atarigames[Cullenetal.,2018],etc). Incloselyrelatedwork,
Cullen, et al [Cullen et al., 2018] demonstrated that active inference agents perform
better in another OpenAI gym environment Doom, compared to reward-maximizing
agents. Their reward-maximizing agents are active inference agents without the epis-
temic value term (G) and can therefore be considered distinct from standard reinforce-
mentlearningagents.
We first describe the environment set-up and then simulate how an agent learns to
navigate the lake to successfully reach the goal. The simulations involve searching for
thereward(i.e.,Frisbee)ina3×3frozenlakeandavoidfallinginahole. Thepurpose
29
of these simulations is to provide an accessible overview —– and accompanying code
(see Software Note) –— of the conceptual (and practical) differences between active
inferenceandstandardreinforcementlearning.
Set-up
The frozen lake has a grid-like structure with four different patches: starting point (S),
frozen surface (F), hole (H) and goal (G) where the Frisbee is located. All patches,
except for (H), are safe. The agent starts each episode at (S); position 1. From there,
to reach the Frisbee location, the agent needs to take a series of actions; e.g. left, right,
down or up. The agent is allowed to continue moving around the frozen lake, with
multiple revisits to the same positions, but each episode ends when either (H) or (G) is
visited. (G) and (H) can be located in one of two locations: position 8 and 6 or 6 and 8
respectively. The objective is to reach (G), the Frisbee location, ideally in as few steps
aspossible,whilstavoidingthehole(H).IftheagentisabletoreachtheFrisbeewithout
falling in the hole, it receives a score of 100 at the end of trial. This scoring metric is
frameworkagnosticandallowsustocompareactiveinferencetoreinforcementlearning
methods. However, it is important to note that maximizing reward is not the definition
ofBayes–optimalbehaviorforanactiveinferenceagent,whereinformationgainisalso
of value. This will become important later. Finally, we limit the maximum number of
timesteps(i.e.,thehorizon)to15.
30
Figure2: Graphicalrepresentationoftheactiveinferencegenerativemodel: Themodel
containsfouractionstatesthatencodedirectionofmovement: right,down,upandleft.
They control the ability to transition between hidden state location factors (one of the
nine locations going from 1 → 9 - only a few states are shown). Each action navigates
the agent to a different location (a select few are shown): e.g., if the agent starts in
position 1 and chooses to turn right, then it will end up in state 2 at the next time–step.
However, if the agent started in location 5, and goes up, it would end up in location 2
instead. Note that both 8 and 6 are absorbing states: only 8 is denoted as such by the
circular arrow. Additionally, if an agent makes an improbable move; i.e., tries to go
left from location 1, it will remain in location 1 (as shown). The hidden states have a
Kronecker tensor product (⊗) form with two factors: location and context (one of the
two goal locations). The context cannot be changed by the agent and corresponds to
the associated Frisbee location: 8 if context 1 or 6 if context 2. Note that in context 1,
the hole location is 6. From each of the two hidden state factors (location and context)
anoutcomeisgenerated. Theagentobservestwotypesofoutcomesateachtimepoint:
its grid position and score. Categorical parameters, that define the generative model,
A (likelihood - P(o|s)): have an identity mapping between hidden state location and
outcome grid position with some uncertainty; e.g. if I have beliefs that I am in position
6, then I will observe myself in position 6, irrespective of context. The score likeli-
hood, given the hidden states, is determined by the context; i.e., for context 1, positive
score received at location 8, and negative or nothing elsewhere. lnP(o) corresponds to
prior preference: the agent expects to find positive score and not remain at the starting
location.
31
Active inference agents
For this paradigm, we define the generative model for the active inference agent as fol-
lows (Figure 2): four action statesthat encode direction ofmovement (left, right, down
andup),18hiddenstates(9locationsfactorizedby2contexts)andoutcomemodalities
includegridposition(9)andscore(3). Theactionstatescontrolthetransitionsbetween
thehiddenstatelocationfactorse.g. whenatlocation4,theagentcanmovetolocation
5(right),7(down),1(up)orstayat4(left). Thehiddenstatefactor,location,elucidates
the agents’ beliefs about its location in the frozen lake. The context hidden state factor
elucidatestheagent’sbeliefsaboutthelocationof(G)and(H):ifcontextis1,then(G)
location is 8 and (H) location is 6. The outcomes correspond to the following: being at
any of the 9 possible grid positions and receiving 3 types of potential reward (positive,
negative or neutral). Positive reward is received if the agent correctly navigates to the
(G)location,negativeiftothe(H)locationandneutralotherwise(F,S).
We define the likelihood P(o|s) as follows: an identity mapping between hidden
state location and outcome grid position; e.g., if I have beliefs that I am located in
position 6, then I will observe myself in position 6, irrespective of context. However,
the likelihood for score, given the hidden states, is determined by the context; i.e. if
the context is 1 (2) then positive score will be received at location 8 (6), and negative
or nothing elsewhere. The action-specific transition probabilities P(s |s ,u) encode
t−1 t
allowable moves, except for the sixth and eight locations, which are absorbing latent
states that the agent cannot leave. We define the agent as having precise beliefs about
the contingencies (i.e., large prior concentration parameters = 100). The utility of the
outcomes, C, is defined by lnP(o) : 4 and −4 nats for rewarding and unrewarding
32
outcome: thiscanberegardedasareplacementforwritingoutanexplicitrewardfunc-
tion. This means, that the agent expects to be rewarded e8 times more, at (G) than (H).
Notice that rewards and losses are specified in terms of nats or natural units, because
we have stipulated reward in terms of the natural logarithms of some outcome. The
prior beliefs about the initial state were initialized: location state (D = 1) for the first
location and zero otherwise, with uniform beliefs for context state. We equip the agent
with deep policies: these are potential permutations of action trajectories e.g., (’Left,
’Left’, ’Right’) or (’Down’, ’Right’, ’Up’). Practically, policies (action sequences) are
removed if the relative posterior probability is of 1/128 or less than the most likely
policy. After each episode, the posteriors about the current state are carried forward
as priors for the next episode. By framing the paradigm in this way we treat solving
thePOMDPasaplanningasinferenceproblem;inordertoactappropriatelytheagent
needstocorrectlyupdateinternalbeliefsaboutthecurrentcontext.
Having specified the state-space and contingencies, we can solve the belief updat-
ing Equations 28 and 29 to simulate appropriate behavior. Pseudo-code for the belief
updating and action selection for this particular type of discrete state-space and time
formulation is presented in supplementary materials. To provide a baseline for purely
exploratory behavior, we also simulated a ‘null’ active inference agent, who had no
priorpreferences(i.e.,wasinsensitivetothereward).
Reinforcement learning agents
Wecomparedtheactiveinferenceagents’performanceagainsttworeinforcementlearn-
ing algorithms: Q-Learning using (cid:15)-greedy exploration [Watkins, 1989, Sutton et al.,
33
1998] and Bayesian model-based reinforcement learning using standard Thompson
sampling [Poupart, 2018, Ghavamzadeh et al., 2016]. Thompson sampling is an ap-
propriate procedure here, because it entails the optimization of dual objectives; reward
maximization and information gain. This is achieved by having a distribution over a
particularfunction,thatisparameterizedbyapriordistributionoverit.
We evaluate two permutations of the Q-learning algorithm, an agent with fixed ex-
ploration ((cid:15) = 0.1) and an agent with decaying exploration ((cid:15) = 1 decaying to 0); the
pseudo-code is presented in supplementary materials. For both Q-learning agents, we
specifythelearningrateas0.5anddiscountfactoras0.99.
TheBayesianRLapproachisastandardDyna-style[Sutton,1990]approach,where
we train Q-learning agents in a belief-based internal model (planning), which accounts
for uncertainty over both the transition model and reward function (i.e., separate prior
distribution over both functions); the pseudo-code is presented in supplementary mate-
rials. The transition model, encodes the probability for the next state, given the current
state and action. These transition probability distributions are the same as the active
inference generative model above: high probability for intended move and extremely
low probability for an implausible move. The reward function, encodes the uncertainty
abouttherewardlocation(animplicitcontextualunderstandingabouttheenvironment).
Thelikelihoods,forthetransitionmodelandrewardfunction,aremodeledviatwosep-
arate Bernoulli distributions; with Beta distributions as the conjugate prior over their
parameters. TheBetadistributionpseudo-counts—fortherewardandtransitionmodel
—areinitializedas1. Theposteriorfortherewardandtransitionmodeldistributionare
evaluatedbyupdatingtheprior(Beta(α,β)). Thus,bytreatingthemaspseudo-counts,
34
theevidenceforintendedmove(likelyrewardlocation),x,isaddedtoαandanimplau-
siblemove(unlikelyrewardlocation),y,isaddedtoβ: posteriorisBeta(α+x,β+y).
Thediscountfactorisspecifiedas0.9.
The Bayesian RL agent is a planning–based RL algorithm that parameterizes the
transitionandrewardmodelusingtwoseparateBernoullidistributionwithaBetaprior.
At each episode, we sample k (=50), θ, from each of the Beta distributions. Using the
sampledpriors,wedefinekMDPsandsolvethemusingvalueiteration. Thissimulation
gives us the Q-value function which is averaged out to get the optimal Q-value. The
optimal Q-value function is used to determine the next action and move to the next
state. This procedure continues till the agent reaches the goal or falls down the hole.
TheprocessofsolvingthekMDPtodeterminethenextactionissimilartohypothetical
roll-outsinotherplanning-basedalgorithms.
Note that more sophisticated reinforcement learning schemes may have been more
apt for solving this task; e.g., [Daw et al., 2006, Fuhs and Touretzky, 2007, Gershman
and Niv, 2010, Daw et al., 2011, Gershman and Daw, 2017]. However, our aim was
to compare standard formulations of active inference and reinforcement learning. The
rationale for this will become clearer when we compare the behavioral performance in
the absence of reward — no motivation for adding any additional heuristics because
they cannot be justified in terms of increasing expected value of reward. Thus, the
adoptedRLagentsaresuitedforourpurpose.
35
Learning to navigate the frozen lake
We evaluate how well the different agents are able to navigate the frozen lake in both
stationary and non-stationary environments, as described below. Each of the environ-
ments were simulated for 200 trials with 500 episodes for the five agents: Q-learning
((cid:15) = 0.1), Q-learning ((cid:15) = 1 decaying to 0), Bayesian model-based reinforcement
learning, Active Inference (Figure 2) and Active Inference (null model; without any
prior outcome preferences i.e. logP(o|C) = 0 for all outcomes). To aid intuition, the
flattening of the prior preferences in the active inference model is equivalent to reclas-
sifying reward as just another state or observation in a reinforcement learning scheme.
Whileanagentwouldstillbe’told’whetherornotithadencounteredarewardingstim-
ulus, this would have no impact on the value function. As noted above, this is not an
exact equivalence, as there is a philosophical distinction between making a change to
the environment (in the reinforcement learning setting) and to the agent (in the active
inferencesetting).
AverageScore[95%CI]
Algorithm Belief-Based DeterministicEnv. StochasticEnv.
Q-Learning((cid:15)=0.1) N 97.79[97.41,98.16] 66.08[63.28,68.88]
Q-Learning((cid:15)=1decayingto0) N 80.44[78.96,81.93] 65.13[62.57,67.68]
BayesianRL Y 99.76[99.45,100.00] 64.39[60.33,68.44]
ActiveInference Y 99.88[99.64,100.00] 98.90[98.00,99.79]
ActiveInference(nullmodel) Y 50.03[49.70,50.35] 50.22[49.89,50.22]
Table 1: Average reward (and 95% confidence interval) for each agent, across both
deterministicandstochasticenvironments. Theresultsarecalculatedfromthe200trials
across500episodes.
36
Stationaryenvironment
For this set-up, the goal (G) exists at position 6 and hole (H) location at 8 for the entire
experiment. We then evaluate the agent performance online, and make no distinction
between offline and online behavior modes. This is to better simulate exploration and
exploitation in the real world, where we use the same policy to gather training data
andact;indeeditisthisexactparadigmwhichisoneofthemajormotivatorsforactive
inference. Theaveragescore(Table1)forallagents,exceptthenullmodelspecification
ofActiveinferencemodel,wasconsiderablyhighat> 80,showingthatallframeworks
wereabletosolvetheMDP.
The low score for the null (active inference) model reflects the lack of prior prefer-
ences for the type of outcomes the agent would like to observe i.e., it does not differ-
entiate between any of the different patches (S, F, G & H) in the frozen lake. The null
modeldoesnotseekoutthegoalstate(i.e.,reward),whichitdoesnotpreferoverother
states. Instead, it either falls in the hole or reaches the goal, with equal probability.
From the perspective of information seeking, this is a very sensible policy, as the same
amount of information can be obtained either by finding the hole (which tells us where
the goal is) or by finding the goal (which tells us where the hole is). As such, there is
nothing to disambiguate between the two. Over subsequent exposures to the environ-
ment, given that it is stationary, this agent will be left with little uncertainty to resolve,
as it will know everything at the start of a trial based upon past experience. Over time,
thiswillleadtoalossofpurposefulbehavior,resemblingwhatwemightexpectfroma
reinforcement learning agent in the absence of any environmental rewards (even in the
presenceofuncertainty).
37
The learning curve, as shown in (Figure 3), highlights that the active inference and
Bayesian model-based reinforcement learning agent learn reward-maximizing behav-
ior (and resolve uncertainty about reward location) in a short amount of time (< 10
episodes). They are able to maintain this for the remaining trials. This is reflected by
the tight confidence intervals around the average reward for both agents. In contrast,
Q-learning ((cid:15) = 0.1), whilst also quickly learning appropriate state-action pairing, has
slightlylargerconfidenceintervalsfortheaveragerewardduetothe10%ofselectinga
randomaction.
Non-stationaryenvironment
We introduce non-stationarity into the environment; the location of the (G) and (H)
are flipped after a certain number of episodes. Initially (G) is located at position 6
and (H) at position 8, and then we swap (G) and (H) at the following time steps:
21,121,141,251,451. This means after episode 451, (G) remains at position 8 until
the end of the simulation. These changes in the reward location test how quickly the
agentcanre-learnthecorrect(G)location. Theaveragescoreforallagentsispresented
inTable1.
As in the stationary set-up, all agents are initially uncertain about the reward loca-
tion. This is quickly resolved, and by episode 20, active inference, Bayesian RL and
Q-learning ((cid:15) = 0.1) exhibit appropriate behavior for solving the task. The null (active
inference) model and Q-learning ((cid:15) = 1 decaying to 0.00) exhibit fairly poor perfor-
mance (consistent with stationary). However, at episode 21, the performance for all
agents drops to 0 due to the change in reward location — except for the agent with no
38
Figure3: Learningcurvefordeterministicenvironment. Thex-axisdenotestheepisode
number and y-axis the average (online) reward. The results presented are calculated
from200trials.
39
Figure 4: Learning curve for stochastic environment. The x-axis denotes the episode
number and y-axis the average (online) reward. The results presented are calculated
from200trials. Thedottedgraylinesrepresentthechangein(G)(and(H))location.
40
preferences, who persists with achieving the reward half of the time. For the reinforce-
ment learning (Q-learning and Bayesian RL) agents, this drop in performance persists
forthenext∼ 40episodes. Thisisbecausebytreatingthisasa‘learning’problem,the
agent has to do the following: 1) reversal learning of its previous understanding of the
reward location and 2) re-learn the current reward location. In contrast, by treating this
asaplanningasinferenceproblem,theactiveinferenceagentisabletoquicklyrecover
performance after a single episode, as the generative model takes into account the con-
textswitch. Inotherwords,theagentsimplyinfersthataswitchhashappened,andacts
accordingly. This quick performance recovery is persistent for all changes in reward
location across the 500 episodes (Figure 3). However, for Bayesian RL the ability to
adapt itsbehavior to thechanging goal locationscontinues to provedifficult; each time
agreaternumberofepisodesarerequiredtoreversethelearningofthepriordistribution
overtherewardfunctionduetotheaccumulationofpseudo-counts. Thiscontrastswith
Q-learning ((cid:15) = 0.1), which adapts fairly quickly to these fluctuating reward locations,
becauseitneedstoonlyupdatetheappropriatestateandactionQ-values.
Therefore,fornon-stationaryenvironmentsactiveinferenceoffersanattractive,nat-
uraladaptationmechanism—fortrainingartificialagents—duetoitsBayesianmodel
updatingproperties. Thisisincontrasttostandardreinforcementlearning,whereissues
of environmental non-stationarity are not accommodated properly, as shown through
the above simulations. They can be dealt with using techniques that involve the in-
clusion of inductive biases; e.g., importance sampling of experiences in multi-agent
environments [Foerster et al., 2017] or using meta-learning to adapt gradient-update
approaches more quickly [Al-Shedivat et al., 2017]. Lastly, we acknowledge that the
41
simulationspresentedlimitthecomparisonbetweenactiveinferenceandstandard(i.e.,
naive) reinforcement learning schemes. We could have introduced further complexity
(i.e., additional distributions to be learnt) within the Bayesian reinforcement learning
model: for example, explicit beliefs about latent contexts instead of implicit context
encoding, via the reward location [Gershman and Niv, 2010, Gershman et al., 2015,
Rakellyetal.,2019]. Toevaluatethecomparisonofmorecomplexreinforcementlearn-
ing agents, to active inference, in non-stationary environments remains an outstanding
research question. We appreciate that with additional design choices the Bayesian re-
inforcement learning agent may exhibit similar behavioral performance to the active
inferenceagent.
Comparing prior preferences and rewards
Inreinforcementlearning,goalsaredefinedthroughrewardfunctionsi.e.,explicitscalar
signal from the environment. In contrast, in active inference, goals are defined through
the agents prior preferences over outcomes. We now illustrate the link between these
definitions of goal-directed behavior by presenting experiments that show the effect
of reward shaping [Ng and Jordan, 2003] in the FrozenLake stationary environment
(Table2).
We apply the following shaping: modifying the reward for reaching the goal (G),
modifying the reward for falling down the hole (F), and modifying the reward for any
state that isn’t a goal (H) (this can be considered a ‘living cost’). In order to convert
theshapedrewardintopriorpreferences,wemanipulatethepriorpreferencessuchthat
theirrelativeweightingmatchesthatintroducedthroughtherewardshapinge.g.,reward
42
of−100isequivalenttopriorpreferencesof−log(5),etc.
Asourexperimentsshow,whenwedefineapriorpreferencethrougharewardfunc-
tion,thebehaviorsofthebelief-basedpolicies(i.e.,BayesianRLandActiveInference)
are nearly identical, and learn to solve the environment as soon as a positive reward is
defined for the goal. On the other hand, the non-probabilistic Q-learning approach ap-
pearsmoresensitivetorewardshaping,withlivingcostscausinggreedierbehavior(i.e.,
taking fewer steps per episode). A possible explanation for this is that the construction
of the generative models for both Bayesian RL and Active Inference clearly define that
the location of the goal/hole is in either states 6 or 8, hence Bayes–optimal behavior
(i.e., getting to the goal in as few steps as possible) can be learned even in the absence
of negative rewards/preferences over certain states. All that is required is some notion
ofwherethegoalstatemightexist,hencetheabilitytolearnBayes–optimalpoliciesby
onlyspecifyingthegoallocation(seethelastrowofTable2).
Another interesting behavior is when there is an absence of preferences/rewards
(i.e.,firstrowofTable2). TheQ-learningapproachlearnsadeterministiccircularpolicy
with little exploration despite the (cid:15) term since it does not update its parameters due to
the lack of reward signal. The belief-based approaches on the other hand maintain
exploration throughout — represented by the average score ranging between 40−45
— as their probabilistic models remain uniform over the beliefs of which transitions
produce preferred outcomes. This suggests that by having an objective function that is
optimizingadualobjective,theagentexhibitssomesortinformationgaini.e.,exploring
the world is intrinsically motivated because it helps the agent build a better model of
the world. We will see more purposeful exploration, under active inference, in the next
43
section.
Finally, we observe that all 3 approaches learn the same circular behavior when
only a negative preference or reward is specified (i.e., second row of Table 2). This is
becausealltheapproacheslearntoavoidtheholestate(H),butsincethereisnonotion
ofgoal-seekingbehavior,donotlearntogotothegoalstate. Interestingly,inthecaseof
the belief-based approaches (Bayesian RL and Active Inference), since the generative
model defines the presence of hole states in either state 6 or 8, and since it receives no
preference for goal states, the generative model assigns non-zero probability with the
holestatebeingineitherstate6or8. Asaresultpoliciesderivedfromthesegenerative
models learn to avoid both states, therefore only terminating when the time limit is
reached.
Throughthisbriefstudy,wehaveillustratedanimplicitequivalencebetweenBayesian
model-based reinforcement learning and active inference. This equivalence rests on
treating prior preferences as a reward function. In other words, by expressing an ar-
bitrary reward function as a potential function (i.e., a log probability over future out-
comes), reward functions can be absorbed into expected free energy. This means one
canelicitidenticalbehaviorsfromreinforcementlearningandactiveinference. Indeed,
if one removes uncertainty — in the form of epistemic value — we are left with prag-
matic value; namely, expected future reward. This shows that reinforcement learning
can be regarded as a limiting or special case of model-based approaches in general —
oractiveinferenceinparticular. However,theFrozenLakeenvironmentisbynomeans
representativeofalldiscreteenvironments,andthismeritsfurtherresearch. Itisimpor-
tanttonotethatbehavioralequivalencesarearesultpurelyoftheenvironmentalset-up
44
and the accompanying reward signal, e.g., changing the FrozenLake environment for a
maze with noisy attractor state, and no reward, might reveal additional behavioral dif-
ferencesbetweenBayesianmodel-basedreinforcementlearningandactiveinference.
Rewards AverageScore(AverageNumberofMoves)
(G) (H) (F) Q-Learning*((cid:15) = 0.1) BayesianRL ActiveInference
0.00 0.00 0.00 0.00(15.00) 39.94(9.17) 44.00(8.67)
0.00 −100 0.00 0.00(15.00) 0.00(15.00) 0.00(15.00)
100 −100 0.00 95.56(3.53) 99.77(3.02) 99.52(3.03)
100 0.00 −10.0 96.00(3.48) 99.89(3.00) 99.47(3.00)
100 −100 −10.0 96.47(3.42) 99.79(3.01) 99.58(3.00)
100 0.00 0.00 95.32(3.58) 99.74(3.00) 99.50(3.07)
Table 2: Reward shaping: average score and number of moves across 100 episodes for
100agents. *Notethatforthisexperimentweevaluateunder(cid:15) = 0.0,i.e.,on-policy.
Learningprioroutcomepreferences
In some settings, explicitly defining prior outcome preferences might be challenging
duetotimedependentpreferences,aninabilitytodisambiguatebetweendifferenttypes
of outcomes, or simply lack of domain knowledge. In those instances, the appropri-
ate distribution of prior outcome preferences can be learned via the agent’s interaction
withtheenvironment. Thisdifficultyextendstoreinforcementlearning,wheredefining
a reward function may not be possible, and in its vanilla formulation, reinforcement
learning offers no natural way to learn behaviors in the absence of a reward function
(seethefirstrowofTable2).
In order to demonstrate the ability of active inference to select policies in the ab-
senceofpre-specifiedpriorpreferences,weallowboththelikelihooddistribution(logP(o|s))
45
and outcome preferences (logP(o|C)) to be learned. This allows us to make explicit
that whether a state is rewarding or not is determined by the agent learning its prior
preferencesanditisnotaspecificsignalfromtheenvironment. Forthis,thegenerative
model is extended to include prior beliefs about the parameters of these two distribu-
tions (a prior over priors in the case of (logP(o|C)), which are learned through belief
updates [Friston et al., 2017a]. The natural choice for the conjugate prior for both dis-
tributionsisaDirichletdistribution,giventhattheprobabilitydistributionsarespecified
as a categorical distribution. This means that the probability can be represented sim-
ply in terms of Dirichlet concentration parameters. We define the Dirichlet distribution
(for both likelihood and prior preferences) as completely flat (initialized as 5 for like-
lihood and 1 for prior preferences for all possible options). This is in contrast to row
one of Table 2, where we specify flat prior preferences, but the agent is not equipped
with (Dirichlet) hyper-priors that enable the agent to learn about the kind of outcomes
itprefers.
Incrementally, we enabled learning of these parameters. First, all outcome prefer-
ences (and their Dirichlet priors) are removed. Therefore, the agent can only learn the
likelihood. As a result, there is no behavioral imperative other than pure exploration
[Schmidhuber, 2006]. This set-up was simulated 15 times and likelihood was learned
in an experience dependent fashion. This results in an initial (exploratory) trajectory
that covers all unchartered territory in the most efficient way possible i.e., there is no
revisiting of locations that have already been encountered (Figure 5.1). Furthermore,
this behavior persists past the initial exploration, with continuous explorations via new
(non-overlapping) trajectories (Figure 5.2). This represents ’true’ exploratory behav-
46
ior - distinct from random action selection - of the sort only possible in a belief-based
scheme. Furthermore, as there are no rewards, this behavior would be impossible to
motivatefromareinforcementlearningperspective,asthislearningisforitsownsake-
nottoimproverewardseeking. Whilesuchanimperativecouldplausiblybeintroduced
to a belief-based reinforcement learning scheme, it would have to appeal to heuristic
argumentslikethepotentialforarewardfunctiontobeintroducedinthefuture.
Next, we equip the agent with the ability to learn outcome preferences (rather than
learn about the environment). This entails updating the outcome preferences via ac-
cumulation of Dirichlet parameters, without learning the likelihood. The set-up was
simulated 10 times, for two separate kinds of outcome. During the first kind, in the
absence of negative preferences, holes become attractive because they are encountered
first – and this is what the agent learns about its behavior (and implicit preferences).
In other words, because holes (H) are absorbing states, and the agent observes itself
falling in a hole recurrently, it learns to prefer this outcome (Figure 5.3). Similarly, in
the second kind of trial, the agent finds itself recurrently acquiring the Frisbee. This
causes it to exhibit preferences for acquiring Frisbee’s (Figure 5.4). These represent
the capacity of active inference agents to develop into hole-seeking or Frisbee-seeking
agents. As one of these outcomes becomes more familiar, the agent observes its own
behaviorandconcludes‘Iamthesortofcreaturethatenjoysspendingtimeinholes(or
withFrisbee’s),’andadjustsfuturebehaviortobeconsistentwiththis.
This capacity is another important point of distinction with reinforcement learning
approaches, where the problem is defined in terms of a pre-specified reward function.
If this is the problem one hopes to solve, it is clearly undesirable for agents to develop
47
Figure5: Parameterlearningforasinglerewardlocation: resultsforlikelihoodlearning
presented in 1&2 and prior preference learning presented in 3&4. Blue arrows denote
the trajectory taken and numbers in the circles denote the trajectory sequence. Cir-
cular arrows represent loops i.e., once in that state, the same outcome is observed till
maximum number of moves reached (15). 5.1 is a pictorial representation of the first
episodetrajectory,withnopriorpreference: right(1 → 2),right(2 → 3),down(3 → 6),
right(6 ↔ 6). 5.2 depicts the next 4 episodes from the trial. 5.3 has two figures: a
pictorialrepresentationoftrajectorytoholeandheat-mapoftheaccumulatedDirichlet
parameters for score (+is positive, — is negative and is neutral). For this trial, there
is a strict preference for holes at time step 4. 5.4 presents similar information, but for
a goal preferring agent; pictorial representation of trajectory to goal and heat-map of
the accumulated Dirichlet parameters for score. There is a strict preference for goals at
timestep4.
48
ulterior motives. This speaks to the fundamental differences in the problems being
solvedbythetwoapproaches. Underactiveinference,theultimate‘goal’istomaintain
a coherent phenotype and persist over time. Hole-seeking agents achieve this, despite
their behavior deviating from what an observer - or the designer of an AI gym game -
mightregardasappropriate.
Finally, we look at the interaction between the epistemic imperatives to resolve un-
certainty about the likelihood mapping and uncertainty about prior preferences. This
set-up was simulated 10 times and both likelihood distribution and prior outcome pref-
erences learned. By parameterizing both the likelihood and prior outcome preferences
withDirichletdistributions,weinduceacontributiontoexpectedfreeenergythatmakes
visiting every location attractive (i.e., every location acquires epistemic affordance or
novelty). However, after a sufficient number of trials, the agent has learned (i.e., re-
duced its uncertainty) that it prefers to hide in holes (Figure 6). This causes the agent
toexhibitexploitativebehaviorofhiding,ratherthancontinueexploring. After5trials,
theagentgoesstraighttothehole.
This is an interesting example of how — by observing one’s own behavior — pref-
erenceformationcontextualizesthefundamentalimperativetoexplore.
Itisimportanttonotethatthelearnedoutcomepreferencesaretime-dependent;i.e.,
theagentpreferstovisitsafe(F)patchesforthefirst3timepointsandthenvisitgoal(G)
patches with high preference (Figure 6). As noted, these are learned by accumulating
experience(intheformofDirichletconcentrationparameters);suchthatuniformpriors
over outcomes become precise posteriors. These precise posteriors then become the
agent’spreferences. Putsimply,ithaslearnedthatthisisthekindofcreatureitis.
49
Figure6: Learningprioroutcomepreferencesforoutcomemodality,score: initial(top)
andafter5episodes(bottom)forasinglerewardlocation
50
We have observed that even in the absence of clearly defined prior preferences,
active inference agents are able to learn these preferences naturally; since prior prefer-
ences are defined in terms of probability distributions, we simply define a distribution
over distributions, and learn these from the data using the standard inference/gradient
updates (Section 2). However, it is important to highlight that these learnt prior prefer-
ence might be at odds with the ’reward’ from the environment. This conceptualization
flips rewarding states on its head, its a matter of preference not a specific scalar signal
from the environment. . Concretely, we can indeed encourage AI agents to ‘solve’ RL
environmentsbyplacingapriorpreferencethatmaximizestheobservationcorrespond-
ing to reward, but definitionally active inference does not require the resultant reward
maximizing behavior to be considered a successful agent. As long as it can learn and
then maintain a consistent set of behaviors over time – through free energy minimiza-
tion – we consider such an agent to be successful under the active inference problem
definition.
Furthermore, by allowing various parts of the active inference framework to be
learned from the environment (i.e., logP(o|s)), we can infer time-dependent prefer-
encesfromtheenvironment. Thisisincontrasttovanillareinforcementlearning,where
it is less clear how to naturally account for learning an intrinsic reward function, with
many competing approaches [Still and Precup, 2012, Mohamed and Rezende, 2015,
Pathaketal.,2017].
51
4 Discussion
We have described active inference — and the underlying minimization of variational
and expected free energy — using a (simplified) discrete state-space and time formu-
lation. Throughout this review, we have suggested that active inference can be used as
framework to understand how agents (biological or artificial) operate in dynamic, non-
stationaryenvironments[Fristonetal.,2017b],viaastandardgradientdescentonafree
energy functional. More generally, active inference can be thought of as a formal way
ofdescribingthebehaviorofrandomdynamicalsystemswithlatentstates.
As noted in the formulation of active inference (see Equation 23), epistemic forag-
ing (or exploration) emerges naturally. This is captured by the desire to maximize the
mutual information between outcomes and the hidden states on the environment. Ex-
ploration means that the agent seeks out states that afford outcomes, which minimize
uncertainty about (hidden) states of affairs. In the FrozenLake simulation, this was
highlightedbytheinitialexploratorymovemadebytheagent,duetouncertaintyabout
reward location. The move resolved the agent’s uncertainty about the reward location
and all subsequent episodes (when the reward location remained consistent) exploited
this information. Note that in the formulation presented, we discussed model param-
eter exploration that might also be carried out by the agent — when learning either
the likelihood or prior preferences — by having priors over the appropriate probabil-
ity distributions and applying the expected free energy derivations to those parameters
[Schwartenbeck et al., 2019]. The simulations showed that in the absence of a reward
signal from the environment, the agent could learn a niche and exhibit self-evidencing
52
behavior. Additionally, it highlighted that due to the fundamental differences in the
conceptual approach, active inference agents may exhibit Bayes–optimal behavior that
is counter-intuitive from the perspective of reinforcement learning i.e., reward mini-
mization. However, from an active inference perspective, reward is simply the sort of
outcomethatispreferred-–andanagentcanlearntopreferothersortsofoutcomes
The canonical properties presented — with respect to decision making under un-
certainty — are usually engineered in conventional reinforcement learning schemes.
However, more sophisticated formulations of reinforcement learning define a central
role for uncertainty over: Q-value functions [Dearden et al., 1998, 2013, Osband et al.,
2016,O’Donoghueetal.,2018],MDP[Deardenetal.,2013,Osbandetal.,2016,Zint-
graf et al., 2019] or even the reward function [Sorg et al., 2012, Fu¨rnkranz et al., 2012,
Haarnoja et al., 2018]. The formulation presented in [Zintgraf et al., 2019], incor-
porates uncertainty over both the model parametrisation and reward function. This
suggests that there is potential to build upon (and remove components of) Bayesian
reinforcement learning algorithms to render them formally equivalent to active infer-
ence. However, this may come at increased algorithmic complexity cost and loss of
generalization. Additionally, these algorithmic design choices are non-trivial and may
demonstrate counter-intuitive behavior [O’Donoghue et al., 2020]. In contrast, active
inferenceenablesdecision-makingunderuncertaintywithnoheuristicsinplay.
The simulations reveal that once the reward signal is removed the active inference
exhibitsinformationseekingbehavior(tobuildabettermodelofitsenvironment),sim-
ilartotheBayesianreinforcement-learningagent. Thistypeofreward-freelearninghas
beencentraltothecuriosityliteratureinreinforcementlearningdespitebydefinitionnot
53
being true to the definition of reinforcement learning. Concretely, these approaches in-
ducean‘intrinsic’rewardusingsomeheuristic(i.e.,dynamicsprediction[Pathaketal.,
2017],randomfeatureprediction[Burdaetal.,2018],informationgain[Mohamedand
Rezende, 2015]), but this does not necessarily align with the axiomatic goal of ‘max-
imizing a numerical reward signal’; they are simply tools (i.e., inductive biases) that
may lead us to achieve this, and in the case of completely absent rewards, it is unclear
whatthegoalofreinforcementlearningis(i.e.,whatbehaviorsarewe“reinforcing”?).
Our treatment emphasizes that —– via a belief-based scheme — active inference
enables us to specify prior beliefs over preferred outcomes or not (to produce purely
epistemicbehavior). Practically,thesecanproducesimilaroutcomes–andhavebehav-
ioral equivalences to the reward function in reinforcement learning, by assigning high
andlowpriorpreferencestooutcomeswithpositiveandnegativerewards,respectively.
Moreover, this highlights a conceptual distinction between prior beliefs over preferred
outcomes in active inference and reward functions in standard reinforcement learning.
While a reward function specifies how an agent should interact with the environment,
prior beliefs over preferred outcomes are a description (via some particular instantia-
tion) of how the agent wishes to behave. Crucially, this description can be learnt over
time; based on relative frequencies of outcomes encountered. This speaks to an elim-
inative use of Bayes optimality, which replaces the notion of reward — as a motivator
of behavior — with prior beliefs about the outcomes an agent works towards. Con-
ceptually speaking, this dissolves the tautology of reinforcement learning, i.e., rewards
reinforce behaviors that secure rewards. Having said this, related formulations can be
found in reinforcement learning: e.g., belief-based reward functions in [Sorg et al.,
54
2012,Fu¨rnkranzetal.,2012,Zintgrafetal.,2019].
In active inference, agent is likely to maximize extrinsic value (c.f., expected re-
ward) by having prior preferences about unsurprising outcomes (see Equation 22) via
the minimization of expected free energy. It is important to note that the minimiza-
tion of expected free energy is achieved by choosing appropriate policies (sequences
of actions). We accounted for this in the initial set-up of the FrozenLake simulation,
where the agent had strong positive preference for finding the Frisbee. Additionally,
holelocationswereassociatedwithstrongnegativepreferences. Incontrast,theActive
inferencenullmodelwithnopriorpreferencesandnoabilitytolearnthem,encouraged
exploratorybehaviorandtheagentendedinthe(G)location44.0%ofthetime.
However,itisworthnotingthatthesepropertiesfollowfromtheformoftheunder-
lying generative model. The challenge is to identify the appropriate generative model
that best explains the generative process (or the empirical responses) of interest [Ger-
shman and Beck, 2017]. In the FrozenLake simulation, by equipping the agents with
beliefs about the current context, we were able (via the generative model and its belief
updating process) to convert a learning problem into a planning as inference problem.
However, this can be treated as a learning problem by specifying a hierarchical MDP
with learning capacity over the problem space. This would allow for slow moving dy-
namics at a higher level that account for changes in context, and fast moving dynamics
atthelowerlevelthatequiptheagentwiththeabilitynavigatethegiveninstantiationof
theFrozenLake[Fristonetal.,2017b]. Whencomparingpriorpreferencesandrewards,
we highlighted that due to no explicit prior preference for goal states, the belief-based
(active inference and Bayesian RL) agents exhibit conservative behaviors; choosing to
55
avoid the (G) state. This behavior is a caveat of the underlying generative model form
—uncertaintymodeledoverthelocationofthe(G)&(H)state—andmanipulatingthe
prior probability distributions (or the factorization of the states) might lead to policies
whereagentschoosestonotavoidthe(G)location. Additionally,thegenerativemodels
underlying this active inference formulation can be equipped with richer forms (e.g.,
via amortization) or learned via structural learning [Gershman and Niv, 2010, Tervo
etal.,2016]. Thus,ifonewastofindtheappropriategenerativemodel,activeinference
could be used for a variety of different problems; e.g. robotic arm movement, dyadic
agents, playing Atari games, etc. We note that the task of defining the appropriate gen-
erativemodel(discreteorcontinuous)mightbedifficult. Thus,futureworkshouldlook
to incorporate implicit generative models (based on feature representation from empir-
icaldata)orshrinkinghiddenstate-spaces,bydefiningtransitionprobabilitiesbasedon
likelihood(ratherthanlatentstates).
56
Software note
Thesimulationspresentedinthispaperareavailableat: https://github.com/ucbtns/dai
Acknowledgments
NSisfundedbytheMedicalResearchCouncil(Ref: MR/S502522/1). PJBisfundedby
theWillowgroveStudentship. KJFisfundedbytheWellcomeTrust(Ref: 088130/Z/09/Z).
We would like to thank the anonymous reviewers for their suggestions and insightful
commentsonthemanuscript.
Disclosure statement
Theauthorshavenodisclosuresorconflictofinterest.
57
References
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Gio-
vanni Pezzulo. Active inference: a process theory. Neural computation, 29(1):1–49,
2017a.
Karl J Friston, Richard Rosch, Thomas Parr, Cathy Price, and Howard Bowman. Deep
temporal models and active inference. Neuroscience & Biobehavioral Reviews, 77:
388–402,2017b.
Karl Friston, Philipp Schwartenbeck, Thomas FitzGerald, Michael Moutoussis, Timo-
thyBehrens,andRaymondJDolan. Theanatomyofchoice: dopamineanddecision-
making. PhilosophicalTransactionsoftheRoyalSocietyB:BiologicalSciences,369
(1655):20130481,2014.
Karl Friston. A free energy principle for a particular physics. arXiv preprint
arXiv:1906.10184,2019.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, Giovanni
Pezzulo, et al. Active inference and learning. Neuroscience & Biobehavioral Re-
views,68:862–879,2016.
Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas Fitzger-
ald, and Giovanni Pezzulo. Active inference and epistemic value. Cognitive neuro-
science,6(4):187–214,2015.
58
Thomas Parr and Karl J Friston. Uncertainty, epistemics and active inference. Journal
ofTheRoyalSocietyInterface,14(136):20170376,2017.
Beren Millidge, Alexander Tschantz, Anil K Seth, and Christopher L Buckley. On
the relationship between active inference and control as inference. arXiv preprint
arXiv:2006.12964,2020a.
HagaiAttias. Planningbyprobabilisticinference. InAISTATS.Citeseer,2003.
Matthew Botvinick and Marc Toussaint. Planning as inference. Trends in cognitive
sciences,16(10):485–488,2012.
Chris L Baker and Joshua B Tenenbaum. Modeling human plan recognition using
bayesiantheoryofmind. Plan,activity,andintentrecognition: Theoryandpractice,
pages177–204,2014.
HansCrauelandFrancoFlandoli. Attractorsforrandomdynamicalsystems. Probabil-
ityTheoryandRelatedFields,100(3):365–393,1994.
UdoSeifert. Stochasticthermodynamics,fluctuationtheoremsandmolecularmachines.
Reportsonprogressinphysics,75(12):126001,2012.
Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, vol-
ume135. MITpressCambridge,1998.
RichardSSuttonandAndrewGBarto. Reinforcementlearning: Anintroduction. MIT
pressCambridge,2018.
59
Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon White-
son. Deep variational reinforcement learning for pomdps. arXiv preprint
arXiv:1806.02426,2018.
PhilippSchwartenbeck,JohannesPassecker,TobiasUHauser,ThomasHBFitzGerald,
Martin Kronbichler, and Karl J Friston. Computational mechanisms of curiosity and
goal-directedexploration. Elife,8:e41703,2019.
DarioAmodei,ChrisOlah,JacobSteinhardt,PaulChristiano,JohnSchulman,andDan
Mane´. Concreteproblemsinaisafety. arXivpreprintarXiv:1606.06565,2016.
Joannes Vermorel and Mehryar Mohri. Multi-armed bandit algorithms and empirical
evaluation. In European conference on machine learning, pages 437–448. Springer,
2005.
VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AlexGraves,IoannisAntonoglou,
Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learn-
ing. arXivpreprintarXiv:1312.5602,2013.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
Lillicrap,TimHarley,DavidSilver,andKorayKavukcuoglu.Asynchronousmethods
for deep reinforcement learning. In International conference on machine learning,
pages1928–1937,2016.
Richard S Sutton. Integrated architectures for learning, planning, and reacting based
on approximating dynamic programming. In Machine learning proceedings 1990,
pages216–224.Elsevier,1990.
60
Marco Wiering and Ju¨rgen Schmidhuber. Efficient model-based exploration. In Pro-
ceedings of the Sixth International Conference on Simulation of Adaptive Behavior:
From Animals to Animats, volume 6, pages 223–228. MIT Press Cambridge, MA,
1998.
Arryon D Tijsma, Madalina M Drugan, and Marco A Wiering. Comparing exploration
strategiesforq-learninginrandomstochasticmazes.In2016IEEESymposiumSeries
onComputationalIntelligence(SSCI),pages1–8.IEEE,2016.
Nicolo` Cesa-Bianchi, Claudio Gentile, Ga´bor Lugosi, and Gergely Neu. Boltzmann
explorationdoneright. InAdvancesinneuralinformationprocessingsystems,pages
6284–6293,2017.
FengCaoandSoumyaRay. Bayesianhierarchicalreinforcementlearning. InAdvances
inneuralinformationprocessingsystems,pages73–81,2012.
Kevin Lloyd and David S Leslie. Context-dependent decision-making: a simple
bayesianmodel. JournalofTheRoyalSocietyInterface,10(82):20130069,2013.
SindhuPadakandla,ShalabhBhatnagar,etal. Reinforcementlearninginnon-stationary
environments. arXivpreprintarXiv:1905.03970,2019.
Brendan O’Donoghue, Ian Osband, and Catalin Ionescu. Making sense of reinforce-
mentlearningandprobabilisticinference. arXivpreprintarXiv:2001.00805,2020.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial
andreview. arXivpreprintarXiv:1805.00909,2018.
61
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):
279–292,1992.
Pascal Poupart. Lecture slides on bayesian reinforcement learning from
cs885:. https://cs.uwaterloo.ca/ ppoupart/teaching/cs885-spring18/slides/cs885-
lecture10.pdf,2018.
Karl Friston, Je´re´mie Mattout, and James Kilner. Action understanding and active in-
ference. Biologicalcybernetics,104(1-2):137–160,2011.
Rafal Bogacz. A tutorial on the free-energy framework for modelling perception and
learning. Journalofmathematicalpsychology,76:198–211,2017.
KarlJFriston,ThomasParr,andBertdeVries. Thegraphicalbrain: beliefpropagation
andactiveinference. NetworkNeuroscience,1(4):381–414,2017c.
KarlJAstrom. Optimalcontrolofmarkovprocesseswithincompletestateinformation.
Journalofmathematicalanalysisandapplications,10(1):174–205,1965.
LisaLee,BenjaminEysenbach,EmilioParisotto,EricXing,SergeyLevine,andRuslan
Salakhutdinov. Efficient exploration via state marginal matching. arXiv preprint
arXiv:1906.05274,2019.
Edmund Chong, Ariana M Familiar, and Won Mok Shim. Reconstructing representa-
tions of dynamic visual objects in early visual cortex. Proceedings of the National
AcademyofSciences,113(5):1453–1458,2016.
62
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review
for statisticians. Journal of the American statistical Association, 112(518):859–877,
2017.
Matthew J Beal. Variational algorithms for approximate Bayesian inference. PhD
thesis,UCL(UniversityCollegeLondon),2003.
Oguzhan Alagoz, Heather Hsu, Andrew J Schaefer, and Mark S Roberts. Markov de-
cision processes: a tool for sequential decision making under uncertainty. Medical
DecisionMaking,30(4):474–483,2010.
RichardBellman. Onthetheoryofdynamicprogramming. ProceedingsoftheNational
AcademyofSciencesoftheUnitedStatesofAmerica,38(8):716,1952.
ThomasParrandKarlJFriston. Generalisedfreeenergyandactiveinference. Biologi-
calcybernetics,113(5-6):495–513,2019a.
BerenMillidge,AlexanderTschantz,andChristopherLBuckley. Whencetheexpected
freeenergy? arXivpreprintarXiv:2004.08128,2020b.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven
explorationbyself-supervisedprediction. InProceedingsoftheIEEEConferenceon
ComputerVisionandPatternRecognitionWorkshops,pages16–17,2017.
SusanneStillandDoinaPrecup. Aninformation-theoreticapproachtocuriosity-driven
reinforcementlearning. TheoryinBiosciences,131(3):139–148,2012.
63
Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation
forintrinsicallymotivatedreinforcementlearning. InAdvancesinneuralinformation
processingsystems,pages2125–2133,2015.
TomBlau,LionelOtt,andFabioRamos. Bayesiancuriosityforefficientexplorationin
reinforcementlearning. arXivpreprintarXiv:1911.08701,2019.
Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and
DeepakPathak. Planningtoexploreviaself-supervisedworldmodels. arXivpreprint
arXiv:2005.05960,2020.
Stephane Ross, Brahim Chaib-draa, and Joelle Pineau. Bayes-adaptive pomdps. In
Advancesinneuralinformationprocessingsystems,pages1225–1232,2008.
J Zico Kolter and Andrew Y Ng. Near-bayesian exploration in polynomial time. In
Proceedingsofthe26thannualinternationalconferenceonmachinelearning,pages
513–520,2009.
Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja
Hofmann, and Shimon Whiteson. Varibad: A very good method for bayes-adaptive
deeprlviameta-learning. arXivpreprintarXiv:1910.08348,2019.
KarlFriston,SpyridonSamothrakis,andReadMontague. Activeinferenceandagency:
optimal control without cost functions. Biological cybernetics, 106(8-9):523–541,
2012.
Maximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek, Cheng Zhang,
64
Sam Devlin, and Katja Hofmann. Generalization in reinforcement learning with se-
lectivenoiseinjectionandinformationbottleneck.InAdvancesinNeuralInformation
ProcessingSystems,pages13978–13990,2019.
Eduardo F Camacho and Carlos Bordons Alba. Model predictive control. Springer
Science&BusinessMedia,2013.
Jeff Beck, Alexandre Pouget, and Katherine A Heller. Complex inference in neural
circuitswithprobabilisticpopulationcodesandtopicmodels. InAdvancesinneural
informationprocessingsystems,pages3059–3067,2012.
Thomas Parr, Dimitrije Markovic, Stefan J Kiebel, and Karl J Friston. Neuronal mes-
sage passing using mean-field, bethe, and marginal approximations. Scientific re-
ports,9(1):1–18,2019.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic:
Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
arXivpreprintarXiv:1801.01290,2018.
Christopher L Buckley, Chang Sub Kim, Simon McGregor, and Anil K Seth. The
free energy principle for action and perception: A mathematical review. Journal of
MathematicalPsychology,81:55–79,2017.
Thomas Parr and Karl J Friston. The computational pharmacology of oculomotion.
Psychopharmacology,236(8):2473–2484,2019b.
KaiUeltzho¨ffer. Deepactiveinference. Biologicalcybernetics,112(6):547–573,2018.
65
Thomas Parr and Karl J Friston. The discrete and continuous brain: from decisions to
movement—andbackagain. NeuralComputation,30(9):2319–2347,2018.
LancelotDaCosta,ThomasParr,NoorSajid,SebastijanVeselic,VictoritaNeacsu,and
Karl Friston. Active inference on discrete state-spaces: a synthesis. arXiv preprint
arXiv:2001.07203,2020.
Michael Moutoussis, Nelson Jesu´s Trujillo-Barreto, Wael El-Deredy, Raymond Dolan,
and Karl Friston. A formal model of interpersonal inference. Frontiers in human
neuroscience,8:160,2014.
Philipp Schwartenbeck, Thomas HB FitzGerald, Christoph Mathys, Ray Dolan,
Friedrich Wurst, Martin Kronbichler, and Karl Friston. Optimal inference with sub-
optimalmodels: addictionandactivebayesianinference. Medicalhypotheses,84(2):
109–117,2015.
MBerkMirza,RickAAdams,ChristophDMathys,andKarlJFriston.Sceneconstruc-
tion, visual foraging, and active inference. Frontiers in computational neuroscience,
10:56,2016.
Maell Cullen, Ben Davey, Karl J Friston, and Rosalyn J Moran. Active inference in
openaigym: aparadigmforcomputationalinvestigationsintopsychiatricillness. Bi-
ologicalpsychiatry: cognitiveneuroscienceandneuroimaging,3(9):809–818,2018.
Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD
thesis,King’sCollege,Cambridge,1989.
66
Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian
reinforcementlearning: Asurvey. FoundationsandTrends®inMachineLearning,8
(5-6):359–483,2016.
Nathaniel D Daw, Aaron C Courville, and David S Touretzky. Representation and
timing in theories of the dopamine system. Neural computation, 18(7):1637–1677,
2006.
Mark C Fuhs and David S Touretzky. Context learning in the rodent hippocampus.
Neuralcomputation,19(12):3173–3215,2007.
SamuelJGershmanandYaelNiv. Learninglatentstructure: carvingnatureatitsjoints.
Currentopinioninneurobiology,20(2):251–256,2010.
Nathaniel D Daw, Samuel J Gershman, Ben Seymour, Peter Dayan, and Raymond J
Dolan. Model-based influences on humans’ choices and striatal prediction errors.
Neuron,69(6):1204–1215,2011.
SamuelJGershmanandNathanielDDaw. Reinforcementlearningandepisodicmem-
ory in humans and animals: an integrative framework. Annual review of psychology,
68:101–128,2017.
Jakob N Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter
Abbeel, and Igor Mordatch. Learning with opponent-learning awareness. arXiv
preprintarXiv:1709.04326,2017.
Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and
67
Pieter Abbeel. Continuous adaptation via meta-learning in nonstationary and com-
petitiveenvironments. arXivpreprintarXiv:1710.03641,2017.
Samuel J Gershman, Kenneth A Norman, and Yael Niv. Discovering latent causes in
reinforcementlearning. CurrentOpinioninBehavioralSciences,5:43–50,2015.
Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Effi-
cient off-policy meta-reinforcement learning via probabilistic context variables. In
Internationalconferenceonmachinelearning,pages5331–5340,2019.
AndrewYNgandMichaelIJordan. Shapingandpolicysearchinreinforcementlearn-
ing. PhDthesis,UniversityofCalifornia,BerkeleyBerkeley,2003.
Ju¨rgen Schmidhuber. Developmental robotics, optimal artificial curiosity, creativity,
music,andthefinearts. ConnectionScience,18(2):173–187,2006.
RichardDearden,NirFriedman,andStuartRussell. Bayesianq-learning. InAaai/iaai,
pages761–768,1998.
Richard Dearden, Nir Friedman, and David Andre. Model-based bayesian exploration.
arXivpreprintarXiv:1301.6690,2013.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep explo-
ration via bootstrapped dqn. In Advances in neural information processing systems,
pages4026–4034,2016.
Brendan O’Donoghue, Ian Osband, Remi Munos, and Volodymyr Mnih. The uncer-
68
tainty bellman equation and exploration. In International Conference on Machine
Learning,pages3836–3845,2018.
Jonathan Sorg, Satinder Singh, and Richard L Lewis. Variance-based rewards for ap-
proximatebayesianreinforcementlearning. arXivpreprintarXiv:1203.3518,2012.
Johannes Fu¨rnkranz, Eyke Hu¨llermeier, Weiwei Cheng, and Sang-Hyeun Park.
Preference-based reinforcement learning: a formal framework and a policy iteration
algorithm. Machinelearning,89(1-2):123–156,2012.
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by ran-
domnetworkdistillation. arXivpreprintarXiv:1810.12894,2018.
Samuel J Gershman and Jeffrey M Beck. Complex probabilistic inference. Computa-
tionalmodelsofbrainandbehavior,453,2017.
D Gowanlock R Tervo, Joshua B Tenenbaum, and Samuel J Gershman. Toward the
neural implementation of structure learning. Current opinion in neurobiology, 37:
99–105,2016.
69

=== REVISE TO ===
PROFESSIONAL TONE: Begin directly with content - NO conversational openings like 'Okay, here's...'

1. Fix all issues above
2. Title: "Active inference: demystified and compared"
3. Include 10-15 quotes from paper text
   - Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
   - Use consistent quote formatting: 'The authors state: "quote"' or vary attribution phrases
   - Vary attribution phrases to avoid repetition
   - CRITICAL: Only extract quotes that actually appear in the paper text
4. ELIMINATE ALL REPETITION - each sentence must be unique
   - Check before each sentence: 'Have I already said this?' If yes, write something new
   - Vary attribution phrases - do NOT repeat 'The authors state' multiple times
5. Extract methodology, results with numbers, key quotes
6. 1000-1500 words, structured with ### headers

Generate COMPLETE revised summary.