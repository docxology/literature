### Deep Active Inference for Partially Observable MDPs – Summary### OverviewThis paper investigates a deep active inference (DAIF) model designed to tackle partially observable Markov decision processes (POMDPs). The authors propose a novel approach that leverages variational autoencoders (VAEs) to encode high-dimensional sensory inputs, enabling the agent to learn successful policies directly from visual data. The core of the model is based on the free-energy principle, originally proposed by Friston [9], which posits that an agent will perceive and act in an environment such as to minimize its free energy [10]. Under this perspective, action is a consequence of top-down proprioceptive predictions coming from higher cortical levels, i.e., motor reflexes minimize prediction errors[11]. The key contribution of this work is the development of a scalable approach to perception and action that deals with large policy and state spaces, addressing a limitation of previous DAIF models.### MethodologyThe authors state: "The authors propose a deep active inference model that can learn successful policies directly from high-dimensional sensory inputs. The deep learning architecture optimizes a variant of the expected free energy and encodes the continuous state representation by means of a variational autoencoder." They note: "We show, in the OpenAI benchmark, that our approach has comparable or better performance than deep Q-learning, a state-of-the-art deep reinforcement learning algorithm." The model is built upon the free-energy principle, where the agent minimizes its variational free energy (VFE) at each time step. They further explain: "Under this perspective, action is a consequence of top-down proprioceptive predictions coming from higher cortical levels, i.e., motor reflexes minimize prediction errors[11]." The model incorporates a transition network to model the dynamics of the environment, and a policy network to generate actions. The authors also describe the use of a target network to stabilize learning. They add: "The authors propose a deep active inference model that can learn successful policies directly from high-dimensional sensory inputs."### ResultsThe authors state: "We show, in the OpenAI benchmark, that our approach has comparable or better performance than deep Q-learning, a state-of-the-art deep reinforcement learning algorithm." In the OpenAI CartPole-v1 environment, the DAIF agent achieved comparable or better performance than DQN. The model’s performance was evaluated using a moving average reward (MAR) metric. The study demonstrates that the proposed approach can learn effective policies in POMDPs. The authors report: "We have comparable or better performance depending on observability."### Deep Active Inference ModelThe authors state: "Deep active inference (dAIF) has been proposed as an alternative to Deep ReinforcementLearning(RL)[7,8]asageneralscalableapproachtoperception, learning and action." The core of the model is based on the free-energy principle, originally proposed by Friston in [9], which relies on the assumption that an agent will perceive and act in an environment such as to minimize its free energy [10]. Under this perspective, action is a consequence of top-down proprioceptive predictions coming from higher cortical levels, i.e., motor reflexes minimize prediction errors[11]. On the one hand, works on dAIF, such as [2,12,13], have focused on scaling the optimization of the Variational Free-Energy bound (VFE), as described in [9,14], to high-dimensional inputs such as images, modelling the generative process with deep learning architectures. This type of approach preserves the optimization framework (i.e., dynamic expectation maximization [15]) under the Laplace approximation by exploiting the forward and backward passes of the neural network. Alternatively, pure end-to-end solutions to VFE optimization can be achieved by approximating all the probability density functions with neural networks [1,3].On the other hand, Expected Free Energy (EFE) and Generalized Free Energy (GFE) were proposed to extend the one-step ahead implicit action computation into an explicit policy formulation, where the agent is able to compute the best action taking into account a time horizon [16]. Initial agent implementations of these approaches needed the enumeration over every possible policy projected forward in time up to the time horizon, resulting in significant scaling limitations. As a solution, deep neural networks were also proposed to approximate the densities comprising the agent’s generative model [1–6], allowing active inference to be scaled up to larger and more complex tasks.However, despite the general theoretical formulation, current state-of-the-art dAIF, has only been successfully tested in toy problems with fully observable state spaces (Markov Decision Processes, MDP). Conversely, Deep Q-learning (DQN) approaches [7] can deal with high-dimensional inputs such as images. Here, we propose a dAIF model that extends the formulation presented in [3] to tackle problems where the state is not observable2 (i.e. Partially Observable Markov Decision Processes, POMDP), in particular, the environment state has to be inferred directly high-dimensional from visual input. The agent’s objective is to minimize its EFE into the future up to some time horizon T similarly as a receding horizon controller. We compared the performance of our proposed dAIF algorithm in the OpenAI CartPole-v1 environment against DQN.### Experimental SetupTo evaluate the proposed algorithm we used the OpenAI Gym’s CartPole-v1, as depicted in Fig.4. In the CartPole-v1 environment, a pole is attached to a cart that moves along a track. The pole is initially upright, and the agent’s objective is to keep the pole from tilting too far to one side or the other by increasing or decreasing the cart’s velocity. Additionally, the position of the cart must remain within certain bound. An episode of the task terminates when the agent fails either of these objectives, or when it has survived for500 time steps. Each time step the agent receives a reward of1.The CartPole state consists of four continuous values: the cart position, the cart velocity, the pole angle and the velocity of the pole at its tip. Each run the state values are initialized at random within a small margin to ensure variability between runs. The agent can exact influence on the next state through two discrete actions, by pushing the cart to the left, or by pushing it to the right.Test were conducted in two scenarios:1)anMDPscenarioinwhichtheagenthas direct access to the state of the environment, and2) a POMDP scenario in which the agent does not have direct access the environment state, and instead receives pixel value from which it must derive meaningful hidden states. By default, rendering the CartPole-v1 environment returns a3×400×600 (color, height,width)arrayofpixelvalues.InourexperimentsweprovidethePOMDPagents with a downscaled and cropped image. There the agents receive a3×37×85 pixel value array in which the cart is centered until it comes near the left or right border.### DiscussionThe authors state: "The authors propose a deep active inference model that can learn successful policies directly from high-dimensional inputs, such as images." Results show that in the MDP case the DAIF agent outperforms the DQN agent, and performs more consistently between runs. Both agents were also shown to be capable of learning(less)successful policies in the POMDP version, where the performance between dAIF and DQN models was found to be comparable. Further work will focus on validating the model on a broader range of more complex problems.