=== IMPORTANT: ISOLATE THIS PAPER ===
You are summarizing ONLY the paper below. Do NOT reference or use content from any other papers.
Do NOT mix information from different papers. Only use information from THIS specific paper.

Paper Title: Contrastive Active Inference
Citation Key: mazzaglia2021contrastive
Authors: Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt

REMEMBER: Extract quotes, claims, and findings ONLY from the paper text provided below.

Year: 2021

Abstract: Activeinferenceisaunifyingtheoryforperceptionandactionrestinguponthe
ideathatthebrainmaintainsaninternalmodeloftheworldbyminimizingfree
energy. Fromabehavioralperspective,activeinferenceagentscanbeseenasself-
evidencingbeingsthatacttofulfilltheiroptimisticpredictions,namelypreferred
outcomesorgoals. Incontrast,reinforcementlearningrequireshuman-designedre-
wardstoaccomplishanydesiredoutcome. Althoughactiveinferencecouldprovide
amorenaturalself-supervisedobjectiveforcontrol,itsapplicabilityhasbee...

Key Terms: ghentuniversity, idlab, pietromazzaglia, active, bartdhoedt, timverbelen, pietro, inference, ugent, contrastive

=== FULL PAPER TEXT ===

Contrastive Active Inference
PietroMazzaglia TimVerbelen BartDhoedt
IDLab IDLab IDLab
GhentUniversity GhentUniversity GhentUniversity
pietro.mazzaglia@ugent.be tim.verbelen@ugent.be bart.dhoedt@ugent.be
Abstract
Activeinferenceisaunifyingtheoryforperceptionandactionrestinguponthe
ideathatthebrainmaintainsaninternalmodeloftheworldbyminimizingfree
energy. Fromabehavioralperspective,activeinferenceagentscanbeseenasself-
evidencingbeingsthatacttofulfilltheiroptimisticpredictions,namelypreferred
outcomesorgoals. Incontrast,reinforcementlearningrequireshuman-designedre-
wardstoaccomplishanydesiredoutcome. Althoughactiveinferencecouldprovide
amorenaturalself-supervisedobjectiveforcontrol,itsapplicabilityhasbeenlim-
itedbecauseoftheshortcomingsinscalingtheapproachtocomplexenvironments.
Inthiswork,weproposeacontrastiveobjectiveforactiveinferencethatstrongly
reduces the computational burden in learning the agent’s generative model and
planningfutureactions. Ourmethodperformsnotablybetterthanlikelihood-based
activeinferenceinimage-basedtasks,whilealsobeingcomputationallycheaper
andeasiertotrain.Wecomparetoreinforcementlearningagentsthathaveaccessto
human-designedrewardfunctions,showingthatourapproachcloselymatchestheir
performance. Finally,wealsoshowthatcontrastivemethodsperformsignificantly
betterinthecaseofdistractorsintheenvironmentandthatourmethodisableto
generalizegoalstovariationsinthebackground.
Websiteandcode: https://contrastive-aif.github.io/
1 Introduction
DeepReinforcementLearning(RL)hasledtosuccessfulresultsinseveraldomains,suchasrobotics,
videogamesandboardgames[42,36,2]. Fromaneuroscienceperspective,therewardprediction
errorsignalthatdriveslearningindeepRLcloselyrelatestotheneuralactivityofdopamineneurons
forreward-basedlearning[44,3]. However,therewardfunctionsusedindeepRLtypicallyrequire
domainandtask-specificdesignfromhumans,spoilingthegeneralizationcapabilitiesofRLagents.
Furthermore,thepossibilityoffaultyrewardfunctionsmakestheapplicationofdeepRLriskyin
real-worldcontexts,giventhepossibleunexpectedbehaviorsthatmayderivefromit[10,29,38].
ActiveInference(AIF)hasrecentlyemergedasaunifyingframeworkforlearningperceptionand
action. InAIF,agentsoperateaccordingtooneabsoluteimperative: minimizetheirfreeenergy[15].
Withrespecttopastexperience,thisencouragestoupdateaninternalmodeloftheworldtomaximize
evidencewithrespecttosensorydata. Withregardtofutureactions,theinferenceprocessbecomes
‘active’ and agents select behaviors that fulfill optimistic predictions of their model, which are
representedaspreferredoutcomesorgoals[17].ComparedtoRL,theAIFframeworkprovidesamore
naturalwayofencodingobjectivesforcontrol. However,itsapplicabilityhasbeenlimitedbecauseof
theshortcomingsinscalingtheapproachtocomplexenvironments,andcurrentimplementationshave
focusedontaskswitheitherlow-dimensionalsensoryinputsand/orsmallsetsofdiscreteactions[12].
Moreover,severalexperimentsintheliteraturehavereplacedtheagent’spreferredoutcomeswith
RL-likerewardsfromtheenvironment,downplayingtheAIFpotentialtoprovideself-supervised
objectives[13,34,49].
35thConferenceonNeuralInformationProcessingSystems(NeurIPS2021).
4202
naJ
51
]GL.sc[
4v38001.0112:viXra
OneofthemajorshortcomingsinscalingAIFtoenvironmentswithhigh-dimensional,e.g. image-
based,environmentscomesfromthenecessityofbuildingaccuratemodelsoftheworld,whichtryto
reconstructeverydetailinthesensorydata. Thiscomplexityisalsoreflectedinthecontrolstage,
whenAIFagentscomparefutureimaginaryoutcomesofpotentialactionswiththeirgoals,toselect
themostconvenientbehaviors. Inparticular,weadvocatethatfulfillinggoalsinimagespacecanbe
poorlyinformativetobuildanobjectiveforcontrol.
In this work, we propose Contrastive Active Inference, a framework for AIF that aims to both
reduce the complexity of the agent’s internal model and to propose a more suitable objective to
fulfillpreferredoutcomes,byexploitingcontrastivelearning. Ourmethodprovidesaself-supervised
objective that constantly informs the agent about the distance from its goal, without needing to
reconstructtheoutputsofpotentialactionsinhigh-dimensionalimagespace.
Thecontributionsofourworkcanbesummarisedasfollows: (i)weproposeaframeworkforAIF
thatdrasticallyreducesthecomputationalpowerrequiredbothforlearningthemodelandplanning
future actions, (ii) we combine our method with value iteration methods for planning, inspired
bythe RLliterature, toamortizethe costofplanning inAIF,(iii)we compareourframeworkto
state-of-the-artRLtechniquesandtoanon-contrastiveAIFformulation,showingthatourmethod
compareswellwithreward-basedsystemsandoutperformsnon-contrastiveAIF,(iv)weshowthat
contrastivemethodsworkbetterthanreconstruction-basedmethodsinpresenceofdistractorsinthe
environment,(v)wefoundthatourcontrastiveobjectiveforcontrolallowsmatchingdesiredgoals,
despitedifferencesinthebackgrounds. Thelatterfindingcouldhaveimportantconsequencesfor
deployingAIFinreal-worldsettings,suchasrobotics,whereperfectlyreconstructingobservations
fromtheenvironmentandmatchingthemwithhigh-dimensionalpreferencesispracticallyunfeasible.
2 Background
ThecontrolsettingcanbeformalizedasaPartiallyObservable
MarkovDecisionProcess(POMDP),whichisdenotedwith a a a
t-1 t t+1
thetupleM = {S,A,T,Ω,O,γ},whereS isthesetofun-
observedstates,Aisthesetofactions,T isthestatetransition
function,alsoreferredtoasthedynamicsoftheenvironment, s t-1 s t s t+1 s t+2
Ωisthesetobservations,Oisasetofconditionalobservation
probabilities,andγ isadiscountfactor(Figure1). Weusethe
o o o o
termsobservationsandoutcomesinterchangeablythroughout t-1 t t+1 t+2
thework. InRL,theagenthasalsoaccesstoarewardfunction
Figure1: POMDPGraphicalModel
R,mappingstate-actionpairstorewards.
ActiveInference. InAIF,thegoaloftheagentistominimize(avariationalboundon)thesurprisal
over observations, −logp(o). With respect to past observations, the upper bound leads to the
variationalfreeenergyF,whichfortimesteptis:
F =E [logq(s )−logp(o ,s )]≥ −logp(o ) (1)
q(st) t t t t
whereq(s )representsanapproximateposterior.
t
The agent hence builds a generative model over states, actions and observations, by defining a
state transition function p(s |s ,a ) and a likelihood mapping p(o |s ), while the posterior
t t−1 t−1 t t
distributionoverstatesisapproximatedbythevariationaldistributionq(s |o ). Thefreeenergycan
t t
thenbedecomposedas:
F =D [q(s |o )||p(s |s ,a )]−E [logp(o |s )]. (2)
AIF KL t t t t−1 t−1 q(st|ot) t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
complexity accuracy
Thisimpliesthatminimizingvariationalfreeenergy,ontheonehand,maximizesthelikelihoodof
observationsunderthelikelihoodmapping(i.e. maximizingaccuracy),whilstminimizingtheKL
divergencebetweentheapproximateposteriorandprior(i.e. complexity). Notethatforthepastwe
assumethatoutcomesandactionsareobserved,henceonlyinferencesaremadeaboutthestates .
t
Alsonotethatthevariationalfreeenergyisdefinedasthenegativeevidencelowerboundasknown
fromthevariationalautoencoderframework[39,27].
Forfuturetimesteps,theagenthastomakeinferencesaboutbothfuturestatesandactionsq(s ,a )=
t t
q(a |s )q(s ),whiletakingintoaccountexpectationsoverfutureobservations. Crucially,inactive
t t t
2
inferencetheagenthasapriordistributionp˜(o )onpreferredoutcomesitexpectstoobtain. Action
t
selectionisthencastasaninferenceproblem,i.e. inferringactionsthatwillyieldpreferredoutcomes,
ormoreformallythatminimizetheexpectedfreeenergyG:
G =E [logq(s ,a )−logp˜(o ,s ,a )], (3)
q(ot,st,at) t t t t t
wherep˜(o ,s ,a )=p(a )p(s |o )p˜(o )istheagent’sbiasedgenerativemodel,andtheexpectation
t t t t t t t
isoverpredictedobservations,statesandactionsq(o ,s ,a )=p(o |s )q(s ,a ).
t t t t t t t
Ifweassumethevariationalposterioroverstatesisagoodapproximationofthetrueposterior,i.e.
q(s |o )≈p(s |o ),andwealsoconsiderauniformpriorp(a )overactions[35],theexpectedfree
t t t t t
energycanbeformulatedas:
G =−E [D [q(s |o )||q(s )]]−E [logp˜(o )]−E [H(q(a |s ))]. (4)
AIF q(ot) KL t t t q(ot) t q(st) t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
intrinsicvalue extrinsicvalue actionentropy
Intuitively,thismeansthattheagentwillinferactionsforwhichobservationshaveahighinformation
gainaboutthestates(i.e. intrinsicvalue),whichwillyieldpreferredoutcomes(i.e. extrinsicvalue),
whilealsokeepingitspossibleactionsasvariedaspossible(i.e. actionentropy).
FullderivationsoftheequationsinthissectionareprovidedintheAppendix.
Reinforcement Learning. In RL, the objective of the agent is to maximize the discounted sum
of rewards, or return, over time
(cid:80)∞γtr
. Deep RL can also be cast as probabilistic inference,
t t
by introducing an optimality variable O which denotes whether the time step t is optimal [30].
t
Thedistributionovertheoptimalityvariableisdefinedintermsofrewardsasp(O = 1|s ,a ) =
t t t
exp(r(s ,a )). Inferenceisthenobtainedbyoptimizingthefollowingvariationallowerbound
t t
−logp(O )≤E [logq(s ,a )−logp(O ,s ,a )]
t q(st,at) t t t t t
(5)
=−E [r(s ,a )]−E [H(q(a |s ))],
q(st,at) t t q(st) t t
wherethereward-maximizingRLobjectiveisaugmentedwithanactionentropyterm,asinmaximum
entropycontrol[20]. Asalsohighlightedin[35],ifweassumelogp˜(o |s )=logp(O |s ),wecan
t t t t
seethatRLworksalikeAIF,butencodingoptimalityvalueinthelikelihoodratherthanintheprior.
Inordertoimprovesample-efficiencyofRL,model-basedapproaches(MBRL),wheretheagent
reliesonaninternalmodeloftheenvironmenttoplanhigh-rewardingactions,havebeenstudied.
ContrastiveLearning. Contrastiverepresentations,whichaimtoorganizethedatadistinguishing
similar and dissimilar pairs, can be learned through Noise Contrastive Estimation (NCE) [19].
Following[37],anNCElosscanbedefinedasalowerboundonthemutualinformationbetweentwo
variables. GiventworandomvariablesX andY,theNCElowerboundis:
(cid:34) 1 (cid:88) K ef(xi,yi) (cid:35)
I(X;Y)≥I (X;Y)≜E log , (6)
NCE K 1 (cid:80)K ef(xi,yj)
i=1 K j=1
(cid:81)
wheretheexpectationisoverK independentsamplesfromthejointdistribution: p(x ,y )and
j j j
f(x,y)isafunction,calledcritic,thatapproximatesthelogdensityratiologp(x|y). Crucially,the
p(x)
criticcanbeunbounded,asin[50],wheretheauthorsshowedthataninnerproductoftransformated
samplesfromXandY,namelyf(x,y)=h(x)Tg(y),withhandgfunctions,workswellasacritic.
3 ContrastiveActiveInference
In this section, we present the Contrastive Active Inference framework, which reformulates the
problemofoptimizingthefreeenergyofthepastF andtheexpectedfreeenergyofthefutureG as
contrastivelearningproblems.
3.1 ContrastiveFreeEnergyofthePast
InordertolearnagenerativemodeloftheenvironmentfollowingAIF,anagentcouldminimizethe
variationalfreeenergyF fromEquation2. Forhigh-dimensionalsignals, suchaspixel-based
AIF
3
images, the model works similarly to a Variational AutoEncoder (VAE) [27], with the informa-
tionencodedinthelatentstates beingusedtoproducereconstructionsofthehigh-dimensional
t
observationso throughthelikelihoodmodel.
t
However,reconstructingimagesatpixellevelhasseveralshortfalls: (a)itrequiresmodelswithhigh
capacity, (b) it can be quite computationally expensive, and (c) there is the risk that most of the
representationcapacityiswastedoncomplexdetailsoftheimagesthatareirrelevantforthetask.
Wecanavoidpredictingobservations,byusinganNCEloss. Optimizingthemutualinformation
betweenstatesandobservations,itbecomespossibletoinfers fromo ,withouthavingtocomputea
t t
reconstruction. Inordertoturnthevariationalfreeenergylossintoacontrastiveloss,weaddthe
constantmarginallog-probabilityofthedatalogp(o )toF,obtaining:
t
+
F =D [q(s |o )||p(s )]−E [logp(o |s )−logp(o )]
KL t t t q(st|ot) t t t (7)
=D [q(s |o )||p(s )]−I(S ;O ).
KL t t t t t
AsforEquation6,wecanapplyalowerboundonthemutualinformationI(S ;O ). Wecandefine
t t
thecontrastivefreeenergyofthepastas:
F =D [q(s |o )||p(s )]−I (S ;O )
NCE KL t t t NCE t t
=D
KL
[q(s
t
|o
t
)||p(s
t
)]−E
q(st|ot)p(ot)
[f(o
t
,s
t
)]+E
q(st|ot)p(o′)
[log
K
1 (cid:80)K
j=1
ef(oj,st)],
(8)
wherethedynamicsp(s )ismodelledasp(s |s ,a ),andtheK samplesfromthedistribution
t t t−1 t−1
p(o′)representobservationsthatdonotmatchwiththestates ,catalyzingthecontrastivemechanism.
t
GiventheinequalityI ≤I,thiscontrastiveutilityprovidesanupperboundonthevariationalfree
NCE
energy,F ≤F ,andthusonsuprisal.
NCE
3.2 ContrastiveFreeEnergyoftheFuture
Performingactiveinferenceforactionselectionmeansinferringactionsthatrealizepreferredout-
comes, byminimizingtheexpectedfreeenergyG. Inordertoassesshowlikelyexpectedfuture
outcomesaretofulfilltheagent’spreferences,inEquation4,theagentusesitsgenerativemodelto
predictfutureobservations.
Reconstructingimaginaryobservationsinthefuturecanbecomputationallyexpensive. Furthermore,
matchingimaginedoutcomeswiththeagent’spreferencesinpixelspacecanbepoorlyinformative,
aspixelsarenotsupposedtocaptureanysemanticsaboutobservations. Also,observationsthatare
“far”inpixelspacearen’tnecessarilyfarintransitionspace. Forexample,whenthegoalisbehinda
door,standingbeforethedooris“far”inpixelspacebutonlyoneactionaway(i.e. openingthedoor).
When the agent learns a contrastive model of the world, following Equation 8, it can exploit its
abilitytomatchobservationswithstateswithoutreconstructions,inordertosearchforthestatesthat
correspondwithitspreferences. Hence,weformulatetheexpectationintheexpectedfreeenergyG
intermsofthepreferredoutcomes,sothatwecanaddtheconstantmarginalp˜(o ),obtaining:
t
+
G =E [logq(s ,a )−logp˜(o ,s ,a )+logp˜(o )]
p˜(ot)q(st,at) t t t t t t
(9)
=D [q(s )||p(s )]−I(S ;O˜ )−E [H(q(a |s ))].
KL t t t t q(st) t t
Withabuseofnotation,themutualinformationbetweenS andO˜ quantifiestheamountofinforma-
t t
tionsharedbetweenfutureimaginarystatesandpreferredoutcomes.
WefurtherassumeD [q(s )||p(s )] = 0, whichconstrainstheagenttoonlymodifyitsactions,
KL t t
preventingittochangethedynamicsoftheworldtoaccomplishitsgoal,aspointedoutin[30]. This
leadstothefollowingobjectiveforthecontrastivefreeenergyofthefuture:
G =−I (S ;O˜ )−E [H(q(a |s ))]
NCE NCE t t q(st) t t
(10)
=−E
q(st)p˜(o)
[f(o˜,s
t
)]+E
q(st)p(o′)
[log
K
1 (cid:80)K
j=1
ef(oj,st)]−E
q(st)
[H(q(a
t
|s
t
))].
SimilarasintheF ,theK samplesfromp(o′)fosterthecontrastivemechanism,ensuringthat
NCE
thestates correspondstothepreferredoutcomes,whilealsobeingasdistinguishableaspossible
t
fromotherobservations. Thiscomponentimpliesasimilarprocessastotheambiguityminimization
aspecttypicallyassociatedwiththeAIFframework[16].
4
4 ModelandAlgorithm
TheAIFframeworkentailsperceptionandaction,inaunifiedview. Inpractice,thisistranslated
intolearningaworldmodel,tocapturetheunderlyingdynamicsoftheenvironment,minimizing
thefreeenergyofthepast,andlearningabehaviormodel,whichproposesactionstoaccomplish
theagent’spreferences,minimizingthefreeenergyofthefuture. Inthiswork,weexploitthehigh
expressivenessofdeepneuralnetworkstolearntheworldandthebehaviormodels.
Theworldmodeliscomposedbythefollowingcomponents:
Priornetwork: p (s |s ,a )
ϕ t t−1 t−1
Posteriornetwork: q (s |s ,a ,o )
ϕ t t−1 t−1 t
Representationmodel: f (o,s)
ϕ
For the prior network, we use a GRU [9] while the posterior network combines a GRU with a
CNN to process observations. Both the prior and the posterior outputs are used to parameterize
Gaussianmultivariatedistributions,whichrepresentastochasticstate,fromwhichwesampleusing
thereparameterizationtrick[27]. Thissetupisinspireduponthemodelspresentedin[21,54,4].
Fortherepresentationmodel,weutilizeanetworkthatfirstprocesseso ands withMLPsandthen
t t
computesthedot-productbetweentheoutputs,obtainingf (o,s)=h (o)Tg (s),analogouslyto
ϕ ϕ ϕ
(cid:80)
[50]. Weindicatetheunifiedworldmodellosswith: J = F (s ,o ).
ϕ t NCE t t
Inordertoamortizethecostoflong-termplanningforbehaviorlearning,weuseanexpectedutility
functiong(s )toestimatetheexpectedfreeenergyinthefutureforthestates ,similarlyto[34]. The
t t
behaviormodelisthencomposedbythefollowingcomponents:
Actionnetwork: q (a |s )
θ t t
Expectedutilitynetwork: g (s )
ψ t
wheretheactionandexpectedutilitynetworksarebothMLPsthatareconcurrentlytrainedasin
actor-criticarchitecturesforRL[28,20]. Theactionmodelaimstominimizetheexpectedutility,
which is an estimate of the expected free energy of the future over a potentially infinite horizon,
whiletheutilitynetworkaimstopredictagoodestimateoftheexpectedfreeenergyofthefuture
thatisobtainablebyfollowingtheactionsoftheactionnetwork. Weindicatetheactionnetworkloss
withJ = (cid:80) G (s )andtheutilitynetworklosswithJ = (cid:80) (g (s )− (cid:80)∞ G (s ))2,
θ t NCE t ψ t ψ t k=T NCE t
where the sum from the current time step to an infinite horizon is obtained by using a TD(λ)
exponentially-weightedestimatorthattradesoffbiasandvariance[43](detailsinAppendix).
The training routine, which alternates updates to the models with data collection, is shown in
Algorithm1. Ateachtrainingiterationofthemodel, wesampleB trajectoriesoflengthLfrom
thereplaybufferD. Negativesamplesforthecontrastivefunctionalsareselected, foreachstate,
bytakingL−1intra-episodenegatives, correspondingtotemporallydifferentobservations, and
(B−1)∗Lextra-episodenegatives,correspondingtoobservationsfromdifferentepisodes.
Mostoftheabovechoices,alongwiththetrainingroutineitself,aredeliberatelyinspiredtocurrent
state-of-the-artapproachesforMBRL[23,22,11]. Themotivationbehindthisistwofold: ontheone
hand,wewanttoshowthatapproachesthathavebeenusedtoscaleRLforcomplexplanningcan
alsostraightforwardlybeappliedforscalingAIF.Ontheotherhand,inthenextsection,weoffera
directcomparisontocurrentstate-of-the-arttechniquesforRLthat,beingunbiasedwithrespectto
themodels’architectureandthetrainingroutine,canfocusontherelevantcontributionsofthispaper,
whichconcernsthecontrastivefunctionalsforperceptionandaction.
Relevantparameterizationfortheexperimentscanbefoundinthenextsection,whilehyperparameters
andadetaileddescriptionofeachnetworkarelefttotheAppendix.
5 Experiments
In this section, we compare the contrastive AIF method to likelihood-based AIF and MBRL in
high-dimensionalimage-basedsettings. Astheexperimentsarebasedinenvironmentsoriginally
designedforRL,wedefinedad-hocpreferredoutcomesforAIF.Ourexperimentationaimstoanswer
thefollowingquestions: (i)isitpossibletoachievehigh-dimensionalgoalswithAIF-basedmethods?
(ii) what is the difference in performance between RL-based and AIF-based methods? (iii) does
5
Algorithm1:TrainingandDataCollection
1: Initializeworldmodelparametersϕandbehaviormodelparametersθandψ.
2: InitializedatasetDwithRrandom-actionepisodes.
3: whilenotdonedo
4: forupdatestepu=1..Udo
5: SampleBtrajectoriesoflengthLfromD.
6: Inferstatess t usingtheworldmodel.
7: UpdatetheworldmodelparametersϕontheBtrajectories,minimizingL θ .
8: ImagineItrajectoriesoflengthHfromeachs t .
9: UpdatetheactionnetworkparametersθontheItrajectories,minimizingL ϕ .
10: UpdatetheexpectedutilitynetworkparametersψontheItrajectories,minimizingL ψ .
11: end
12: Resettheenvironment.
13: Initstates t =s 0 andsett=0
14: InitnewtrajectorywiththefirstobservationT ={o 1 }
15: whileenvironmentnotdonedo
16: Inferactiona t usingtheactionnetworkq θ (a t |s t ).
17: Actontheenvironmentwitha t ,andreceiveobservationo t+1 .
18: AddtransitiontothebufferT =T ∪{a t ,o t+1 }andsett=t+1
19: Inferstates t usingtheworldmodel
20: end
21: end
contrastiveAIFperformbetterthanlikelihood-basedAIF?(iv)inwhatcontextscontrastivemethods
aremoredesirablethanlikelihood-basedmethods? (v)areAIF-basedmethodsresilienttovariations
intheenvironmentbackground?
WecomparethefollowingfourflavorsofMBRLandAIF,sharingsimilarmodelarchitecturesandall
trainedaccordingtoAlgorithm1:
• Dreamer: theagentsbuildaworldmodelabletoreconstructbothobservationsandrewards
fromthestate. Reconstructedrewardsforimaginedtrajectoriesarethenusedtooptimize
thebehaviormodelinanMBRLfashion[23,22].
• ContrastiveDreamer: thismethodisanalogtoitsreconstruction-basedcounterpart,apart
fromthatitusesacontrastiverepresentationmodel,likeourapproach. Similarmethods
havebeenstudiedin[23,32].
• Likelihood-AIF:theagentminimizestheAIFfunctionals,usingobservationreconstructions.
Therepresentationmodelfromtheprevioussectionisreplacedwithanobservationlike-
lihoodmodelp (o |s ),whichwemodelasatransposedCNN.Similarapproacheshave
ϕ t t
beenpresentedin[13,34].
• Contrastive-AIF(ours): theagentminimizesthecontrastivefreeenergyfunctionals.
InTable1,wecomparethenumberofparametersandofmultiply-accumulate(MAC)operations
requiredforthetwoflavorsoftherepresentationmodelinourimplementation: likelihood-basedand
contrastive(ours). Usingacontrastiverepresentationmakesthemodel13.8timesmoreefficientin
termsofMACoperationsandreducesthenumberofparametersbyafactor3.5.
InTable2,wecomparethecomputationspeedinourexperiments,measuringwall-clocktimeand
usingDreamerasareference. Contrastivemethodsareonaverage16%faster,whileLikelihood-AIF,
whichinadditiontoDreamerreconstructsobservationsforbehaviorlearning,is224%slower.
Table1: ComputationalEfficiency Table2: ComputationTime
MMACs #Params w.r.t. Dreamer
Likelihood 212.2 4485.7k ContrastiveDreamer/AIF 0.84
Ours 15.4 1266.7k Likelihood-AIF 3.24
6
Figure 2: MiniGrid Experiments. (left) Empty task goal image. (right) Results: shaded areas
indicatestandarddeviationacrossseveralruns.
5.1 MiniGridNavigation
WeperformedexperimentsontheEmpty6×6andtheEmpty8×8environmentsfromtheMiniGrid
suite[8]. Inthesetasks,theagent,representedasaredarrow,shouldreachthegoalgreensquare
navigatingablackgrid(seeFigure3a). Theagentonlyseesapartoftheenvironment,corresponding
toa7×7gridcenteredontheagent(inthebottomcentertile). Werenderobservationsas64×64
pixels.ForRL,apositiverewardbetween0and1isprovidedtotheagentassoonastheagentreaches
thegoaltile: thefastertheagentreachesthegoal,thehigherthereward. ForAIFagents,wedefined
thepreferredoutcomeastheagentseeingitselfonthegoalgreentile,asshowninFigure2(left).
For the 6×6 task, the world model is trained by sampling B = 50 trajectories of length L = 7,
whilethebehaviormodelistrainedbyimaginingH =6stepslongtrajectories. Forthe8×8task,
we increased the length L to 11 and the imagination horizon H to 10. For both tasks, we first
collectedR=50randomepisodes,topopulatethereplaybuffer,andtrainforU =100stepsafter
collectinganewtrajectory. Beingtheactionsetdiscrete,weoptimizedtheactionnetworkemploying
REINFORCEgradients[52]withrespecttotheexpectedutilitynetwork’sestimates.
We assess performance in terms of the rewards achieved along one trajectory, stressing that AIF
methodsdidnothaveaccesstotherewardfunctionbutonlytothegoalobservation,duringtraining.
Theresults,displayedinFigure2(right),showtheaveragesumofrewardsobtainedalongtraining,
overthenumberoftrajectoriescollected. Wechosetocompareoverthenumberoftrajectoriesasthe
trajectories’lengthdependsonwhethertheagentcompletedthetaskornot.
Inthisbenchmark,weseethatMBRLalgorithmsrapidlyconvergetohighlyrewardingtrajectories,
inboththe6×6andthe8×8tasks. Likelihood-AIFstrugglestoconvergetotrajectoriesthatreach
the goal consistently and fast, mostly achieving a reward mean lower than 0.4. In contrast, our
method performs comparably to the MBRL methods in the 6×6 grid and reaches the goal twice
moreconsistentlythanLikelihood-AIFinthe8×8grid,leaningtowardsDreamerandContrastive
Dreamer’sresults.
UtilityFunctionAnalysis. Inordertounderstandthedifferencesbetweentheutilityfunctionswe
experimentedwith,weanalyzethevaluesassignedtoeachtileinthe8×8taskbyeverymethod.
(a)GridTask (b)Rewards (c)AIFModel (d)NCEModel(ours)
Figure3: UtilityValuesMiniGrid. (b-c-d)Darkertilescorrespondtohigherutilityvalues.
7
(a)ReacherEasyGoal (b)ReacherHardGoal (c)DistractingReacherEasy
Figure4: Continuoustaskssetup. NotethattheReacherEasyGoalisalsousedfortheDistracting
ReacherEasytask,withoutchangingthegoal’sbackground.
FortheAIFmethods,wecollectedallpossibletransitionsintheenvironmentandusedthemodelto
computeutilityvaluesforeachtile. TheresultsareshowninFigure3.
TherewardsignalfortheEmptyenvironmentisverysparseandinformativeonlyoncetheagent
reachesthegoal. Incontrast,AIFmethodsprovidedenserutilityvalues. Inparticular,wenoticedthat
theLikelihood-AIFmodelprovidesaverystrongsignalforthegoalposition,whereasothervalues
arelessinformativeofthegoal. Instead,theContrastive-AIFmodelseemstocapturesomesemantic
information about the environment: it assigns high values to all corners, which are conceptually
closeroutcomes tothe goal, whilealso providing thesteepestsignal forthe greencorner andits
neighbortiles. Asalsosupportedbytheresultsobtainedintermsofrewards,ourmethodprovidesa
denserandmoreinformativesignaltoreachthegoalinthistask.
5.2 ReacherTask
Weperformedcontinuous-controlexperimentsontheReacherEasyandHardtasksfromtheDeep-
MindControl(DMC)Suite[48]andonReacherEasyfromtheDistractingControlSuite[47]. Inthis
task,atwo-linkarmshouldpenetrateagoalspherewithitstipinordertoobtainrewards,withthe
spherebeingbiggerintheEasytaskandsmallerintheHardone. TheDistractingSuiteaddsanextra
layerofcomplexitytotheenvironment,alteringthecameraangle,thearmandthegoalcolors,and
thebackground. Inparticular,weusedthe‘easy’versionofthisbenchmark,correspondingtosmaller
changesinthecameraanglesandinthecolors,andchoosingthebackgroundfromoneoffourvideos
(exampleinFigure4c).
InordertoprovideconsistentgoalsfortheAIFagents,wefixedthegoalspherepositionasshownin
Figure4band4a. AsthereisnofixedbackgroundintheDistractingSuitetask,wecouldnotusea
goalimagewiththecorrectbackground,asthatwouldhavemeantchangingitateverytrajectory. To
notintroduce‘external’interventionsintotheAIFexperiments,wedecidedtouseagoalimagewith
theoriginalbluebackgroundfromtheDMCSuitetotestouttheAIFcapabilitytogeneralizegoalsto
environmentshavingthesamedynamicsbutdifferentbackgrounds.
Forbothtasks,theworldmodelistrainedbysamplingB =30trajectoriesoflengthL=30,while
thebehaviormodelistrainedbyimaginingH =10stepslongtrajectories. WefirstcollectR=50
randomepisodes,topopulatethereplaybuffer,andtrainforU =100stepsaftereverynewtrajectory.
Beingtheactionsetcontinuous,weoptimizedtheactionnetworkbackpropagatingtheexpectedutility
valuethroughthedynamics,byusingthereparameterizationtrickforsamplingactions[23,11].
TheresultsarepresentedinFigure5,evaluatingagentsintermoftherewardsobtainedpertrajectory.
Thelengthofatrajectoryisfixedto1·103steps.
ReacherEasy/Hard. TheresultsontheReacherEasyandHardtasksshowthatourmethodwas
thefastesttoconvergetostablehighrewards,withContrastiveDreamerandDreamerfollowing. In
particular,Dreamer’sdelaytoconvergenceshouldbeassociatedwiththemorecomplexmodel,that
took more epochs of training than the contrastive ones to provide good imagined trajectories for
planning,especiallyfortheHardtask. TheLikelihood-AIFfailedtoconvergeinallruns,becauseof
thedifficultyofmatchingthegoalstateinpixelspace,whichonlydiffersasmallnumberofpixels
fromanyotherenvironmentobservation.
DistractingReacherEasy. OntheDistractingtask,wefoundthatDreamerfailedtosucceed. As
weshowinAppendix,thereconstructionmodel’scapacitywasentirelyspentonreconstructingthe
complexbackgrounds,failingtocapturerelevantinformationforthetask. Conversely,Contrastive
Dreamerwasabletoignorethecomplexityoftheobservationsandthedistractionspresentinthe
8
Figure5: ReacherResults. Shadedareasindicatestandarddeviationacrossseveralruns.
environment,eventuallysucceedinginthetask. Surprisingly,alsoourContrastive-AIFmethodwas
abletosucceed,showinggeneralizationcapabilitiesthatarenotsharedbythelikelihoodcounterpart.
Webelievethisresultisimportantfortworeasons: (1)itprovidesevidencethatcontrastivefeatures
better capture semantic information in the environment, potentially ignoring complex irrelevant
details,(2)contrastiveobjectivesforplanningcanbeinvarianttochangesinthebackground,when
theunderlyingdynamicsofthetaskstaysthesame.
Utility Function Analysis. To collect further insights on the different methods’ objectives, we
analyzetheutilityvaluesassignedtoobservationswithdifferentposesintheReacherHardtask. In
Figure6,weshowacomparisonwhereallthevaluesarenormalizedintherange[0,1],considering
themaximumandminimumvaluesachievablebyeachmethod.
The reward signal is sparse and provided only when the arm is penetrating the goal sphere with
hisorangetip. Inparticular,arewardof+1isobtainedonlywhenthetipisentirelycontainedin
thesphere. TheLikelihood-AIFutilitylooksveryflatduetothestaticbackground,whichcauses
anyobservationtobeverysimilartothepreferredoutcomeinpixelspace. Evenaposethatisvery
differentfromthegoal,suchasthetopleftone,isseparatedonlybyarelativelysmallnumberof
pixelsfromthegoalone,inthebottomrightcorner,andthistranslatesintoveryminordifferencesin
utilityvalues(i.e. 0.98vs1.00). ForContrastive-AIF,weseethatthemodelprovideshigherutility
valuesforobservationsthatlookperceptuallysimilartothegoalandlowervaluesformoredistant
states, providing a denser signal to optimize for reaching the goal. This was certainly crucial in
achievingthetaskinthisexperiment,thoughoverly-shapedutilityfunctionscanbemoredifficultto
optimize[1],andfutureworkshouldanalyzetheconsequencesofsuchdenseshaping.
Figure6: UtilityValuesReacher. NormalizedutilityvaluesformultipleposesinReacherHard.
9
6 RelatedWork
ContrastiveLearning. Contrastivelearningmethodshaverecentlyledtoimportantbreakthroughs
intheunsupervisedlearninglandscape. TechniqueslikeMoCO[7,24]andSimCLR[5,6]have
progressivelyimprovedperformanceinimagerecognition,byusingonlyafewsupervisedlabels.
Contrastivelearningrepresentationshavealsoshownsuccessfulwhenemployedfornaturallanguage
processing[50]andmodel-freeRL[46].
Model-basedControl. Improvementsinthedynamicsgenerativemodel[21],haverecentlyallowed
model-basedRLmethodstoreachstate-of-the-artperformance,bothincontroltasks[23]andon
video games [22, 26]. An important line of research focuses on correctly balancing real-world
experiencewithdatageneratedfromtheinternalmodeloftheagent[25,11].
Outcome-DrivenControl. Theideaofusingdesiredoutcomestogeneratecontrolobjectiveshas
beenexploredinRLaswell[41,18,40]. In[31],theauthorsproposeasystemthat,givenadesired
goal, cansampleplansofactionfromalatentspaceanddecodethemtoactontheenvironment.
DISCERN[51]maximizesmutualinformationtothegoal,usingcosinesimilaritybetweenthegoal
andagivenobservation,inthefeaturespaceofaCNNmodel.
ActiveInference.Inourwork,weusedactiveinferencetoderiveactions,whichisjustonepossibility
toperformAIF,asdiscussedin[14,35]. Inotherworks,theexpectedfreeenergyispassivelyusedas
theutilityfunctiontoselectthebestbehavioramongpotentialsequencesofactions[15,16]. Methods
that combine the expressiveness of neural networks with AIF have been raising in popularity in
thelastyears[53]. In[13],theauthorsproposeanamortizedversionofMonteCarloTreeSearch,
throughanhabitnetwork,forplanning. In[49],AIFisseenperformingbetterthanRLalgorithms
in terms of reward maximization and exploration, on small-scale tasks. In [34], they propose an
objectivetoamortizeplanninginavalueiterationfashion.
7 Discussion
WepresentedtheContrastiveActiveInferenceframework,acontrastivelearningapproachforactive
inference,thatcaststhefreeenergyminimizationimperativesofAIFascontrastivelearningproblems.
We derived the contrastive objective functionals and we corroborated their applicability through
empiricalexperimentation,inbothcontinuousanddiscreteactionsettings,withhigh-dimensional
observations. Combiningourmethodwithmodelsandlearningroutinesinspiredfromthemodel-
basedRLscene,wefoundthatourapproachcanperformcomparablytomodelsthathaveaccess
to human-designed rewards. Our results show that contrastive features better capture relevant
informationaboutthedynamicsofthetask,whichcanbeexploitedbothtofindconceptuallysimilar
statestopreferredoutcomesandtomaketheagent’spreferencesinvarianttoirrelevantchangesinthe
environment(e.g. background,colors,cameraangle).
Whilethepossibilitytomatchstatestooutcomesintermsofsimilarfeaturesisratherconvenient
inimage-basedtasks,theriskisthat,iftheagentneversawthedesiredoutcome,itwouldconverge
tothesemanticallycloseststateintheenvironmentthatitknows. Thisraisesimportantconcerns
aboutthenecessitytoprovidegoodexploratorydataabouttheenvironment,inordertopreventthe
agentfromhanginginlocalminima. Forthisreason,weaimtolookintocombiningouragentwith
exploration-drivendatacollection,forzero-shotgoalachievement[33,45]. Anothercomplementary
lineofresearchwouldbeequippingourmethodwithbetterexperiencereplaymechanisms,suchas
HER[1],toimprovethegeneralizationcapabilitiesofthesystem.
Broaderimpact
Activeinferenceisabiologically-plausibleunifyingtheoryforperceptionandaction. Implementa-
tionsofactiveinferencethatarebothtractableandcomputationallycheapareimportanttofoster
furtherresearchtowardspotentiallybettertheoriesofthehumanbrain. Bystronglyreducingthe
computationalrequirementsofoursystem,comparedtootherdeepactiveinferenceimplementations,
weaimtomakethestudyofthisframeworkmoreaccessible. Furthermore,oursuccessfulresultson
theroboticmanipulatortaskwithvaryingrealisticbackgroundsshowthatcontrastivemethodsare
promisingforreal-worldapplicationswithcomplexobservationsanddistractingelements.
10
AcknowledgmentsandDisclosureofFunding
ThisresearchreceivedfundingfromtheFlemishGovernment(AIResearchProgram).
References
[1] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin,
O.PieterAbbeel,andW.Zaremba. Hindsightexperiencereplay. InI.Guyon,U.V.Luxburg,S.Ben-
gio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,editors,AdvancesinNeuralInformation
ProcessingSystems,volume30.CurranAssociates,Inc.,2017.
[2] A.P.Badia,B.Piot,S.Kapturowski,P.Sprechmann,A.Vitvitskyi,D.Guo,andC.Blundell. Agent57:
Outperformingtheatarihumanbenchmark,2020.
[3] R.Bogacz. Dopamineroleinlearningandactioninference. eLife,9:e53262,jul2020.
[4] L.Buesing, T.Weber, S.Racaniere, S.M.A.Eslami, D.Rezende, D.P.Reichert, F.Viola, F.Besse,
K.Gregor,D.Hassabis,andD.Wierstra. Learningandqueryingfastgenerativemodelsforreinforcement
learning,2018.
[5] T.Chen,S.Kornblith,M.Norouzi,andG.Hinton. Asimpleframeworkforcontrastivelearningofvisual
representations. arXivpreprintarXiv:2002.05709,2020.
[6] T.Chen,S.Kornblith,K.Swersky,M.Norouzi,andG.Hinton. Bigself-supervisedmodelsarestrong
semi-supervisedlearners. arXivpreprintarXiv:2006.10029,2020.
[7] X.Chen,H.Fan,R.Girshick,andK.He. Improvedbaselineswithmomentumcontrastivelearning,2020.
[8] M.Chevalier-Boisvert, L.Willems, andS.Pal. Minimalisticgridworldenvironmentforopenaigym.
https://github.com/maximecb/gym-minigrid,2018.
[9] J.Chung,C.Gulcehre,K.Cho,andY.Bengio. Empiricalevaluationofgatedrecurrentneuralnetworkson
sequencemodeling. InNIPS2014WorkshoponDeepLearning,December2014,2014.
[10] J.ClarkandD.Amodei. Faultyrewardfunctionsinthewild,2016.
[11] I.Clavera,V.Fu,andP.Abbeel. Model-augmentedactor-critic:Backpropagatingthroughpaths,2020.
[12] L. Da Costa, T. Parr, N. Sajid, S. Veselic, V. Neacsu, and K. Friston. Active inference on discrete
state-spaces:Asynthesis. JournalofMathematicalPsychology,99:102447,2020.
[13] Z.Fountas,N.Sajid,P.Mediano,andK.Friston. Deepactiveinferenceagentsusingmonte-carlomethods.
InH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin,editors,AdvancesinNeuralInformation
ProcessingSystems,volume33,pages11662–11675.CurranAssociates,Inc.,2020.
[14] K.Friston,L.DaCosta,D.Hafner,C.Hesp,andT.Parr. SophisticatedInference. NeuralComputation,
33(3):713–763,032021.
[15] K.Friston,T.FitzGerald,F.Rigoli,P.Schwartenbeck,J.O’Doherty,andG.Pezzulo. Activeinferenceand
learning. Neuroscience&BiobehavioralReviews,68:862–879,2016.
[16] K. Friston, F. Rigoli, D. Ognibene, C. Mathys, T. Fitzgerald, and G. Pezzulo. Active inference and
epistemicvalue. CognNeurosci,6(4):187–214,2015.
[17] K.J.Friston,J.Daunizeau,J.Kilner,andS.J.Kiebel. Actionandbehavior:afree-energyformulation.
BiologicalCybernetics,102(3):227–260,Mar2010.
[18] Y.Ganin,T.Kulkarni,I.Babuschkin,S.M.A.Eslami,andO.Vinyals. Synthesizingprogramsforimages
usingreinforcedadversariallearning. InProceedingsofthe35thInternationalConferenceonMachine
Learning,volume80ofProceedingsofMachineLearningResearch.PMLR,2018.
[19] M.GutmannandA.Hyvärinen.Noise-contrastiveestimation:Anewestimationprincipleforunnormalized
statisticalmodels. InY.W.TehandM.Titterington,editors,ProceedingsoftheThirteenthInternational
ConferenceonArtificialIntelligenceandStatistics,volume9ofProceedingsofMachineLearningResearch,
pages297–304,ChiaLagunaResort,Sardinia,Italy,13–15May2010.PMLR.
[20] T.Haarnoja,A.Zhou,P.Abbeel,andS.Levine. Softactor-critic: Off-policymaximumentropydeep
reinforcementlearningwithastochasticactor. InJ.DyandA.Krause,editors,Proceedingsofthe35th
InternationalConferenceonMachineLearning,volume80ofProceedingsofMachineLearningResearch,
pages1861–1870.PMLR,10–15Jul2018.
[21] D.Hafner,T.Lillicrap,I.Fischer,R.Villegas,D.Ha,H.Lee,andJ.Davidson.Learninglatentdynamicsfor
planningfrompixels.InK.ChaudhuriandR.Salakhutdinov,editors,Proceedingsofthe36thInternational
Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages
2555–2565.PMLR,09–15Jun2019.
[22] D.Hafner,T.Lillicrap,M.Norouzi,andJ.Ba. Masteringatariwithdiscreteworldmodels,2021.
11
[23] D. Hafner, T. P. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent
imagination. InICLR,2020.
[24] K.He,H.Fan,Y.Wu,S.Xie,andR.B.Girshick.Momentumcontrastforunsupervisedvisualrepresentation
learning.In2020IEEE/CVFConferenceonComputerVisionandPatternRecognition,CVPR2020,Seattle,
WA,USA,June13-19,2020,pages9726–9735.IEEE,2020.
[25] M.Janner,J.Fu,M.Zhang,andS.Levine. Whentotrustyourmodel:Model-basedpolicyoptimization.
InH.Wallach,H.Larochelle,A.Beygelzimer,F.d'Alché-Buc,E.Fox,andR.Garnett,editors,Advances
inNeuralInformationProcessingSystems,volume32.CurranAssociates,Inc.,2019.
[26] L.Kaiser,M.Babaeizadeh,P.Milos,B.Osinski,R.H.Campbell,K.Czechowski,D.Erhan,C.Finn,
P.Kozakowski, S.Levine, A.Mohiuddin, R.Sepassi, G.Tucker, andH.Michalewski. Model-based
reinforcementlearningforatari,2020.
[27] D.P.KingmaandM.Welling. Auto-encodingvariationalbayes,2014.
[28] V.R.KondaandJ.N.Tsitsiklis. Actor-criticalgorithms. InAdvancesinneuralinformationprocessing
systems,pages1008–1014.Citeseer,2000.
[29] V.Krakovnaetal. Specificationgaming:theflipsideofaiingenuity,2020.
[30] S.Levine. Reinforcementlearningandcontrolasprobabilisticinference:Tutorialandreview,2018.
[31] C.Lynch,M.Khansari,T.Xiao,V.Kumar,J.Tompson,S.Levine,andP.Sermanet. Learninglatentplans
fromplay,2019.
[32] X.Ma,S.Chen,D.Hsu,andW.S.Lee. Contrastivevariationalmodel-basedreinforcementlearningfor
complexobservations. InProceedingsofthe4thConferenceonRobotLearning,2020.
[33] P.Mazzaglia,O.Catal,T.Verbelen,andB.Dhoedt.Self-supervisedexplorationvialatentbayesiansurprise,
2021.
[34] B.Millidge. Deepactiveinferenceasvariationalpolicygradients,2019.
[35] B.Millidge,A.Tschantz,A.K.Seth,andC.L.Buckley. Ontherelationshipbetweenactiveinferenceand
controlasinference,2020.
[36] OpenAI,I.Akkaya,M.Andrychowicz,M.Chociej,M.Litwin,B.McGrew,A.Petron,A.Paino,M.Plap-
pert,G.Powell,R.Ribas,J.Schneider,N.Tezak,J.Tworek,P.Welinder,L.Weng,Q.Yuan,W.Zaremba,
andL.Zhang. Solvingrubik’scubewitharobothand,2019.
[37] B. Poole, S. Ozair, A. Van Den Oord, A. Alemi, and G. Tucker. On variational bounds of mutual
information. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International
Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages
5171–5180.PMLR,09–15Jun2019.
[38] I.Popovetal. Data-efficientdeepreinforcementlearningfordexterousmanipulation,2017.
[39] D.J.Rezende,S.Mohamed,andD.Wierstra. Stochasticbackpropagationandapproximateinference
indeepgenerativemodels. InProceedingsofthe31stInternationalConferenceonMachineLearning
(ICML),volume32,pages1278–1286,2014.
[40] T.G.J.Rudner,V.H.Pong,R.McAllister,Y.Gal,andS.Levine. Outcome-drivenreinforcementlearning
viavariationalinference,2021.
[41] T.Schaul,D.Horgan,K.Gregor,andD.Silver. Universalvaluefunctionapproximators. InF.Bachand
D.Blei,editors,Proceedingsofthe32ndInternationalConferenceonMachineLearning,volume37of
ProceedingsofMachineLearningResearch,pages1312–1320,Lille,France,07–09Jul2015.PMLR.
[42] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart,
D.Hassabis,T.Graepel,andetal. Masteringatari,go,chessandshogibyplanningwithalearnedmodel.
Nature,588(7839):604–609,Dec2020.
[43] J.Schulman,P.Moritz,S.Levine,M.Jordan,andP.Abbeel. High-dimensionalcontinuouscontrolusing
generalizedadvantageestimation,2018.
[44] W.Schultz. Predictiverewardsignalofdopamineneurons. JNeurophysiol,80(1):1–27,Jul1998.
[45] R.Sekar,O.Rybkin,K.Daniilidis,P.Abbeel,D.Hafner,andD.Pathak. Planningtoexploreviaself-
supervisedworldmodels. InICML,2020.
[46] A.Srinivas,M.Laskin,andP.Abbeel. Curl:Contrastiveunsupervisedrepresentationsforreinforcement
learning,2020.
[47] A.Stone,O.Ramirez,K.Konolige,andR.Jonschkowski. Thedistractingcontrolsuite–achallenging
benchmarkforreinforcementlearningfrompixels,2021.
[48] Y.Tassa,Y.Doron,A.Muldal,T.Erez,Y.Li,D.deLasCasas,D.Budden,A.Abdolmaleki,J.Merel,
A.Lefrancq,T.Lillicrap,andM.Riedmiller. Deepmindcontrolsuite,2018.
12
[49] A.Tschantz,B.Millidge,A.K.Seth,andC.L.Buckley. Reinforcementlearningthroughactiveinference,
2020.
[50] A.vandenOord,Y.Li,andO.Vinyals. Representationlearningwithcontrastivepredictivecoding,2019.
[51] D.Warde-Farley,T.V.deWiele,T.D.Kulkarni,C.Ionescu,S.Hansen,andV.Mnih. Unsupervised
controlthroughnon-parametricdiscriminativerewards. In7thInternationalConferenceonLearning
Representations,ICLR2019,NewOrleans,LA,USA,May6-9,2019.OpenReview.net,2019.
[52] R.J.Williams. Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcementlearning.
Mach.Learn.,8(3–4):229–256,May1992.
[53] O.Çatal,T.Verbelen,J.Nauta,C.D.Boom,andB.Dhoedt. Learningperceptionandplanningwithdeep
activeinference. InICASSP2020-2020IEEEInternationalConferenceonAcoustics,SpeechandSignal
Processing(ICASSP),pages3952–3956,2020.
[54] O.Çatal,S.Wauthier,C.DeBoom,T.Verbelen,andB.Dhoedt. Learninggenerativestatespacemodels
foractiveinference. FrontiersinComputationalNeuroscience,14:103,2020.
13
A BackgroundDerivations
Inthissection,weprovidethederivationsoftheequationsprovidedinsection2.
Inallequations,bothforthepastandthefuture,weconsideronlyonetimestept.Thisispossiblethankstothe
Markovassumption,statingthattheenvironmentpropertiesexclusivelydependontheprevioustimestep.This
makespossibletowritestep-wiseformulas,byapplyingancestralsampling,i.e.forthestatedynamicsuntilT:
T
(cid:88)
logp(s |a )= logp(s |s ,a ).
≤T ≤T t t−1 t−1
t=1
TosimplifyandshortentheEquations,wemostlyomitconditioningonpaststatesandactions. However,as
showninsection4,thetransitiondynamicsexplicitlytakeancestralsamplingintoaccount,byusingrecurrent
neuralnetworksthatprocessmultipletimesteps.
A.1 FreeEnergyofthePast
Forpastobservations,theobjectiveistobuildamodeloftheenvironmentforperception.Sincecomputingthe
posteriorp(s |o )isintractable,welearntoapproximateitwithavariationaldistributionq(s ).Asweshow,
t t t
thisprocessprovidesanupperboundonthesurprisal(logevidence)ofthemodel:
(cid:88)
−logp(o )=−log p(o ,s )
t t t
st
=−log
(cid:88)p(o
t
,s
t
)q(s
t
)
q(s )
t
st
(cid:20) (cid:20) (cid:21)(cid:21)
p(o ,s )
=−log E t t
q(st) q(s )
t
(cid:20) (cid:21)
p(o ,s )
≤−E log t t
q(st) q(s )
t
=E [logq(s )−logp(o ,s )],
q(st) t t t
whereweappliedJensen’sinequalityinthefourthrow,obtainingthevariationalfreeenergyF (Equation1).
Thefreeenergyofthepastcanbemainlyrewrittenintwoways:
F =E [logq(s )−logp(o ,s )]
q(st) t t t
=D [q(s )||p(s |o )]−logp(o ).
KL t t t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
evidencebound logevidence
=D [q(s )||p(s )]−E [logp(o |s )],
KL t t q(st) t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
complexity accuracy
wherethefirstexpressionhighlightstheevidenceboundonthemodel’sevidence,andthesecondexpression
showsthebalancebetweenthecomplexityofthestatemodelandtheaccuracyofthelikelihoodone.Fromthe
latter,theF (Equation2)canbeobtainedbyexplicitingp(s )asp(s |s ,a ),accordingtotheMarkov
AIF t t t−1 t−1
assumption,andbychoosingq(s )=q(s |o )astheapproximatevariationaldistribution.
t t t
A.2 FreeEnergyoftheFuture
Forthefuture,theagentselectsactionsthatitexpectstominimizethefreeenergy.Inparticular,activeinference
assumesthatthefuture’smodeloftheagentisbiasedtowardsitspreferredoutcomes,distributedaccordingtothe
priorp˜(o ).Thus,wedefinetheagent’sgenerativemodelasp˜(o ,s ,a )=p(a )p(s |o )p˜(o )andweaimto
t t t t t t t t
findthedistributionsoffuturestatesandactionsbyapplyingvariationalinference,withthevariationaldistribution
q(s ,a ). Ifweconsiderexpectationstakenovertrajectoriessampledfromq(o ,s ,a )=p(o |s )q(s ,a ),
t t t t t t t t t
theexpectedfreeenergyG(Equation3)becomes:
G =E [logq(s ,a )−logp˜(o ,s ,a )]
q(ot,st,at) t t t t t
=E [logq(a |s )+logq(s |s ,a )−logp(a )−logp(s |o )−logp˜(o )],
q(ot,st,at) t t t t−1 t−1 t t t t
whereweexplicitconditioningonthepreviousstate-action(s ,a )forthesakeofclarity.
t−1 t−1
Wenowassumep(s |o )≈q(s |o ),whichmeansassumingthevariationalstateposteriormodelapproximates
t t t t
thetrueposterioroverstates,asaconsequenceofminimizingF.Thus,wecanrewritetheaboveresultas:
G ≈E [logq(a |s )+logq(s |s ,a )−logp(a )−logq(s |o )−logp˜(o )].
q(ot,st,at) t t t t−1 t−1 t t t t
14
Then,weassumethattheagent’smodellikelihoodoveractionsisuniformandconstant,namelyp(a )= 1 :
t |A|
1
G ≈E [logq(a |s )+logq(s |s ,a )−logq(s |o )−logp˜(o )]−log .
q(ot,st,at) t t t t−1 t−1 t t t |A|
Finally,bydroppingtheconstantandrewritingalltermsasKLdivergencesandentropies,weobtain:
G =−E [D [q(s |o )||q(s |s ,a )]]−E [logp˜(o )]−E [H(q(a |s ))]
AIF q(ot) KL t t t t−1 t−1 q(ot) t q(st) t t
thatistheexpectedfreeenergyasdescribedinEquation4.
B ModelDetails
Theworldmodel,composedbythepriornetworkp (s |s ,a ),theposteriornetworkq (s |s ,a ,o )
ϕ t t−1 t−1 ϕ t t−1 t−1 t
andtherepresentationmodelf (o,s),ispresentedinFigure7.
ϕ
ThepriorandtheposteriornetworkshareaGRUcell,usedtorememberinformationfromthepast.Theprior
networkfirstcombinespreviousstatesandactionsusingalinearlayer,thenitprocessestheoutputwiththe
GRUcell,andfinallyusesa2-layerMLPtocomputethestochasticstatefromthehiddenstateoftheGRU.
Theposteriornetworkalsohasaccesstothefeaturescomputedbya4-layerCNNoverobservations. This
setupisinspiredonthemodelspresentedin[21,54,4]. Fortherepresentationmodel,ontheonehand,we
takethefeaturescomputedfromtheobservationsbytheposterior’sCNN,processthemwitha2-layerMLP
andapplyatanhnon-linearity,obtainingh (o).Ontheotherhand,wetakethestates ,weprocessitwitha
ϕ t
2-layerMLPandapplyatanhnon-linearity,obtainingg (s). Finally,wecomputeadot-product,obtaining
ϕ
f (o,s)=h (o)Tg (s).Intheworldmodel’sloss,J = (cid:80) F (s ,o ),wecliptheKLdivergencetermin
ϕ ϕ ϕ ϕ t NCE t t
theF below3freenats,toavoidposteriorcollapse.
NCE
Thebehaviormodeliscomposedbytheactionnetworkq (a |s )andtheexpectedutilitynetworkg (s ),
θ t t ψ t
whichareboth3-layerMLPs.Inordertogetagoodestimateoffutureutility,abletotradeoffbetweenbiasand
variance,weusedGAE(λ)estimation[43].Inpracticethistranslatesintoapproximatingtheinfinite-horizon
utility
(cid:80)∞
G (s )with:
k=T NCE t
(cid:40)
(1−λ)g (s )+λGλ if t<H,
Gλ =G (s )+γ ψ t+1 t+1
t NCE t t g (s ) if t=H,
ψ H
where λ is an hyperparameter and H is the imagination horizon for future trajectories. Given the above
definition, we can rewrite the actor network loss as: J = (cid:80) Gλ and the utility network loss with
θ t t
J = (cid:80) (g (s )−Gλ)2.InG ,wescaletheactionentropyby3·10−4,toprevententropymaximization
ψ t ψ t t NCE
fromtakingovertherestoftheobjective.Inordertostabilizetraining,whenupdatingtheactornetwork,weuse
theexpectedutilitynetworkandtheworldmodelfromthepreviousepochoftraining.
s s
t t
s
t-1 Representation
Prior Network Posterior Network Model
a GRU cell
t-1
(shared)
conv 4x4, 256 channels, stride 2
Posterior conv 4x4, 128 channels, stride 2
CNN conv 4x4, 64 channels, stride 2
conv 4x4, 32 channels, stride 2
o
t
Figure7: WorldModel. Prior, posteriorandrepresentationmodels. FortheposteriorCNN,the
configurationofeachlayerisprovided.
15
C HyperParameters
Name Value
WorldModel
Latentstatedimension 30
GRUcelldimension 200
Adamlearningrate 6·10−4
BehaviorModel
γ parameter 0.99
λparameter 0.95
Adamlearningrate 8·10−5
Common
Hiddenlayersdimension 200
Gradientclipping 100
Table3: Worldandbehaviormodelshyperparameters.
D ExperimentDetails
Hardware.WerantheexperimentsonaTitan-XGPU,withani5-2400CPUand16GBofRAM.
PreferredOutcomes.Forthetasksofourexperiments,thepreferredoutcomesare64x64x3images(displayed
inFigure2,4b,4a).Correspondingp(o˜)distributionsaredefinedas64x64x3multivariateLaplacedistributions,
t
centeredontheimages’pixelvalues. Wealsoexperimentedwith64x64x3multivariateGaussianswithunit
variance,obtainingsimilarresults.
Baselines.Insection5,wecomparefourdifferentflavorsofmodel-basedcontrol:Dreamer,ContrastiveDreamer,
Likelihood-AIFandContrastive-AIF.LossesforeachofthesemethodsareprovidedinTable4,adoptingthe
followingadditionaldefinitions:
J =−logp(r |s )
R t t
G =−E [r(s ,a )]−E [H(q(a |s ))],
RL q(st,at) t t q(st) t t
whereG isthesameasinEquation5.
RL
J J J
ϕ θ ψ
Dreamer F +J G (g − (cid:80)∞G )2
AIF R RL ψ t RL
ContrastiveDreamer F +J G (g − (cid:80)∞G )2
NCE R RL ψ t RL
Likelihood-AIF F G (g − (cid:80)∞G )2
AIF AIF ψ t AIF
Contrastive-AIF F G (g − (cid:80)∞G )2
NCE NCE ψ t NCE
Table4: Baselinesoverview. Alllossesaresummedovermultipletimesteps.
16
DistractingSuiteReconstructions.IntheReacherEasyexperimentfromtheDistractingControlSuite,we
found that Dreamer, a state-of-the-art algorithm on the DeepMind Control Suite, was not able to succeed.
Wehypothesizedthatthiswasduetotheworldmodelspendingmostofitscapacitytopredictthecomplex
background,beingthenunabletocapturerelevantinformationaboutthetask.
InFigure8,wecomparegroundtruthobservationsandreconstructionsfromtheDreamerposteriormodel.Aswe
expected,wefoundthatdespitethemodelcorrectlystoredinformationaboutseveraldetailsofthebackground,
itmissedcrucialinformationaboutthearmpose.Althoughbetterworldmodelscouldalleviateproblemslike
this,westronglybelievethatdifferentrepresentationlearningapproaches,likecontrastivelearning,providea
bettersolutiontotheissue.
Epoch: 0 Epoch: 100 Epoch: 200
t = 0
t = 10
Observations
t = 20
t = 30
t = 0
t = 10
Reconstructions
t = 20 -
t = 30
Figure8: DreamerReconstructions
17

=== INSTRUCTIONS ===

0. PROFESSIONAL TONE REQUIREMENTS:
   - Begin directly with the paper title or content - NO conversational openings
   - Do NOT use phrases like: 'Okay, here's...', 'Here's a summary...',
     'Let me summarize...', 'I'll extract...', or similar conversational language
   - Start immediately with substantive content in formal academic tone
   - Example BAD: 'Okay, here's a summary of the paper...'
   - Example GOOD: 'This paper investigates [topic]...'

1. Start with exact title: "Contrastive Active Inference"

2. EXTRACT QUOTES:
   - Extract 10-15 direct quotes from the paper that support key claims
   - QUOTE EXTRACTION AND FORMATTING:
     * Extract quotes VERBATIM from the paper text - do NOT modify or "correct" them
     * Extract quotes exactly as they appear in the source text
     * Preserve all aspects of the quote exactly as written, including spacing
     * Use proper quotation marks: "quote text" (double quotes)
     * CRITICAL: Only extract quotes that actually appear in the paper text
     * Do NOT generate, invent, or "fix" quotes - extract them exactly as written
   - QUOTE FORMATTING STANDARD:
     * Attribution format: 'The authors state: "quote text"' OR 'According to the paper: "quote text"'
     * Vary attribution phrases to avoid repetition (use: 'The authors state', 'They note',
       'The paper argues', 'According to the research', 'The study demonstrates')
     * Include section context when available: 'In the Introduction, the authors state: "quote text"'
     * Ensure proper spacing around quotes and punctuation
   - Search the full paper text to find relevant quotes
   - Each quote must be verbatim from the paper text (with spacing normalized)

3. IDENTIFY CLAIMS:
   - Identify the main claims and arguments made by the authors
   - State each claim clearly and support it with quotes from the paper
   - Distinguish between primary claims and supporting arguments

4. SUMMARIZE KEY FINDINGS:
   - Summarize the key findings with specific numbers, metrics, and results
   - Include quantitative data: percentages, statistics, measurements
   - Extract numerical results from the results section
   - Present findings with supporting evidence from the paper

5. DESCRIBE METHODS:
   - Describe the methodology, experimental setup, and approach used
   - Include details about: algorithms, procedures, experimental design
   - Explain how the research was conducted
   - Extract specific methodological details from the methods section

6. PRESENT RESULTS:
   - Present the results with quantitative data and statistical significance
   - Include specific numbers, tables, figures mentioned in the paper
   - Extract results from the results section with exact values
   - Support results with quotes or data from the paper

7. NO REPETITION - CRITICAL REQUIREMENT (ENHANCED):
   - CRITICAL: Before writing EACH sentence, check: 'Have I already said this exact idea?'
   - If you've already stated an idea, DO NOT repeat it - move to the next unique point
   - Each sentence must be COMPLETELY UNIQUE - no duplicate ideas, even with different words
   - Each claim appears EXACTLY ONCE - if you've stated it, move to the next unique point
   - Each paragraph must be COMPLETELY UNIQUE - no duplicate paragraphs
   - Do NOT repeat the same sentence, even with slight variations or word changes
   - Do NOT repeat paragraphs or sections - each section must have unique content
   - Each claim should appear only ONCE in the entire summary
   - Vary attribution phrases: use 'The authors state', 'They note', 'The paper argues',
     'According to the research', 'The study demonstrates' - do NOT repeat the same phrase
   - If you find yourself writing similar content, STOP immediately and write something completely different
   - Before each sentence, ask: 'Have I already said this?' If yes, write something new
   - Vary your language: use synonyms, different sentence structures, different perspectives
   - REPETITION CHECKLIST: After writing each sentence, verify it's not a duplicate of any previous sentence

   EXAMPLES OF WHAT NOT TO DO:
   ❌ BAD: 'The authors state: "X". The authors state: "Y". The authors state: "Z".'
   ✅ GOOD: 'The authors state: "X". They further note: "Y". The paper argues: "Z".'

   ❌ BAD: Repeating the same claim 3+ times with slight variations
   ✅ GOOD: State each claim once, then move to the next unique point

8. STRUCTURE:
   - Use markdown headers: ### Overview, ### Methodology, ### Results, ### Discussion
   - Target length: 1000-1500 words
   - Ensure all requested elements (quotes, claims, findings, methods, results) are included
