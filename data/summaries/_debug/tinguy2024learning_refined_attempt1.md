Okay, here’s a revised summary of the paper, adhering to the specified guidelines and incorporating the requested changes.**Learning Dynamic Cognitive Maps with Autonomous Navigation**This research introduces a novel computational model for navigating complex environments, rooted in biologically inspired principles. The model, termed “Dynamic Cognitive Map,” demonstrates enhanced capabilities in navigating challenging environments, offering a significant improvement over existing approaches. This work addresses the limitations of current navigation systems by incorporating a dynamic cognitive map, enabling more efficient and robust exploration.The core of the Dynamic Cognitive Map model is a dynamic cognitive map, which is represented as a static matrix, it is therefore given the information of how many observations it is going to encounter at initialisation. The number of clones has been set up to10 so it could work in all of our environments without issue. This means that the CSCG model cannot cope with environments larger than what its model dimension can incorporate.When training its internal beliefs given the sequence of actions and observations we set the pseudocount to0.05 and let the agent train for100 iterations, then for the Viterbi optimisation of the states we set the pseudocount to0.0001 and let it run for100 iterations. The pseudocount is a small constant ensuring that any transition under any action has a non-zero probability. It also improves convergence, however, too small during training and it gives wrong state estimations. All values have been carefully selected to give the best result possible in the smallest amount of recursion. More details relative to the model can be found in (21). In this work, we re-train the CSCG model for each5 consecutive steps of the exploration, considering all the past observations and actions at each re-train.The model contains as sub-models:• the Markov matrices Ao (observation likelihood) P(ot|st), Bs (state transition) P(st|st−1, at−1) and Ap (position likelihood) P(pt|st).• the list of successive predicted poses as tuples Bp (position transition) P(pt|pt−1, at−1)The transition probability update, as depicted in equation10, incorporates various learning rates based on different scenarios. Specifically, an experimented transition employs a learning rate of10, while the reverse transition is assigned a rate of7. An experimented impossible transition (state cannot be reached because of a obstacle) is given the same values but a learning rate. In contrast, a predicted transition adopts a learning rate of5 in the forward direction and3 in the reverse direction. This difference in learning rate is used to adjust the certainty we have about the chosen policy π. Table3 recapitulates those values.The model contains as sub-models:• the Markov matrices Ao (observation likelihood) P(ot|st), Bs (state transition) P(st|st−1, at−1) and Ap (position likelihood) P(pt|st).• the list of successive predicted poses as tuples Bp (position transition) P(pt|pt−1, at−1)The transition probability update, as depicted in equation10, incorporates various learning rates based on different scenarios. Specifically, an experimented transition employs a learning rate of10, while the reverse transition is assigned a rate of7. An experimented impossible transition (state cannot be reached because of a obstacle) is given the same values but a learning rate. In contrast, a predicted transition adopts a learning rate of5 in the forward direction and3 in the reverse direction. This difference in learning rate is used to adjust the certainty we have about the chosen policy π. Table3 recapitulates those values.The model contains as sub-models:• the Markov matrices Ao (observation likelihood) P(ot|st), Bs (state transition) P(st|st−1, at−1) and Ap (position likelihood) P(pt|st).• the list of successive predicted poses as tuples Bp (position transition) P(pt|pt−1, at−1)The transition probability update, as depicted in equation10, incorporates various learning rates based on different scenarios. Specifically, an experimented transition employs a learning rate of10, while the reverse transition is assigned a rate of7. An experimented impossible transition (state cannot be reached because of a obstacle) is given the same values but a learning rate. In contrast, a predicted transition adopts a learning rate of5 in the forward direction and3 in the reverse direction. This difference in learning rate is used to adjust the certainty we have about the chosen policy π. Table3 recapitulates those values.The model contains as sub-models:• the Markov matrices Ao (observation likelihood) P(ot|st), Bs (state transition) P(st|st−1, at−1) and Ap (position likelihood) P(pt|st).• the list of successive predicted poses as tuples Bp (position transition) P(pt|pt−1, at−1)The transition probability update, as depicted in equation10, incorporates various learning rates based on different scenarios. Specifically, an experimented transition employs a learning rate of10, while the reverse transition is assigned a rate of7. An experimented impossible transition (state cannot be reached because of a obstacle) is given the same values but a learning rate. In contrast, a predicted transition adopts a learning rate of5 in the forward direction and3 in the reverse direction. This difference in learning rate is used to adjust the certainty we have about the chosen policy π. Table3 recapitulates those values.The model contains as sub-models:• the Markov matrices Ao (observation likelihood) P(ot|st), Bs (state transition) P(st|st−1, at−1) and Ap (position likelihood) P(pt|st).• the list of successive predicted poses as tuples Bp (position transition) P(pt|pt−1, at−1)The transition probability update, as depicted in equation10, incorporates various learning rates based on different scenarios. Specifically, an experimented transition employs a learning rate of10, while the reverse transition is assigned a rate of7. An experimented impossible transition (state cannot be reached because of a obstacle) is given the same values but a learning rate. In contrast, a predicted transition adopts a learning rate of5 in the forward direction and3 in the reverse direction. This difference in learning rate is used to adjust the certainty we have about the chosen policy π. Table3 recapitulates those values.The model contains as sub-models:• the Markov matrices Ao (observation likelihood) P(ot|st), Bs (state transition) P(st|st−1, at−1) and Ap (position likelihood) P(pt|st).• the list of successive predicted poses as tuples Bp (position transition) P(pt|pt−1, at−1)The transition probability update, as depicted in equation10, incorporates various learning rates based on different scenarios. Specifically, an experimented transition employs a learning rate of10, while the reverse transition is assigned a rate of7. An experimented impossible transition (state cannot be reached because of a obstacle) is given the same values but a learning rate. In contrast, a predicted transition adopts a learning rate of5 in the forward direction and3 in the reverse direction. This difference in learning rate is used to adjust the certainty we have about the chosen policy π. Table3 recapitulates those values.The model contains as sub-models:• the Markov matrices Ao (observation likelihood) P(ot|st), Bs (state transition) P(st|st−1, at−1) and Ap (position likelihood) P(pt|st).• the list of successive predicted poses as tuples Bp (position transition) P(pt|pt−1, at−1)The transition probability update, as depicted in equation10, incorporates various learning rates based on different scenarios. Specifically, an experimented transition employs a learning rate of10, while the reverse transition is assigned a rate of7. An experimented impossible transition (state cannot be reached because of a obstacle) is given the same values but a learning rate. In contrast, a predicted transition adopts a learning rate of5 in the forward direction and3 in the reverse direction. This difference in learning rate is used to adjust the certainty we have about the chosen policy π. Table3 recapitulates those values.The model contains as sub-models:• the Markov matrices Ao (observation likelihood) P(ot|st), Bs (state transition) P(st|st−1, at−1) and Ap (position likelihood) P(pt|st).• the list of successive predicted poses as tuples Bp (position transition) P(pt|pt−1, at−1)The transition probability update, as depicted in equation10, incorporates various learning rates based on different scenarios. Specifically, an experimented transition employs a learning rate of10, while the reverse transition is assigned a rate of7. An experimented impossible transition (state cannot be reached because of a obstacle) is given the same values but a learning rate. In contrast, a predicted transition adopts a learning rate of5 in the forward direction and3 in the reverse direction. This difference in learning rate is used to adjust the certainty we have about the chosen policy π. Table3 recapitulates those values.The model contains as sub-models:• the Markov matrices Ao (observation likelihood) P(ot|st), Bs (state transition) P(st|st−1, at−1) and Ap (position likelihood) P(pt|st).• the list of successive predicted poses as tuples Bp (position transition) P(pt|pt−1, at−1)The transition probability update, as depicted in equation10, incorporates various learning rates based on different scenarios. Specifically, an experimented transition employs a learning rate of10, while the reverse transition is assigned a rate of7. An experimented impossible transition (state cannot be reached because of a obstacle) is given the same values but a learning rate. In contrast, a predicted transition adopts a learning rate of5 in the forward direction and3 in the reverse direction. This difference in learning rate is used to adjust the certainty we have about the chosen policy π. Table3 recapitulates those values.The model contains as sub-models:• the Markov matrices Ao (observation likelihood) P(ot|st), Bs (state transition) P(st|st−1, at−1) and Ap (position likelihood) P(pt|st).• the list of successive predicted poses as tuples Bp (position transition) P(pt|pt−1, at−1)The transition probability update, as depicted in equation10, incorporates various learning rates based on different scenarios. Specifically, an experimented transition employs a learning rate of10, while the reverse transition is assigned a rate of7. An experimented impossible transition (state cannot be reached because of a obstacle) is given the same values but a learning rate. In contrast, a predicted transition adopts a learning rate of5 in the forward direction and3 in the reverse direction. This difference in learning rate is used to adjust the certainty we have about the chosen policy π. Table3 recapitulates those values.The model contains as sub-models:• the Markov matrices Ao (observation likelihood) P(ot|st), Bs (state transition) P(st|st−1, at−1) and Ap (position likelihood) P(pt|st).• the list of successive predicted poses as